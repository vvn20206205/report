@@@
1
00:00:00,000 --> 00:00:16,004
Hi, I'm Liz. Welcome to the AI for trading nano degree program. And I'm Eddie. Financial markets have always been complex and in the past few decades, the level of complexity has grown with advances in modern technology and the growth of available data.

2
00:00:16,004 --> 00:00:24,750
But there's so much to process. From stock prices, earnings reports, and news articles, to social media, the list is seemingly endless.

3
00:00:24,750 --> 00:00:33,175
Is it even possible to make sense of it all? This is where quantitative financial analysts, or quants play an important role.

4
00:00:33,174 --> 00:00:49,719
But what does a quant? And what do quants do? In the next video, you'll hear from Jonathan. A quant who has held senior roles in several major financial institutions, and who has been instrumental in building the nano degree program that you are now a part of.


@@@
1
00:00:00,000 --> 00:00:06,044
You're probably really excited to get started, but maybe you have a few questions, like what's a quant?

2
00:00:06,044 --> 00:00:14,474
And what does a quant do? We were really lucky when building this nanodegree program. We got to work with some of the leading players in the quant world.

3
00:00:14,474 --> 00:00:20,340
To tell you a little more about that world, we have Jonathan Larkin, a strategist in quantitative equities.

4
00:00:20,339 --> 00:00:26,240
He's held senior positions in quantitative equity trading at several of the leading financial institutions.


@@@
1
00:00:00,000 --> 00:00:06,714
In his book, The Quants, Scott Patterson really hit the nail on the head. He described Quants as those searching for the truth.

2
00:00:06,714 --> 00:00:12,849
The truth being the universal underlying hidden dynamics in the markets and that has always captivated me.

3
00:00:12,849 --> 00:00:23,114
Quants build computational models of the world. Specifically, that could be about financial instruments or markets, and Quants apply the scientific method to finance.

4
00:00:23,114 --> 00:00:47,734
You'll find Quants working across the entire financial industry these days. Most people associate the job of a Quant with very well known quantitative hedge funds and it is certainly true that Quants work in that type of organization, but Quants also work in commercial banks, proprietary trading firms, asset management firms, data vendor firms, and they also work across functions.

5
00:00:47,734 --> 00:00:55,925
You'll find Quants working in many aspects of a investment organization. You'll find Quants in the research function.

6
00:00:55,924 --> 00:01:01,755
Those are Quants looking for predictive signals and data that can be applied downstream to a portfolio.

7
00:01:01,755 --> 00:01:09,890
You'll find Quants working in risk management trying to predict the risk of a particular portfolio or the risk firm wide.

8
00:01:09,890 --> 00:01:17,549
You'll find Quants working in portfolio construction, combining the signals and models produced by other Quants.

9
00:01:17,549 --> 00:01:26,039
You'll also find Quants working in data vendors, data vendors who are preparing unstructured data, then to sell to the financial industry.

10
00:01:26,040 --> 00:01:37,170
One of the reasons that I'm so fascinated with this field is that there really is no typical day. That being said, the work product of a Quant is to create a model.

11
00:01:37,170 --> 00:01:51,834
That could be a model of financial time series or it could be the model of a portfolio or firm-wide risk, or it could be a model to reduce unstructured data to structured form for further use in the investment process.

12
00:01:51,834 --> 00:01:59,935
One thing that makes this field fascinating is that things are always changing. There's new developments in markets.

13
00:01:59,935 --> 00:02:09,444
There are new datasets. There are new computational techniques. You always need to keep learning and you always need to continually up your game in order to compete.

14
00:02:09,444 --> 00:02:19,879
People who have achieved substantial success in other scientific disciplines are often humbled when they come to quantitative finance.

15
00:02:19,879 --> 00:02:38,425
The reason for this is that signals in financial data are very, very faint, and you need to combine not just a scientific approach, but you need to combine that with understanding of markets, intuition of markets, and domain knowledge about markets.


@@@
1
00:00:00,000 --> 00:00:08,214
Hi, I'm Miriam. Every nanodegree program created at Udacity comes to life through the efforts of many, many hard-working people.

2
00:00:08,214 --> 00:00:14,910
It's my very great pleasure to introduce you to the team of instructors and industry experts who will guide you on your learning journey.

3
00:00:14,910 --> 00:00:24,820
First step, your Udacity instructors. Jonathan is an expert in quantitative investment strategies, he has a ton of experience relevant to what you'll learn in this program.

4
00:00:24,820 --> 00:00:31,620
He was Global Head of Equities at Millennium Management and co-head of America's Equity Derivatives Trading at JPMorgan.

5
00:00:31,620 --> 00:00:37,914
Cindy was a quantitative analyst at Bank of America, Merrill Lynch, Morgan Stanley, and Ping An securities.

6
00:00:37,914 --> 00:00:50,989
Cindy oversees this nanodegree program as the curriculum lead. Brok has designed several coding projects that students build in Udacity's deep learning self-driving car and artificial intelligence nanodegree programs, and now this one as well.

7
00:00:50,990 --> 00:00:58,059
Liz completed her PhD in Physics and experimental neuroscience, she was also a data science instructor at the data incubator.

8
00:00:58,060 --> 00:01:04,680
Eddie rect at BlackRock, Thompson writers and Morgan Stanley and has degrees in financial engineering and computer science.

9
00:01:04,680 --> 00:01:12,050
Miriam, that's me, I'm the senior learning strategist here at Udacity. That I felt courses in mathematics physics and programming here.

10
00:01:12,049 --> 00:01:17,894
I've also worked on our community student experience and motivational design, and you'll be hearing from me throughout this program.

11
00:01:17,894 --> 00:01:25,479
Arpan has taught computer science courses for Georgia Tech and also for you Udacity's deep learning and natural language processing nanodegree programs.

12
00:01:25,480 --> 00:01:34,045
We also worked with a team of industry experts who helped to put together the course materials that will introduce you to the exciting and fast-moving world of quant training.

13
00:01:34,045 --> 00:01:41,890
Kendall has worked as a quant trainer at hedge funds such as Citadel and Millennium partners. He also worked as a quantitative researcher at JPMorgan.

14
00:01:41,890 --> 00:01:48,739
He holds degrees in electrical engineering and financial mathematics. Justin has held a variety of roles in industry.

15
00:01:48,739 --> 00:01:57,564
He was an Investment Strategist at BlackRock Scientific Active Equity Group and most recently, he was a quant research analyst at MUFG Highmark capital.

16
00:01:57,564 --> 00:02:04,159
Harry developed algorithmic trading software at Morgan Stanley and was also a portfolio manager at apogee fund management.

17
00:02:04,159 --> 00:02:13,235
He is also the Chief Technology Officer at Carlisle blue wave. Murat has a PhD in statistics and has spent his entire career in systematic electronic trading.

18
00:02:13,235 --> 00:02:25,370
He's currently a quant researcher at redex training and has worked for JP Morgan and Citadel. All of us are so excited to support you as you learn and grow, and now I hand you over to Liz and Eddie to talk about the program overview.


@@@
1
00:00:00,000 --> 00:00:05,324
Now let's take a deep dive into the program, and talk about what you will learn in the next few months.

2
00:00:05,325 --> 00:00:16,449
First of all, to get the most out of this program, you'll ideally have Intermediate Python skills including Numpy and Pandas, and prior exposure to statistics, linear algebra, and calculus.

3
00:00:16,449 --> 00:00:25,829
We've designed this program to be accessible to anyone who is passionate about using advanced data analytics to make sense of financial data.

4
00:00:25,829 --> 00:00:36,310
With that in mind, we will review certain bath encoding concepts as they are used or point you to other Udacity resources to refresh your background knowledge.

5
00:00:36,310 --> 00:00:43,594
The Nadir degree program has two terms. Each term contains four projects and will take three months to complete.

6
00:00:43,594 --> 00:00:52,275
Prior to each project, there's a set of lessons, quizzes, and exercises that will introduce you to a topic and prepare you for the project.

7
00:00:52,274 --> 00:01:05,379
You will then practice what you've learned by completing the project. Note that much of your learning will come from working on the project, trying different things and even going beyond what is required as you explore what's possible.

8
00:01:05,379 --> 00:01:14,844
We highly encourage you to take the time, to make each project your own unique reflection of what you have learned and what you are capable of.

9
00:01:14,844 --> 00:01:30,034
Students who successfully complete term one are eligible to enroll in term two. At the end of term two, you will embark on a Capstone Project which will give you the opportunity to use everything you've learned to design and evaluate quantitative trading strategies.

10
00:01:30,034 --> 00:01:44,820
In term one, you will learn about the Fundamentals of Quantitative Finance. You will apply quantitative methods that are practiced in the financial industry including regression, optimization, and trading signal generation.

11
00:01:44,819 --> 00:01:52,814
You will work on projects that reflect the actual quant workflow as it's done in financial firms, with real data sets.

12
00:01:52,814 --> 00:02:00,009
Term one is designed to prepare you for term two, which applies more advanced methods to the quant workflow.

13
00:02:00,010 --> 00:02:11,775
In term two, you'll use machine learning algorithms to generate trading signals. You'll use natural language processing to analyze financial statements and apply recurrent neural networks to analyze news data.

14
00:02:11,775 --> 00:02:19,585
You'll get hands on practice with backtesting, which is a realistic simulation designed to evaluate the effectiveness of a strategy.

15
00:02:19,585 --> 00:02:26,365
Finally, you will use advanced techniques to combine several trading signals together to optimize a portfolio.


@@@
1
00:00:00,000 --> 00:00:11,550
Hey there. I'm Miriam and I work on student motivation and learning science at Udacity. I'll pop up at different moments throughout your needed degree experience, to give you context for why we've designed your learning experience as we have.

2
00:00:11,550 --> 00:00:21,369
The goal of the entire Eudacity team is to support you in meeting your goals, whether you're here to expand your understanding of topics you're interested in, or unlock a whole new stage of your career.

3
00:00:21,370 --> 00:00:29,275
To help you grow in the ways that you want to, we're going to provide you with productive challenges that will stretch your brain, as well as powerful support from your community.

4
00:00:29,274 --> 00:00:38,664
We'll guide you in learning by doing, through your projects, through helping out your fellow students, and through questions we asked you in the classroom to let you play around with your new knowledge and skills.

5
00:00:38,664 --> 00:00:44,530
Actively engaging your brain in these ways will help you develop deep, flexible, long-lasting learning.


@@@
1
00:00:00,000 --> 00:00:14,580
Projects and project reviews are a super important part of your learning experience at Udacity. Completion of projects enables you to graduate from the nanodegree program, but more importantly your projects also act as substantial building blocks for your portfolio of work.

2
00:00:14,580 --> 00:00:19,855
Once you've finished each project, you'll submit it to us, and we'll send it back to you with feedback from one of our mentors.

3
00:00:19,855 --> 00:00:24,945
You can submit each project as many times as you need to until it meets the specifications of the rubric.

4
00:00:24,945 --> 00:00:31,710
This process of getting feedback from experts in your community and making changes in accordance with that, will really strengthen your learning.

5
00:00:31,710 --> 00:00:39,079
It will also help you produce super high quality work. It gives you practice at the kind of iterative process that happens in the tech world all day every day.


@@@
1
00:00:00,000 --> 00:00:06,330
Your study group is a small circle of students in your degree program who will stick together throughout your whole experience.

2
00:00:06,330 --> 00:00:15,974
You'll find some mentors there too. Your study group is the place to go to talk about goal setting, work through problems with others in real time, and just get to know some really cool, supportive people.

3
00:00:15,974 --> 00:00:25,600
Research shows that teaching others allows you to apply your learning in new ways that powerfully strengthen your mastery of material, and your study group will give you plenty of opportunities for that.


@@@
1
00:00:00,000 --> 00:00:10,205
In today's students like you have access to a technical community driven Q and A platform called Knowledge, where we curate the most useful content to help you grow your skills and expertise.

2
00:00:10,205 --> 00:00:17,984
You can search for, ask, and answer questions in Knowledge, helping to expand your learning while simultaneously contributing to communities collected mastery.

3
00:00:17,984 --> 00:00:28,839
Knowledge is the place to go for efficient help with content related issues. Whether you're stuck wrestling with a tricky problem or want to get some practice applying your learning to issues others have faced, Knowledge can help.


@@@
1
00:00:00,000 --> 00:00:11,460
My name is Jonathan Larkin, and I've worked in quantitative finance now for about 20 years at Investment Banks Hedge Funds and Fintech startups, and I'm very excited to be advising Udacity on this program.

2
00:00:11,460 --> 00:00:36,045
I wanted to become a quant trader, because I was genuinely very, very fascinated with the field. It's an incredibly interdisciplinary field, to be successful, you have to draw upon pure technical skills in areas like computer science and mathematics, but you also have to understand markets, understand psychology, and it needs to be able to bring that together in a commercial result.

3
00:00:36,045 --> 00:00:49,229
I really enjoy working with complex dynamic systems, and I've always enjoyed that in various endeavors and there's nothing more complex than financial markets.

4
00:00:49,229 --> 00:00:59,744
There's always something to learn. If I look back on my career spanning over 20 years, I've seen things change in an extraordinary way.

5
00:00:59,744 --> 00:01:06,935
When I first started in the field, there was no algorithmic trading, now everything is completely automated.

6
00:01:06,935 --> 00:01:31,385
You'll find quants working across the entire financial industry these days. Most people associate the job of a quant with very well known quantitative hedge funds, and it is certainly true that quants work in that type of organization, but quants also work in commercial banks, proprietary trading firms, asset management firms, data vendor firms, and they also work across functions.

7
00:01:31,385 --> 00:01:53,905
You'll find quants working in many aspects of an investment organization. You'll find quants in the research function, those are quants looking for predictive signals and data that can be applied downstream to a portfolio, you'll find quants working in risk management trying to predict the risk of a particular portfolio or the risk firm wide.

8
00:01:53,905 --> 00:02:10,300
You'll find quants working in portfolio construction combining the signals and models produced by other quants, you'll also find quants working in data vendors, and data vendors who are preparing unstructured data than to sell to the financial industry.

9
00:02:10,300 --> 00:02:30,775
The curriculum in this program is specifically designed to be very very practical and very applicable to the quant trading process, both in a classical sense, but also in the analysis of new data and then the application of machine learning and artificial intelligence techniques.

10
00:02:30,775 --> 00:02:38,590
As such, students coming out of this program should be very attractive candidates to go into quant-trading rules.

11
00:02:38,590 --> 00:02:55,409
The traditional way to become a quant is to get a stem based degree at a top university. Go to your Career Center and they'll introduce you to a host of financial firms, you interview, you get an offer, you join an analyst program and then you get training and mentorship and you take it from there.

12
00:02:55,409 --> 00:03:09,080
Quant firms have traditionally hired from say the top 20 universities. The problem with that is say for circumstantial reasons, the top talent in the world may not be at those 20 universities.

13
00:03:09,080 --> 00:03:23,199
Given that quant firms are in a war for talent, and their success is dependent on new innovative and diverse ideas coming from their employee base, it really behooves them to cast the widest possible net.


@@@
1
00:00:00,020 --> 00:00:15,960
Hi, am Kathleen, I lead the career team at Udacity. We support students in their job search. The careers team knows what employers are looking for in a job candidate and we want to work with you to market yourself as the best person for the job, whether it's a new role or a promotion at your current company.

2
00:00:15,960 --> 00:00:24,285
You need to be proactive in your career development to be job ready at graduation. For this reason, career services are available throughout your Nanodegree experience.

3
00:00:24,285 --> 00:00:35,729
It's important to remember, while you're here to learn the technical skills for a job, it's equally important to know how to communicate those skills to an employer through an application, your online profiles, and in interviews.

4
00:00:35,729 --> 00:00:46,965
Let's meet the Careers team will be with you along the way. Hi I'm Rachel. Along with a network of career mentors, our team is here to help you from when you begin to look for jobs until you accept your job offer.

5
00:00:46,965 --> 00:00:55,379
Career support is present at different stages throughout your Nanodegree and we encourage you to go through all of them regardless of where you are in your job search.

6
00:00:55,380 --> 00:01:04,930
Udacity career services include guides, lessons, and personalized feedback on topics like talking to recruiters, interviewing, and optimizing your online profiles.

7
00:01:04,930 --> 00:01:10,790
I'm Trinh. In order to graduate career ready, you should continuously work on your career development.

8
00:01:10,790 --> 00:01:16,450
Like with all learning, beginning something earlier means you're more prepared when it's time to apply that learning.

9
00:01:16,450 --> 00:01:23,179
When your dream job pops up, you want to be ready for it. Hi, I'm Kirsten, and I head up employer relations at Udacity.

10
00:01:23,180 --> 00:01:28,540
Once you've completed the career curriculum, you'll be ready to apply to jobs and talk with employers.

11
00:01:28,540 --> 00:01:35,405
Udacity partners with industry leading companies, who are looking to hire skilled and passionate people, like you.

12
00:01:35,405 --> 00:01:51,120
Having hiring partners means you'll have direct and preferential exposure at these companies. This will help give you a leg up in your job search, as you explore your career options and develop a career path you need to you, your passion, your location, and other career goals.

13
00:01:51,120 --> 00:01:56,189
After you complete your first core Nanodegree projects, you'll then be prompted to begin your career development with us.

14
00:01:56,189 --> 00:02:03,030
We'll be there with you from then on as you explore your career options and pursue your dream role. We look forward to working with you.


@@@
1
00:00:00,000 --> 00:00:12,910
The single biggest contributor to student success in nanodegree programs is time management. We at Udacity aim to create a learning experience that will motivate you to stick with your learning and help you get excited to engage with content all the time.

2
00:00:12,910 --> 00:00:18,949
But, realistically, many students have a lot of other things going on in their lives like jobs, families, or other big commitments.

3
00:00:18,949 --> 00:00:28,425
Because of that, we provided you with a timeline in your program to help you pace yourself. This will allow you to make substantial progress, and will also act as a guide to how you're doing.

4
00:00:28,425 --> 00:00:36,234
You can think of the suggested deadlines as learning milestones to shoot for. Each one represents a tremendous amount of effort and growth toward your goals.

5
00:00:36,234 --> 00:00:47,294
If you're not able to meet a deadline all is not lost. The most important thing is to stick to a rhythm of learning that you can sustain and to dive right back into your nanodegree program as soon as you can if you have to take a break.

6
00:00:47,295 --> 00:00:58,119
To figure nanodegree learning into your life effectively, I recommend a two-pronged approach. Tip number one, continue to block out time on your calendar each week for your nanodegree program.

7
00:00:58,119 --> 00:01:04,265
The most successful Udacity students work on their content and projects multiple times a week, not just in one big block.

8
00:01:04,265 --> 00:01:12,984
Having at least three substantial chunks of learning times sprinkled throughout your weekly schedule will help you create a sustainable routine and will help your learning stick much better.

9
00:01:12,984 --> 00:01:25,729
Tip number two, slot in smaller chunks of learning whenever you have a few minutes free. Using Udacity's mobile apps is a great way to turn your train ride, time waiting in a line, or random free moment into a productive step toward your goals.

10
00:01:25,730 --> 00:01:35,155
Every little bit counts. Following these best practices will give your brain frequent chances to digest new material, which will help you develop strong skills and deep knowledge.

11
00:01:35,155 --> 00:01:40,640
Before you know it, jumping into new challenges and keeping up with your timeline will feel much more natural.


@@@
1
00:00:00,000 --> 00:00:06,290
You've probably heard of stocks and shares before. In this lesson, we're going to discuss stocks and stock prices.

2
00:00:06,290 --> 00:00:14,014
We'll also clarify the meaning of some of the words you'll hear when discussing them. Shares of stock represent fractional ownership in a company.

3
00:00:14,015 --> 00:00:23,765
If you own one share of a company stock and the stock has been split into eight shares in total or eight shares outstanding, you own one-eighth of the company.

4
00:00:23,765 --> 00:00:37,769
These days however, companies split their stock into millions or even billions of shares. Facebook for example, has issued just under three billion shares, while Alphabet, the company that owns Google, has issued about 700 million shares.

5
00:00:37,770 --> 00:00:45,570
But what does it really mean to own part of a company as a shareholder? Can you take the company car out for a spin? Not usually.

6
00:00:45,570 --> 00:00:57,064
There are actually two types of stock, common and preferred stock. Shareholders who own common stock usually get a portion of the company's profits in the form of dividends if the company distributes them.

7
00:00:57,064 --> 00:01:03,400
A dividend is a payment made by a corporation to its investors, usually as a distribution of profits.

8
00:01:03,399 --> 00:01:08,614
Shareholders also get the right to vote on matters like who should be on the company's board of directors.

9
00:01:08,614 --> 00:01:16,319
If the company is liquidated due to bankruptcy, they also get a portion of the remaining assets after all other stakeholders are compensated.

10
00:01:16,319 --> 00:01:26,594
Generally, ownership of shares in publicly traded companies, companies whose stock is owned and traded by the general public, entails what's called limited liability.

11
00:01:26,594 --> 00:01:33,140
In part, limited liability means that shareholders cannot be made to pay the company's debts if it goes bankrupt.

12
00:01:33,140 --> 00:01:45,424
Shareholders who own preferred stock get a slightly different deal, they're promised a fixed amount of income each year and get paid before owners of common stock get paid dividends, but they usually do not have voting rights.

13
00:01:45,424 --> 00:01:52,954
But of course, one of the main benefits of owning stock is the potential opportunity to profit from selling it after it's value has increased.


@@@
1
00:00:00,000 --> 00:00:09,155
Let's take a moment to clarify the meaning of some words you might hear when discussing stock. A security is a financial instrument that has some type of monetary value.

2
00:00:09,154 --> 00:00:20,655
Some securities that you might have heard of are stocks, bonds, and options. Securities can be classified into three broad types; debt securities, derivative securities, and equity securities.

3
00:00:20,655 --> 00:00:27,879
Debt securities represent money that is owed and must be repaid like government or corporate bonds, or certificates of deposit.

4
00:00:27,879 --> 00:00:38,860
Debt securities are also called fixed-income securities because they promise a stream of income over time in the form of interest either at a fixed rate or at a rate determined by a specific formula.

5
00:00:38,859 --> 00:00:46,870
Derivative securities such as options or futures contracts are so-named because their values depend on the prices of other assets.

6
00:00:46,869 --> 00:00:57,859
For example, an option is a contract which gives the buyer the right but not the obligation to buy or sell an underlying asset at a specified price on a specified date.

7
00:00:57,859 --> 00:01:14,560
An example of an option contract would be employee stock options. As part of employee incentive programs, employers frequently offer employees the option to buy companies stock at a lower price called the strike price, and later sell it on the open market to make a profit.

8
00:01:14,560 --> 00:01:24,544
On the other hand, a futures contract obligates the buyer to buy or the seller to sell an asset at a predetermined price at a specified time in the future.

9
00:01:24,545 --> 00:01:38,465
For example, say, a dairy farmer concurrently sell milk for $2 a gallon. Next month, he plans to sell 200 gallons of milk, but he's concerned that fluctuations in the market price for milk will erode his profits.

10
00:01:38,465 --> 00:01:54,625
So he enters into a futures contract with an investor. By doing so, he is obligated to sell 200 gallons of milk on the agreed date, but he's guaranteed to receive $400 regardless of the market price of milk at that time.

11
00:01:54,625 --> 00:02:04,689
This type of agreement would protect him from the risk of catastrophic loss if the price of milk tanks, but will also limit his ability to profit if the price of milk rises.

12
00:02:04,689 --> 00:02:15,784
Then there's that last category, equity securities. Let's break that phrase down. Equity is the value of an owned asset minus the amount of all debts or liabilities on that asset.

13
00:02:15,784 --> 00:02:23,965
So if you own a $15,000 car, but you owe a $5,000 loan against it, the car represents $10,000 of equity.

14
00:02:23,965 --> 00:02:31,689
The word equity can have somewhat different meanings in different contexts. But in general, you can think of equity as the net value of something owned.

15
00:02:31,689 --> 00:02:43,330
Stocks are called equity securities because they represent ownership in a firm. One more thing, a security representing ownership interest in a private company is called private equity.


@@@
1
00:00:00,000 --> 00:00:09,720
So say, you want a chance to reap the potential financial rewards of investment in stock. Where do you start? Well, usually, you find a brokerage.

2
00:00:09,720 --> 00:00:17,480
A brokerage company functions as a middleman connecting buyers and sellers. However, they usually charge fees to complete transactions.

3
00:00:17,480 --> 00:00:29,175
It is also sometimes possible to buy stock directly from companies. In this case, you don't have to use or pay commissions to a broker, but you have less control over the price and timing of your purchase.

4
00:00:29,175 --> 00:00:43,060
You can't buy or sell at a specific market price or at a specific time. Typically, the company buys or sells shares for the plan at set times and at an average market price, but how the shares are purchased depends on the company and the details of the plan.

5
00:00:43,060 --> 00:00:53,420
Once you've bought some stock, you'll be very interested in the patterns of changes in its price because in order to make money from your investment, you want to sell it when its price is higher than when you bought it.

6
00:00:53,420 --> 00:01:01,625
A great deal of effort has gone into predicting when the price of a stock will go up. In order to follow your stock, you'll look up its unique symbol or ticker.

7
00:01:01,625 --> 00:01:12,184
Alphabets unique symbol is GOOG, Apples ticker is AAPL. This is a price graph of Alphabet stock from 2005 until September 2017.

8
00:01:12,185 --> 00:01:23,370
While the graph seems to progress upwards, it isn't a steady or linear climb. The history of a stock's price is important and potentially an indicator of how the stock will do in the future.

9
00:01:23,370 --> 00:01:30,579
As such, price histories have been studied by investors and traders since stocks were first traded. But when was that?

10
00:01:30,579 --> 00:01:43,764
How did all this get started in the first place? Some people trace the history of stock back to the Roman Republic when the state contracted out many of its services to private companies which issued shares to individuals to help with government services.

11
00:01:43,765 --> 00:01:52,174
In the early 1600s, the Dutch East India Company became the first company in history to issue bonds and shares of stock to the general public.

12
00:01:52,174 --> 00:02:04,614
The company established the Amsterdam Stock Exchange in 1602 as a forum for trade of securities. The Amsterdam Stock Exchange pictured here could be called the first incarnation of a modern stock market.

13
00:02:04,614 --> 00:02:10,990
Patterns in market activity have been studied since markets earliest days and some patterns have been observed to repeat.

14
00:02:10,990 --> 00:02:21,755
One of the most interesting market phenomena is the market bubble. Bubbles occur when market participants drive stock prices above their value in relation to some system evaluation.

15
00:02:21,754 --> 00:02:36,294
There are notable similarities between the tulip bulb bubble which took place for centuries ago and saw the value of a single tulip bulb grow to nearly 10 times the annual wage of a skilled worker and the.com bubble which occurred in the early 2000s.

16
00:02:36,294 --> 00:02:46,699
The similarities in the graphs are fascinating and academics from behavioral economists to theoretical mathematicians have devised captivating theories as to why bubbles might happen.

17
00:02:46,699 --> 00:02:55,935
One of the earliest works Extraordinary Popular Delusions and the Madness of Crowds was published by Scottish journalists Charles Mackay way back in 1841.

18
00:02:55,935 --> 00:03:07,400
If you're interested in reading more about market bubbles or understanding the forces driving markets which are not explained by standard quantitative models, I'd suggest the work of Robert Shiller and Nassim Taleb.


@@@
1
00:00:00,000 --> 00:00:14,324
This lesson will introduce you to the mechanics behind modern-day stock markets. But not only that, you also build a simple market simulator that matches and executes orders, and sets prices just like a real market.

2
00:00:14,324 --> 00:00:22,244
This will give you a better idea of all the activities that keep a market running. These activities generate a lot of data.

3
00:00:22,245 --> 00:00:38,540
Market data, which may hold important clues that you can use to decide when to buy or sell stocks. As you get introduced to the various dimensions of data available to us, try to build an intuition for why they might be important, what signals they might carry.

4
00:00:38,539 --> 00:00:49,094
Finally, you will learn to deal with data during the closing and opening of global markets. If you pay close attention, there might be some training opportunities there too.


@@@
1
00:00:00,000 --> 00:00:07,120
This is Betty. She sells the best fruit. Let's ask her how she decides what price to sell at. Good morning.

2
00:00:07,120 --> 00:00:13,064
Morning. So, if you don't mind me asking, I was wondering how do you choose the prices for your oranges.

3
00:00:13,064 --> 00:00:18,795
Sometimes I see they're selling at two bucks a pound, some days they're four bucks a pound, what's going on?

4
00:00:18,795 --> 00:00:28,500
Well, I obviously try to cover my costs and then keeps some profit. That gives me a starting point. Sometimes people will buy it at that price which makes me think I could get a bit more money for them.

5
00:00:28,500 --> 00:00:34,189
So, I bump up the price a little bit. If I don't see anyone buying, I might lower the price later in the day.

6
00:00:34,189 --> 00:00:46,674
Does that always work or do people come back and bargain with you as well? Yes, all the time. They'll often want to pay a lower price and then I tell them what price I can sell at, and we come to a middle ground.

7
00:00:46,674 --> 00:00:51,715
Some people are smart. They'll ask around what prices others are selling it and then pick the lowest price.

8
00:00:51,715 --> 00:01:05,495
Eventually, we`re all forced to sell at that lowest price. I see. So, the prices of oranges are set by actual transactions, like the maximum price a buyer is willing to buy at and the minimum price the seller is willing to sell at?


@@@
1
00:00:00,000 --> 00:00:13,700
Stock markets function in a very similar way. When a company first begins trading publicly it typically picks a value for its stock based on certain company metrics like revenue, profits, assets et cetera.

2
00:00:13,699 --> 00:00:21,015
But from that point on, it's stock prices almost entirely driven by how much demand exists for its stock.

3
00:00:21,015 --> 00:00:31,109
But how do you quantify demand? Think about it. Demand is nothing but how many people want to own some stock, or how much money they're willing to pay for it.

4
00:00:31,109 --> 00:00:42,724
In market speak you want to see how many orders people are placing for it and at what price. Modern markets keep an electronic record of all orders that have been submitted.

5
00:00:42,725 --> 00:00:48,939
People who want to buy a stock can specify how much or the limit that they're willing to pay for it.

6
00:00:48,939 --> 00:00:55,479
People who want to sell can specify the least amount. Again the limit they're willing to sell it for.

7
00:00:55,479 --> 00:01:04,655
Whenever a suitable match is found between a buyer and seller, a trade is executed. It's important to understand how orders are matched.

8
00:01:04,655 --> 00:01:14,575
Let say I submit a limit buy order for two shares of Netflix at $180 each. Think of this as an offer or commitment to buy.

9
00:01:14,575 --> 00:01:28,055
It is also known as placing a bid where $180 is my bid price. This order gets added to the market's queue where all the different buyer orders for Netflix are listed sorted by price.

10
00:01:28,055 --> 00:01:41,295
Someone else submits a limit sell order for four shares at $179.90. This is a commitment to sell. $179.90 is their asking price.

11
00:01:41,295 --> 00:01:48,995
Since that is less than the top most buy order, the one with the highest bid price a transaction cannot take place.

12
00:01:48,995 --> 00:01:56,079
If there are enough stocks to fulfill that order then the next bid could also be fulfilled with that same seller.

13
00:01:56,079 --> 00:02:03,525
In this case the top to buy orders are executed and three out of the four shares offered by the seller are sold.

14
00:02:03,525 --> 00:02:13,324
The remaining one share can be used to partially fulfilled my buy order, and the rest of it can be satisfied by the next seller who offers a suitable price.

15
00:02:13,324 --> 00:02:23,019
Say someone else offers to sell three shares at $179.80. This order rises to the top of the sell queue due to its low price.

16
00:02:23,020 --> 00:02:46,865
Since this is less than my bid price my remaining one share can come from the seller. So, in the end, with my limit order of my two Netflix shares at $180 I got one share at $179.90, and one at $179.80 for an average price of $179.85 per share.

17
00:02:46,865 --> 00:03:02,254
This sounds a lot like an auction doesn't it? In fact, multiple auctions going on simultaneously orchestrated by some authority that ensures every buyer and seller gets the best available price for the stock they want to trade.

18
00:03:02,254 --> 00:03:14,490
This authority is the stock exchange. One problem with this model is that trading can take a lot of effort if everyone has to participate in an auction each time they want to buy or sell a stock.

19
00:03:14,490 --> 00:03:26,219
What if you wanted to purchase some right away, how do you make sure you're getting the best price? Well, if you're at a farmers' market you might go around asking every vendor, but that's very inefficient.

20
00:03:26,218 --> 00:03:33,490
Fortunately, since stock market transactions are carried out electronically we have better ways of dealing with this.

21
00:03:33,490 --> 00:03:48,369
The stock exchange keeps track of the last price at which a stock was traded. This is publicly posted as the current price of the stock, and if you wish to buy or sell shares around that price you should be able to do that immediately.

22
00:03:48,370 --> 00:03:59,829
This is known as placing a market order. But it does introduce one complication, someone has to be willing to sell or buy that stock on the other end at all times.

23
00:03:59,830 --> 00:04:11,854
That's where a market maker comes into the picture. A market maker is a financial firm. Usually a brokerage that continuously offers to buy and sell stocks at publicly advertised prices.

24
00:04:11,854 --> 00:04:24,280
For example, the New York Stock Exchange has designated market makers such as Citadel securities and J Stryker and Cole who are obligated to make large volumes of shares available for trading.

25
00:04:24,279 --> 00:04:32,759
You might think why would someone want to do that? What if a market maker buys a bunch of shares and then the price starts falling. That's right.

26
00:04:32,759 --> 00:04:42,780
They do take a risk by continuously buying and selling. But they are usually compensated for that risk by commissions and fees that they earn on those traits.

27
00:04:42,779 --> 00:04:58,230
They also maintain a small difference between the bid and ask prices for each stock. Say you want to sell some Microsoft shares at market price, a market maker is bidding $74 per stock which happens to be the best bid price available.

28
00:04:58,230 --> 00:05:07,720
So the market executes your order with that market maker. Later, another investor comes in and wants to buy some Microsoft shares at market price.

29
00:05:07,720 --> 00:05:20,604
The same market maker maybe asking for a slightly higher price to sell say $74.10. Assuming that's the best price available they just made $0.10 on each share, that might not seem much.

30
00:05:20,605 --> 00:05:28,375
But over the course of a day, a market maker typically trades thousands or millions of shares. That easily adds up.

31
00:05:28,375 --> 00:05:39,729
The difference between these prices is known as the Bid-Ask Spread. This is a very important concept to understand in trading and comes into play in other situations as well.


@@@
1
00:00:00,000 --> 00:00:08,109
Market makers play an important role in keeping a stock market running smoothly. Imagine what would happen without market-makers?

2
00:00:08,109 --> 00:00:17,039
It would be harder to buy or sell stocks at a consistent price. As people start selling a stock, its price would start falling and vice versa.

3
00:00:17,039 --> 00:00:29,355
Formally, we say that a market maker provides liquidity. Liquidity is the property of a financial asset like a stock, to be bought or sold without causing sharp changes in its price.

4
00:00:29,355 --> 00:00:45,350
For instance, Apple stock sees a lot of day-to-day activity. So, we say that it is liquid. If you have a large number of Apple shares and you want to sell them, there's a good chance you will get a consistent and predictable price for them.

5
00:00:45,350 --> 00:00:55,334
On the other hand, stocks that are relatively difficult to buy or sell are said to be illiquid. Penny stocks, for example, are typically very thinly traded.

6
00:00:55,335 --> 00:01:02,509
Consequently, buying or selling them may be difficult. Liquidity can also vary from market to market.

7
00:01:02,509 --> 00:01:11,989
Oh yes, the same stock can be bought or sold in different markets. HSBC is one such stock. It's listed in both New York and Hong Kong.

8
00:01:11,989 --> 00:01:23,700
Let's look at how its price changed during a month of trading. Each market maintains its own book of orders so the same stock can trade at different prices in different markets.

9
00:01:23,700 --> 00:01:31,140
The difference is usually small, as you can see, and any big changes permeate across markets pretty quickly.

10
00:01:31,140 --> 00:01:43,405
That being said, it's possible to profit from market inefficiencies by simultaneously purchasing and selling an asset in different markets, thereby exploiting the differences in price.


@@@
1
00:00:00,000 --> 00:00:07,849
Stock exchanges publish a stream of data that includes each individual trade. This is known as tick data.

2
00:00:07,849 --> 00:00:14,129
Ticks are an intuitive way to gauge the health of a stock or even an entire market. Are investors happy?

3
00:00:14,130 --> 00:00:25,589
Are they making money? Are prices generally rising or falling, ect.? For example, you can compare each stock's latest tick price with its previous price to see if it is going up or down.

4
00:00:25,589 --> 00:00:33,090
You can then aggregate this information by comparing the number of stocks on an up-tick with the number of stocks on a down-tick.

5
00:00:33,090 --> 00:00:50,820
These ticks also form the basis for all market data that is available for analysis. Tick data can provide further insight into how a particular stock is behaving and can help you take better intraday decisions, but you're talking about a massive volume of data.

6
00:00:50,820 --> 00:00:58,695
Have you ever wondered how busy stock markets are? The New York Stock Exchange carries out billions of trades every day.

7
00:00:58,695 --> 00:01:08,495
That's billions with a B. If you want to keep track of all these transactions and use all that data to make trading decisions, that might slow you down.

8
00:01:08,495 --> 00:01:17,325
Even if you want to use historical tick data to build statistical models or train a learning algorithm, it might be too much data to process.

9
00:01:17,325 --> 00:01:26,325
Consider that over $42 trillion worth of stocks and shares were traded in the US in 2016. A single year.

10
00:01:26,325 --> 00:01:41,530
That's more than double the GDP of the entire country. Fortunately, market data can be summarized in a way that drastically reduces the volume of information to be processed and yet retains most of the important characteristics.


@@@
1
00:00:00,000 --> 00:00:10,334
Investors like to focus on pieces of data that are most important. One way to do that, is to take the tick data and bucket it into regular intervals of time.

2
00:00:10,335 --> 00:00:19,945
It could be minutes, hours, days, months, extra. Then summarize all the trades that happened during each interval using measures that are relevant to you.

3
00:00:19,945 --> 00:00:31,535
Let's take a closer look at one of these intervals. Let's say it's a minute. The most common set of measures used in practice are Open High Low Close or OHLC.

4
00:00:31,535 --> 00:00:40,149
Open is the stock price at the beginning of the period, high and low capture its range of movement, and close is where it ends.

5
00:00:40,149 --> 00:00:48,895
These measures are often visualized using OHLC bars, which show these four measures for each time interval in a compact form.

6
00:00:48,895 --> 00:00:55,500
This gives you a high level summary that might better represent price movements than looking at lower prices.

7
00:00:55,500 --> 00:01:08,854
Here's an OHLC chart, that shows a month of price data bucketed into single days. You can spot days where the intraday prices jumped beyond opening and closing prices, indicating a lot of volatility.

8
00:01:08,855 --> 00:01:15,835
If you try to plot all the individual takes over this period, it would be hard to identify these patterns.

9
00:01:15,834 --> 00:01:23,579
Sometimes these measures are used for a very specific purposes. The daily closing price is the one that is coded most often.

10
00:01:23,579 --> 00:01:31,844
It is used by casual traders and investors who are interested in long-term gains. It is also used for accounting purposes.

11
00:01:31,844 --> 00:01:43,653
The opening price, is where you would expect the first trade of the day to take place. There might be a gap from last day's closing price, because of pre-market trading or trading in other markets.

12
00:01:43,653 --> 00:01:52,370
Some trading strategies try to utilize this difference. High-low, the high and low prices capture the movement of the stock.

13
00:01:52,370 --> 00:02:00,070
These peaks are easier to remember for humans compared to individual trades or the distribution of prices during that period.


@@@
1
00:00:00,000 --> 00:00:11,730
Besides the current price of a particular stock. Another metric that traders liked to keep an eye on is the number of shares that are being traded over a period of time known as volume.

2
00:00:11,730 --> 00:00:22,980
Often shown along the bottom of an OHLC chart. The sum of unit price times volume gives you a more accurate measure of the total amount of money moving around.

3
00:00:22,980 --> 00:00:31,094
Volume is also important because it can affect how sharply its price might rise or fall. Let's take a look at an example.

4
00:00:31,094 --> 00:00:47,849
Acme Inc is trading at $100 per share. Several investors speculate that Acme is going to launch a new product and expecting the new product to increase Acme's value significantly, they place large orders to buy a total of 100,000 shares.

5
00:00:47,850 --> 00:00:57,050
The action of buying the shares increases the apparent demand for the stock and consequently the price jumps to $120 per share.

6
00:00:57,049 --> 00:01:06,069
That is, someone actually buys the shares at $120. Why does it jump? Because there are more people who want to buy the stock than those who want to sell.

7
00:01:06,069 --> 00:01:17,555
So, the sellers can charge a premium and you bet they do. Let's assume that over the next few days there aren't any announcements and no significant new activity in Acme's stock.

8
00:01:17,555 --> 00:01:25,694
Some shareholders want to pull out their money and invest in another opportunity they hear about. So the price begins to fall gradually.

9
00:01:25,694 --> 00:01:34,240
Noticing this trend other investors decide to sell the majority of shares they bought. So that they can make a profit while the price is still high.

10
00:01:34,239 --> 00:01:50,064
As expected, this results in a sharp drop in Acme's stock price it comes down to $90 per share. If you're monitoring the price and volume of Acme's stock, you might have noticed the relationship between volume of transactions and price changes.

11
00:01:50,064 --> 00:01:59,000
In general, large volumes of buy orders tend to sharply increase the stock price and large volumes of sell orders make it fall.

12
00:01:59,000 --> 00:02:09,604
This is another signal that you can use to decide when to trade a given stock. It is also something to keep in mind if you're a major investor and you trade in large volumes.

13
00:02:09,604 --> 00:02:18,280
Some hedge funds for instance will spread their transactions over multiple days, so that their own actions don't end up affecting the price adversely.

14
00:02:18,280 --> 00:02:28,050
Note that the volume of transactions also varies throughout any trading day. This is something to be aware of if you want to use volume as a trading signal.

15
00:02:28,050 --> 00:02:33,174
Stocks that are of active interest will typically see a lot of training at the beginning of the day.

16
00:02:33,175 --> 00:02:40,555
Since that is when traders get to act on, all the new information gathered and analyzed since the previous day's market close.

17
00:02:40,555 --> 00:02:49,504
This initial activity known as price discovery, helps buyers and sellers converge on a mutually acceptable market price for each stock.

18
00:02:49,504 --> 00:03:01,579
Then the volume falls to a lower level as typical daily trading activity resumes. Finally, towards the end of the trading day, activity tends to increase a little resulting in a higher volume.

19
00:03:01,580 --> 00:03:11,284
This can be due to several reasons including day traders who want to close out any open positions and funds that typically update their holdings at the end of the day.

20
00:03:11,284 --> 00:03:18,094
In fact, some traders who have decided to buy-sell particular stocks, may not want to risk waiting for the next day.

21
00:03:18,094 --> 00:03:28,335
Who knows what events might take place in the meantime and how they might affect prices. Volume carries a lot of information about a stock and is an often overlooked metric.

22
00:03:28,335 --> 00:03:39,130
High volume trade, is more likely to affect the price of a stock than a low volume trade. Therefore, volume is an important factor to consider within your trading strategy.


@@@
1
00:00:00,000 --> 00:00:10,300
Stock markets typically remain open for a finite number of hours every day, say 9:30 AM to 4:00 PM. This is when the majority of the transactions take place.

2
00:00:10,300 --> 00:00:19,945
Why you ask? Well, traders and exchange operators are humans just like you and me. They need to go back to their families, sleep, and live a life.

3
00:00:19,945 --> 00:00:30,280
Okay, there is another reason. With the growing popularity of algorithmic trading, a large portion of transactions today are being carried out by automated systems.

4
00:00:30,280 --> 00:00:39,525
They can be much more responsive to market conditions than humans, and result in a sharp increase or decrease in stock prices over a very short period of time.

5
00:00:39,524 --> 00:00:53,109
If left unchecked, this can result in things spiraling out of control. Closing market operations at a regular time every day provides an artificial barrier that can limit the damage such events can cause.

6
00:00:53,109 --> 00:00:59,960
This also give stock market regulators some time to analyze the situation and implement control measures.

7
00:00:59,960 --> 00:01:09,939
Now, some exchanges do allow for pre-market and post-market trading. Typically 4:00 AM to 9:30 AM for pre-market, and 4:00 PM to 8:00 PM post-market.

8
00:01:09,939 --> 00:01:25,909
However, not many traders participate in the session. So, trading volume is typically low. Either way, the takeaway from this is that stock markets close operations at a certain time every day, and they typically remain closed over weekends and holidays.

9
00:01:25,909 --> 00:01:33,560
As a result, when you look at stock data, you'll notice these gaps or discontinuities during periods when the markets are closed.

10
00:01:33,560 --> 00:01:52,829
Depending on your analysis, this may or may not have an impact on the conclusions you draw. For instance, if you treat market data as a simple sequence of observations and ignore the timestamps as if the market's just smoothly continue from one end of day to the next opening bell, then these gaps are not as significant.

11
00:01:52,829 --> 00:02:02,370
But, if you compute a metric that involves time like the number of transactions per hour, then ignoring these gaps can give you inaccurate results.

12
00:02:02,370 --> 00:02:10,605
The discontinuities can produce spikes or unusual flats in calculated values, or just outright wrong information.


@@@
1
00:00:00,000 --> 00:00:17,019
Another aspect worth considering is that most major stock markets are actual physical buildings located in a city somewhere on the planet, and because different cities around the globe are at different time zones, the actual opening and closing times also vary.

2
00:00:17,019 --> 00:00:28,449
On any given date, the Hong Kong Stock Exchange opens at 09:30 AM local time. Seven hours later, the London Stock Exchange opens at 09:30 AM their local time.

3
00:00:28,449 --> 00:00:37,033
And the New York Stock Exchange opens five hours after that. This produces some additional complications and opportunities for traders.

4
00:00:37,033 --> 00:00:48,549
For example, consider the stocks that are listed on multiple global exchanges. The price behavior of stock on the Hong Kong exchange is likely to have an effect on how it performs in London.

5
00:00:48,549 --> 00:00:56,255
With a full seven hours between them, you have plenty of time to decide how you want to trade that day on the London Exchange.

6
00:00:56,255 --> 00:01:08,435
Imagine the kinds of strategies you can devise. If HSBC rises in Hong Kong, you can buy HSBC in London the moment the market opens, knowing that it is likely to rise there too.

7
00:01:08,435 --> 00:01:20,530
Of course, you have to be careful about how you execute such strategies. Remember that if you are trying to take advantage of markets in different time zones, chances are high that other traders are too.


@@@
1
00:00:00,000 --> 00:00:11,120
Now that you know how stock markets work, how trades are executed and how prices are set. We'll move on and take a closer look at the data you need to pay attention to.

2
00:00:11,119 --> 00:00:25,765
Market data forms the basis for quantitative analysis and trading. In addition, there are several other sources of information that you also need to take into account, such as corporate actions, data derived from fundamental information et cetera.

3
00:00:25,765 --> 00:00:33,129
All of these affect stock prices. So you need to factor them in when designing your quantitative trading pipeline.


@@@
1
00:00:00,000 --> 00:00:09,644
Let's use the knowledge you took from Marketplace Mechanics, and dive deep into the data. The data is the most important thing to quantitative analysis.

2
00:00:09,644 --> 00:00:18,869
Without good data, you don't have good predictions. That is sometimes referred to as the garbage in, garbage out principle.

3
00:00:18,870 --> 00:00:26,654
That is the quality of your data directly impacts the quality of your predictions, no matter what model you use.

4
00:00:26,655 --> 00:00:35,439
In this lesson, we'll talk about a few common data sets that you'll use and the difficulty with working with that data.


@@@
1
00:00:00,000 --> 00:00:07,634
Let's start with market data. This is any data that you receive from the market typically generated from trades.

2
00:00:07,634 --> 00:00:21,570
This is perhaps the most important form of data relevant to quantitative analysis. Market data is temporal in nature, it's a series of trading events that happen in a moment of time, each moment is called a tick.

3
00:00:21,570 --> 00:00:31,070
A ticker contains the time of the event, information about which stock or ticker symbol is traded trade data and quote data.

4
00:00:31,070 --> 00:00:39,299
The trade data is the price and the amount of the transaction. The quote data is the price and size of the bid and ask.

5
00:00:39,299 --> 00:00:51,910
A bid is request to buy stock at a price for a set amount of shares. A ask is the other side of the trade it's the request to sell stock at a price for a set amount of shares.

6
00:00:51,909 --> 00:01:02,434
In large marketplaces the number of trades can easily hit 200 trades per second. This is a massive amount of data when using historical tech data.

7
00:01:02,435 --> 00:01:14,145
Analyzing individual texts may not be visible with this much data. So, we bucket these texts into equally space time integrals such as minutes or days.

8
00:01:14,144 --> 00:01:25,829
For each bucket, we can compute open high low close prices and total volume of transactions. This sort of minute or day level data is much easier to work with.

9
00:01:25,829 --> 00:01:35,914
You have the ability to ignore the timestamps and treat the data as equally spaced sequences. In some cases, you may still need to use the timestamps.

10
00:01:35,915 --> 00:01:45,000
But we'll save that for another lesson. In the next video, you will learn about another type of finance data called corporate actions.


@@@
1
00:00:00,000 --> 00:00:07,455
The next set of data we'll talk about is related to events a company can take that affects its shareholders.

2
00:00:07,455 --> 00:00:15,089
This data is called Corporate Actions. For this lesson, we'll describe only two of the many corporate actions.

3
00:00:15,089 --> 00:00:26,859
Stock splits and dividends. Let's start with stock splits. On June 2nd, 1998, you could buy a share of Amazon for about $85.

4
00:00:26,859 --> 00:00:35,534
On this date, Amazon decided to perform a two to one stock split. Every Amazon shareholder, had their shares doubled.

5
00:00:35,534 --> 00:00:43,799
This does not mean all of their money in Amazon doubled. When a stock is split into two, its price drops by half.

6
00:00:43,799 --> 00:00:55,274
This makes sure that the total market capitalization of a stock has not changed by a split. Market capitalization is the dollar value of a company's outstanding shares.

7
00:00:55,274 --> 00:01:05,679
This is calculated by multiplying the stock price by the total number of shares outstanding. In this case, there are twice as many outstanding shares.

8
00:01:05,680 --> 00:01:14,739
To keep the market capitalization the same, the stock price has to drop by half. Why would you ever want to split a stock?

9
00:01:14,739 --> 00:01:21,979
One reason could be to make the stock more liquid. Less expense of stocks are believed to be more liquid.

10
00:01:21,980 --> 00:01:32,265
Current shareholders and potential investors can now buy and sell and more granularity. This helps maintain a healthy volume of transactions.

11
00:01:32,265 --> 00:01:45,060
Amazon later went on to have two more stock splits. A three-to-one split on January 5th, 1999. A two-to-one split on September 2nd of the same year.

12
00:01:45,060 --> 00:01:54,055
The price dropped to roughly one-third and one-half respectively. I say roughly, because there is still some trading during the day.

13
00:01:54,055 --> 00:02:05,015
The closing price ends up being a little different from exactly one-third or one-half, typically, a little higher due to the renewed interest in the stock.

14
00:02:05,015 --> 00:02:13,159
If you look at the graph of the stock prices at that time, you'll notice that the large drops are due to the stock splits.

15
00:02:13,159 --> 00:02:23,454
Looking at this graph makes it look like the company dropped in value. Using these prices, a computer would read the data the same way.

16
00:02:23,455 --> 00:02:32,504
However, this is not true. The value of the company has not changed from the split. How do you correct for this?

17
00:02:32,504 --> 00:02:47,239
One common approach is to normalize the prices to reduce the sudden changes. For instance, you could double the stock price after each two-to-one split, triple after each three-to-one split, and so on.

18
00:02:47,240 --> 00:02:54,085
The trouble with that is, your current normalized price will be very different from what the stock is actually trading at.

19
00:02:54,085 --> 00:03:04,585
Instead, we'll do the opposite. We'll have the price before each two-to-one split, turn it into thirds before any three-to-one split, and so on.

20
00:03:04,585 --> 00:03:15,085
Now, the newest adjusted price matches the price currently on the market. Stock prices normalized in this manner is called adjusted price.

21
00:03:15,085 --> 00:03:24,420
This is typically provided in most historical data sets. Needless to say, you must keep in mind that this is a distortion of reality.

22
00:03:24,419 --> 00:03:34,765
In June of 1988, before the first split, Amazon was not trading at seven dollars. It was trading around $85.

23
00:03:34,764 --> 00:03:48,870
It is the repeated adjustments to historical prices that have reduced 85 to $7. With this in mind, let's learn about the next corporate action, dividend.


@@@
1
00:00:00,000 --> 00:00:12,195
The next corporate action we'll talk about is dividend, specifically cash dividends. Dividends are when companies share some fraction of their profits with their shareholders.

2
00:00:12,195 --> 00:00:23,894
Let's take a look at Qualcomm which pays out dividends pretty much every quarter. On May, 21st of 2017, they paid out $0.57 per share.

3
00:00:23,894 --> 00:00:35,195
However, dividends are given to everyone that hold shares in the stock at the time of the payout. A shareholder must have bought the share before the ex-dividend date.

4
00:00:35,195 --> 00:00:44,844
An ex-dividend date is the cutoff point to receive the future dividends. Let's take a look at an ex-dividend date for AIG.

5
00:00:44,844 --> 00:01:03,510
AIG had announced on February 8th, 2018 that they will be giving out dividends in the future. The date the dividends will be given out is March 29th, 2018, the ex-dividend date is March 14th, 2018.

6
00:01:03,509 --> 00:01:14,854
Whoever held the stock before the state will receive the dividend. With the basics of dividends out of the way, let's go over adjusting the prices to dividends.

7
00:01:14,855 --> 00:01:27,579
We'll start with why we adjust the prices based on dividends. Imagine that you own one share of company A and one share of company B, both are worth $50.

8
00:01:27,579 --> 00:01:38,170
The next day is an ex-dividend date of $1 in dividends for company A. That day, company A closes at $49.50.

9
00:01:38,739 --> 00:01:53,584
Company B does not have ex-dividend date, but also closes at $49.50. If you only looked at the prices, you might think you lost $0.50 on both stocks.

10
00:01:53,584 --> 00:02:06,724
In reality, you made $0.50 on company A and lost $0.50 on company B. Just like the stock splits, we'll normalize the prices to reflect this.

11
00:02:06,724 --> 00:02:22,365
To get the normalised prices, we first need to calculate the adjusted price factor. The formula for this is 1+D over S. D is the dividend, S is the stock price at ex-dividend date.

12
00:02:22,365 --> 00:02:32,875
To normalize the price, you would divide the historical price by the adjusted price factor up until the day before the ex-dividend date.


@@@
1
00:00:00,000 --> 00:00:09,619
So, you have your stock prices adjusted for corporate actions like splits and dividends. How do you use this information to perform trading?

2
00:00:09,619 --> 00:00:21,339
When you buy, when you sell, or even which stocks do you buy or sell. You can take these decisions based on signals that can be derived from historical price data.

3
00:00:21,339 --> 00:00:33,265
The first step in this process is to compute statistical measures that are called indicators. You can think of the raw price of a stock as the most basic kind of indicator.

4
00:00:33,265 --> 00:00:45,885
Here are the stock prices for Facebook over a one-year period. The latest price is about $115. Is that good? Should we buy some Facebook shares?

5
00:00:45,884 --> 00:00:53,204
Well, it's not clear. The price seems to be jumping around a lot and we don't have a sense of where we should expect to be.

6
00:00:53,204 --> 00:01:05,795
If we had that, then we could check if their current price is significantly higher or lower than the expected price and make a decision based on that.

7
00:01:05,795 --> 00:01:16,730
So, what is the expected price of a stock? Is it the average price from when it began trading? That seems a little too extreme.

8
00:01:16,730 --> 00:01:23,899
Stocks can grow in orders of magnitude over the years. Most current prices will be well over the average.

9
00:01:23,899 --> 00:01:39,494
What might be relevant is the recent average price of the stock. Maybe over the past week or month, we can extend this idea throughout the history of the stock and compute the average of a fixed length window of time.

10
00:01:39,495 --> 00:01:51,494
It's like moving the window one unit at a time and taking the average price within that window. This is known as the simple moving average or rolling mean.

11
00:01:51,495 --> 00:02:00,450
We could devise a trading strategy that looks for large deviations from the moving average, and generate trading signals based on that.

12
00:02:00,450 --> 00:02:10,590
For instance, if a stock falls too far below its average, then we should buy it, or if it rises too far above, then we should sell.

13
00:02:10,590 --> 00:02:21,025
But, how much is too much. Using a constant number or a threshold does not seem like a good idea. Different stocks trade at different price levels.

14
00:02:21,025 --> 00:02:30,840
We need a measure that is tied to the price of the stock, maybe some fraction of the stock price. But again, what fraction? We don't know.

15
00:02:30,840 --> 00:02:40,995
Some stocks jump around a lot. Some are more stable. A better idea might be to compute the threshold from the jumpiness or volatility of the stock.

16
00:02:40,995 --> 00:02:52,964
How about standard deviation? In fact, we can reuse the windowing idea and compute standard deviation over the same fixed length window across time.

17
00:02:52,965 --> 00:03:03,305
These lines are called Bollinger bands. All these peaks sticking above the upper band are signaling that the stock is trading at a higher price than normal.

18
00:03:03,305 --> 00:03:13,120
The dips below the lower band are signaling abnormally low price. One problem that you might notice is that there's too many such points.

19
00:03:13,120 --> 00:03:21,160
We can reduce them by increasing the width of the bands. That is, by considering a wider range of variation to be normal.

20
00:03:21,159 --> 00:03:30,949
A common threshold is to choose two standard deviations above and below. Now, we have far fewer outliers.

21
00:03:30,949 --> 00:03:39,235
But the question remains what do we do with these outlying points? Sometimes we get short burst of outliers.

22
00:03:39,235 --> 00:03:52,924
So, should we buy or sell each of these points? If you think about it, when the price falls below the lower band, you don't really know if it's going to keep falling or is it going to rise back up.

23
00:03:52,925 --> 00:04:04,199
Maybe it's wiser to focus on these inflection points. When the price is below the lower band and starts to cross back inside towards the mean, that should be a good time to buy.

24
00:04:04,199 --> 00:04:14,090
The price is still fairly low and on a rise. On the other side, we can sell when the price crosses down the upper band to the inside.

25
00:04:14,090 --> 00:04:23,980
This gives us a series of buy and sell signals. At this point, you must be wondering, how much money would I make from following these signals.

26
00:04:23,980 --> 00:04:34,304
Well, that opens up a whole can of worms. We'll have to decide how much money you invest to account for the brokerage charges and for a few other things.

27
00:04:34,305 --> 00:04:43,625
Let's just kick this can down the road and we will look at again when we talk about backtesting. For now, think about the indicators we just computed.

28
00:04:43,625 --> 00:04:52,985
Simple moving average and standard deviation and the trading signals we derived from them. This was a fairly simple approach.


@@@
1
00:00:00,000 --> 00:00:13,169
Up until now, you've been treating stock prices as a continuous time series. For instance, the end of day data for stock includes a row for every day or does it?

2
00:00:13,169 --> 00:00:26,500
Here's how Facebook traded over the month of May in 2017. As you can see, there's these clumps of five samples and then a gap and then another clump and so on.

3
00:00:26,500 --> 00:00:33,585
Take a look at the calendar for that month each clump of five dates in the plot corresponds to a workweek.

4
00:00:33,585 --> 00:00:49,884
These gaps are simply weekends when the market was closed. Two months later, it's July and we see a similar pattern, but here at the beginning, there seems to be a date missing, its fourth of July a holiday in the United States.

5
00:00:49,884 --> 00:00:56,620
So, gaps in the data can result from weekends, holidays, and other reasons the market might be closed.

6
00:00:56,619 --> 00:01:06,530
You might be thinking why is this important? After all, if we forget about these missing days, the data is still continuous in terms of trading days.

7
00:01:06,530 --> 00:01:15,070
Well, that's true if you treat the price data as simple sequence and ignore the timestamps, then you don't need to worry about the gaps.

8
00:01:15,069 --> 00:01:22,320
Say, you're computing daily returns. Take the price on each day and subtract it from the price on the previous day.

9
00:01:22,319 --> 00:01:31,125
Well, previous trading day that is. For more robust approach to trading, you may not want to ignore the missing days.

10
00:01:31,125 --> 00:01:38,215
Even if the market is closed, other events can occur that might influence stock prices when the market reopens.

11
00:01:38,215 --> 00:01:46,325
For example, company announcements, news articles, geopolitical events, natural disasters, anything and everything can affect the stock price.

12
00:01:46,325 --> 00:01:59,094
The more time between two trading days, the bigger the window for things to happen. So, you could try to normalize returns by dividing the actual number of days between any two samples.

13
00:01:59,094 --> 00:02:13,510
This may work in certain applications, but can reduce the genuine large differences, or you could just use that information about these irregular gaps between samples when trying to make trading decisions.

14
00:02:13,509 --> 00:02:22,248
Okay, let's recap, weekends, holidays, and other events can cause market to be closed on certain dates.

15
00:02:22,248 --> 00:02:32,885
These dates may be missing in stock market data, you can choose to ignore these gaps, normalize for them, or identify and deal with them as needed.

16
00:02:32,884 --> 00:02:43,390
Another kind of gap you should be aware of is the time between the market closes for the day and when it reopens the next day.

17
00:02:43,389 --> 00:02:57,310
Markets often allow some additional trading during the pre and post market sessions. Few traders participate in these sessions, so the volume is low, but these transactions can still affect stock prices.

18
00:02:57,310 --> 00:03:08,180
Moreover, when a stock is listed on multiple exchanges around the globe, its price may be actually changing around the clock on another exchange.

19
00:03:08,180 --> 00:03:16,539
When a particular market opens for trading, the price of that stock can be different from the closing price on that market from the previous day.

20
00:03:16,539 --> 00:03:28,104
Again, depending on how you are using the price information, you may not need to worry about these differences, but they may give you an additional clue that you can use for trading.

21
00:03:28,104 --> 00:03:35,869
A more significant case of missing values is produced by a major corporate action like listings and mergers.

22
00:03:35,870 --> 00:03:48,675
For instance, say you're analyzing stock data from the year 2000 to 2016, Google only IPOed in 2004, there is no existence of the stock prior to that.

23
00:03:48,675 --> 00:04:04,379
So, what do you do? If you absolutely need a value to work with, you can backfill Google's opening price from its IPO date to the beginning of your analysis period using the same price for open, high, low, and close.

24
00:04:04,379 --> 00:04:13,430
Since no trading actually happened on these days, you can set volume to zero. But, this may not be necessary, and it can be misleading.

25
00:04:13,430 --> 00:04:23,814
So instead, you can maintain a list of valid ticker symbols that form the universe of stocks you're considering and this list can change from day to day.

26
00:04:23,814 --> 00:04:35,680
A more bizarre case happens when a company is delisted from exchange. Perhaps because they went bankrupt or got bought out entirely by private investors.

27
00:04:35,680 --> 00:04:45,805
Dell went private in 2013 by buying back all its public shares. No record of Dell stock exists from that point onwards.

28
00:04:45,805 --> 00:04:55,540
If you held a share of Dell stock on that day it went private, it's not like you'd lost that investment, you would have been paid by Dell for that share.

29
00:04:55,540 --> 00:05:17,474
So, it would be wrong to assume that the price dropped to zero. One way to mitigate this is to fill the last known price on the stock forward till the end of your analysis period, or if you're simulating trades over that time period, you can force sell the stock on that date and remove Dell from the stock universe going forward.

30
00:05:17,475 --> 00:05:27,529
How you ultimately deal with this misleading values will depend on exactly what you're trying to do with the data, but completely ignoring them, might not be the right choice.


@@@
1
00:00:00,000 --> 00:00:10,960
The average return from experiment A is indeed higher than from B. Why is that? It's due to a phenomenon known as survivor bias or survivorship bias.

2
00:00:10,960 --> 00:00:18,510
If you only picked from the stocks that have survived until today, that already filtered out all the other stocks that failed during that period.

3
00:00:18,510 --> 00:00:25,804
Another way to think about experiment A, is that you essentially traveled forward in time from 2005 to the present day.

4
00:00:25,804 --> 00:00:35,565
You know then which companies have survived, went back and bought from among those stocks. Of course you are going to do well, but what determines you?

5
00:00:35,564 --> 00:00:50,964
Experiment B is more honest. You buy stocks based on information available in 2005. No time travel mumbo jumbo and as expected some of these stocks will fail giving you a lower return on average.

6
00:00:50,965 --> 00:00:58,260
This is very critical when you're testing the efficiency of a trading signal or strategy using historical prices.

7
00:00:58,259 --> 00:01:07,439
If you allow survivor bias to creep into your analysis, you will get results that are overly optimistic and probably won't work in the real world.


@@@
1
00:00:00,000 --> 00:00:21,015
Let's take a moment to think about what a trading algorithms goal should be. The obvious part is making money i.e, we should try to generate as much positive returns from trading as possible but the stock market is inherently very unpredictable as we have already seen.

2
00:00:21,015 --> 00:00:29,109
One approach is to buy stocks that have been showing consistent growth and hold onto them for long periods of time.

3
00:00:29,109 --> 00:00:38,329
This usually generates some returns but nothing spectacular unless you happen to pick stocks that grow spectacularly.

4
00:00:38,329 --> 00:00:47,990
But, how you tell ahead of time? What if they don't do as well or even fall? This inherent risk in the market is very real.

5
00:00:47,990 --> 00:00:59,015
It is as much part of the game as the actual stock prices. To mitigate this risk, you should maintain a fairly broad portfolio stocks instead of investing a handful.

6
00:00:59,015 --> 00:01:13,215
How you choose these stocks can affect how much your returns are affected by market behavior. For instance, you could buy a collection of different technology stocks, so the value of your portfolio will go up.

7
00:01:13,215 --> 00:01:27,530
If there's general growth across the sector, you would be somewhat immune to the chance of individual companies failing badly or you can bind stocks from different sectors to create a more diverse portfolio.

8
00:01:27,530 --> 00:01:42,234
It will be less sensitive to any particular sector but is also not likely to go through the roof. This is because in order to generate large returns, all these sectors will need to perform well together.

9
00:01:42,234 --> 00:01:53,359
It is much more likely for only some stocks or sectors to perform strongly. Which means the average performance of all the other sectors will reduce any such effect.

10
00:01:53,359 --> 00:02:07,465
So, how do you go about picking your basket of stocks? You could perform complex statistical analysis to find collections that are likely to generate good returns but also reduce the overall risk.

11
00:02:07,465 --> 00:02:19,094
If you're just investing your personal money, this might be too much work. Not everyone who wants to invest they have the knowledge and skills to conduct a proper analysis.

12
00:02:19,094 --> 00:02:27,705
For this reason, many banks and other financial institutions offer investment funds which are managed by professionals.

13
00:02:27,705 --> 00:02:36,288
Mutual funds are one such option which pulling money from multiple investors and then buy shares on their behalf.

14
00:02:36,288 --> 00:02:49,250
You can choose funds according to your investment goals. Some are designed to reduce risk, yield a lower expected rate of return while others are configured to give a higher rate of return at increased risk.

15
00:02:49,250 --> 00:03:02,580
Some funds track the performance of specific sectors such as infrastructure, technology, communications, et cetera while others may be tied to specific indices.

16
00:03:02,580 --> 00:03:14,514
In addition to combining multiple stocks, some funds are traded on stock exchanges themselves. That is, in order to invest money in these funds, you buy their shares on the market.

17
00:03:14,514 --> 00:03:28,854
Hence, they are known as Exchange Traded Funds or ETFs. They are very popular investments for stock market investors as it tend to produce some growth as long as the sector or index they're tracking does well.

18
00:03:28,854 --> 00:03:42,944
In addition to mitigating risk, they are also much more economically compared to investing in many stocks individually, because you typically have to pay brokerage and other transaction fees on them separately.

19
00:03:42,944 --> 00:04:01,305
A popular ETF is Standard &amp; Poor's 500 or S&amp;P 500 which trades under the ticker symbol SPY. The S&amp;P 500 include 500 stocks with the large market capitalization that trade on the New York Stock Exchange or Nasdaq, selected from diverse sectors.

20
00:04:01,305 --> 00:04:14,790
The composition of an ETF, the stocks and their proportions can vary over time. This gives rise to another source of information that can be useful in making trading decisions.

21
00:04:14,789 --> 00:04:23,310
ETF compositional data. For example, let's say you want to diversify your investment to reduce market risk.

22
00:04:23,310 --> 00:04:33,414
You do that by analyzing the individual performance or correlations between stocks. This generates a portfolio that is well-balanced and not too correlated.

23
00:04:33,415 --> 00:04:45,200
If you include ETFs like SPY in your portfolio, then there will be some correlation between the ETFs and the individual company stocks that they are made up of.

24
00:04:45,199 --> 00:04:54,800
Now, instead of trying to compute these correlations from historic data, wouldn't it be better if you had access to the exact proportion of stocks used.

25
00:04:54,800 --> 00:05:09,600
That's precisely what an ETF compositional data provides. Using this information, you can obtain a much more accurate measure of how correlated the stocks in a portfolio are and balance them out to further mitigate risks.


@@@
1
00:00:00,000 --> 00:00:11,410
Think about all the sources of information we have considered so far, market data, corporate actions, fundamental information, compositional data.

2
00:00:11,410 --> 00:00:20,054
This is the standard information that everyone uses, it's small compared to the amount of data that can affect the market.

3
00:00:20,054 --> 00:00:43,730
There is useful information all around you to help you predict the market, information like news articles that have the ability to shape investor opinion, social media posts that can convey sentiment towards companies, satellite images used to estimate crop yield, consumer data that can predict sales and revenue long before official announcements.

4
00:00:43,729 --> 00:00:53,344
You may not see the world through the eyes of a clock right now, but once it clicks, we hope you'll have a new appreciation for how the world works.


@@@
1
00:00:00,000 --> 00:00:16,469
Albert Einstein reportedly said, "It's not that I'm so smart, it's just that I stay with problems longer." Your drive to stick with things and see them through in a large part will determine your success, not just with this Nanodegree program, but in your job, and in your life.

2
00:00:16,469 --> 00:00:26,644
Our goal at Udacity is to ensure that you meet your goals., so we're here to help you learn like Einstein did to tackle challenges and persevere through them.

3
00:00:26,644 --> 00:00:40,029
Our students come from all walks of life, and their goals are as varied and different as they are. But whatever your personal goals are, we have content, resources, and support mechanisms to make sure that you reach them.

4
00:00:40,030 --> 00:00:46,145
If you're stuck, or even if you just fancy saying hello, remember that your community is here for you.


@@@
1
00:00:00,000 --> 00:00:09,984
So far, you've seen some stock market data and learned about some of the market processes that generate these data, so you're beginning to see what these long streams of numbers actually mean.

2
00:00:09,984 --> 00:00:21,754
As is true of all types of data, stock data represent measurements of things in the real world that have certain properties, and people often take particular steps or make certain assumptions on analyzing them.

3
00:00:21,754 --> 00:00:28,320
In this lesson, we're going to talk about some of the important properties of stock data that lead people to analyze them in certain ways.


@@@
1
00:00:00,000 --> 00:00:08,550
Say we're looking at a time series of prices for a stock. The ups and downs are interesting if we're vaguely curious about how the company is doing.

2
00:00:08,550 --> 00:00:16,289
But more likely than not, we're looking at the price series because we own some of the stock or we've invested money in the stock on someone else's behalf.

3
00:00:16,289 --> 00:00:23,774
What we care about is how our investment has increased or decreased in value. So, how do we measure that increase or decrease?

4
00:00:23,774 --> 00:00:31,435
There are actually several metrics we might use to quantify changes in price over time. One is the simple difference in price.

5
00:00:31,434 --> 00:00:40,695
P_t minus P_t minus one. This might represent the difference between the price of a stock this month and it's price one month ago for example.

6
00:00:40,695 --> 00:00:53,165
Another is the percentage return or raw return. This is the difference divided by the starting price, P_t minus P_t minus one divided by P_t minus one.

7
00:00:53,164 --> 00:01:05,025
Imagine you bought a $1,000 of stock one month ago and sold it this month for $1050. Your return on your initial investment of $50 is five percent of your original investment.

8
00:01:05,025 --> 00:01:15,325
If you buy $5,000 of stock this month and sell it for $5,300 next month, you might want to compare the success of this new investment to the previous one.

9
00:01:15,325 --> 00:01:24,579
Instead of comparing $300 to $50, you can calculate the return and compare six percent to five percent to see that as a proportion of your original investment.

10
00:01:24,579 --> 00:01:31,965
You did slightly better with the second investment. That's the advantage of using returns. Prices are normalized and thus similar in scale.


@@@
1
00:00:00,000 --> 00:00:08,279
Quantitative analysts frequently work with a quantity related to but slightly different from the raw return, the natural logarithm of return.

2
00:00:08,279 --> 00:00:22,254
Remember how the return was defined as P sub t minus P sub t minus one divided by P sub t minus 1. Well, the log return is defined slightly differently, as the natural logarithm of P sub t divided by P sub t minus one.

3
00:00:22,254 --> 00:00:41,115
To see how to convert between these two quantities, note that if we add one to P sub t minus P sub t minus one divided by P sub t minus one, we can convert one to P sub t minus one, divided by P sub t minus one, and the expression simplifies to P sub t divided by P sub t minus one.

4
00:00:41,115 --> 00:00:53,810
So, if we denote the log return as capital R, and the raw return as little r, then capital R equals the log of little r plus one and little r equals e to the capital R minus one.

5
00:00:53,810 --> 00:01:00,560
Why the quantitative analysts use log returns? Well, it turns out that log returns have a number of appealing properties.

6
00:01:00,560 --> 00:01:06,775
Instead of listing them all now, we're going to highlight them as they arise while we talk about the remaining content in this lesson.

7
00:01:06,775 --> 00:01:22,000
For one thing, if the value of R is small, or much less than one, then the natural log of one plus r approximately equals r. Since returns are typically small percentages, the values of log returns are typically close to the values of returns.

8
00:01:22,000 --> 00:01:34,495
To understand this intuitively, take a look at the graph of the natural log of x plus one, it goes through the point 00, and it has a slope of one there, so at 00 it's tangent to the line y equals x.

9
00:01:34,495 --> 00:01:44,409
If we were to zoom into a tiny neighborhood around x equals 0, the two lines practically overlie each other, so the natural log of x plus one approximately equals X.


@@@
1
00:00:00,000 --> 00:00:06,884
Let's talk about how a series of daily price values arises. It has to do with our earlier discussion of compounding.

2
00:00:06,884 --> 00:00:13,949
Let's say, a stock starts at P sub zero, and each day the price changes by some small percent, the return.

3
00:00:13,949 --> 00:00:21,394
We saw earlier how the price at time T could be written as a product of all of these one plus little r sub i terms.

4
00:00:21,394 --> 00:00:37,040
Remember, one plus little r equals P sub t divided by P sub t minus one. If we take the log of both sides of this equation, we get log of P sub t on one side and a sum of log of one plus little r sub i terms on the other side.

5
00:00:37,039 --> 00:00:47,000
Here, for convenience, I'm going to move log of P sub zero back to the left-hand side. Remember, P sub zero is a constant, it's not dependent on time.

6
00:00:47,000 --> 00:00:56,859
Now, we're going to use the central limit theorem. You might remember the central limit theorem from a class on probability, but if you don't I'll just explain it briefly here.

7
00:00:56,859 --> 00:01:10,490
It says that, the sum of random variables that have the same distribution and are not dependent on each other approaches a normal distribution and the limit that the number of random variables and the sum goes to infinity.

8
00:01:10,489 --> 00:01:16,875
So, let's make the assumption that the returns on each day are independent, but come from the same underlying distribution.

9
00:01:16,875 --> 00:01:26,575
This seems reasonable certainly more reasonable for returns and for prices. After all, the price of a stock on one day almost certainly depends on its price the day before.

10
00:01:26,575 --> 00:01:35,209
So, if we make that assumption, we can see that the right hand side, the wholesome is a random variable that follows a normal distribution.

11
00:01:35,209 --> 00:01:53,049
So, we have that log of P sub t minus log of P sub zero is distributed normally. But if P sub 0 is a constant, then log of P sub t is also distributed normally, because a normally distributed random variable minus the constant is still normally distributed.

12
00:01:53,049 --> 00:02:03,259
Now, I'm going to tell you about a new distribution called the log-normal distribution. Guess what, it looks kind of like that right-skewed distribution we talked about earlier.

13
00:02:03,260 --> 00:02:11,034
What you need to know about the log normal distribution is that if Y is distributed normally then e to the y is distributed log normally.

14
00:02:11,034 --> 00:02:27,175
I'll say that again, if Y is distributed normally, then e to the y is distributed log normally. So, if log of P sub t is distributed normally, then P sub t which equals e to the log of P sub t is distributed log normally.

15
00:02:27,175 --> 00:02:39,640
In fact, prices are frequently assumed to arise from a log normal distribution. This seems reasonable from just eyeballing the distribution because we can see that it doesn't go below zero and it's skewed to the right.

16
00:02:39,639 --> 00:02:50,319
Those properties match our actual price distribution well. However, when working with real data, the assumption that prices are distributed log normally, may or may not be a good one.

17
00:02:50,319 --> 00:02:57,479
These distributions can be convenient models, but don't confuse them with actual distributions of stock prices or returns.


@@@
1
00:00:00,000 --> 00:00:10,274
A trading strategy is a set of steps and rules that help you decide what stocks to buy or sell, when to perform these trades and how much money to invest in them.

2
00:00:10,275 --> 00:00:25,445
Designing a successful training strategy that reliably generates profits for you while minimizing the risk of losing money is indeed a complex task, but it starts with looking for signals that might hold a clue about the future performance of stocks.

3
00:00:25,445 --> 00:00:39,840
You might be wondering, where do we find such signals? Is there a book that I can read? Well, yes. There are some ideas that have led to good strategies, but if an idea becomes popular, chances are, everyone will try to exploit it.

4
00:00:39,840 --> 00:00:46,849
In order to have a competitive advantage over other traders, you should try and come up with your own ideas or variations.

5
00:00:46,850 --> 00:01:01,564
Your idea could be something simple. For example, retail stocks rise during the holiday shopping season or something more complicated, like oil stocks that deviate from the oil sector index beyond a certain threshold are bound to fall back.

6
00:01:01,564 --> 00:01:09,840
Whatever your idea might be, it is important to treat it as a hypothesis and test it against data to see if it holds up.


@@@
1
00:00:00,530 --> 00:00:15,219
Newton's first law of motion states that an object at rest stays at rest, and an object in motion stays in motion with the same speed and in the same direction, unless acted upon by an unbalanced force.

2
00:00:15,220 --> 00:00:31,589
While Newton's laws may seem to have little to do with the vagaries of the stock market, a similar behavior is often noted in security prices, that is, rising stock prices seem to continue to rise for some time, while falling stock prices continue to fall.

3
00:00:31,589 --> 00:00:40,579
This observed phenomenon is popularly known as momentum. But, stocks are not like physical objects, so what causes this effect?

4
00:00:40,579 --> 00:00:47,715
Honestly, no one really knows. There are several factors that seem to contribute to it including human behavior.

5
00:00:47,715 --> 00:00:55,744
People don't want to miss out on an opportunity to make money, and tend to follow the heart. People also tend to under-react to news.

6
00:00:55,744 --> 00:01:21,839
New information propagates over time leading to a prolonged effect on stock prices. There is a branch of trading strategies that attempts to capitalize on such trends, and while there is no standard method to quantify momentum signals, a few common techniques include: technical indicators such as moving averages, large price movements with volume, and stocks making new highs.

7
00:01:21,840 --> 00:01:29,679
In the next section, we shall look at one example momentum strategy and evaluate its potential using statistical analysis.

8
00:01:29,680 --> 00:01:43,379
The general premise of this trading signal is that, out-performing stocks tend to keep their momentum and continue to remain out-performers for some more time in a particular market and vice versa for under-performers.

9
00:01:43,379 --> 00:01:55,730
If you believe in momentum being a repeating market phenomenon, it may be a good opportunity to buy out performers and sell under-performers, capitalizing on the continuation of their movement.


@@@
1
00:00:00,000 --> 00:00:06,919
Once you have found a signal that seems to indicate the future performance of a stock, it is time to put it to action.

2
00:00:06,919 --> 00:00:17,484
For instance, if you think that a stock has upward momentum, you might want to buy some shares and hold onto it for a fixed period of time, or until you start seeing the stock fall.

3
00:00:17,484 --> 00:00:27,030
This is known as taking a long position on the stock. When you sell your stock, hopefully, at a higher price than you bought it, that is known as closing your position.

4
00:00:27,030 --> 00:00:33,289
But what if the stock has a downward momentum? And you believe that it is going to keep falling for some time?

5
00:00:33,289 --> 00:00:42,024
In this case, you can take what is known as a short position on the stock, selling first and then buying back later, hopefully, at a lower price.

6
00:00:42,024 --> 00:00:55,734
If this is the first time you're hearing about shorting, it might sound bewildering. But what it boils down to is borrowing shares from someone, usually your broker, and then promising to return them once your short position is closed.

7
00:00:55,734 --> 00:01:02,130
The brokers incentive here is that they typically earn a commission on the profit you make from the short sale.

8
00:01:02,130 --> 00:01:15,885
At the same time, they are taking a risk. What if you bail out and never buy back the shares? For this reason, brokers typically require you to keep some money in a margin account that they can charge if you fail to keep your promise.

9
00:01:15,885 --> 00:01:22,790
The whole process of short selling is a bit more complex with interests, fees, and margin calls coming into the picture.

10
00:01:22,790 --> 00:01:30,594
But, all you need to know when evaluating a potential trading signal is that shorting is one possible action you can take.

11
00:01:30,594 --> 00:01:38,604
So, when you think a stock is going up, you can buy or go long, and if you believe it is going to keep falling, you can short it.

12
00:01:38,605 --> 00:01:46,254
However, dealing with individual stocks can be tricky and unpredictable. You don't want to put all your eggs in one basket.

13
00:01:46,254 --> 00:01:54,109
Therefore, it is recommended to adopt a cross-sectional strategy, where you invest in multiple stocks at the same time.

14
00:01:54,109 --> 00:02:03,770
One smart way to do this is to use your signal to rank order the stocks, and use the rankings to select stocks for long and or short positions.

15
00:02:03,769 --> 00:02:15,400
For instance, if you're using a momentum signal, the top performers are likely to keep rising, so you can go long on them, and the bottom performers, that are falling, are likely to keep falling.

16
00:02:15,400 --> 00:02:22,270
So, you can short them. This has the added benefit that you're using the relative performance of stocks to compare them.


@@@
1
00:00:00,000 --> 00:00:17,714
Here's how you might approach the problem of formulating the complete trading strategy. Our goal is to construct a stock portfolio of long and short positions, and the selection process for stocks to go into the portfolio is based on the stock's returns performance relative to other stocks.

2
00:00:17,714 --> 00:00:24,855
For this example, we shall assume we are periodically re-evaluating and re-balancing stock holdings every month-end.

3
00:00:24,855 --> 00:00:33,405
Let's use the S&amp;P stocks universe. The top 500 stocks trading in the US tracked by the S&amp;P 500 index.

4
00:00:33,405 --> 00:00:48,625
A stock universe, is a general term in finance that refers to a group of stocks that share certain common features, belong to the same market or simply a set of stocks that are used in verifying or simulating trading strategies.

5
00:00:48,625 --> 00:01:00,134
First, we fetch the daily closing prices of each toxins mid-2013. Please note that it is important that we use adjusted closing prices for analysis purposes.

6
00:01:00,134 --> 00:01:10,510
Also note that the composition of stock universes changes over time. For example, around 10 to 40 companies leave or enter the S&amp;P 500 every year.

7
00:01:10,510 --> 00:01:19,724
When you test your strategy, it is important that your dataset for 2013 for example, contained the companies that were part of the universe in 2013.

8
00:01:19,724 --> 00:01:26,579
In fact, your dataset for any point in time, should contain the companies that were in the universe at that time.

9
00:01:26,579 --> 00:01:39,004
If you use the current composition for example, the current S&amp;P 500 index in 2017, then you will effectively be testing your strategy on the subset of stocks that survive to 2017.

10
00:01:39,004 --> 00:01:46,819
These stocks probably performed better than the ones that left the universe. So the performance of your strategy will look better than it should.

11
00:01:46,819 --> 00:02:05,150
This is a subtle error that analysts often make, and is known as survivorship bias. Now, since we are only interested in month-end prices, we can re-sample the daily closing prices into monthly prices, then we can form a log returns time-series from the monthly prices.

12
00:02:05,150 --> 00:02:12,750
For each month-end observation period, we rank the stocks by their month-end returns from the highest to the lowest.

13
00:02:12,750 --> 00:02:20,194
Select the top performing end stocks into our long portfolio and the bottom performing end stocks into our short portfolio.

14
00:02:20,194 --> 00:02:31,270
You could also choose a fraction of stock say, the top and bottom 10 percent. To simplify this example, we'll assume every stock gets an equal dollar amount of investment.

15
00:02:31,270 --> 00:02:39,020
This makes it easier to compute the portfolios monthly returns performance as the simple arithmetic average of the stock returns.

16
00:02:39,020 --> 00:02:47,189
Lastly, the combined portfolios monthly returns is the difference between the long portfolio's return and that of the short portfolio.


@@@
1
00:00:00,000 --> 00:00:09,254
Now we are ready to perform our analysis. The resulting returns time series is the theoretical monthly performance of our long-short portfolio.

2
00:00:09,255 --> 00:00:27,519
Our goal is to see if the mean monthly return is greater than zero. Let's calculate that. So, our mean is indeed greater than zero at 0.53 percent, but does this mean that we got this value because on average this trading strategy yields positive returns?

3
00:00:27,519 --> 00:00:36,000
That is to say that the true mean is greater than zero? It could be that on average this trading strategy will not yield positive returns.

4
00:00:36,000 --> 00:00:47,474
Maybe the true mean is less than or equal to zero, and this value is just a random fluctuation. Well, one approach to assess this is to perform a statistical test on our hypothesis.

5
00:00:47,475 --> 00:00:57,980
A t-test is a way of testing the probability of getting as bigger mean as we did, assuming all the assumptions we made to build our model of strategy returns were correct.

6
00:00:57,979 --> 00:01:06,775
In our case, we can compute the t-statistic by dividing the mean return x bar by the standard error of the mean SE x bar.

7
00:01:06,775 --> 00:01:21,655
Using this t-statistic, we can measure the probability of getting a mean monthly return of 0.53 percent or larger if the true mean monthly return is zero given that the assumptions we made to build our model are correct.

8
00:01:21,655 --> 00:01:30,879
This probability is called the p-value. If the probability is very small, we might infer that it's unlikely that the true mean is zero.

9
00:01:30,879 --> 00:01:39,405
Now, before running the test, we should decide how small the p-value needs to be for us to conclude that the true mean is not zero.

10
00:01:39,405 --> 00:01:51,005
To denote this threshold, we usually use the Greek symbol alpha. A commonly used value is 0.1. Setting alpha sets the false positive rate for the test.

11
00:01:51,004 --> 00:02:06,795
So, if we set alpha equals 0.1, we're effectively saying that if we use this threshold, we are incorrectly rejecting the null hypothesis when the true mean is zero in 10 percent of many hypothetical users of this test.

12
00:02:06,795 --> 00:02:28,875
By performing a one-tail t-test on the alternative hypothesis that the mean of these returns is statistically greater than zero, we calculate the p-value with the stats package using a degrees of freedom of n minus one equals 47, and we find that the t-statistic is about 1.618, with the p-value of 0.0566.

13
00:02:28,875 --> 00:02:37,254
Meaning that the result is not significant at P less than 0.05 level, but insignificant at P less than 0.1.

14
00:02:37,254 --> 00:02:59,925
This means that we are fairly unlikely to get a mean of 0.53 percent if the true mean is zero. This result shows some initial promise that there could be some alpha contained in the strategy, and thus serves as a quick sanity check to justify investigating further and fine-tuning the strategy leading to a full backtesting exercise in the end.

15
00:02:59,925 --> 00:03:16,155
If we measure the monthly performance of the SPDR, S&amp;P 500 ETF, SPY over the same period, we would find that the mean monthly return is 0.83 percent, which is actually greater than that of the strategy, at least at this preliminary stage.

16
00:03:16,155 --> 00:03:25,175
To be clear, comparing our momentum strategy to the strategy of investing directly in S&amp;P 500 ETF is not really a fair comparison.

17
00:03:25,175 --> 00:03:35,330
These are two very different strategies. For one thing, ours involves taking long and short positions, while investing in the S&amp;P only involves taking long positions.

18
00:03:35,330 --> 00:03:52,820
However, this gives us a vague idea of what values portfolio returns might take. Also, while our strategy's obviously simplistic to begin with, this shows that it is actually not an easy task to find a strategy that can outperform the market.

19
00:03:52,819 --> 00:04:04,430
Note that if you get a very large p-value at this early stage, you wouldn't want to go back and make meaningless changes to your strategy like changing a parameter a little until you got a very low p-value.

20
00:04:04,430 --> 00:04:14,090
Doing this is called data snooping or p-hacking, and would just fine tune your strategy to exploit the particular random fluctuations in the dataset you have.

21
00:04:14,090 --> 00:04:23,159
A strategy device that way would be unlikely to perform well on future data. What you should do instead is make meaningful changes to your strategy.


@@@
1
00:00:00,000 --> 00:00:08,310
Formulating trading strategies often starts with an observation. A pattern that seems to be recurring in the market over time.

2
00:00:08,310 --> 00:00:15,105
At that point, your creativity and intuition tell you that there might be an opportunity for monetization.

3
00:00:15,105 --> 00:00:25,849
Your job as a contrator then, is to turn this observation into an expression, both mathematically and programmatically and verify it using historical data.

4
00:00:25,850 --> 00:00:40,379
This is alpha research. Statistical analysis lets you very quickly and scientifically test whether an observed pattern or trading signal that you come up with has the potential to turn into a profitable trading strategy.

5
00:00:40,380 --> 00:00:46,545
Once this is proven, then you can proceed to define your trading strategy in a more detailed manner.

6
00:00:46,545 --> 00:00:59,884
Which will lead to a full back-testing exercise, as the last step of the research process. As a point of clarification, the Alpha of the t-test is not the same as the Alpha in Alpha analysis.

7
00:00:59,884 --> 00:01:12,920
The same Greek symbols are just used to represent many different things. Later in the program, we shall look more closely at the caveats and techniques of finding trading signals using time series.


@@@
1
00:00:00,000 --> 00:00:08,595
We believe that the community of students who are taking this nanodegree are doing more than just getting themselves ready to enter the quant trading industry.

2
00:00:08,595 --> 00:00:15,724
You and your community of classmates are helping us to prove that talent exists in every corner of the world.

3
00:00:15,724 --> 00:00:39,339
I have to admit that from the outside of the finance world there's a sense that banking and finance tends to be more suited for students from elite universities, but when we step inside the financial industry, we actually see that there are people from all kinds of backgrounds, former nonprofit policy analysts, chemists, and also very successful people who never got a college degree.

4
00:00:39,340 --> 00:00:52,920
So, I think that your community of classmates are collectively setting out to prove that no matter where you're from or what your parents did or what school you went to, you have the capability to find Alpha.


@@@
1
00:00:00,000 --> 00:00:15,950
Congratulations on getting through your first batch of lessons. Next on your agenda is your first project, trading with momentum, where you'll implement a momentum trading strategy and perform a statistical test to conclude if there's alpha in the signal.

2
00:00:15,949 --> 00:00:20,440
That sounds like a really interesting project. Does a bode on the learning we've been doing up until now?

3
00:00:20,440 --> 00:00:31,059
It does. This project will give you an overview of the Quant workflow. By preprocessing data with pandas, implementing a trading signal, and evaluating it's performance.

4
00:00:31,059 --> 00:00:36,945
Sounds like students will get a chance to use all the skills they've been building up until now, but in a really industry-relevant way.

5
00:00:36,945 --> 00:00:48,405
Completely. We'll use this foundation in later lessons where you'll learn more advanced techniques to generate trading signals, construct a portfolio, and evaluate its performance.

6
00:00:48,405 --> 00:00:57,240
That's awesome. Projects are a super powerful learning tool, they give you a chance to bring together all the skills and knowledge you've built up until now and apply them in new and interesting ways.

7
00:00:57,240 --> 00:01:06,839
Totally. I love doing projects. They certainly can feel a bit challenging, but even though I don't always see the solution straight away, they help me grow in really meaningful ways.

8
00:01:06,840 --> 00:01:12,814
Totally. That's a great point. Since projects are these big learning opportunities, it's totally normal to find them challenging.

9
00:01:12,814 --> 00:01:17,799
Luckily, in your Nanodegree Program, you get to iterate on your projects just like you do with work in the real world.

10
00:01:17,799 --> 00:01:30,129
That's right. You'll get personalized feedback on each project you do from our expert project reviewers that will help you see what you've learned already and how you can keep learning and improving your work.

11
00:01:30,129 --> 00:01:36,779
If your project doesn't meet the specifications the first time or even a number times beyond that, that's totally normal.

12
00:01:36,780 --> 00:01:43,064
Yeah. Again, projects themselves are meant to stretch your knowledge and skills, not just give you a chance to show off what you've already learned.

13
00:01:43,064 --> 00:01:50,540
It's the same as with the quizzes in the Udacity classroom, you get to play around and practice with them, trying them as many times as you want to or need to to keep growing.

14
00:01:50,540 --> 00:01:59,050
Everything you do at Udacity is intended to help you learn, and you are in the driver's seat. Good luck with project one, and have fun.


@@@
1
00:00:00,000 --> 00:00:07,480
Welcome back. In this set of lessons, we're going to expand on the fundamentals even building, to prepare you for some neat stuff we'll do later.

2
00:00:07,480 --> 00:00:17,030
We're going to begin with a bird's eye view of strategy development, so that you understand the stages of this and how the activities of each stage fit together.

3
00:00:17,030 --> 00:00:24,980
We'll talk about weird data points, outliers, a discussion that will help you start to get a feel for the type of data used in quantitative trading.

4
00:00:24,980 --> 00:00:34,214
We're also going to discuss the very important concepts of regression and time series analysis, which are basic building blocks we'll need to introduce later concepts.

5
00:00:34,215 --> 00:00:45,094
We'll discuss the very important concept of volatility or variability in returns, which is frequently used as a measure of risk, which people in the trading world are constantly talking about.

6
00:00:45,094 --> 00:01:01,239
Finally, we'll discuss Paris trading, another very important type of trading strategy. This set of lessons is designed to help you build out your understanding of the ideas, data, tools, and concepts, fundamental to trading strategies. Let's go for it.


@@@
1
00:00:00,000 --> 00:00:09,940
Congratulations on making it this far. The content you've seen previously, will serve as a strong foundation of knowledge from which to build your quantitative finance career.

2
00:00:09,939 --> 00:00:17,969
We have a few more concepts to discuss. But before we get into the weeds, let's take this opportunity to step back for a moment.

3
00:00:17,969 --> 00:00:24,839
I want to describe the series of steps, quants go through when developing trading strategies from a big picture of perspective.

4
00:00:24,839 --> 00:00:31,935
So you'll be able to see, how each step can contribute to the ultimate goal of profiting from a successful trading strategy.

5
00:00:31,934 --> 00:00:38,664
This will help you understand the point of each school year learning and make concepts discussed in the next few videos more clear.

6
00:00:38,664 --> 00:00:51,395
Fundamentally, quantitative trading is the process of using statistical analysis and modeling to predict market behavior and using those predictions to make trades with the goal of profit.

7
00:00:51,395 --> 00:00:59,545
Like any research process understanding and predicting market behavior begins with a new idea for how the world might work.

8
00:00:59,545 --> 00:01:07,584
In other words, a hypothesis. A good hypothesis is the first step to making predictions about future behavior.

9
00:01:07,584 --> 00:01:23,729
Coming up with a good hypothesis means being brutally specific. For example, the hypothesis stocks that are discussed in the news are likely to go up is not very useful by itself, because it's so vague that it doesn't lead to testable predictions.

10
00:01:23,730 --> 00:01:32,085
In what news outlet? At what time? In what manner must the stocks we discussed and for how long will they go up?

11
00:01:32,084 --> 00:01:46,100
An improved hypothesis would be stocks whose company name or ticker appear in the landing page of The Wall Street Journal website will increase by price by one percent, one day following this appearance.

12
00:01:46,099 --> 00:01:53,810
Although it's far from perfect, we can actually test this hypothesis by finding the tickers and checking for the change in price.

13
00:01:53,810 --> 00:02:01,625
So how do you come up with a good hypothesis? Well, your hypothesis should be based on your current understanding of how markets work.

14
00:02:01,625 --> 00:02:08,039
So the more you know about the markets, the more likely you are able to come up with a new potentially profitable idea.

15
00:02:08,039 --> 00:02:16,819
It can take time to build up the domain knowledge to come up with a good hypothesis in any field of study and quantitative finance is no exception.

16
00:02:16,819 --> 00:02:35,364
So how do you build up this domain expertise? Well, you can try to immerse yourself in the markets, observe the markets, watch financial news, read financial newspapers, read books, read blogs, and study the known strategies of famous quants and discretionary traders and investors.

17
00:02:35,365 --> 00:02:42,230
Attend meetings and conferences and read academic papers such as those on the Social Science Research Network.

18
00:02:42,229 --> 00:02:58,925
An online repository of scholarly research in the social sciences including economics and finance. But no matter who you are or what you know, if your hypothesis makes predictions that are inconsistent with the real observed data, your hypothesis is wrong.

19
00:02:58,925 --> 00:03:05,400
The precise manner in which it is wrong, when and how, we'll show you how to focus your efforts to improve it.


@@@
1
00:00:00,000 --> 00:00:08,179
A hypothesis that forms the basis of a trading strategy must survive several phases of increasingly rigorous testing.

2
00:00:08,179 --> 00:00:20,429
This process can be different at different companies, banks, or hedge funds, but usually, there is an initial exploratory research phase during which ideas are generated and validated in a fairly basic way.

3
00:00:20,429 --> 00:00:32,774
The goal here is to come up with promising ideas relatively quickly. In the research phase, you will focus on determining what set of positions to enter, on which assets at which times.

4
00:00:32,774 --> 00:00:41,794
Such that you have the potential to get positive future returns. Later on, you'll incorporate other factors critical to a full-fledged trading strategy.

5
00:00:41,795 --> 00:00:52,100
How much money to invest in each asset, under what conditions to exit positions, factoring in the costs of making trades, and what risks constraints to impose?

6
00:00:52,100 --> 00:01:00,500
The process of rigorously simulating the entire flow in an automated fashion using historical data is called Backtesting.

7
00:01:00,500 --> 00:01:10,399
It is important to resist the temptation to jump right into full-fledged backtesting. We'll talk about the reasons for this when we discuss overfitting in depth.


@@@
1
00:00:00,000 --> 00:00:07,495
There are several flavors of trading strategies. The most basic type is based on buying and selling a single asset.

2
00:00:07,495 --> 00:00:18,224
An example of this type of strategy might be just trading the S&amp;P 500. You could track the performance of the S&amp;P over time and enter positions based on past performance.

3
00:00:18,225 --> 00:00:25,995
For example, if the S&amp;P has been doing well for a while, you might enter a long position under the assumption that it has momentum.

4
00:00:25,995 --> 00:00:33,675
Another common strategy is to find pairs of assets that seemed to be related and trade based on their relative movements.

5
00:00:33,674 --> 00:00:47,570
These are called pairwise strategies. For example, say there are two big companies in the beverage industry, these companies are probably subject to many in the same market affects, the price of ingredients, the cost of shipping et cetera.

6
00:00:47,570 --> 00:00:53,480
If one stock starts to appreciate more than the other, you might think that the laggard will eventually catch up.

7
00:00:53,479 --> 00:01:11,099
A pairwise strategy would make trades to try to capture this differential. Another class of trading strategy extends this idea to groups of stocks, these are called cross-sectional strategies also known as equity statistical arbitrage or equity market neutral investing.

8
00:01:11,099 --> 00:01:27,170
This is a popular type of strategy and involves comparing hundreds to thousands of stocks to determine which to hold in long and short portfolios with the goal of benefiting from transient market phenomena without being subject to overall market movements.

9
00:01:27,170 --> 00:01:41,940
The comparison is often based on price and or volume or fundamental information. A momentum signal where you rank stocks based on the strength of prior returns over a given period is an example of a cross-sectional strategy.

10
00:01:41,939 --> 00:01:52,924
Finally, there's a class of strategies based primarily on new types of data, such as satellite imagery, social media, geolocation or consumer transaction data.

11
00:01:52,924 --> 00:02:02,134
Large hedge funds and asset managers are most interested in strategies three and four. These are the two we spend most of the time on in this nano-degree.

12
00:02:02,135 --> 00:02:10,919
Why is this? Two reasons: first, large professional market participants by definition have large amounts of capital to put to work.

13
00:02:10,919 --> 00:02:23,234
As such, they desire strategies with high capacity, this means the strategy can be put to work at a meaningful asset level and this is most often achieved by trading very many stocks.

14
00:02:23,235 --> 00:02:33,965
Cross-sectional strategies all other things being equal, have the highest capacity. Second, professional market participants are looking for differentiated ideas.

15
00:02:33,965 --> 00:02:43,009
Given the proliferation of alternative data, many funds hope to uncover signals in hard-to-find, expensive and difficult to work with data.


@@@
1
00:00:00,000 --> 00:00:06,929
Let's zoom in on one of the types of trading strategies we talked about earlier, cross-sectional equity investing.

2
00:00:06,929 --> 00:00:15,359
This is a deep and important type of trading strategy. So, let's talk in detail about the pieces needed to build this type of strategy.

3
00:00:15,359 --> 00:00:31,184
In general, you can think of the process of developing this type of strategy as having six stages: data selection, universe definition, alpha discovery, alpha combination, portfolio construction, and trading.

4
00:00:31,184 --> 00:00:40,010
In the first stage, you need to decide what data set or data sets you want to use. As we've said, you should start with a hypothesis.

5
00:00:40,009 --> 00:00:51,395
So, you're going to want to get access to the data relevant to testing it. In the second stage, you need to pare down your data set to a subset that contains the stocks you wanted potentially trade.

6
00:00:51,395 --> 00:01:04,185
You might exclude stocks that have low training volume, and are therefore, hard to trade. But you also want to construct your portfolio, so that the stocks in it have similarities, so that ranking them is a reasonable thing to do.

7
00:01:04,185 --> 00:01:10,774
However, since you're looking to benefit from their movements relative to each other, they shouldn't be too similar.

8
00:01:10,775 --> 00:01:17,259
You're also going to want to limit your universe to the stocks to which your hypothesis logically applies.

9
00:01:17,260 --> 00:01:24,909
For example, let's imagine you want to work with the hypothesis discussed in the paper, Geographic Momentum by Quoc Nguyen.

10
00:01:24,909 --> 00:01:35,445
This hypothesis is that a momentum effect is created for multinational US companies, because investors do not pay attention to foreign market developments.

11
00:01:35,444 --> 00:01:43,700
So, there is an opportunity to predict increases in these company stock prices when markets and countries they're operating in improve.

12
00:01:43,700 --> 00:01:51,670
If you're working with this hypothesis, you would only want to work with companies who actually have significant foreign market operations.


@@@
1
00:00:00,000 --> 00:00:08,560
The alpha discovery phase is where the fun really starts. This is when you start looking for alphas, but what are alphas exactly?

2
00:00:08,560 --> 00:00:22,665
An alpha is an expression applied to the cross-section over your universe of stocks which returns a vector of real numbers whose values are proportional to the size of the position you will take on each asset.

3
00:00:22,664 --> 00:00:33,880
You can think of the numerical output of an alpha as providing an indication, or metric of conviction about future returns which will ultimately inform a trading decision.

4
00:00:33,880 --> 00:00:45,844
When you have discovered a successful alpha, the value in that vector for each stock is directly proportional to the rank of its return at a future time across the universe of stocks.

5
00:00:45,844 --> 00:00:55,429
For example, in a cross-sectional momentum strategy, we rank the stocks according to how much momentum they have as measured by a momentum indicator.

6
00:00:55,429 --> 00:01:03,615
We use the ranks to decide which stocks to put in along portfolio and which stocks to put in a short portfolio at every time interval.

7
00:01:03,615 --> 00:01:11,929
In this scenario, you can think of the logic that produces the ranks as the alpha, and the ranks themselves as the alpha vector.

8
00:01:11,930 --> 00:01:22,170
Taken together, the ranks are a vector of numbers that help inform the trading decision of which docs to hold long and short, and in what amounts.

9
00:01:22,170 --> 00:01:31,659
An alpha is one type of trading signal. A trading signal is a general term for any numerical signal that can be used to inform a trade.

10
00:01:31,659 --> 00:01:39,200
It could just be a single number. Therefore, alphas which are vectors are a subset of trading signals.

11
00:01:39,200 --> 00:01:52,254
The alpha discovery phase can also be called the signal research phase. This stage is where you test your hypothesis and see if you can come up with evidence that your idea may lead to strong future returns.

12
00:01:52,254 --> 00:02:04,814
This will be an iterative process. However, in modern markets, it's rare that a single alpha will provide sufficiently consistent positive returns to provide the sole basis of an investment strategy.

13
00:02:04,814 --> 00:02:13,125
Typically, several alphas will be combined together to generate an overall alpha that has better performance than the best individual alpha.

14
00:02:13,125 --> 00:02:26,730
This is akin to the ideas of model stacking and ensembling in traditional machine learning. Combining alphas that have diverse inputs and underlying hypotheses can lead to a high-quality combined alpha vector.

15
00:02:26,729 --> 00:02:37,674
For example, a price-driven alpha such as the momentum alpha may combined well with an alpha based on company fundamentals since the inputs are very different.

16
00:02:37,675 --> 00:02:51,965
In this phase, alphas may be combined using simple logic like adding ranks are averaging, or through more complicated waiting schemes like finding the weights that lead to the lowest possible variance for the combined alpha.

17
00:02:51,965 --> 00:03:01,524
Another possible method is to translate your alphas into features and use them as inputs to a machine learning classifier to capture the relationships between the alphas.

18
00:03:01,525 --> 00:03:09,270
The output of this stage is a single alpha vector that incorporates the information of many individual alphas.


@@@
1
00:00:00,000 --> 00:00:07,125
Eventually, you will arrive at the stage where you want to start thinking about how your strategy will work in practice.

2
00:00:07,125 --> 00:00:16,064
This is the portfolio construction stage, he stage where you have to think about using your combined alpha vector to generate and update an actual portfolio.

3
00:00:16,065 --> 00:00:31,194
At every time iteration, your strategy is going to take an existing portfolio, make predictions about market behavior using your combined alpha vector and use this information to make trades that move the existing portfolio to an updated portfolio.

4
00:00:31,193 --> 00:00:41,474
There are more questions that must be answered when turning your market hypothesis into traits such as, how does your strategy take into account in various forms of risk?

5
00:00:41,475 --> 00:00:48,270
What quantities should your portfolio seek to optimize? Are there any additional constraints on your portfolio?

6
00:00:48,270 --> 00:00:55,554
How do real-world constraints like transaction costs eat into the theoretical return and how can these be mitigated?

7
00:00:55,554 --> 00:01:03,020
Let's talk in a bit more detail about some of these questions. You probably already have an intuitive sense of what a risk is.

8
00:01:03,020 --> 00:01:11,620
It's uncertainty about the future, and in particular, the possibility that something bad will happen, like losing the money you invested.

9
00:01:11,620 --> 00:01:20,724
In finance, risk usually refers to uncertainty or variability in returns and there are many different ways to quantify it mathematically.

10
00:01:20,724 --> 00:01:27,325
There are different types of risks. Risks inherent in the entire market are called systematic risks.

11
00:01:27,325 --> 00:01:37,689
One form of systematic risk is risk inherent to individual sectors, like the technology sector or the energy sector called sector-specific risk.

12
00:01:37,689 --> 00:01:49,150
For example, the success of the energy sector as a whole might be affected by fluctuations in the price of oil, but it's unlikely that this would affect the technology sector nearly as much.

13
00:01:49,150 --> 00:02:03,560
Risk inherent to individual stocks is called idiosyncratic or specific risk. For example, a single oil company might be uniquely affected by political events in a region of the world in which it is exploring for oil.

14
00:02:03,560 --> 00:02:15,110
Your strategy will likely incorporate a mathematical model in these risks, which will help you understand and manage the different types of risks and attempt to avoid being overly exposed to anyone.

15
00:02:15,110 --> 00:02:25,460
Different investment funds have different mandates driven by the goals of their investors. For example, some funds will want to be neutral to all systematic risk.

16
00:02:25,460 --> 00:02:34,039
Whereas, other funds have a specific target for that risk. Your portfolio could be designed to maximize different objectives.

17
00:02:34,039 --> 00:02:46,449
As one example, you could seek to maximize expected return and minimize return variance. In this case, if two portfolios have the same expected return, you would choose the one with lower variance.

18
00:02:46,449 --> 00:03:11,120
Finally, your portfolio may be subject to additional requirements or constraints. For example, the policies of your investment firm may dictate that you can't hold stocks below a certain level of market capitalization, or you may only be allowed to enter long positions due to government regulations in certain markets, or there may be a limit on what percentage of your portfolio can be invested in any single stock.

19
00:03:11,120 --> 00:03:21,094
These must be taken into account during portfolio design. Finally, in the training stage, you make actual trades in the market.

20
00:03:21,094 --> 00:03:35,734
After portfolio construction, you'll have an ideal portfolio and a list of trades to make. In this stage, you'll have to figure out mechanics like how fast to trade and to which time horizon you believe your alpha applies.

21
00:03:35,735 --> 00:03:48,125
You may decide to trade quickly and aggressively, or more slowly and passively. You'll also have to take into account the fact that the trades themselves incur costs which will eat into your returns.

22
00:03:48,125 --> 00:03:58,144
This is the realm of market microstructure and high-frequency trading. Finally, spends market conditions don't allow you to place the exact trades you intend.


@@@
1
00:00:00,000 --> 00:00:08,744
Welcome back. In this lesson, we're going to discuss common sources of Outliers, a few ways to spot them and some methods you can use to handle them.

2
00:00:08,744 --> 00:00:15,669
Outliers can be a real nuisance. If not handled intelligently, they can make results look better than they really are.

3
00:00:15,669 --> 00:00:27,684
They can lead to false trading signals which when used to construct trading strategies result in profits and losses that at best, differ from simulation results and at worst, are completely negative.


@@@
1
00:00:00,000 --> 00:00:10,109
What are we talking about when we talk about outliers? We're talking about extreme or unexpected values in market data which may or may not represent real events.

2
00:00:10,109 --> 00:00:18,984
For example, if we're tracking a solar energy company stock price, we might see unexpected movements in the price on a day when a solar eclipse occurs.

3
00:00:18,984 --> 00:00:29,195
Outliers can show up for a million different reasons but some reasons are more common than others. In this video, we're going to talk about a few of the most common types of outliers.

4
00:00:29,195 --> 00:00:36,304
Outliers can always appear due to human error, either during manual data entry or in the form of computer bugs.

5
00:00:36,304 --> 00:00:57,869
These are known as fat finger errors. To cite a few examples that had newsworthy consequences, in 2001 a Japanese traitor cost his employer UBS Warburg, 71 million pounds when he sold 610,000 shares at 60 yen instead of six shares at 610,000 yen as he had intended.

6
00:00:57,869 --> 00:01:13,430
In 2015, a junior member of Deutsche Bank's foreign exchange sales team processed a trade using a gross figure rather than a net figure causing a payment to a U.S. hedge fund of six billion dollars, orders of magnitude too high.

7
00:01:13,430 --> 00:01:26,759
Sometimes data can be missing completely, entered as zeros or duplicated from previous states. This can happen due to lax quality control of market data vendors but sometimes exchanges themselves can be missing data.

8
00:01:26,760 --> 00:01:33,110
Data maybe missing because the stock was suspended from trading for a day or more often, for part of a day.

9
00:01:33,109 --> 00:01:45,719
Trading on a stock can be stopped for regulatory reasons, to reduce volatile trading prior to a companies news announcement or when there's uncertainty about whether the stock continues to meet listing standards.

10
00:01:45,719 --> 00:01:54,004
It can also be stopped for non-regulatory reasons. For example, if there's a significant imbalance and pending buy and sell orders.

11
00:01:54,004 --> 00:02:00,204
In these cases, one way to check if trading actually occurred is to check if the trade volume was zero.

12
00:02:00,204 --> 00:02:10,895
Trading for an entire exchange can also be stopped. There are mechanisms called trading curves or circuit breakers that halt trading if an index drops by a certain percent.

13
00:02:10,895 --> 00:02:21,460
The idea is to prevent stock market crashes, events when the prices of stocks across a significant cross section of the market drops suddenly and significantly.

14
00:02:21,460 --> 00:02:28,715
You might also see extreme price movements that are due to real events such as earnings, mergers or other announcements.

15
00:02:28,715 --> 00:02:40,840
Announcements represent information that the market previously did not know. So these can surprise the general market if the results are much better or worse than the market expected, so to speak.

16
00:02:40,840 --> 00:02:48,469
When the market adjusts to this information, the stock price fluctuates as the new information starts to be reflected in the trading price.

17
00:02:48,469 --> 00:03:00,430
For example, the athletic apparel company, Puma, made an unexpected earnings announcement on October 18th, 2017 in which it reported stronger than expected third-quarter earnings.

18
00:03:00,430 --> 00:03:09,454
That day, its stock closed four percent higher than the previous day's close and the subsequent days saw fluctuations as the price rose still further.

19
00:03:09,455 --> 00:03:23,900
Outliers can occur due to the intended or unintended actions of computer trading programs. For example, in 2010 the market experienced a flash crash which lasted only 36 minutes and was later linked to trading algorithms.

20
00:03:23,900 --> 00:03:43,259
While extreme price fluctuations occurred during this short period, they were due to real events. Sometimes quants must work with unadjusted or nominal price data because adjusted price data are unavailable through their data source, when working with real time data directly from a stock exchange for example.

21
00:03:43,259 --> 00:03:54,858
If you are working with unadjusted price data, you will see discontinuities in prices on dates when companies issue dividends, split their stock or took other corporate actions.

22
00:03:54,859 --> 00:04:02,439
Companies may issue dividends on a regular schedule or may decide to issue one-off so-called special dividends.

23
00:04:02,439 --> 00:04:12,590
In this example, you can see that in 2017, Costco issued several regular dividends as well as a large seven dollar per share special dividend.

24
00:04:12,590 --> 00:04:26,324
These discontinuities can look like outliers if returns are calculated on these data. A stock trades ex-dividend on or after the ex-dividend date, so on this date, non-adjusted prices decrease.

25
00:04:26,324 --> 00:04:34,519
This represents the fact that the net company value has decreased because cash has been transferred from the company to shareholders.

26
00:04:34,519 --> 00:04:42,379
However, after prices are adjusted, change demand for company stock may also cause prices to change.

27
00:04:42,379 --> 00:04:59,670
Price changes due to change demand are real market effects. To deal with discontinuities in unadjusted prices, whoever is cleaning the data must go back, look at corporate events and cumulatively adjust for each dividend distribution to generate adjusted prices.


@@@
1
00:00:00,000 --> 00:00:06,644
Thinking about how to deal with Outliers is an important part of signal research. Let me give you a couple examples.

2
00:00:06,644 --> 00:00:14,494
Sometimes, the prices of stocks that aren't traded very much undergo dramatic changes when they are actually traded.

3
00:00:14,494 --> 00:00:21,625
As an example, let's look at some small market capitalization stocks traded on the Hong Kong Stock Exchange.

4
00:00:21,625 --> 00:00:37,324
One example is Easyknit International Holdings Limited which has the ticker symbol 1218.HK. I'm using Easyknit as an example of a thinly or infrequently traded stock, but what does it mean to be thinly traded.

5
00:00:37,325 --> 00:00:46,804
Well, one way to quantify how much a stock is traded is to look at its typical volume. This is the number of shares that were traded during a given time period.

6
00:00:46,804 --> 00:00:53,475
However, the number of shares traded may not tell you very much since a company can issue arbitrary numbers of shares.

7
00:00:53,475 --> 00:01:06,719
A better metric called turnover is the volume traded multiplied by the price per share, in other words, the total amount of money that actually changed hands when the stock was traded during this period.

8
00:01:06,719 --> 00:01:24,444
This metric is more comparable across companies since it's in units of currency. Easyknit's average volume was only about 10,000 shares at the time we made this video and traded at around 4.8 Hong Kong dollars for a turnover of around 48,000 Hong Kong dollars.

9
00:01:24,444 --> 00:01:38,500
For comparison, the turnover of Tencent Holdings, a leading provider of Internet related services in China and one of the most traded stocks on the Hong Kong Stock Exchange, was around five billion Hong Kong dollars.

10
00:01:38,500 --> 00:01:50,075
It doesn't take much trading to move the prices of thinly traded stocks up and down. A single trade will cause a significant change in price, so the prices can fluctuate unpredictably.

11
00:01:50,075 --> 00:02:01,704
These fluctuations are real, they are due to market activity, but they are unpredictable. You could avoid exposure to this unpredictability by excluding these stocks from your trading universe.

12
00:02:01,704 --> 00:02:14,915
Alternatively, if you want to trade these stocks, it might be a good idea to keep these price fluctuations in your trading data in order to be sure that your strategy performs well despite this unpredictability.

13
00:02:14,914 --> 00:02:22,354
This example illustrates why it's important to be careful which stocks you chose to include in your trading universe in the first place.

14
00:02:22,354 --> 00:02:35,520
Including data from periods when the market is behaving in an unusual way can skew your signals. For example, during and after market crashes, stock prices are volatile and can reach extreme values.

15
00:02:35,520 --> 00:02:53,939
Because market crashes do not happen often, they are statistically infrequent outlying events. If we calibrate trading signals including data from these periods, the results will be highly skewed and the signals won't perform optimally on normal trading days, in other words, most of the time.

16
00:02:53,939 --> 00:03:04,405
But if we don't include such periods when calibrating trading signals, and one of these rare events happens, the signal may perform really poorly or even in a toxic manner.

17
00:03:04,405 --> 00:03:23,540
There is no golden rule to resolve this, some traders prefer to alleviate these problems in the strategy formulation phase by controlling the sizes of the long and short positions they take and establishing thresholds called stop loss levels at which they will exit positions to prevent further losses.

18
00:03:23,539 --> 00:03:30,394
It's also possible to design training strategies that attempt to take advantage of sudden extreme price movements.

19
00:03:30,395 --> 00:03:46,490
People who trade with so-called contrarian strategies seek to identify sudden price movements or dislocations that they think are not justified by real information or events and therefore won't last, and they trade against such movements.

20
00:03:46,490 --> 00:03:56,859
One could try to apply this strategy to data of any level of time resolution, but minute level data are more prone to these sudden movements than daily data or monthly data.

21
00:03:56,860 --> 00:04:10,670
This is a risky strategy, it can be difficult to determine whether the price will settle back to where it was, in which case the trader profits, or whether the price will continue to move in the same direction which would not benefit the trader.


@@@
1
00:00:00,000 --> 00:00:06,389
Finding and handling Outliers in raw price data and signal returns are slightly different scenarios.

2
00:00:06,389 --> 00:00:20,550
In this video, we're going to talk about spotting Outliers in raw data. Outliers to look for in raw data include large changes in stock prices and volumes, missing dates, missing prices and missing volumes.

3
00:00:20,550 --> 00:00:34,825
One basic approach to finding extreme values in raw data is to screen the data for them. Doing this by brute force, by looking at every row, for example, is very inefficient but maybe necessary if time is tight.

4
00:00:34,825 --> 00:00:46,125
Plots can be helpful but not by much if the data-set contains hundreds of stocks. One way to screen for Outliers is to create rule-based searching and filtering methods.

5
00:00:46,125 --> 00:00:57,090
For example, you might set up a filter to catch instances when prices change by more than some value that seems reasonable given your signal and the scale of typical price movements.

6
00:00:57,090 --> 00:01:07,465
Percent change thresholds should not be relied upon too heavily, as they are likely to yield many false positives, extreme yet legitimate price movement data-points.

7
00:01:07,465 --> 00:01:18,270
Nonetheless, using such thresholds is one way to screen data quickly. If the price change is accompanied by a large change in volume, it's less likely to be wrong.

8
00:01:18,269 --> 00:01:33,670
So, you can use volume information to improve the accuracy of your filter. The challenges of this task will always be the need to process large amounts of data, minimize false positives and decide how to deal with data values that are missing.


@@@
1
00:00:00,000 --> 00:00:11,129
So, what do you do when you find outliers and raw price data? Well, although at least large institutions, co- traders usually have a team of people to clean data for them.

2
00:00:11,130 --> 00:00:24,250
They may run into situations where they need results fast. The easiest and quickest way to determine if the extreme value is real, or fill in a missing data point is to cross check with another data source.

3
00:00:24,250 --> 00:00:36,469
If it's a one off point, you can remove or replace it with data from a secondary source manually. You might think of replacing these data with some mean of the surrounding data.

4
00:00:36,469 --> 00:00:43,545
This is not done often because it risks incorporating information from the future into those days data.

5
00:00:43,545 --> 00:00:53,335
Inaccuracy due to the use of information that would not have been known or available during the period being analyzed is called Lookahead Bias.

6
00:00:53,335 --> 00:01:08,694
Think about it. If you were trying to fill in that missing datum by averaging data from the surrounding days on the day for which the datum is missing, you wouldn't be able to because you wouldn't know what the price would be in the future.

7
00:01:08,694 --> 00:01:17,139
Lookahead bias is a bias because using unknowable data from the future will consistently make your results look better.

8
00:01:17,140 --> 00:01:26,150
The main problem with using future data in signal research is that any kind of strategy based on that research would be impossible to execute.

9
00:01:26,150 --> 00:01:36,030
One common mistake is to use closing prices of the current day or a future date to calculate the trading signal for the same day.

10
00:01:36,030 --> 00:01:45,939
But of course, a strategy that says to make a trade during the day based on a closing price for that same day would be impossible to implement.

11
00:01:45,939 --> 00:02:00,204
It may not seem to make much of a difference in cases where you were using the data for some kind of historical analysis, but to be safe, it makes sense to simply substitute a missing closing price with the previous closing price during signal research.

12
00:02:00,204 --> 00:02:09,715
However, keep in mind that it is recommended to keep the missing data during back testing because they may represent a real non-tradable event.

13
00:02:09,715 --> 00:02:16,864
For example, stock A may not have traded for a day and thus it's closing price would have been missing.

14
00:02:16,865 --> 00:02:24,670
In your back test your data should reflect this and your strategy should not attempt to place trades on stock a during that day.


@@@
1
00:00:00,000 --> 00:00:09,029
After you calculate returns from a trading signal, you may suspect the presence of outliers when you examine the distribution of your signal returns.

2
00:00:09,029 --> 00:00:25,195
Let me explain what I mean by that. Imagine that we've written some code to generate a trading signal and we've calculated our monthly returns from a couple years of trading based on this signal, what would we expect the distribution of returns to look like?

3
00:00:25,195 --> 00:00:33,760
Let's figure out what we would expect to see in a few limiting scenarios. What if we picked our buy and sell times at random?

4
00:00:33,759 --> 00:00:42,144
In that case, we wouldn't expect to make any money because we'll be equally likely to buy or sell when the market is going up or down.

5
00:00:42,145 --> 00:00:50,804
We'd expect our returns to have a normal distribution with mean zero, but what if we designed a trading strategy that performed well?

6
00:00:50,804 --> 00:01:01,274
Ideally, if a trading signal looks like it will perform well, the distribution of its returns should look like a slightly positively skewed normal distribution.

7
00:01:01,274 --> 00:01:10,420
For the signal to make money, returns should be positive and non-zero on average. So, the distributions mean should be above zero.

8
00:01:10,420 --> 00:01:20,440
However, sometimes the return distribution can look a little too good or just plain weird. This should arouse your skepticism.

9
00:01:20,439 --> 00:01:35,060
Extremely skewed shapes or bumps at either tail of the histogram spell trouble. One tool you can use to compare your distribution of returns to another distribution like the normal distribution is the QQ plot.

10
00:01:35,060 --> 00:01:42,449
A QQ plot is a plot of the quantiles of the first data set against the quantiles of the second data set.

11
00:01:42,450 --> 00:01:57,245
What are quantiles? Well, if we split a data set into four equally sized groups, the dividing lines are at the 25th, 50th, and 75th percentiles, these are usually called quartiles.

12
00:01:57,245 --> 00:02:12,814
The 50th percentile is the median, the value below which 50% the data fall. Quartiles divide the data set into four groups, but with quantiles, the dataset can be divided into any number of equally-sized groups.

13
00:02:12,814 --> 00:02:25,610
For example, you could have ten quantiles, these are usually called deciles. The word quantile is usually used to denote the cut points, but it's sometimes used to refer to the groups themselves.

14
00:02:25,610 --> 00:02:58,390
Let's get back to the goal of comparing your distribution of returns to the normal distribution. If you wanted to use QQ plots, you would first first the set of quantiles you want to use, then you would plot the nth quantile of your distribution against the nth quantile of the normal distribution and continue for all the values of n. If you're comparing your distribution to the normal distribution, and your distribution is approximately normal, points in the QQ plot should fall along a straight line.

15
00:02:58,389 --> 00:03:08,444
If the distribution has fatter tails than the normal distribution, the QQ plot will reveal deviations from a straight line at the extremities of the graph.

16
00:03:08,444 --> 00:03:19,330
Distributions with skew will also have QQ plots that curve away from a straight line. A good quant should try to understand the root cause of outliers and returns.

17
00:03:19,330 --> 00:03:30,139
The first step in dealing with a situation like this is to find out where and when the outlying data points occurred, for which stock or stocks and for which dates.

18
00:03:30,139 --> 00:03:38,390
The next step is to ask why. Depending upon the nature of the extreme data, the source might be obvious or less so.

19
00:03:38,389 --> 00:03:48,880
Was it a data error? Was an illegitimate movement due to a real event? If an extreme datum looks like it could be real, you can check the news for the stock on that day.

20
00:03:48,879 --> 00:03:56,764
Was there an announcement? You'll want to think of every possibility you can to explain why this might not be a market data vendor problem.


@@@
1
00:00:00,000 --> 00:00:06,904
After you've identified anomalous skew in your signal returns and you think you know the cause, what do you do?

2
00:00:06,905 --> 00:00:21,614
The best strategy for dealing with these types of outliers varies on a case-by-case basis. If you know it's just bad data from the data vendor, you can try to fix it by replacing the affected data with correct values from a different vendor.

3
00:00:21,614 --> 00:00:30,134
If you can't do this, you might try to determine if your research result will be greatly affected if you replace the value with any reasonable value.

4
00:00:30,135 --> 00:00:43,144
For example, the previous day's values. If this isn't going to work, you might try to figure out whether removing this data row, all together, will have any significant effect on your signal research results.

5
00:00:43,145 --> 00:00:50,039
You are the one in the best position to make this judgment call. After all, you know the signal the best.

6
00:00:50,039 --> 00:00:56,670
If you think the extreme values are due to legit market events, there are a few different things you should think about.

7
00:00:56,670 --> 00:01:11,805
You might try to find out whether there are many similar occurrences in other stocks. If there are and if you can identify a common cause for all the extreme values, you could consider isolating this group of stocks out of the research stock universe.

8
00:01:11,805 --> 00:01:26,284
You should think about what effect this might have on your signal overall. Here, we're showing several thinly traded small market cap Asian stocks experiencing large sudden unpredictable price fluctuations.

9
00:01:26,284 --> 00:01:35,950
Another type of stock that experiences dramatic unpredictable price fluctuations is the stock of single drug biotechnology companies.

10
00:01:35,950 --> 00:01:48,215
These type of companies evaluation hinges upon the success of various milestones: phase one,two, and three drug efficacy trials and FDA approval.

11
00:01:48,215 --> 00:02:01,170
With only one product, the company is worth a lot if it is successful and very little if it fails. However, the outcome of these binary events is very difficult to predict and to handle.

12
00:02:01,170 --> 00:02:21,125
So, quants frequently exclude these stocks from their trading universes. As one example, the price of the stock of Sage Theraputics, a company that makes a postpartum depression drug, jumped from $93 to $167 in a single day last year when the company announced positive test results.

13
00:02:21,125 --> 00:02:29,365
You should also think about whether events you think are legitimate are due to special market events like corporate earnings or central bank announcement's.

14
00:02:29,365 --> 00:02:42,210
Realistically, if you eventually use this signal to build a trading strategy, you might want to think about whether you can avoid these events by, for example, pausing the strategy in advance of them.

15
00:02:42,210 --> 00:02:57,530
If it's possible to do this, then you can probably remove these rows and proceed with the research. If you can't pause the strategy ahead of these events, then you're going to need to think about how you will be able to avoid losing money when executing this strategy.

16
00:02:57,530 --> 00:03:11,530
As you can see, a lot of this is common sense. You'll always need to think about how you can prevent the quality of your research results being degraded by outliers and how you can avoid losing money when trading your final strategy.


@@@
1
00:00:00,000 --> 00:00:12,404
If possible, we'd like to generate trading signals that are robust to outlying data points. For example, imagine you have an outlier in the closing price time series, a huge positive spike.

2
00:00:12,404 --> 00:00:24,745
If you're using a momentum based strategy and your strategies buy or sell decision is based solely on individual closing price values, then the single value might induce a buy decision.

3
00:00:24,745 --> 00:00:41,784
If instead you take the moving average of closing prices averaged with a fixed duration rolling time window as the input to your signal, it will average out that single extreme datum and reduce the effect of individual outliers on buy or sell decisions.

4
00:00:41,784 --> 00:00:49,039
The trade-off here is that your signal actions might be generated with a slight delay relative to the stock price movement.

5
00:00:49,039 --> 00:01:02,994
The duration of this delay will depend on the window size you use. You can take an analogous approach for portfolio level strategies, or strategies that are based on the movements of an entire portfolio of stocks,.

6
00:01:02,994 --> 00:01:14,215
You can average out the extreme movements of individual stocks by basing your buy or sell decision on the accumulated movements of many stocks, or even an entire sector.

7
00:01:14,215 --> 00:01:28,655
This will also reduce the effect of outliers on your signal. If you are looking for alternative solutions, there have been ongoing efforts from within the industry to incorporate Bayesian methods, or machine-learning, into outlier detection.


@@@
1
00:00:00,000 --> 00:00:08,765
In this lesson, we've discussed common sources of outliers and data, some ways to spot them, and a few things you can do to handle them.

2
00:00:08,765 --> 00:00:16,280
However, it's worth noting that your strategy for dealing with outliers will probably vary on a case-by-case basis.


@@@
1
00:00:58,429 --> 00:01:25,210
This is what we do in Statistical Arbitrage. Statistical Arbitrage is a trading technique that involves simultaneously buying and selling two related assets based on the analysis of how these two assets move in relation to one another.

2
00:01:25,209 --> 00:01:36,189
It's important to keep in mind, the signal to noise ratio in our data. The signal is the meaningful part of our input data that helps us predict our dependent variable.

3
00:01:36,189 --> 00:01:48,390
The noise is the random part of our data, that does not help us make better predictions. With financial data, the signal is relatively low and there's a lot of noise.

4
00:01:48,390 --> 00:02:00,100
In other words, the signal to noise ratio tends to be low. When the signal to noise ratio is low, predictive models tend to overfit the data that it's trained on.

5
00:02:00,099 --> 00:02:11,344
This means that when the model is used to make forecasts in real life, they often pay too much attention to aspects of the data that are not actually useful in prediction.

6
00:02:11,344 --> 00:02:23,325
This results in predictions that are not accurate enough to be useful. Moreover, the relationship between the independent variables and the stock returns can change over time.

7
00:02:23,324 --> 00:02:36,205
In other words, their relationships are not stationary. In practice, this means that your models become stale after sometime and need to be periodically retrained with more recent data.

8
00:02:36,205 --> 00:02:47,305
This also means that some independent variables, that produced a useful predictive signals in the past may not necessarily do so in the future and vice versa.

9
00:02:47,305 --> 00:02:54,330
This is partly why a certain trading strategies fade after awhile and be become useful again in the future.


@@@
1
00:00:00,000 --> 00:00:09,330
Many statistical models assume that the data follows a normal distribution, also referred to as a Gaussian or a bell curve.

2
00:00:09,330 --> 00:00:19,365
This is important when checking whether our models are valid. There are various tests that we use to check that our models describe a meaningful relationship.

3
00:00:19,364 --> 00:00:32,204
These tests assume that the data are normally distributed. If the data are not normally distributed, then these tests tend to conclude that the model is valid when in fact it is not.

4
00:00:32,204 --> 00:00:42,335
We will review the concepts of normality, show how to check for normality, and introduce some ways to transform our data so that they follow a normal distribution.

5
00:00:42,335 --> 00:00:58,145
What is a random variable? A random variable is a variable that can take on a random value. The probability that the random variable takes a particular value is determined by its probability distribution.

6
00:00:58,145 --> 00:01:05,555
You can think of a probability distribution as a range of numbers each with a probability associated with it.

7
00:01:05,555 --> 00:01:17,704
With data from the real world, we don't actually know its underlying probability distribution. However, we can often approximate its probability distribution with an equation.

8
00:01:17,704 --> 00:01:27,680
If we say that a random variable is normally distributed, how do we visualize what this means? We can imagine the random variable as a tennis ball machine.

9
00:01:27,680 --> 00:01:42,369
This machine shoots tennis falls onto a number line that ranges from negative infinity to infinity. The number line has buckets of equal size placed on the line, so that tennis balls will collect in these buckets.

10
00:01:42,370 --> 00:01:54,140
If the random variables distribution is centered around zero, this means that tennis balls are more likely to hit the number line at zero, rather than say 500.

11
00:01:54,140 --> 00:02:01,444
If we shoot enough tennis balls along this number line, then we end up with a hill shaped pile of tennis balls.


@@@
1
00:00:00,000 --> 00:00:10,259
A probability distribution is defined in math by an equation. We call this equation a probability density function or PDF.

2
00:00:10,259 --> 00:00:19,815
For every number from negative infinity to infinity, the probability density function gives a probability that this number will be generated.

3
00:00:19,815 --> 00:00:44,854
Using math notation, X tilde D means the random variable X follows a probability distribution D. The capital P with a lowercase x vertical bar and capital D is read as the probability of x given D. So, what does it mean when we say the probability of x given D?

4
00:00:44,854 --> 00:00:56,685
Let's say we have a number two that we observed from a dataset. We assume that the dataset follows a particular probability distribution such as the normal distribution.

5
00:00:56,685 --> 00:01:09,515
The equation P of x tells us how likely the random variable would take on the value two. This P of x outputs a number between zero and one which represents this probability.

6
00:01:09,515 --> 00:01:26,600
For instance, here is the standard normal distribution written as a fancy-looking capital N. The standard normal distribution has a mean of zero and a standard deviation of one, but what if you had a distribution that was not centered around zero?

7
00:01:26,599 --> 00:01:38,300
Or, what if you had a distribution that had a flatter wider distribution? These distributions can still be modeled by the same equation by adjusting its parameters.

8
00:01:38,299 --> 00:01:48,604
To describe these variations of the normal distribution, we can adjust two of its parameters. Calculate the mean of the data and assign that value to mu.

9
00:01:48,605 --> 00:01:57,730
Calculate the standard deviation and assign it to sigma. The resulting function approximates the distribution of our data.


@@@
1
00:00:00,000 --> 00:00:11,530
We've just seen how to model our data if we assume that it is normally distributed. But how do we decide if our data can be described by a normal distribution in the first place?

2
00:00:11,529 --> 00:00:20,540
A quick way to visually check, is to plot a histogram of our data. We can then compare the histogram to a plot of the normal distribution.

3
00:00:20,539 --> 00:00:29,210
Does this data's distribution look normal? How about this one? Does it look like we can still describe it with a normal distribution?

4
00:00:29,210 --> 00:00:38,209
We can also use a boxplot to check for normality. As you'll soon see, the boxplot helps to check for symmetry in your data.

5
00:00:38,210 --> 00:00:46,469
Normal distributions are symmetric around their mean. Here is a boxplot. There is a box with a line inside.

6
00:00:46,469 --> 00:00:55,135
The bottom edge of the box, the middle line in the box, and the top edge of the box, are quantiles that divide the data in two groups.

7
00:00:55,134 --> 00:01:02,620
We call the dividing lines quantiles because they split the data into groups with the same number of points.

8
00:01:02,619 --> 00:01:15,015
Since the three dividing lines create four groups, we call these lines quartiles. Remember that the quartiles are the three dividing lines, not the four groups themselves.

9
00:01:15,015 --> 00:01:26,255
The line inside the box is the median. If you ordered all of your dataset from smallest to largest, 50 percent of your data points are less than the median.

10
00:01:26,254 --> 00:01:35,105
The bottom side of the box represents the first quartile. Twenty five percent of the data are less than the first quartile.

11
00:01:35,105 --> 00:01:43,194
The top side of the box is the third quartile. Seventy five percent of the data are less than the third quartile.

12
00:01:43,194 --> 00:01:54,189
We can calculate the interquartile range by taking the third quartile, minus the first quartile. The boxplot has small lines on either end.

13
00:01:54,189 --> 00:02:05,485
We call these lines whiskers. The lower bound whisker is defined as the first quartile, minus 1.5 times the interquartile range.

14
00:02:05,484 --> 00:02:15,079
Similarly, the upper bound whisker is set to the third quartile, plus 1.5 times the interquartile range.

15
00:02:15,080 --> 00:02:22,844
Points lying outside of the whiskers can be considered outliers and are visualized as individual points.

16
00:02:22,844 --> 00:02:33,889
Going back to our test for normality, if you look at the normal distribution, it is symmetric. When a distribution is symmetric, it's median is equal to it's mean.

17
00:02:33,889 --> 00:02:44,045
The first quartile and third quartile are also the same distance away from the median. Also, the whiskers are the same distance away from the median.

18
00:02:44,044 --> 00:02:53,114
By visualizing your data with a boxplot, you can see whether it's symmetric or not. If it's not symmetric, it's not normally distributed.

19
00:02:53,115 --> 00:03:01,855
We can look at the data using both a box whisker plot and a histogram. Here, it looks like it follows a normal distribution.

20
00:03:01,854 --> 00:03:13,099
This other dataset is not normal. In fact, we say that it is skewed because it has a long tail of data to only one side of the distribution.

21
00:03:13,099 --> 00:03:20,204
When there is a long tail of data to the left, the mean of the distribution lies to the left of the median.

22
00:03:20,205 --> 00:03:29,655
We say that the distribution is left skewed. In fact, stock returns tend to exhibit left skew and fat tails.

23
00:03:29,655 --> 00:03:37,805
This means that extreme negative returns occur with greater frequency than would be expected by a normal distribution.

24
00:03:37,805 --> 00:03:45,500
Please note that even if your data is symmetric, it's not necessarily a bell-shaped curve of a normal distribution.

25
00:03:45,500 --> 00:03:55,629
So, it helps to see the box whisker plot, as well as the histogram. To do a more thorough check for normality, we can use a QQ plot.

26
00:03:55,629 --> 00:04:07,125
The QQ plot checks if the shape of the data matches the shape of our probability density function. The first Q in QQ plot stands for quantile.

27
00:04:07,125 --> 00:04:17,449
The second Q in QQ plot also stands for quantile. Common quantiles are quartiles, deciles, and percentiles.

28
00:04:17,449 --> 00:04:27,660
Recall that when we sort the data points from smallest to largest, we can find boundaries that divide the data into groups with equal number of points.

29
00:04:27,660 --> 00:04:41,724
If we divide a data into 10 buckets, the nine boundaries are deciles. If we divide the data into 100 buckets, the 99 boundaries are called percentiles.

30
00:04:41,725 --> 00:04:50,329
Notice, that this is not the same as plotting a histogram. With a histogram, the buckets have the same width.

31
00:04:50,329 --> 00:05:01,444
With quantiles, the buckets all have the same number of data points. QQ plots let us compare any two distributions to check if they have the same shape.

32
00:05:01,444 --> 00:05:11,009
Since we want to check if a data is shaped like a normal distribution, we can plot it's quantiles against the quantiles of a normal distribution.

33
00:05:11,009 --> 00:05:24,435
As an example, let's take the 50th percentile of our data as our y-coordinate. Next, we take the 50th percentile of the standard normal distribution as our x-coordinate.

34
00:05:24,435 --> 00:05:41,519
We plot this xy point on the QQ plot. We repeat this for all the other percentiles. If the two distributions have the same shape, then the plotted points will follow a straight line that goes from the bottom left to the top right.

35
00:05:41,519 --> 00:05:52,015
In this QQ plot, we can see that the data does not look like a normal distribution. This is because the plotted points form a curve instead of a straight line.

36
00:05:52,014 --> 00:06:03,754
So far, we've seen plots that help us visually check if our data looks like it's normal. What if we wanted a single number to represent how normal the data is?

37
00:06:03,754 --> 00:06:13,330
If we had a single number, we could choose a cutoff point. Anything on one side of this cutoff point, and we decide that it's normal.

38
00:06:13,329 --> 00:06:26,860
Anything on the other side of the cutoff point, and we decide that it's not normal. The Shapiro-Wilk test and the D'Augustino-Pearson tests, are hypotheses tests that give a p-value.

39
00:06:26,860 --> 00:06:36,775
The p-value ranges from zero to one. These are hypotheses tests in which the null hypothesis assumes that the data is normally distributed.

40
00:06:36,774 --> 00:06:51,275
If the p-value is larger than 0.05, then we will assume that it follows a normal distribution. If the p-value is less than 0.05, then it likely does not follow a normal distribution.

41
00:06:51,274 --> 00:07:02,419
There is a another more general test to decide whether any two distributions are the same. This test is called the Kolmogorov-Smirnov test.

42
00:07:02,420 --> 00:07:16,084
Given two distributions, the Kolmogorov-Smirnov test returns a p-value. If the p-value is greater than 0.05, we will assume that the two distributions are the same.

43
00:07:16,084 --> 00:07:26,915
If the p-value is less than 0.05, then we say the two distributions are not the same. Remember the reason why we care about the data being normal.

44
00:07:26,915 --> 00:07:36,074
When we use statistical models such as regression, we use hypotheses tests to check if we can trust the model parameters of the model.

45
00:07:36,074 --> 00:07:49,069
These tests assume that our data is normally distributed. If the data are not normally distributed, these tests tend to tell us that our model is valid when in fact it is not.


@@@
1
00:00:00,000 --> 00:00:12,515
Recall that we also need to check if our data is stationary over time. By stationary, we mean that the mean, variance, and covariance are the same over time.

2
00:00:12,515 --> 00:00:26,719
In particular, we want to check that the variance of our data is stable over time. This is important because if the variance changes over time, then the tests that we use to validate our model will be incorrect.

3
00:00:26,719 --> 00:00:37,250
The term for constant variance over time is homoscedasticity. The term for a changing variance over time is heteroskedasticity.

4
00:00:37,250 --> 00:00:55,175
To check if our data is homoscedastic or heteroskedastic, we can use the Breusch-Pagan test. If the Breusch-Pagan test, yields a small P value, say less than 0.05, then we have to assume that the data is heteroskedastic.

5
00:00:55,174 --> 00:01:03,729
If the Breusch-Pagan test yields a P value greater than 0.05, we can assume that the data is homoscedastic.


@@@
1
00:00:00,000 --> 00:00:11,064
So, now the question is, what do we do when our data is not normally distributed? Similarly, what do we do when our data is heteroscedastic?

2
00:00:11,064 --> 00:00:23,300
To reshape our data and make it more normal, we can feed our data into the log function. To get data that is homoscedastic, we can take the time difference between periods.

3
00:00:23,300 --> 00:00:30,434
By time difference, I mean that we can view our data as the rate of return from one day to the next.

4
00:00:30,434 --> 00:00:38,049
Similarly, we could get the time difference by subtracting each previous day's value from the current day's value.

5
00:00:38,049 --> 00:00:51,515
In practice, we take the rate of change for each period and then apply the log function. You may have seen this earlier when learning about why we model financial data with log returns.

6
00:00:51,515 --> 00:01:00,425
A way to make our data both normally distributed and homoscedastic, is by applying the Box-Cox transformation.

7
00:01:00,424 --> 00:01:08,234
To preview what the Box-Cox transformation does, let's get a conceptual idea of what a transformation looks like.

8
00:01:08,234 --> 00:01:19,519
Imagine you place all your data points on a number line, they are not evenly spaced and have odd clusters in some parts and no data points in other places.

9
00:01:19,519 --> 00:01:34,129
Think of this as a necklace in which the beads are not spaced out in a very nice way. Now, imagine if we could nudge each of these beads a little bit to the left and to the right to even out the spacing in a nicer way.

10
00:01:34,129 --> 00:01:46,164
Notice that since these beads are all on the same string, the relative order of each data point remains the same, we're just spacing them out in a way that's easier to work with.

11
00:01:46,165 --> 00:01:58,835
In math terms, we just applied a monotonic transformation. A monotonic transformation changes the values of a dataset but preserves the relative order.

12
00:01:58,834 --> 00:02:12,590
The Box-Cox transformation takes a dataset and outputs a dataset that is more normally distributed. You can see that the Box-Cox transformation has one constant value, lambda.

13
00:02:12,590 --> 00:02:28,079
If you choose lambda to be zero, then the transformation function is defined as the natural log. You can try different values for lambda to transform the data, then perform tests for normality and homoscedasticity.

14
00:02:28,080 --> 00:02:36,070
Once you can say your data is normally distributed, you can use it in the various models that we'll discuss in the rest of the lesson.


@@@
1
00:00:00,000 --> 00:00:15,599
Now, we'll look at how to use one random variable to predict another random variable. We'll cover the basics of regression as this forms the basis for several models that are used to analyze stock returns over time.

2
00:00:15,599 --> 00:00:25,664
If we want to estimate the price of a house, we may assume that home buyers are willing to pay more for a bigger house, all other things being equal.

3
00:00:25,664 --> 00:00:40,275
So, we may find data on the area covered by each house as well as its price. We want to find a coefficient that we can multiply by the area, then add a constant term, which we refer to as the intercept.

4
00:00:40,274 --> 00:00:48,888
This is the equation for a straight line. Fortunately, we don't have to guess the best values for the coefficient and intercept.

5
00:00:48,889 --> 00:00:56,490
When we plot the price against the area, we can draw a line that tries its best to pass through most of the points.

6
00:00:56,490 --> 00:01:03,734
We can measure how well the line fits the data by measuring the vertical difference between the point and the line.

7
00:01:03,734 --> 00:01:14,793
An optimal regression line is one that manages to reduce these differences. This process that finds the optimal regression line is called ordinary least squares.

8
00:01:14,793 --> 00:01:22,034
Even after we find the best regression line, we can expect to see differences between the data points and the line.

9
00:01:22,034 --> 00:01:30,069
These differences between the best fit regression line at each point are called residuals or error terms.

10
00:01:30,069 --> 00:01:40,870
In other words, the residual is the difference between the actual value, and the predicted value. We can check if the residuals follow a normal distribution.

11
00:01:40,870 --> 00:01:51,004
If the residuals follow a normal distribution with a mean of zero and a constant standard deviation, then these residuals can be considered random.

12
00:01:51,004 --> 00:01:59,775
By random, I mean that the model's predicted value is equally likely to be higher or lower than the actual value.

13
00:01:59,775 --> 00:02:08,990
If however, the average of the residuals is not zero, this gives us a hint that the model has a bias in its prediction errors.

14
00:02:08,990 --> 00:02:22,349
One way to improve our model is to look for other independent variables. This is called multiple regression, when we use more than one independent variable to predict a dependent variable.

15
00:02:22,349 --> 00:02:33,274
Now that we fit a regression model, we want to check if we can rely on it for future predictions. One measure of our model's ability to fit the data is the R-squared value.

16
00:02:33,275 --> 00:02:48,360
The R-squared is a metric that ranges from zero to one. R-squared of one means that all the variation in the dependent variable, can be explained by all the variation in the independent variables.

17
00:02:48,360 --> 00:02:58,875
A better metric is the adjusted R-squared, which helps us to find the minimum combination of independent variables that are most relevant for our model.

18
00:02:58,875 --> 00:03:10,489
Another way to check our model, is by performing an F-test. The F-test checks whether our coefficients and intercepts are not zero, and therefore, meaningful.

19
00:03:10,490 --> 00:03:24,870
If we get a P-value of 0.05 or smaller, we can assume that our parameters are not zero. When parameters are not zero, then we can say that our model describes a meaningful relationship.


@@@
1
00:00:00,000 --> 00:00:09,599
Earlier, we touched on using multiple independent variables to predict a single dependent variable. We call this multiple regression.

2
00:00:09,599 --> 00:00:17,265
In this example, we are using a house's area, number of rooms and number of years since the house was built.

3
00:00:17,265 --> 00:00:28,379
All three are variables that we use to predict the house price. We can go one step further and choose more than one dependent variable that we wish to predict.

4
00:00:28,379 --> 00:00:36,704
Now, we're not just predicting the house price, but also trying to predict the home's electricity consumption and gas consumption.

5
00:00:36,704 --> 00:00:44,670
When we tried to predict more than one dependent variable at the same time, this is called multivariate regression.

6
00:00:44,670 --> 00:00:55,314
When we're using more than one independent variable to predict more than one dependent variable, this is called multivariate multiple regression.

7
00:00:55,314 --> 00:01:04,930
This appears rather complicated, but being familiar with multivariate regression will help you later when you learn about time series and pairs trading.


@@@
1
00:00:00,000 --> 00:00:11,995
So, how is regression used in stock trading? In practice, using regression to predict a stock's return is difficult because the signal and financial data is low compared to the noise.

2
00:00:11,994 --> 00:00:26,035
Moreover, the models are sensitive to some choices you make about the model. For instance, you'll have to decide how much of your previous data to use since more recent data is more relevant than very old data.

3
00:00:26,035 --> 00:00:36,509
Regression models are pretty sensitive to these design choices. Regression is also sensitive to outliers in the data, as it adds more noise to the training data.

4
00:00:36,509 --> 00:00:49,580
However, there are still important reasons to learn regression to analyze stock returns. Learning regression is useful because we can apply the same regression techniques to analyze time-series data.

5
00:00:49,579 --> 00:00:58,715
Time series analysis looks at data that is collected at regular intervals over time and uses that to predict its value in the near future.

6
00:00:58,715 --> 00:01:08,495
One more note about why regression is important to learn, if we learn the details of how neural networks work, many of the same principles apply.

7
00:01:08,495 --> 00:01:18,439
In fact, if we think of regression as a fundamental building block, then a neural network is a building that is composed of many regression building blocks.


@@@
1
00:00:00,000 --> 00:00:07,250
Congratulations on making it through this lesson. You've built a foundation that will help you with the next few lessons.

2
00:00:07,250 --> 00:00:18,719
Coming up, we'll discuss time series analysis in more depth. Time series analysis includes statistical techniques such as auto-regression and moving averages.

3
00:00:18,719 --> 00:00:27,015
Time series analysis also includes machine learning techniques such as Kalman Filters and Recurrent Neural Networks.


@@@
1
00:00:00,000 --> 00:00:07,935
You've been busy learning here, building up your knowledge and skills through your nanodegree work. But, what's actually happening inside your head to enable this growth?

2
00:00:07,934 --> 00:00:15,680
Well, each time you think about something, you activate particular neural pathways in your brain, and that activation strengthens the pathways themselves.

3
00:00:15,679 --> 00:00:28,940
As a simple example, if I just learned basic addition so that I can figure out that three plus two equals five, the more I do problems similar to this like, one plus six equals seven, the easier it will be for me to remember how to add digits together.

4
00:00:28,940 --> 00:00:34,715
Essentially, we grow our knowledge and make it easier to access the more we recall relevant information.

5
00:00:34,715 --> 00:00:41,060
Research also shows that applying new learning in different contexts makes it even stronger, and it allows you to retain it better.

6
00:00:41,060 --> 00:00:54,189
So, if I move from problems like three plus two equals five to trying out something like 4 plus 20, plus 1, plus 6, I have the opportunity to apply my addition skills in a new context with bigger numbers and more things to add up.

7
00:00:54,189 --> 00:01:05,894
This will make my brain distill the core concepts necessary for understanding how addition really works, which means that in the future, when faced with even trickier addition problems, I'll have a strong foundation to tackle those challenges.

8
00:01:05,894 --> 00:01:18,019
This is exactly what we want to prepare you to do, albeit with much tougher problems. Our job at Udacity is to help you build deep knowledge and flexible skills that you can apply to all sorts of novel problems in the future.


@@@
1
00:00:00,000 --> 00:00:07,345
Welcome to the lesson on time series analysis. Time series are data that are collected at regular intervals.

2
00:00:07,344 --> 00:00:21,504
We will cover two statistical methods, autoregression and moving averages. This will give us the foundation to cover two more advanced methods, autoregressive moving averages and autoregressive integrated moving averages.

3
00:00:21,504 --> 00:00:29,490
From there, we will cover two machine-learning methods. The first is Kalman filters, and the more generalized, particle filters.

4
00:00:29,489 --> 00:00:38,299
The second is recurrent neural networks. Let's get started. Let's think a bit about what a stock price time series looks like.

5
00:00:38,299 --> 00:00:48,875
Most stock prices increase over time. Sadly, some stock prices also decrease over time. This means that the price shows a trend.

6
00:00:48,875 --> 00:01:00,679
This makes analyzing the prices difficult since the prices are non-stationary. By non-stationary, we mean that the data's mean and standard deviation change over time.

7
00:01:00,679 --> 00:01:13,240
The goal of time series analysis is to use past data to predict future values. So, if the properties of the data change over time, the past is less useful in predicting the future.

8
00:01:13,239 --> 00:01:22,579
To work with data that is more likely to be stationary, and therefore, easier to model, we use stock returns and not stock prices.

9
00:01:22,579 --> 00:01:33,129
Moreover, to work with data that is more stationary and more normally distributed, we use the log of stock returns, which we call log returns.


@@@
1
00:00:00,000 --> 00:00:08,855
When we look at log returns of a stock, we assume that the previous period's value gives us some insight into the next period's value.

2
00:00:08,855 --> 00:00:16,350
It's also reasonable to assume that the past couple of data points give us hints as to what the next value will be.

3
00:00:16,350 --> 00:00:29,524
This is the main principle behind Auto-regressive Models. An autoregressive model, also called an AR model, tries to fit a line that is a linear combination of previous values.

4
00:00:29,524 --> 00:00:36,905
The AR model includes an Intercept which represents some constant that is independent of the previous values.

5
00:00:36,905 --> 00:00:45,064
The AR model also includes an Error term, which represents movements that cannot be predicted using the previous values.

6
00:00:45,064 --> 00:00:56,990
Let's see how we prepare the time-series data, to feed it into an Autoregressive model. As an example, our AR model will use the previous two values to predict the current value.

7
00:00:56,990 --> 00:01:06,920
For example, if we have daily data for Monday, Tuesday, and Wednesday, then we use Monday and Tuesday as the independent variables.

8
00:01:06,920 --> 00:01:19,155
We also define Wednesday as the dependent variable that the Model tries to predict. Then, when we move on to Thursday, we use the previous two days to predict Thursday's value.

9
00:01:19,155 --> 00:01:29,689
If data from a previous period has some predictive value, then this coefficient will be non-zero. The number of past values used in the model is known as the Lag.

10
00:01:29,689 --> 00:01:39,730
We define an AR model by its Lag. For example, a model that only uses yesterday's value and ignores the rest, is an AR 1 Model.

11
00:01:39,730 --> 00:01:52,870
Whereas, a model that uses the two previous days' values and ignores the rest is an AR 2 Model, and a model that uses the previous three days' values and ignores the rest is an AR 3 Model.

12
00:01:52,870 --> 00:02:05,174
We write Auto-regressive Models as ARP, where p stands for the lag. We can choose different lag values to train your model and see how they perform on test data.

13
00:02:05,174 --> 00:02:17,150
We can check if the coefficients are significantly different from zero. If some coefficients are likely zero, then you can reduce the lag and focus the model on the more recent values.

14
00:02:17,150 --> 00:02:27,889
As with any regression model, you can also check the adjusted R squared to get a sense of how well the independent variables explain the movements of the dependent variables.

15
00:02:27,889 --> 00:02:38,254
Note that an AR model is designed to represent a single time series. If you have several different time series, you could build a separate AR model for each.

16
00:02:38,254 --> 00:02:48,534
But what if the movement of one stock has some relation to the movement of another? You may be seeing something similar with multivariate multiple regression.

17
00:02:48,534 --> 00:02:56,840
To account for interdependence among more than one time series, we can use the multivariate version of autoregression.

18
00:02:56,840 --> 00:03:08,129
This is called a vector autoregressive model. Note that having some familiarity with the vector autoregressive model will help you as you learn about pairs trading in a later lesson.


@@@
1
00:00:00,000 --> 00:00:18,910
Another way to model time series is to think of the stock return hovering around in moving average. As an analogy, imagine that you're walking at night while holding a lantern, a moth flies around the lantern, moving a bit randomly, but still following the general path of your lantern.

2
00:00:18,910 --> 00:00:28,285
In this analogy, the lantern represents the average position. The moth's smaller movements relative to the lantern are the residuals.

3
00:00:28,285 --> 00:00:36,479
The moth's movement relative to the ground is a combination of the lantern's movement and the moth's relative movements.

4
00:00:36,479 --> 00:00:50,660
In the moving average model, often called an MA model, we started with the average Mu. To get the value at time T, we add a linear combination of residuals from the previous time periods.

5
00:00:50,659 --> 00:00:59,900
In financial time series, the residuals represent the new unpredictable information that cannot be captured by the past data points.

6
00:00:59,899 --> 00:01:06,109
The residuals are the difference between the model's past predictions with the actual values that occurred.

7
00:01:06,109 --> 00:01:19,920
Moving average models are defined as MAQ where Q is the lag. To decide the best value for Q in an MAQ model, you can draw an autocorrelation plot.

8
00:01:19,920 --> 00:01:26,969
Correlation between two variables is a measure of how much one variable moves when the other variable moves.

9
00:01:26,969 --> 00:01:38,125
Correlation ranges between negative one and one. Autocorrelation is a measure of how much the current value moves in relation to one of its previous values.

10
00:01:38,125 --> 00:01:47,025
For example, let's say we noticed that the current stock return is usually positive when its previous value is positive, and vice versa.

11
00:01:47,025 --> 00:01:58,689
Then, we can say that the stock has a positive autocorrelation with it's T minus 1 value. Note how an autocorrelation plot is different from performing a multiple regression.

12
00:01:58,689 --> 00:02:12,435
Correlation measures the pairwise relationship between exactly two periods at a time. Multiple regression measures how a set of independent variables collectively influence the value of the dependent variable.

13
00:02:12,435 --> 00:02:20,289
When we view an autocorrelation plot, we want to use the lag that contains highly positive or highly negative correlations.

14
00:02:20,289 --> 00:02:29,400
When we reach some time periods with little correlation, we can choose the lag to ignore those values and any other time periods further back in time.


@@@
1
00:00:00,000 --> 00:00:10,099
Autoregressive and moving average models tend to capture different relationships. The nice thing is, you can get the best of both by adding them together.

2
00:00:10,099 --> 00:00:20,699
An autoregressive moving average is defined with a p and q. The p is the lag for the autoregression, the q is the lag for moving average.

3
00:00:20,699 --> 00:00:31,434
A variation of the autoregressive moving average is the autoregressive integrated moving average. This concept is used in a trading strategy called Pairs trading.

4
00:00:31,434 --> 00:00:38,269
First, let's build some intuition that will help us when we study autoregressive integrated moving averages.

5
00:00:38,270 --> 00:00:49,500
Let's say we want to describe the position of a turtle. The turtle is walking at a regular pace. So, its position goes from zero, then one, then two, and so on.

6
00:00:49,500 --> 00:00:57,295
We noticed that the turtle is moving at a regular pace. So, we can describe its speed, or the distance over time, as a single number.

7
00:00:57,295 --> 00:01:05,645
For example, this turtle is moving at one meter per second. If we plot the turtle's position over time, it is not constant.

8
00:01:05,644 --> 00:01:16,730
Its position over time will look like a line that slopes upward. However, if we plot the turtle's speed over time, it is a constant one meter per second.

9
00:01:16,730 --> 00:01:25,105
The plot of speed over time is a horizontal line. Now let's think about how the turtle's position is related to its speed.

10
00:01:25,105 --> 00:01:31,954
If we look at the turtle's position over time, the slope of that line is its speed, or meters per second.

11
00:01:31,954 --> 00:01:38,384
Going the other way, we look at the plot of the turtle's speed over time and take the cumulative sum.

12
00:01:38,385 --> 00:01:47,885
So, for instance, at each second, we add one meter. So, we get one meter, two meters, three meters, and so on.

13
00:01:47,885 --> 00:01:54,754
This is actually the area under the horizontal line, and is also the position of the turtle as time goes on.

14
00:01:54,754 --> 00:02:04,180
So, what did we learn here? In general, taking the difference between each period is called the time difference or item wise difference.

15
00:02:04,180 --> 00:02:11,254
If you take the time difference of your data, you may be able to describe your data more easily as a constant number.

16
00:02:11,254 --> 00:02:17,900
If you remember from calculus, taking the derivative of a straight line gives the slope of that line.

17
00:02:17,900 --> 00:02:26,944
This slope gives us a way to describe the line with a constant. To go from speed back to position, we can take the integral.

18
00:02:26,944 --> 00:02:37,320
This is finding the area under the curve or taking the cumulative sum. Taking the integral lets us translate from the speed back to position.

19
00:02:37,319 --> 00:02:44,135
We will now apply these concepts of time difference to learn about the autoregressive integrated moving average.

20
00:02:44,134 --> 00:02:59,390
Recall that regression-based time series models require the data to be stationary. When data is not stationary, the mean, variants, or co-variants may change over time, and it's hard to use the past to predict the future.

21
00:02:59,389 --> 00:03:05,739
One way to get a stationary time series is by taking the difference between points in the time series.

22
00:03:05,740 --> 00:03:22,349
The time difference may also be called the rate of change or the item wise difference. We call that the rate of change between periods, or the rate of return, can be calculated by taking the ratio of the current price divided by the previous price.

23
00:03:22,349 --> 00:03:31,085
When you express your data in terms of logs, this ratio becomes a difference between your current log price and the previous log price.

24
00:03:31,085 --> 00:03:40,344
When working with financial data, we usually find that asset price time series have a property such that their time difference is stationary.

25
00:03:40,344 --> 00:03:48,094
In other words, we like working with returns and not prices because the time series are more stable.

26
00:03:48,094 --> 00:04:01,395
In math terms, we say that the original price data is integrated of order one. We also say that the log returns of this data is integrated of order zero.

27
00:04:01,395 --> 00:04:10,479
So, when working the time series, you can check if it is stationary using a statistical test called the augmented Dickey Fuller test.

28
00:04:10,479 --> 00:04:20,380
If the augmented Dickey Fuller test gives a p-value that is 0.05 or less, then we can assume that the time series is stationary.

29
00:04:20,379 --> 00:04:30,310
If the data is not stationary, we can take the time difference then run the augmented Dickey-Fuller test to see if the time difference is stationary.

30
00:04:30,310 --> 00:04:42,854
If it is stationary, then we can say that this time difference is integrated of order zero. We can also say that the original time series is integrated of order one.

31
00:04:42,855 --> 00:04:57,610
You may need to take the time difference multiple times. Then, once you find a time difference that is stationary, you refer to the original data as integrated of order d where d is the number of times that you had to take the time difference.

32
00:04:57,610 --> 00:05:14,469
Also, once we have a stationary time series, we can model it with an autoregressive moving average. Note that being familiar with the integrated order one and integrated ordered zero time series will help you as you learn about co-integration and pairs trading.


@@@
1
00:00:00,000 --> 00:00:08,210
We'll now discuss the Kalman filter, which is used for time series in self-driving cars and even flying cars.

2
00:00:08,210 --> 00:00:15,390
But first, let's look back at regression and autoregressive moving averages to see how Kalman filters are different.

3
00:00:15,390 --> 00:00:25,670
Recall that with autoregressive moving averages, we must choose the lag number for autoregression and also the lag number for moving averages.

4
00:00:25,670 --> 00:00:38,980
Our choice of the lag parameters can affect how the model performs. What if, instead of trying to find the best lag parameters, we had a single state that represented all of the relevant information from the past?

5
00:00:38,979 --> 00:00:48,825
In other words, instead of choosing values for P and Q, what if we had a set of variables at T minus 1 to represent the past?

6
00:00:48,825 --> 00:00:59,250
I'll give you a hint, Kalman filters. Also, you may recall that financial data has a lot of noise relative to its useful signal information.

7
00:00:59,250 --> 00:01:06,359
It's often the case that we want to measure a specific thing that can only measure something else that's related.

8
00:01:06,359 --> 00:01:14,959
For example, we may wish to measure oil production levels but can only measure oil pipeline flows near the production sites.

9
00:01:14,959 --> 00:01:23,175
So, how do we make predictions when we have noisy indirect measurements? If you guessed Kalman filters, then you're right.

10
00:01:23,174 --> 00:01:30,395
When using Kalman filters, we can assume that the stock returns properties can be summarized by set of values.

11
00:01:30,394 --> 00:01:39,884
We call these set of values the state. Within the state of the time series, there is some hidden property that we can't measure directly.

12
00:01:39,885 --> 00:01:47,334
We can think of this hidden property as a smooth curve which represents the value of the stock return if there was no noise.

13
00:01:47,334 --> 00:01:54,530
On the other hand, where we actually measure the stock return includes this hidden state plus noise.

14
00:01:54,530 --> 00:02:04,245
So, what we have to work with is a more jagged curve with some randomness in it. The Kalman filter is designed to handle this kind of real life noisy data.

15
00:02:04,245 --> 00:02:15,375
The Kalman filter repeats the following steps in a loop. The first step is called the predict step, the second step is called the measurement update step.

16
00:02:15,375 --> 00:02:22,764
First, the Kalman filter predicts the hidden state or value of the stock return as a probability distribution.

17
00:02:22,764 --> 00:02:30,129
Next, it takes measurements such as the actual stock return data and then updates its belief about the hidden state.

18
00:02:30,129 --> 00:02:42,284
Note that the Kalman filter stores the relevant information in what's called the state. Also notice how the Kalman filter is dynamically updating its underlying model every time it performs a measurement.

19
00:02:42,284 --> 00:02:51,780
The Kalman filter uses both the previous time period's state and the measurement of the latest stock return to predict the next state.

20
00:02:51,780 --> 00:03:01,069
So all the relevant prior history of the time series is stored in the T minus 1 state, and there's no need to look at the earlier time periods.


@@@
1
00:00:00,000 --> 00:00:10,019
Next, we'll discuss the particle filter, a type of genetic algorithm that is also used for self-driving cars as well as time series.

2
00:00:10,019 --> 00:00:18,480
By genetic algorithm, I mean that we apply natural selection to improve our estimates. This will become more clear in a bit.

3
00:00:18,480 --> 00:00:29,795
Let's start with a thought experiment. Imagine that we can hire many little helpers, each with a certain view on where the stock returns are going based on market data.

4
00:00:29,795 --> 00:00:37,544
Each of these little helpers predict the stock return for the next day, and on the following day, you can see how correct they were.

5
00:00:37,545 --> 00:00:50,245
Each day, you pay more to the little helpers who are correct and pay less to the incorrect helpers. Over time, only the accurate helpers remain to make predictions.

6
00:00:50,244 --> 00:00:58,354
So by the process of natural selection, we find the helpers who are most accurate and average their predictions as your best estimate.

7
00:00:58,354 --> 00:01:05,629
Since this process looks like natural selection and evolution, particle filters are considered a type of genetic algorithm.

8
00:01:05,629 --> 00:01:13,025
The little helpers are called particles. These particles are individual models whose parameters are set randomly.

9
00:01:13,025 --> 00:01:19,599
When most particles make very similar predictions, this also shows more confidence in the average prediction.

10
00:01:19,599 --> 00:01:27,579
When there are significant changes in the data distribution at any point in time, the particles may make predictions that are more different from each other.

11
00:01:27,579 --> 00:01:34,239
In those cases, when the individual predictions are more spread out, we are less confident of the prediction.

12
00:01:34,239 --> 00:01:41,429
Particle filters are pretty good at handling a variety of data because they don't assume the data to be normally distributed.


@@@
1
00:00:00,000 --> 00:00:08,369
The last model we'll introduce here are recurrent neural networks, which are used in natural language processing as well as time series.

2
00:00:08,369 --> 00:00:20,535
Recurrent neural networks are as the name implies neural networks. Neural networks are models that can be thought of as many regressions stacked in series and in parallel.

3
00:00:20,535 --> 00:00:30,000
When I say the regressions are in a series, I mean that the output of one regression is fed in as the input to another regression in a chain.

4
00:00:30,000 --> 00:00:38,460
Also, when I say parallel, I mean that there are multiple regressions whose outputs are fed into another layer of regressions.

5
00:00:38,460 --> 00:00:50,894
A recurrent neural network is called recurrent because it takes some of its intermediate output and uses this as part of its input as it trains itself on incoming data.

6
00:00:50,895 --> 00:01:04,344
The recurrent neural network takes an input and outputs in prediction. The recurrent neural network also outputs an additional signal and intermediate output which it feeds back into itself.

7
00:01:04,344 --> 00:01:17,759
At the next time step when the RNN receives another input from the data source, it uses both the input as well as its previous intermediate output to help it calculate its next prediction.

8
00:01:17,760 --> 00:01:36,795
You can think of this signal as a way for the RNN to remember relevant information from the past. To train a recurrent neural network, you feed it lots of data from the past, then you see how well it predicts data from the future using a process called gradient descent to adjust the coefficients in the network.

9
00:01:36,795 --> 00:01:44,420
One commonly used version of recurrent neural network is made up of one or more long short-term memory cells.

10
00:01:44,420 --> 00:01:53,959
The long short-term memory cell, or LSTM cell consists of several neural networks each that perform a specific task.

11
00:01:53,959 --> 00:02:02,750
The LSTM cell can be thought of as an assembly line, and specific tasks are performed by different assembly line workers.

12
00:02:02,750 --> 00:02:11,305
The LSTM cell takes data as input and also takes its own signals that it generated from the previous period.

13
00:02:11,305 --> 00:02:21,514
The signal that it takes from its previous period can be thought of as its memory from the past. Some of the assembly line workers, remove some memories.

14
00:02:21,514 --> 00:02:31,584
Other assembly line workers add more memories based on incoming data. Still other assembly line workers decide what to output.

15
00:02:31,585 --> 00:02:39,370
Remember that the recurrent neural network has two kinds of outputs. One, is its prediction for the variable in question.

16
00:02:39,370 --> 00:02:49,470
The other is an intermediate output for its future self. You can think of this intermediate output as the memory that it wishes to pass on to the next time period.


@@@
1
00:00:00,430 --> 00:00:08,189
In this lesson, we've seen statistical methods such as regression, autoregression, and moving averages.


@@@
1
00:00:00,000 --> 00:00:08,015
Welcome back. In this lesson, we're going to tell you all about volatility. Volatility is an important measure of risk, but what is risk?

2
00:00:08,015 --> 00:00:21,619
We've talked a little about it already, and you may have an intuitive idea for what it means. It's basically uncertainty about the future and in particular, uncertainty about bad things happening, like losing all the money you've invested in something.

3
00:00:21,620 --> 00:00:30,074
In finance, risks can be thought of as uncertainty about future returns. Uncertainty, that sounds kind of hard to measure.

4
00:00:30,074 --> 00:00:40,640
How do you measure what you don't know? Well, we can get started by thinking of the log return as a random variable that can assume different values with different probabilities.

5
00:00:40,640 --> 00:00:57,315
An important way of characterizing a probability distribution is with it's expected or mean value. The mean is the average of many measurements of the random variable, and you already know that the standard deviation is a measure of the spread of a distribution.

6
00:00:57,314 --> 00:01:07,250
Volatility is simply the standard deviation of the probability distribution of log returns. It's a measure of the spread of this particular distribution.

7
00:01:07,250 --> 00:01:16,694
It measures the dispersion of log returns around the expected log return. Volatility gives you a sense of the range of values log returns are likely to fall into.

8
00:01:16,694 --> 00:01:32,689
For example, say you measured log returns on investment for several years and found that they happen to be distributed more or less normally with a mean or expected log return of about 5%, and an annual volatility of about 6%.

9
00:01:32,689 --> 00:01:43,590
If you remember, that for a normal distribution, about 95% of observations fall in a range from negative two to positive two standard deviations around the mean.

10
00:01:43,590 --> 00:01:52,534
This would mean that 95% of the annual returns would be between approximately negative 7% and positive 17%.

11
00:01:52,534 --> 00:02:07,790
So, why spend all this time learning about volatility? Basically, every investor trying to understand what level of return they can expect to get on average will also care about how often and by how much their returns are going to differ from that average.

12
00:02:07,790 --> 00:02:16,690
In essence, volatility is a simple metric that characterizes variability in returns, that has many different users all over the financial world.

13
00:02:16,689 --> 00:02:24,145
Let's list a few examples of the ways you might see it used. As I said, volatility is one way to measure risk.

14
00:02:24,145 --> 00:02:39,390
Traders can use volatility of a trading instrument to define position sizing in a portfolio. Volatility can be used in the design of alphas, that is, as part of the logic that defines which stocks to trade and when to trade them.

15
00:02:39,389 --> 00:02:47,835
Volatility is very important for determining the prices of options contracts. Finally, it's possible to trade volatility directly.

16
00:02:47,835 --> 00:02:56,069
For example, you can trade an instrument that tracks the volatility of the S&amp;P 500, we'll learn a little more about this later on.


@@@
1
00:00:00,000 --> 00:00:09,519
Say we want to get a sense of how our returns are going to vary in the future. That is to say, we want to estimate the volatility of a stock we've invested in.

2
00:00:09,519 --> 00:00:20,685
How do we do it? Well, the simplest way is to calculate the standard deviation of the log returns of some price data that we have for this stock, eg data from the past.

3
00:00:20,684 --> 00:00:33,765
This is called historical volatility. The first step is to calculate log returns from prices. The first log return datum will be undefined since there is no price datum from the time point proceeding it.

4
00:00:33,765 --> 00:00:46,030
So, if you start with n plus one price data, you will now have n log returns. Then, the formula for volatility is the formula for standard deviation, where the observations are log returns.

5
00:00:46,030 --> 00:00:55,744
One thing to keep in mind is that, since volatility is defined as the standard deviation, it treats log returns that are above and below the mean the same way.

6
00:00:55,744 --> 00:01:07,064
This is because in the calculation of standard deviation, price differences relative to the mean are squared, so that negative and positive differences are combined into one quantity.

7
00:01:07,064 --> 00:01:20,030
So, after doing these calculations for this small dataset, we'd get sigma equals 0.025. Since our dataset consisted of daily prices, this is the estimate of the daily volatility.

8
00:01:20,030 --> 00:01:28,350
Sometimes though, instead of having a price for every day, you might only have a price for every week, or a price for every month.

9
00:01:28,349 --> 00:01:37,094
If you take these data, calculate the log returns, and then calculate the standard deviation, you would get a different number for the volatility.

10
00:01:37,094 --> 00:01:52,060
For example, for stocks, the standard deviation of daily log returns is typically around 0.006 to 0.03, while the standard deviation of weekly log returns is typically around 0.01 to 0.07.

11
00:01:52,060 --> 00:02:01,500
If you think about it, this makes sense. This is just telling you that stock prices change more over the course of a week than over the course of a day.

12
00:02:01,500 --> 00:02:12,719
So weekly returns vary more widely than daily returns. So your volatility estimate depends on the time frequency of the underlying price data.

13
00:02:13,009 --> 00:02:20,784
But we want to be able to come up with a volatility number that we can compare across different situations and datasets.

14
00:02:20,784 --> 00:02:29,159
In order to do this, people typically calculate a value that corresponds to the standard deviation of annual log returns.

15
00:02:29,159 --> 00:02:42,910
If you don't have several years of data, and you can't calculate the standard deviation of annual log returns, you can calculate the standard deviation of log returns of another frequency and extrapolate it to an annual one.

16
00:02:42,909 --> 00:02:48,930
This is called calculating the annualized volatility. And we're going to talk about it in the next lesson.


@@@
1
00:00:00,000 --> 00:00:12,189
So, we know how to calculate volatility for a given period. This gives us one number to capture return variability for this period, but we also know that the market changes over time.

2
00:00:12,189 --> 00:00:20,210
We know the market goes up and down, and while sometimes it seems highly variable, other times it seems relatively placid.

3
00:00:20,210 --> 00:00:32,130
For example, during the last financial crisis in 2008, stock prices were extremely volatile. Whereas 2016 and 2017 have marked a relatively low volatility period.

4
00:00:32,130 --> 00:00:41,079
So, what do you do if you want to understand how volatility is changing over time? One option is to use a rolling window.

5
00:00:41,079 --> 00:00:50,894
So, if you wanted to estimate today's volatility, you would calculate the standard deviation of a set of log returns from sometime in the past up to yesterday.

6
00:00:50,895 --> 00:01:05,859
This is a practical method that is commonly implemented, but how long should the time window be? When you take a look at the volatility of previous days to estimate today's volatility, how important is the volatility from one day ago?

7
00:01:05,859 --> 00:01:17,599
How about from one week ago? Or how about from one month ago? A long window will mean that the value you compute may not react to changes in market conditions quickly enough.

8
00:01:17,599 --> 00:01:26,209
To short window may mean that the computed value becomes two variable over different window periods to be used reliably.

9
00:01:26,209 --> 00:01:43,450
Your choice of time window is going to depend on your application. In general, if your signal or strategy involves holding onto purchases for long periods, you can afford to use a longer time window for computation and vice versa for short holding periods.


@@@
1
00:00:00,000 --> 00:00:11,199
When we're predicting today's volatility, we may think that data from the past month are important, but what happened yesterday is even more important than what happened last week.

2
00:00:11,199 --> 00:00:25,210
This introduces another possibility for how to estimate volatility. What we can do to increase the influence of the more recent past is weight, log return observations from different days, differently.

3
00:00:25,210 --> 00:00:37,350
One common method is to weigh the observation from yesterday, the highest, the one from the day before a little less, the day before that a little less, and so on into the past.

4
00:00:37,350 --> 00:00:44,750
In fact, the weights decrease by the same percent with every step which means they decrease exponentially.

5
00:00:44,750 --> 00:00:51,500
As a result, this is called an exponentially weighted moving average. Let's see how to calculate it.

6
00:00:51,500 --> 00:01:01,304
Let's start with the formula for historical volatility. Let's imagine we're estimating today's volatility using a window of n past days.

7
00:01:01,304 --> 00:01:09,109
We'll add the subscript t to indicate that we are calculating today's volatility. Let's square both sides.

8
00:01:09,109 --> 00:01:17,290
Remember, Sigma squared is the variance but we can always take its square root to get the standard deviation i.e. the volatility.

9
00:01:17,290 --> 00:01:35,474
Now, let's make some simplifications. Let's assume that the mean log return is zero. This is okay to do because when you have log returns calculated over a short time interval, like daily log returns, their mean is generally small compared to their standard deviation.

10
00:01:35,474 --> 00:01:46,570
Now, let's change n minus one to n, this simplification won't influence the result much when n is large, and allows us to write the estimate of the variance this way.

11
00:01:46,569 --> 00:01:54,920
As you can see, this is just an average of the squared log returns in which each squared log return is weighted equally.

12
00:01:54,920 --> 00:02:03,995
Let's change the arithmetic average to a weighted average and make the weights decrease with each step into the past like we discussed earlier.

13
00:02:03,995 --> 00:02:17,334
We're going to introduce a new parameter and lets call it Lambda. Lambda is a number between zero and one, and it represents the factor by which the weights in the weighted average decrease as we go back in time.

14
00:02:17,335 --> 00:02:27,365
So, let's construct the sum. The first term will be yesterday's log return squared, multiplied by Lambda to the zeroth power which equals one.

15
00:02:27,365 --> 00:02:36,109
The next term will be Lambda times the log return of the day before squared. And the next term will have another factor of Lambda.

16
00:02:36,110 --> 00:02:47,830
This keeps going until we have a term for every log return in our window. Now, to make it a weighted average, we just have to divide by the sum of the weights, and that's it.

17
00:02:47,830 --> 00:02:54,634
That's the formula for an exponentially weighted, moving average estimate of the variance of the log returns.


@@@
1
00:00:00,000 --> 00:00:09,085
It would be great to know what will happen before it happens, wouldn't it? Traders sometimes try to forecast volatility in order to anticipate the market.

2
00:00:09,085 --> 00:00:15,974
The general thinking is that it's easier to predict volatility than price, volatility tends to be sticky.

3
00:00:15,974 --> 00:00:32,950
A volatile market trading day is often followed by another volatile one. When you're trying to model volatility, you can use a special form of the autoregressive model called ARCH, which stands for, autoregressive conditionally heteroscedastic.

4
00:00:32,950 --> 00:00:44,469
Okay, now those are some really big words, so let's break it down. Autoregressive simply means that the current value is somehow related to the recent past values.

5
00:00:44,469 --> 00:00:52,864
Heteroscedastic means that the variable we're trying to model may have different magnitudes of variability at different time points.

6
00:00:52,865 --> 00:01:10,715
The magnitude of variability is commonly measured using variance. Finally, conditional here, refers to a constraint we place in this model, which limits the heteroscedastic property to be conditionally dependent on the previous value or values of the variable.

7
00:01:10,715 --> 00:01:17,084
An arch model looks really similar to the formula we had for an exponentially weighted moving average.

8
00:01:17,084 --> 00:01:26,390
The current variance is a weighted sum of past squared log returns, but now we think of the weights as being parameters of a model.

9
00:01:26,390 --> 00:01:42,079
For example, in an arch one model, if rt represents the log return of a stock at time t, then we say that its variance is equal to alpha zero, plus alpha one, times r t minus one squared.

10
00:01:42,079 --> 00:01:57,744
Alpha zero and alpha one are the two parameters of the model. You can think about alpha zero as representing a baseline variance, and alpha one as waiting the contribution to the model of the log return at the previous time point.

11
00:01:57,745 --> 00:02:05,590
This can be extended to an arch m model, where the variance is conditionally dependent on m past timesteps.

12
00:02:05,590 --> 00:02:18,659
Note that here, the variance is being modeled only based on the squared log returns. We can also add terms to make the current variance dependent on previous estimates of the variance.

13
00:02:18,659 --> 00:02:27,015
To address this, you could modify the expression to include terms with previous variance values denoted as sigma squared.

14
00:02:27,014 --> 00:02:42,484
This is known as the GARCH, or generalized arch model. And you can parameterize it as GARCH m, n, where m is the number of log return terms, and n is the number of variants terms to include in the model.

15
00:02:42,485 --> 00:02:48,990
Once you're satisfied with a model you've built, it's time to think about how you might use it to make trading decisions.

16
00:02:48,990 --> 00:02:57,245
You may choose to generate buy, sell signals directly based on your predictions, or you may use it in an indirect way.

17
00:02:57,245 --> 00:03:14,120
You can analyze past signal returns and see if it has somehow correlated with volatility. For example, it's well known that certain strategies do better with higher volatility, and then you can choose to activate that strategy only during high volatility periods.


@@@
1
00:00:00,000 --> 00:00:12,390
At this point, you may be wondering, well, what causes volatility anyway? A natural assumption may be that volatility of a stock is caused by new information reaching the market.

2
00:00:12,390 --> 00:00:21,294
New information causes people to revise their opinions about the value of a stock. The price of the stock changes which causes volatility.

3
00:00:21,294 --> 00:00:41,855
However, research hasn't supported this conclusion. Research studies have compared the variance of stock price returns between the closing price on one day and the closing price on the next trading day, when there are no non-trading days in between, and the closing price on a Friday and the closing price on a Monday.

4
00:00:41,854 --> 00:00:50,280
You might expect the weekend variance to be three times the midweek variance because the real elapsed time is three times as long.

5
00:00:50,280 --> 00:01:00,710
However, this has not been found to be the case, even for assets affected by news that is equally likely to arrive midweek or on a weekend.

6
00:01:00,710 --> 00:01:10,585
The weekend variants of orange juice futures prices which are mainly affected by weather news is only about 1.5 times the midweek variance.

7
00:01:10,584 --> 00:01:23,614
The implication of all this is that volatility is, to a large extent, caused by trading itself. This idea is consistent with the observation that volatility and volume are frequently correlated.

8
00:01:23,614 --> 00:01:39,290
All other things being equal, greater trading volume means greater volatility. But during a time when volatility is high because lots of trading is happening and the price goes up, you might reasonably expect the price to go back down.

9
00:01:39,290 --> 00:01:45,129
If there was no news, the price probably didn't increase because the company's performance improved.

10
00:01:45,129 --> 00:01:55,055
It probably increased because people wanted to trade large numbers of stocks and sellers charged a premium for that ability to trade, or liquidity.

11
00:01:55,055 --> 00:02:09,854
This means that for equity strategies in high volatility times, a type of strategy considered more likely to do well is a type of strategy based on the idea that prices will return to their running mean when they go way up or down.

12
00:02:09,854 --> 00:02:17,479
These are called mean reversion strategies. In low volatility times, momentum strategies may work better.

13
00:02:17,479 --> 00:02:29,754
There are other general patterns in market behavior with respect to volatility. One is that market volatility tends to be high when the market is going down and low when the market is going up.

14
00:02:29,754 --> 00:02:42,349
Because of this, volatility is often quoted in the popular media as the gauge for fear. To see this most clearly, let's look at an index that essentially measures market volatility.

15
00:02:42,349 --> 00:02:51,530
You might have heard of the VIX Index, which is a market index measuring the volatility of 30-day S&amp;P 500 Index options.

16
00:02:51,530 --> 00:03:06,465
The value of the VIX index represents the annualized volatility of the S&amp;P 500 Index. This plot of the VIX Index and S&amp;P 500 Index over the same periods shows that the VIX spikes when the S&amp;P dips.

17
00:03:06,465 --> 00:03:19,129
For example, between 1990 and 2015, VIX spiked to a record 80 during October and November of 2008 when, as you remember, the markets were roiling from a huge crisis.

18
00:03:19,129 --> 00:03:26,885
I know what you're thinking, can you trade the VIX itself? You guessed it. Humans will turn anything into tradeable securities.

19
00:03:26,884 --> 00:03:35,539
You can trade futures and options on the VIX Index which essentially amounts to making a bet on the volatility of the S&amp;P 500.

20
00:03:35,539 --> 00:03:48,215
The VIX is published by the Chicago Board Options Exchange, which publishes a range of other volatility indices on stock and commodity indices currencies and some individual stocks.


@@@
1
00:00:00,000 --> 00:00:13,469
Volatility is a very general metric of variability in returns, so it's used in many ways. In this video, we're going to mention a few possible ways that might be used in trading, but there are many more possibilities.

2
00:00:13,470 --> 00:00:26,654
First, you can classify stocks as highly volatile or less volatile. Some strategies might work best on low volatility stocks, so you could use the volatility metric to limit your universe.

3
00:00:26,655 --> 00:00:37,119
For example, one type of strategy called a mean reverting strategy is to take a short position on an asset when it appreciates because you think it will depreciate again.

4
00:00:37,119 --> 00:00:49,560
If a stock has low volatility, you may think there is less uncertainty in its fair value. So, after a large deviation from its fair value, its price is more likely to revert.

5
00:00:49,560 --> 00:00:57,899
Another interesting observation about low volatility stocks is that they actually seem to perform well compared to high volatility stocks.

6
00:00:57,899 --> 00:01:10,430
In finance in general, greater risk is thought to accompany greater returns. So, the empirical observation that low volatility stocks outperform high-volatility stocks is considered a mystery.

7
00:01:10,430 --> 00:01:18,569
One explanation is that people ignore really boring things in favor of exciting stocks, like the tortoise and the hare.

8
00:01:18,569 --> 00:01:25,129
The low volatility tortoise is chugging along, but people are buying the high-volatility hare stocks.

9
00:01:25,129 --> 00:01:33,804
Some exchange traded funds specifically optimized for low volatility and seek to include low volatility stocks.

10
00:01:33,805 --> 00:01:42,289
We'll talk more about exchange traded funds in future lessons. Another way to use volatility is to normalize another metric.

11
00:01:42,290 --> 00:01:55,325
Let me explain what I mean by that. Say you have a momentum signal consisting of returns over the previous year, say both Netflix and Walmart have a return of 30%.

12
00:01:55,325 --> 00:02:04,155
This may seem to mean they're doing equally well, but in fact, Netflix is much more volatile, it can move 30% in a week.

13
00:02:04,155 --> 00:02:13,130
So, you could normalize your momentum signal by dividing by the volatility. The idea is to make the comparison apples to apples.

14
00:02:13,129 --> 00:02:24,159
This theme exists all over the place. If you want to compare any signal across your entire universe, you usually get some benefit by standardizing it in this way.

15
00:02:24,159 --> 00:02:41,430
You can also use volatility to help you determine position sizes. In general, quant traders utilize smaller position sizes in their strategies when markets are volatile to minimize the volatility of net profits, commonly called profit and loss, or P&amp;L.

16
00:02:41,430 --> 00:02:52,639
Moreover, the volatility of overall portfolio, P&amp;L, is often used as a gauge of the professional ability of portfolio managers managing multiple trading strategies.

17
00:02:52,639 --> 00:03:08,724
So, the ability to allocate capital, taking volatility into account becomes very important. If you believe you can forecast volatility with a certain level of confidence, you can adjust your portfolio sizing in anticipation of a pending storm in the market.

18
00:03:08,724 --> 00:03:18,060
But how would you decide how much money to move? Well, you can include volatility in a formula used to determine the size of your positions.

19
00:03:18,060 --> 00:03:24,639
Every trader might do this slightly differently, but as an example, a trader might use a formula like this.

20
00:03:24,639 --> 00:03:40,100
R divided by sigma times M times last close equals position size, where r is the dollar amount the trader is willing to lose if an M sigma event occurs against her or his position.

21
00:03:40,099 --> 00:03:59,594
Sigma is the annualized volatility of the security or strategy in question, M is a trader to find integer, last close is the last closing price of the security or the equivalent for a portfolio of stocks, and position size is the number of stocks to trade.

22
00:03:59,594 --> 00:04:15,469
This will lead to smaller position sizes for more expensive and more volatile assets. If you determine your position using a formula like this, then when you notice market conditions changing, you might decide to recalculate and adjust your position sizes accordingly.

23
00:04:15,469 --> 00:04:23,949
Sometimes trading strategies define thresholds at which to automatically sell a stock to take a guaranteed profit or limit losses.

24
00:04:23,949 --> 00:04:30,540
An upper threshold is called the take profit level, while a lower threshold is called a stop-loss level.

25
00:04:30,540 --> 00:04:45,753
These might be defined as a target price or as a percent change from an entry price. However, if market volatility increases, many stocks might reach these levels more frequently causing more frequent trades.


@@@
1
00:00:00,000 --> 00:00:13,350
Trading might use metrics availability to define trading signals. For example, you can take a fix length rolling window and calculate the mean and standard deviation of prices within that window.

2
00:00:13,349 --> 00:00:27,100
If we draw a line, two standard deviations above and below the rolling mean, we can get a sense of the trend and tendency to fluctuate of prices, these lines are called Bollinger Bands.

3
00:00:27,100 --> 00:00:44,975
We can use Bollinger Bands to generate trading signals. For example, we might think that if the price is below the lower band and starts to cross back inside towards the mean, that the price is still fairly low and on the rise, this might be a good time to buy.

4
00:00:44,975 --> 00:00:53,339
We might also think that if the price is above the upper band and starts to move back towards the mean, that this might be a good time to sell.

5
00:00:53,340 --> 00:01:01,310
This is one type of strategy you might use to take advantage of situations when the stock price continually returns to its running mean.

6
00:01:01,310 --> 00:01:13,694
Alternatively, when the price hits the upper band, you may think that is going to keep going that it's breaking out to take advantage of these situations, you can define a slightly different type of strategy.

7
00:01:13,694 --> 00:01:27,384
If the price of a stock is not very variable but continues to increase, you can get a situation where it keeps increasing but never hits the upper Bollinger Band defined by the standard deviation of the prices.

8
00:01:27,385 --> 00:01:41,234
Instead, you can use an upper band defined by the maximum value within a rolling window. If the price continues to increase, it will continue to reach new maxima and will hit that upper band.

9
00:01:41,234 --> 00:01:53,800
One strategy you might take defined by this signal is to enter a long position on the stock when it hits the rolling max and enter a short position when it hits the rolling min.


@@@
1
00:00:00,000 --> 00:00:12,464
Well done on finishing the lesson. In this lesson, we talked about a lot of different things. We gave you some key insights into volatility that will really help you construct trading strategies.

2
00:00:12,464 --> 00:00:20,239
Understanding volatility is essential to understanding the markets, and this lesson provides a solid foundation.


@@@
1
00:00:00,000 --> 00:00:07,480
In this lesson, we'll learn about two properties of financial assets, mean reversion, and co-integration.

2
00:00:07,480 --> 00:00:18,300
Based on these properties, we will learn about the trading strategy called, Paris trading. From there, we will examine the more general strategy called, mean reversion trading.

3
00:00:18,300 --> 00:00:31,475
First, let's preview some concepts. What is mean reversion? A mean reverting time series is one that tends to move back and forth around some constant value.

4
00:00:31,475 --> 00:00:39,185
When applying mean reversion to a single stock, we could check when the stock price deviates significantly from the mean.

5
00:00:39,185 --> 00:00:54,325
For example, we may see a stock decline significantly below its historical average. Assuming the stock will revert to its mean, we may buy the stock and anticipate that it will increase back to its original price.

6
00:00:54,325 --> 00:01:04,034
There is still the risk that the stock price does not return to its historical mean. For example, the stock may settle at its new, lower value.

7
00:01:04,034 --> 00:01:13,264
Even worse, it might just keep declining. In practice it's more common to apply mean reversion to a pair of stocks.

8
00:01:13,265 --> 00:01:28,385
Let's imagine that two companies are economically linked. By economically linked, I mean that they may be in the same line of business, may be operating in the same country, and may have similar types of products or customers.

9
00:01:28,385 --> 00:01:35,310
We can imagine that the stock prices of these two companies may also move up and down in a similar way.

10
00:01:35,310 --> 00:01:45,209
Since these companies move up together and down together, they appear to have a consistent relationship that we can expect will continue into the near future.

11
00:01:45,209 --> 00:01:56,885
So, when we see the prices of these two similar companies diverge significantly, we may make trading decisions based on the assumption that this divergence is temporary.

12
00:01:56,885 --> 00:02:05,060
The view of our trading strategy is that later in the future, the prices will settle back to their original relationship.


@@@
1
00:00:00,000 --> 00:00:07,169
Let's look at mean reversion in detail. We can plot a stock's price movements against its moving average.

2
00:00:07,169 --> 00:00:17,039
We tend to see that when the price increases or decreases significantly from this moving average, the price then reverts back towards this moving average.

3
00:00:17,039 --> 00:00:24,140
To describe a mean reverting process with some math, we'll use what's called the Drift and Volatility model.

4
00:00:24,140 --> 00:00:31,309
To guide our understanding, we'll imagine the stock movement as a boat that is floating passively in the ocean.

5
00:00:31,309 --> 00:00:39,469
The change in price of an asset over a period of time is a sum of a long-term average plus some randomness.

6
00:00:39,469 --> 00:00:46,554
This long-term average is called the drift term because it changes over time, but relatively slowly.

7
00:00:46,554 --> 00:00:59,500
Hence, the long-term average is like the ocean current that slowly and steadily carries the boat. The amount of drift depends upon the current price and the amount of time that has passed.

8
00:00:59,500 --> 00:01:12,795
The random part of the price change is called the Volatility Term. We can think of this volatility component as all the fish, dolphins or even whales that bump into the boat.

9
00:01:12,795 --> 00:01:26,015
All of these little bumps move the boat in a more random path around it stripped. The volatility term depends on the current price, a standard deviation and the amount of time that passes.

10
00:01:26,015 --> 00:01:33,130
The standard deviation is small when the markets are calm, so it's like encountering a small group of fish.

11
00:01:33,129 --> 00:01:41,245
The standard deviation may become large during a market crash. This would be like encountering a group of wales.

12
00:01:41,245 --> 00:01:57,219
With math, the Drift and Volatility model looks like this. The dp_t on the left represents the change in price over the time t. The first term to the right of the equal sign a is the drift term.

13
00:01:57,219 --> 00:02:08,104
The drift term is made up of the current price, a constant average mu, and the change in time. The second term on the right is the volatility term.

14
00:02:08,104 --> 00:02:19,159
The volatility term is made up of the current price, a standard deviation sigma, a random noise factor epsilon and the square root of the change in time.

15
00:02:19,159 --> 00:02:31,129
If this equation feels intimidating, don't worry. For now, just think about the boat drifting in the ocean, encountering fish, dolphins and maybe even some tropical islands.


@@@
1
00:00:00,000 --> 00:00:11,025
Let's take a closer look at pairs trade. When two companies have economic links, this implies that their stock prices may also move in a similar manner.

2
00:00:11,025 --> 00:00:28,204
For example, company A sells frozen green peas, and company B cells frozen carrots. Since frozen peas and carrots are often combined and sold together, we can see how an increase in sales of peas and carrots would affect both companies.

3
00:00:28,204 --> 00:00:35,859
Most of the time we see the two stocks moving similarly. However, we may see the two start to diverge.

4
00:00:35,859 --> 00:00:43,625
When this happens, we use our assumption about their economic ties, and also our assumption about mean-reversion.

5
00:00:43,625 --> 00:00:52,185
We assume that the divergence is temporary and that the two stock values will revert back to their original long-term relationship.

6
00:00:52,185 --> 00:01:03,765
The pairs trade involves going long the asset that is priced relatively low. At the same time, the strategy involves going short the asset that is priced relatively high.

7
00:01:03,765 --> 00:01:17,900
If and when the prices start to converge we can close our positions. When I say we close our positions, I mean that we sell the asset that we originally buy and we buy the asset that we initially shorted.

8
00:01:17,900 --> 00:01:28,875
Also, if we see the stocks do not converge but continue to diverge over a period of time, we may also decide to close our positions to cut our losses.

9
00:01:28,875 --> 00:01:37,219
You may be wondering how long we should wait before expecting to see the pairs converge. It helps to look at historical trends.

10
00:01:37,219 --> 00:01:50,700
Sometimes pairs of socks diverge and then converge in cycles that may last weeks or months. Some traders may decide to trade in shorter time horizons such as in days or even hours.

11
00:01:50,700 --> 00:01:59,145
For these shorter time periods traders may look for smaller movements to determine what they consider diverging, or converging trends.

12
00:01:59,144 --> 00:02:07,825
There are benefits to trading a pair rather than a single stock. Trading a pair reduces our exposure to the overall market.

13
00:02:07,825 --> 00:02:24,444
For example, with a single stock, we may expect a stock's recent price decline to be temporary. However, if a negative jobs report causes most stocks to decline then this overall market movement would carry this single stock downward with it.

14
00:02:24,444 --> 00:02:39,425
With partnering, we reduce some of the impact of large market movements. So, even if both stocks in the pair are pulled down by a general market downturn, what matters to us is the relative difference between the pair of stocks.

15
00:02:39,425 --> 00:02:46,250
To describe this relative difference between a pair of stocks we will look into the spread and the hedge ratio.

16
00:02:46,250 --> 00:02:55,819
While the pair of stocks are still converged or linked, we can imagine holding a long position in stock A and a short position in stock B.

17
00:02:55,819 --> 00:03:02,895
Our positions are in a certain proportion so that our combined position has a constant value over time.

18
00:03:02,895 --> 00:03:14,034
In other words, movements in our long position and short position cancel each other out. The proportion that lets us create this neutral position is called the hedge ratio.

19
00:03:14,034 --> 00:03:26,050
There are two ways to calculate the hedge ratio and they tend to have similar results. One way is to take the price of stock B divided by the price of stock A, or the price ratio.

20
00:03:26,050 --> 00:03:35,460
A second way is to run a linear regression in which stock A is the independent variable and stock B is the dependent variable.

21
00:03:35,460 --> 00:03:51,185
The coefficient of this regression is our hedge ratio and is similar to the price ratio. The difference is that the regression accounts for a series of previous prices while the price ratio uses just the most recent price of each stock.

22
00:03:51,185 --> 00:04:01,329
Once we have a hedge ratio we can calculate the spread. The spread is the price of B minus the product of the hedge ratio and price of A.

23
00:04:01,330 --> 00:04:15,694
This may look familiar from our studies of regression. When we used a regression model to estimate the dependent variable we compared the prediction with the actual value and called it a residual or error term.

24
00:04:15,694 --> 00:04:27,625
Here, we can think of using the hedge ratio to estimate the price of B using the price of A. We still expect a difference between the estimation and the actual price of B.


@@@
1
00:00:00,000 --> 00:00:10,509
To find good pairs to trade, we want to find pairs of companies that have economic links. It's common to find stocks that are in the same sector and country.

2
00:00:10,509 --> 00:00:21,920
It's also good to look for a less obvious but still meaningful connections. For instance, companies that trade internationally maybe affected by trade decisions by their respective governments.

3
00:00:21,920 --> 00:00:32,350
For example, steel companies in the United States and steel companies in China maybe influenced by political decisions made in either or both countries.

4
00:00:32,350 --> 00:00:42,064
Two companies may be suppliers and customers within the same supply chain. For instance, a textile supplier sells to a clothing retailer.

5
00:00:42,064 --> 00:00:49,614
A decline in clothing sales may impact the retailer first and then impact the textile supplier a bit later.

6
00:00:49,615 --> 00:01:02,605
Two similar companies maybe trading in separate countries and time zones. For example, a pharmaceutical company in India may see its stock rise due to strong international demand for its medicines.

7
00:01:02,604 --> 00:01:17,155
A few hours later, a similar pharmaceutical company in Germany may also see its stock rise. Notice in the last two examples, we see a lag between when one stock moves and the other moves in the same direction.

8
00:01:17,155 --> 00:01:34,069
A time lag is a good scenario to look for in pairs trading. If we see stock A move upwards and expect Stock B to move upwards later as well, then we can place a trade after Stock A moves, but before stock B moves.

9
00:01:34,069 --> 00:01:47,780
After Stock B moves as expected, we can close our positions. Once we found a pair of stocks based on economic links, we can check if those stocks are in fact good candidates for pairs trading.

10
00:01:47,780 --> 00:01:55,355
We check the pair by calculating the spread. If the spread appears constant over time, it's likely stationary.


@@@
1
00:00:00,000 --> 00:00:11,314
Let's add more detail to describe the relationship between a pair of stocks. Recall that some time series such as stock prices are integrated of order one.

2
00:00:11,314 --> 00:00:17,899
By this, I mean that if we take the time difference of this time series, we can get a stationary series.

3
00:00:17,899 --> 00:00:29,835
When a series is stationary, it is also integrated of order zero. Another way to get a stationery process is to find some linear combination of a pair of stocks.

4
00:00:29,835 --> 00:00:40,875
For example, if stock X and stock Y have an economic link, then if we subtract one from the other, we may get a time series that is stationary.

5
00:00:40,875 --> 00:00:52,829
More generally, we multiply X by hedge ratio using regression. Then we take Y minus the estimate of Y based on the hedge ratio and the value of X.

6
00:00:52,829 --> 00:01:08,555
This difference is the spread. If the spread is stationary, then it is integrated of order zero. Also if the spread is stationary, then we say that the original series of X and Y are cointegrated.

7
00:01:08,555 --> 00:01:18,099
The hedge ratio is also called the coefficient of cointegration. Note that cointegration is not the same as correlation.

8
00:01:18,099 --> 00:01:33,225
Correlation is the covariance re-scaled to range from negative one to positive one. A strong positive correlation means that when stock A moves up, stock B moves up at the same time and vice versa.

9
00:01:33,224 --> 00:01:42,594
However, if the magnitude of movement in one stock is greater than the other, then they can still drift apart even though they are correlated.

10
00:01:42,594 --> 00:01:55,845
In other words, two correlated stocks may not be cointegrated. Cointegration means that, over a range of days, the relative increase in A is matched by a relative increase in B.

11
00:01:55,844 --> 00:02:08,599
Let's imagine that we use $100 to buy stock A and $100 to buy stock B. Assuming these are cointegrated, the value of both positions would remain similar over time.

12
00:02:08,599 --> 00:02:20,129
So keep in mind that a pair of stocks may be correlated but not cointegrated. Similarly, a pair of stocks may be cointegrated but not correlated.

13
00:02:20,129 --> 00:02:32,165
For pairs trading, we want to find pairs of stocks that are cointegrated. The method for checking if two series are cointegrated is called the Engle-Granger Test.

14
00:02:32,164 --> 00:02:41,105
The Engle-Granger Test involves two steps that we saw previously. One, find the hedge ratio using regression.

15
00:02:41,104 --> 00:02:51,335
Two, calculate the spread and test if the spread is stationary. If the spread is stationary, then the two series are cointegrated.

16
00:02:51,335 --> 00:03:00,500
To check if the spread is stationary, we use the Augmented Dickey-FullerTest. The Augmented Dickey-Fuller Test outputs a P-value.

17
00:03:00,500 --> 00:03:12,669
If the P-value is small, say, 0.05 or less, then we can assume that the spread is stationary. Therefore, we can assume that the two are cointegrated.


@@@
1
00:00:00,000 --> 00:00:10,339
Let's see how we would find cointegrated pairs from a universe of stocks. For example, we might look at companies listed in the S&amp;P 500.

2
00:00:10,339 --> 00:00:20,839
If we were to compare every pair of stocks this would take a long time. More importantly, we would end up with spurious false positives just by chance.

3
00:00:20,839 --> 00:00:28,994
It's important to partition our universe of stocks into meaningful groups. One way is to group stocks by sector or industry.

4
00:00:28,995 --> 00:00:37,569
However, since grouping by sector is quite popular, we want to find other relationships that are not as obvious but still meaningful.

5
00:00:37,570 --> 00:00:45,939
One way to find meaningful but less obvious groups is to use a class of unsupervised machine learning algorithms called clustering.

6
00:00:45,939 --> 00:00:53,300
The inputs to the clustering algorithm are the time series themselves. More similar time series get grouped together.

7
00:00:53,299 --> 00:01:01,799
Note that after clustering we still want to analyze the fundamental relationships between pairs of stocks that exhibit co integration.


@@@
1
00:00:00,000 --> 00:00:09,625
It's also important to decide how we define when two stocks are diverging from there spread. For this, we want to choose a threshold.

2
00:00:09,625 --> 00:00:17,394
If the spread exceeds the threshold, then we can assume that the spread is temporary and will revert back to its historical mean.

3
00:00:17,394 --> 00:00:25,364
We can have two kinds of thresholds. One threshold, tells us that the spread is much wider than its historical average.

4
00:00:25,364 --> 00:00:36,835
Another threshold, can tell us that the spread is much narrower than its historical average. When the spread is unusually wide, the action we take is called shorting the spread.

5
00:00:36,835 --> 00:00:49,620
We say we're shorting the spread, because we expect the spread to get smaller in the future. Conversely, when the spread is unusually narrow, the action we take is called going long the spread.

6
00:00:49,619 --> 00:00:57,439
We say we're going long the spread or buying the spread because we expect the spread to get larger in the future.

7
00:00:57,439 --> 00:01:05,394
Let's discuss what it means to go long or short spread. When the spread widens, we decide to short the spread.

8
00:01:05,394 --> 00:01:16,250
This means, that we short the asset that has increased relative to the spread. We also buy the asset that has declined relative to the spread.

9
00:01:16,250 --> 00:01:25,950
Some of our previous examples involved shorting the spread. Conversely, when we see this spread narrow, we decided to go long the spread.

10
00:01:25,950 --> 00:01:35,689
This means that we buy the asset that has declined relative to the spread. We also short the asset that has increased relative to the spread.

11
00:01:35,689 --> 00:01:46,969
If this concept sounds a bit confusing, just to remember to buy low and sell high. In other words, buy when it's on sale, and sell when it's overpriced.

12
00:01:46,969 --> 00:01:55,100
The way we define these thresholds, is by calculating how many standard deviations the spread is from its historical average.

13
00:01:55,099 --> 00:02:05,655
This is called the Z-score of spread. A Z-score of positive one, means that a value is one standard deviation greater than its historical mean.

14
00:02:05,655 --> 00:02:17,120
To calculate the Z-score, we subtracted the mean and divide by the standard deviation. As with all modeling, is necessary to perform backtesting.

15
00:02:17,120 --> 00:02:28,129
By backtesting, I mean that we want to test our model on historical data. We want to test in a way that simulates the model making predictions in real time.

16
00:02:28,129 --> 00:02:41,659
We don't want to expose the model to data that occurs in the future. To perform backtesting, we divide our historical data into a training set, a validation set, and a test set.

17
00:02:41,659 --> 00:02:57,784
The training set has earlier data up to a cutoff date, the validation set has data that occurs after the training set up to and even later cutoff date, the test set contains data that occurs after the validation set.

18
00:02:57,784 --> 00:03:10,449
We train our model on the training set, then do intermediate checks using the validation set. This allows us to adjust our model to help it perform more accurate estimates.

19
00:03:10,449 --> 00:03:19,490
When we've adjusted our model to the point that we think it's ready to use, we then check it against the test set to see if it really works.

20
00:03:19,490 --> 00:03:34,530
Note that, it's important not to use the test set until we're done optimizing the model. If we were to adjust the model after using the final test set, this would be like letting our model cheat by showing it information about the future.


@@@
1
00:00:00,000 --> 00:00:07,934
Now that you've reached the end of this lesson, we hope you've gained a better understanding of pairs trading and mean reversion in general.

2
00:00:07,934 --> 00:00:33,600
Remember that mean reversion trading follows these general steps. Find a pair of assets or a pair of portfolios that have some economic link, compute the hedge ratio, use the hedge ratio to compute the spread, test whether the spread is stationary, if the spread is stationary, consider the pair a candidate for mean reversion trading.

3
00:00:33,600 --> 00:00:42,135
Next, choose thresholds for the spread. When the spread widens beyond its historical average, short the spread.

4
00:00:42,134 --> 00:00:52,009
When the spread narrows beyond its historical average, go along the spread. Most important of all, remember to backtest your model.


@@@
1
00:00:00,000 --> 00:00:09,285
Now that you made your way through the material, you have a solid foundation in researching a trading strategy which will help you with finding an alpha signal.

2
00:00:09,285 --> 00:00:19,390
The next step on your journey is to do Project 2 where you'll implement a breakout strategy. You'll take everything you learned from Project 1 to build this strategy, along with finding and filtering Outliers.

3
00:00:19,390 --> 00:00:26,530
Transitions are really easy. In transitioning from lessons to projects and back again, sometimes it's difficult for an entity students.

4
00:00:26,530 --> 00:00:36,515
That's true. I've gone through multiple Eudacity Nanodegree programs as a student, and I have to admit that starting a project was one of the loneliest parts of the process.

5
00:00:36,515 --> 00:00:44,824
I was a Eudacity student in the data analyst, machine learning and deep learning Nanodegrees. Every time I started a new project, I struggled.

6
00:00:44,825 --> 00:00:54,320
I didn't really know if I would ever finish. Yeah. I would spend hours alone in the library just begging my code to magically work, and searching the web and online forums for hints.

7
00:00:54,320 --> 00:01:02,280
So, there are definitely moments when I started to ask myself, "What am I doing here? Why am I doing this to myself?" That sounds really tough.

8
00:01:02,280 --> 00:01:14,060
How did you handle those moments? Well, whenever I felt like I couldn't bring myself to keep trying, I would watch online guest speakers or interviews by people who did something really cool in the field that I was studying.

9
00:01:14,060 --> 00:01:20,519
So, it helped to remind me why I was interested in the Nanodegree program to begin with, and helped me to keep going forward.

10
00:01:20,519 --> 00:01:31,799
Yeah. That's a really great idea. If you're ever feeling burnt out or tired from a day of debugging code or working on tough problems, take a few seconds to remind yourself of what initially brought you to this Nanodegree program.

11
00:01:31,799 --> 00:01:45,025
What do you want to do? Who do you hope to become? Watch some YouTube videos about this innovative field in which you're growing, maybe throw in a few cat and puppy videos along the way, and when you feel that urge to move forward with your learning, dive back into your program.


@@@
1
00:00:00,000 --> 00:00:09,899
Starting open-ended projects like some of the projects in this nanodegree program or even like the project of learning a whole new subject can feel bewildering.

2
00:00:09,900 --> 00:00:21,719
For example, starting the project of earning a PhD will certainly bewildering to me. It can be hard to proceed when you don't know what parts of the information you're trying to learn will be important in the future.

3
00:00:21,719 --> 00:00:27,750
Frankly, there's an infinite amount of information in the world and any part of it could lead to interesting places.

4
00:00:27,750 --> 00:00:38,429
That's one of the reasons why we try in this nanodegree program, to give you context for why you're learning things, how they're relevant to your goals and how they'll be useful in the future.

5
00:00:38,429 --> 00:00:45,280
Also, with unstructured problems, you may not really know when you have found the answer and are done.

6
00:00:45,280 --> 00:00:55,480
When I worked on unstructured problems during my PhD, it felt like trying to climb a set of stairs in the dark, feeling around with my foot until I found a stair to step onto.

7
00:00:55,479 --> 00:01:05,310
What was helpful to me in this type of situation was to let goals be my guide. So, if I was trying to build something, I would know I was done when it was built and it worked.

8
00:01:05,310 --> 00:01:15,245
If I was trying to analyze some data, it was important to decide what question I was trying to answer, and so I would know I was done when I had an answer to that question.

9
00:01:15,245 --> 00:01:25,295
Sometimes, the goals are obvious, like finish a course, but sometimes you have to decide what the goals are, like when you have to decide what questions you're trying to answer.

10
00:01:25,295 --> 00:01:38,209
We've provided you with some goals in your projects in the form of rubric criteria to help you direct your efforts, but bigger picture, deciding what you are trying to achieve can give you a way to measure your progress.

11
00:01:38,209 --> 00:01:51,299
So, now that you've got some substantial progress under your belt in this big project that is your nanodegree program, and growth journey that that's a part of, how are you feeling about the goals you've set out to achieve?

12
00:01:51,299 --> 00:01:59,439
Recognize the headway that you've made so far, in terms of what you've learned, reasoned through and created, you're making great progress.


@@@
1
00:00:00,000 --> 00:00:08,769
Hi there. These next few lessons we'll give you a broad overview of mutual funds, hedge funds, and exchange traded funds.

2
00:00:08,769 --> 00:00:19,589
The second half will be a deep dive into portfolio optimization, which is really exciting because it will help you build the mathematical skill sets that you'll want to have as a quant.

3
00:00:19,589 --> 00:00:28,134
Now, I have to admit that math tends to scare me sometimes. Well actually, okay, math always scares me a lot.

4
00:00:28,135 --> 00:00:37,204
But math is also very cool. When you learn the math, its like gaining superpowers that let you extract meaning from the chaos of numbers.

5
00:00:37,204 --> 00:00:49,429
With that said, we'd still like to acknowledge that diving into the math can still feel scary or overwhelming, especially when you feel like it's just you alone in a library or a coffee shop.

6
00:00:49,429 --> 00:00:57,049
I've been through several Eudacity Nano degrees as a student, and I also felt like sometimes it was just me and my computer in a library.

7
00:00:57,049 --> 00:01:03,290
I'd like to share a bit of advice for getting through the more challenging material in an online learning environment.

8
00:01:03,289 --> 00:01:10,024
What really helped me get through some of the more challenging subjects was reaching out online to interact with my classmates.

9
00:01:10,025 --> 00:01:20,554
I'd either ask questions when I was stuck, or tried to answer other student questions. I would also post curriculum suggestions, and sometimes I'd get a response from someone at Eudacity.

10
00:01:20,555 --> 00:01:29,924
Whether your online chats directly answered the question you're trying to solve, they help you by building a community that supports you, and also motivates you.

11
00:01:29,924 --> 00:01:43,174
It's one of the important side benefits that you've already experienced when you take in-person classes, because you're surrounded by other people with shared goals and shared experiences, because they have empathy for you and your struggles.

12
00:01:43,174 --> 00:01:50,954
In the Eudacity classroom, your classmates and mentors are there with you too just connected by modern technology.

13
00:01:50,954 --> 00:02:02,049
Also, we want to remind you that there's a whole team of people at Eudacity, many of whom may not necessarily be on camera, that are working to support you on your journey.

14
00:02:02,049 --> 00:02:10,000
We encourage you to regularly connect with your fellow classmates so that you can support each other on your journey through the program.


@@@
1
00:00:00,000 --> 00:00:10,150
Welcome to this module, and congratulations on getting through the previous modules and projects. In this lesson, we'll learn about Stocks, Indices and Funds.

2
00:00:10,150 --> 00:00:21,299
We'll cover how Indices are defined and categorized. We'll also learn about market capitalization and price-weighted indices, and walk through an example of constructing an index.

3
00:00:21,300 --> 00:00:31,200
Then, we'll learn about funds, such as hedge funds and mutual funds. We'll cover active versus passive fund management as well as smart beta.

4
00:00:31,199 --> 00:00:38,850
We'll also learn about open-end and close-end mutual funds which will prepare us for the next lesson about exchange traded funds.

5
00:00:38,850 --> 00:00:45,884
In later lessons of this module we'll calculate portfolio risk and return and study optimal risky portfolios.

6
00:00:45,884 --> 00:00:55,329
Finally, we'll cover portfolio optimization which will equip you with the skills to construct portfolios with a goal of maximizing returns and minimizing risk.


@@@
1
00:00:00,000 --> 00:00:07,835
Let's first define the terms Equity, Stocks and Shares. What does it mean when an investor buys stocks.

2
00:00:07,835 --> 00:00:21,990
When you get a new job and the company pays you in shares what does that mean? Equity, is the value of a company's assets minus it's liabilities, is the net value of the company after counting for everything in the company owes.

3
00:00:21,989 --> 00:00:31,844
Such as debt, payments to suppliers, or employee wages. A company's equity, is divided into stock certificates that investors can buy.

4
00:00:31,844 --> 00:00:39,640
These can be called shares of the company, or stock in the company. Stock and shares pretty much refer to the same concept.

5
00:00:39,640 --> 00:00:50,449
When you say you own stock, people know you're referring to shares in one or more companies. When you say you own shares, you normally specify shares, any specific company.

6
00:00:50,450 --> 00:00:59,554
So, when an investors purchase stocks, or if employees receive shares from their employers, they're receiving partial ownership of a company's equity.

7
00:00:59,554 --> 00:01:17,689
Now, let's say you're trying to follow how well the overall markets are doing. So, you look at a bunch of stocks and you may see that Google was down yesterday but up today, while IBM was up yesterday and then down today, and Amazon was up both yesterday and today.

8
00:01:17,689 --> 00:01:34,039
How do you consolidate all this information into a single number in a way that's meaningful? As you'll see next, we can use an index to help us track not only these companies, but also hundreds and even thousands of companies using a single number.


@@@
1
00:00:00,000 --> 00:00:08,894
What is an index? If you're listening to financial news about the U.S. markets, you may hear about the S&amp;P or the Dow.

2
00:00:08,894 --> 00:00:20,704
For news about Latin America, there is IBOVESPA or MERVAL. For markets in Asia, there's the NIKKEI or HANG SENG, and in Europe, there's the FTSE or EURO STOXX.

3
00:00:20,704 --> 00:00:32,670
These are all examples of indices. Indices are created by financial research, and credit rating companies such as Standard and Poor's, Dow Jones and Financial Times.

4
00:00:32,670 --> 00:00:43,390
They're also published by stock exchanges such as the Nasdaq or the London Stock Exchange. Indices can also be created by banks such as Hang Seng bank.

5
00:00:43,390 --> 00:00:53,459
Business news will often refer to whether the index went up or down. For instance, we may hear that the Dow went up by 200 points since yesterday.

6
00:00:53,460 --> 00:01:06,409
If the Dow was at 25,000 yesterday, that means it is now 25,200 points today. The 200-point increase, a bounce to an increase of about 0.8 percent.

7
00:01:06,409 --> 00:01:19,269
An index is a metric that represents the aggregated value of a group of stocks as a single number. Note that you can refer to more than one index as indexes or indices.


@@@
1
00:00:00,000 --> 00:00:08,410
An important point to keep in mind is that an index is a virtual portfolio, and not an actual fund that people invest in.

2
00:00:08,410 --> 00:00:17,144
We can think of an index has a fantasy football team in which sports fans pick their favorite players and put them in a virtual team.

3
00:00:17,144 --> 00:00:26,074
We can track the performance of this artificially constructed team by combining the performance metrics such as number of points scored for each player.

4
00:00:26,074 --> 00:00:37,484
But keep in mind that the team only exists in paper not in real life. There's no team owner, no coach and the members in the virtual team don't actually practice together.

5
00:00:37,484 --> 00:00:52,469
Similarly, an index represents the aggregated performance of a group of stocks. But they're not funds they're not managed by professional money managers, and investors don't pay index creators to invest their money in the indices.


@@@
1
00:00:00,000 --> 00:00:13,144
Indices may track stocks within a specific stock exchange or country. For example the CSI 300 includes stocks that are listed on the Shanghai and Shenzhen Stock Exchanges in China.

2
00:00:13,144 --> 00:00:26,769
Stocks can also be related by sector for example the Nasdaq 100 technology index or Nasdaq financial 100 index track the performance of stocks in their respective sectors.

3
00:00:26,769 --> 00:00:33,975
Indices are important to investors because at a glance we can quickly check the status of various markets.

4
00:00:33,975 --> 00:00:50,070
In a way it's checking the pulse of a region or sector to evaluate the health of that market. Professional investment managers also use indices as benchmarks that they either tried to match or outperform with their own portfolios.


@@@
1
00:00:00,000 --> 00:00:10,710
So far we've seen some major indices that track a large collection of stocks, from big to small with different sectors such as technology or banking.

2
00:00:10,710 --> 00:00:19,734
What if we wanted a more refined look at just the smaller companies or what if we wanted to track just high-growth companies.

3
00:00:19,734 --> 00:00:27,955
Having a more specific index would help if our investment research is focused on companies with similar characteristics.

4
00:00:27,954 --> 00:00:37,990
Index creators often make indices that group stocks by market capitalization defined as large-cap, mid-cap or small-cap.

5
00:00:37,990 --> 00:00:46,424
Market capitalization or market cap of a company is its number of shares outstanding times the price per share.


@@@
1
00:00:00,000 --> 00:00:10,224
Indices can also include stocks based on whether companies are considered growth or value stocks. So, what's a growth stock, and what's a value stock?

2
00:00:10,224 --> 00:00:29,850
A growth stock tends to have high growth in sales or earnings and potential for future growth. For example, if a company creates new ways for people to make purchases with bitcoin or new ways for people to commute to work, it might be considered a growth stock because of its potential for increased future sales.

3
00:00:29,850 --> 00:00:46,399
A value stock tends to be a mature company that has stable sales, revenue, and earnings. For example, a company that sells household and bathroom products may have a stable flow of earnings, but has fewer expectations for rapid growth.


@@@
1
00:00:00,000 --> 00:00:13,540
So, how do we define whether a company is growth or value? We can look at valuation metrics such as price to earnings ratio, price to sales ratio, and price to book ratio.

2
00:00:13,539 --> 00:00:24,964
Then, within a list of companies in an index, we can rank these stocks by these metrics and say that a stock is more on one end of the spectrum relative to the other stocks.

3
00:00:24,964 --> 00:00:32,620
The price to earnings ratio, is the stock price divided by the company's earnings per share over the past four quarters.

4
00:00:32,619 --> 00:00:39,155
The price to sales ratio, is the stock price divided by the sales per share over the past four quarters.

5
00:00:39,155 --> 00:00:50,665
The price to book ratio, is the stock price divided by the book value per share. The book value is the company's accounting value which is assets minus liabilities.

6
00:00:50,664 --> 00:00:59,689
The book value differs from the market value, which we've been referring to as the market capitalization, which varies as the stock price changes.

7
00:00:59,689 --> 00:01:13,669
Growth stocks tend to have high price to earnings, price to sales and price to book ratios. Conversely, value stocks tend to have lower price to Earnings, price to sales, and price to book ratios.

8
00:01:13,668 --> 00:01:28,670
Note that these ratios tend to vary by sector and industry. Generally speaking, software and biotechnology stocks have higher PE ratios, while agriculture and construction companies tend to have lower PE ratios.


@@@
1
00:00:00,000 --> 00:00:08,330
Indices may use both the market cap and the growth versus the value classification, to decide which stocks to include.

2
00:00:08,330 --> 00:00:20,769
So as an example, the S&amp;P devised a list of stocks into the S&amp;P 500, S&amp;P MidCap 400 and the S&amp;P SmallCap 600.

3
00:00:20,769 --> 00:00:34,730
The S&amp;P 500 contains large-cap stocks such as Lockheed Martin, an aerospace company. The S&amp;P MidCap 400 contains stocks such as Delfi and automotive technology company.

4
00:00:34,729 --> 00:00:52,189
The S&amp;P SmallCap 600 contains a small-cap stocks such as The New York Times. Within each of these indices, the stocks are then ranked as more growth or more value according to price-to-earnings, price-to-sales, for price-to-book value ratios.

5
00:00:52,189 --> 00:01:13,129
These rankings are used to create more subgroups defined as either growth or value. For example, from the S&amp;P SmallCap 600 list, stocks are selected for the S&amp;P SmallCap 600 growth index or the S&amp;P SmallCap 600 Value Index.


@@@
1
00:00:00,000 --> 00:00:17,410
So, let's say an Index like the Nikkei 225, tracks the price movements of 225 stocks. To get a single Index number, we could add up the single share price of all of the 225 stocks, and there's your index number.

2
00:00:17,410 --> 00:00:24,530
That's more or less what the Nikkei 225 Index does, and this is called Price Weighting, or Equal Weighting.

3
00:00:24,530 --> 00:00:34,730
The Dow Jones Index is also a Price-Weighted Index. Note, that Index creators usually rescale the number, but we'll see that later in the lesson.


@@@
1
00:00:00,000 --> 00:00:07,040
We can also take a weighted average of the stocks in a way that does not treat all the stocks the same.

2
00:00:07,040 --> 00:00:20,795
A sensible choice for weights is to give more weight to stocks of bigger companies rather than smaller companies because a larger company's price moving has a greater effect on the overall change in the stock market.

3
00:00:20,795 --> 00:00:30,679
This is called market cap weighting. Remember, that market capitalization of a company is the number of shares times the market price per share.

4
00:00:30,679 --> 00:00:39,884
So, for example, the Wilshire 5000 Index is market cap weighted and contains both Nvidia and iRobot.

5
00:00:39,884 --> 00:01:00,295
Nvidia has a market cap of about 160 billion. While, iRobot has a market cap of about 1.8 billion. This means that a one percent change in the price of Nvidia will affect the Wilshire 5000 Index more than a one percent change in the price of iRobot.

6
00:01:00,295 --> 00:01:12,869
Major indices that are market-capitalization weighted are the S&amp;P 500, Ibovespa, MERVAL, Hang Seng, FTSE and EURO STOXX.


@@@
1
00:00:00,000 --> 00:00:09,370
You may wonder what happens to an index if one of its listed companies goes bankrupt, is acquired, or merges with another company.

2
00:00:09,369 --> 00:00:20,670
For example, in June of 2018, Monsanto, a US based agriculture company was acquired by Bayer, a pharmaceutical company based in Germany.

3
00:00:20,670 --> 00:00:33,074
Monsanto was listed in the S&amp;P 500, but due to the acquisition, Standard &amp; Poor's removed Monsanto from the S&amp;P 500, which is called an Index delete.

4
00:00:33,075 --> 00:00:45,500
Standard &amp; Poor's also added Twitter to the S&amp;P 500, which is called an Index add. When the list of companies in an index changes, the index needs to be rebalanced.

5
00:00:45,500 --> 00:00:53,885
In other words, the weights applies to each stock must be recalculated based on the new total market cap of all stocks in the index.

6
00:00:53,884 --> 00:01:05,780
Now, we'll cover how an index is constructed and calculated next. Industry balancing can be scheduled at predetermined intervals, such as every month, every quarter, or every year.

7
00:01:05,780 --> 00:01:15,069
An index may also be rebalanced on the day of a major event such as during a merger, acquisition, privatization, or bankruptcy.


@@@
1
00:00:00,000 --> 00:00:19,065
Let's see how an index is constructed and updated to see where the index number comes from. The Hang Seng Index was created by a banker, Stan Lee Kuan at Hang Seng Bank in 1969, and tracks the 50 largest companies that are listed on the Hong Kong Stock Exchange.

2
00:00:19,065 --> 00:00:26,925
The index selected the 50 largest companies that were traded on the Hong Kong Stock Exchange and added up their market capitalization.

3
00:00:26,925 --> 00:00:37,035
Then, they set that sum equal to 100 points for the index. So the index began in 1969 with the number 100.

4
00:00:37,034 --> 00:00:44,574
Note that we will refer to the sum of market caps of all these 50 companies as the total market cap.

5
00:00:44,575 --> 00:00:55,250
Each trading day, the percent change of the total market cap is multiplied to the previous day's index, to calculate the index for that day.

6
00:00:55,250 --> 00:01:03,265
For example, let's pretend the total market cap on the very first day was 100 billion Hong Kong dollars.

7
00:01:03,265 --> 00:01:15,204
The following day, let's say the total market cap changed to 102 billion. Recall that we can get the percent change by taking the current day divided by the previous day.

8
00:01:15,204 --> 00:01:35,670
So in this case, 102 billion divided by 100 billion is 1.02 or a 2% increase. If we multiply the first day's index of 100 points by 1.02, the second day's index value is now 102 points.


@@@
1
00:00:00,000 --> 00:00:09,509
Note that the Hang Seng Index is a capped free float adjusted market cap weighted index. Okay, that was a lot of words.

2
00:00:09,509 --> 00:00:25,399
Let's start with free float adjusted market cap, in particular the free float part. When we measure the market cap of a company, we take all outstanding shares held by investors and multiply by the price per share.

3
00:00:25,399 --> 00:00:36,605
However, some of these shares are not readily available in the market because some stockholders are legally prevented from selling their shares during a lock-up period.

4
00:00:36,604 --> 00:00:49,640
For example, employees who are paid in their company's shares may not be allowed to sell their shares a few days before the company's quarterly earnings report or for a year after an initial public offering.

5
00:00:49,640 --> 00:01:03,115
By using only their readily tradable or free float shares to calculate market cap, we can get a better measure of how the market is moving, since the non-free float shares can't be bought or sold at that time.

6
00:01:03,115 --> 00:01:13,730
So in practice, an index will estimate what fraction of shares are free float and use that in this calculation of free float shares from day to day.

7
00:01:13,730 --> 00:01:30,200
For example, let's say about 90 percent of $0.10 outstanding shares are free float. If today, we see that $0.10 has 10 billion shares outstanding, we'll estimate that nine billion of those are free float.


@@@
1
00:00:00,000 --> 00:00:11,359
Going back to the issue of index deletes and index adds, when Monsanto was removed from the SMP 500 and replaced by Twitter, what happened to the index?

2
00:00:11,359 --> 00:00:21,439
Was it reset back to 100? Is there's some fancy math to update the index? Well in fact, we use the same math as we did before.

3
00:00:21,440 --> 00:00:30,845
Prior to the replacement Monsanto was worth about $50 billion. On the day of replacement Twitter was worth about $30 billion.

4
00:00:30,844 --> 00:00:37,890
Let's make up a fake company to serve as a placeholder for Monsanto before, and Twitter after the change.

5
00:00:37,890 --> 00:00:50,390
Let's call this fake company, MonsanTwitter. Between the previous day and the current day, our placeholder MonsanTwitter changed its market cap from $50 billion to $30 billion.

6
00:00:50,390 --> 00:00:59,130
We calculate the percent change of the total market cap as usual and apply that to the previous index to get the current day's index.


@@@
1
00:00:00,000 --> 00:00:12,169
Now, let's define an investment fund. An investment fund is a collection of investor money that a professional money manager allocates toward a portfolio of assets.

2
00:00:12,169 --> 00:00:20,629
So, what's the benefit of investing in a portfolio of stocks? Why not just pick your favorite 10 stocks and watch them grow?

3
00:00:20,629 --> 00:00:34,375
Well, portfolio's benefit from diversification enable investors to get the same level of return with lower risk or similarly more return for the same level of risk.

4
00:00:34,375 --> 00:00:45,994
In effect, a portfolio let's you optimize your Sharpe ratio, which is the measure of how much you're rewarded for buying risky assets divided by the risk of that portfolio.

5
00:00:45,994 --> 00:00:58,789
You'll learn more about the Sharpe ratio in a later lesson, but for now, just remember that a smart combination of assets together can give you better returns or less risk than a single stock can achieve.

6
00:00:58,789 --> 00:01:11,375
You may notice that there are many mutual funds to choose from. Each of these target a certain subset of assets, have specific investment strategies and also aim for a certain level of risk.

7
00:01:11,375 --> 00:01:22,269
Investors can choose different funds based on their level of risk tolerance, the amount of fees charged by the fund and also based on the fund's past performance.


@@@
1
00:00:00,000 --> 00:00:12,074
Let's learn about active fund management versus passive fund management. When the goal of a fund is to outperform an index, it's called an actively managed fund.

2
00:00:12,074 --> 00:00:21,375
For example the Fidelity Contrafund targets large cap growth stocks and seeks to outperform the S and P 500 index.

3
00:00:21,375 --> 00:00:33,340
When the goal is to match the performance of an index, it's called a passively managed fund. A passive fund usually chooses an index as a benchmark that it tries to track.

4
00:00:33,340 --> 00:00:44,479
So they're often called index funds. The fund managers do this by buying the same stocks that are listed by the index and in the same proportion as the index.

5
00:00:44,479 --> 00:00:52,594
Note that an index fund is not an index. It's a fund that attempts to match the performance of an index.

6
00:00:52,594 --> 00:01:05,329
For example: The Vanguard 500 index Investor Fund is a fund that allocates investor money towards a portfolio that tries to match the S and P 500 index.


@@@
1
00:00:00,000 --> 00:00:08,529
Note that Active Funds, are sometimes called Alpha Funds. While Passive Funds are sometimes called Beta Funds.

2
00:00:08,529 --> 00:00:18,464
So, where do the names alpha and beta come from? Well, alpha and beta refer to the intercept and coefficient of a regression line.

3
00:00:18,464 --> 00:00:26,515
In particular, this is the regression of a portfolio's return against the markets return in excess of the risk-free rate.

4
00:00:26,515 --> 00:00:42,844
In general, a portfolio's excess return, is the portfolio's return minus the risk-free rate. A beta coefficient of one means that a two percent increase in the market, will also result in a two percent increase in the portfolio.

5
00:00:42,844 --> 00:00:53,664
An intercept or alpha of one means that the portfolio has a one percent return that exists, even if the market's excess return is zero.

6
00:00:53,664 --> 00:01:00,324
You'll get more details about this particular regression line in a later lesson about portfolio optimization.

7
00:01:00,325 --> 00:01:09,830
But the goal of an active fund, is to generate excess returns above the market's excess return. Hence, is called an Alpha Fund.

8
00:01:09,829 --> 00:01:18,730
The goal of a passive fund is to closely match the returns of the market that is tracking. So, it's called a Beta Fund.


@@@
1
00:00:00,000 --> 00:00:14,240
So, here's the question, can a fund only be active versus passive, only alpha or beta? Well, if you're at a restaurant, do you have to choose between soup or salad?

2
00:00:14,240 --> 00:00:25,815
Does your beverage have to be either milk or tea? What if you wanted a little bit of both? In fact, you can order a half soup and a half salad combo.

3
00:00:25,815 --> 00:00:33,984
You can also order a milk tea. Similarly, a portfolio can combine both active and passive management.

4
00:00:33,984 --> 00:00:47,890
This is often referred to as Smart Beta. Generally speaking, the idea is to start with the weights based on an existing index and use that as a starting point from which we can try to make improvements.

5
00:00:47,890 --> 00:00:59,040
So, for instance, if we can nudge the weights a little up or down from their original values, then we can aim to either improve returns or reduce risk.

6
00:00:59,039 --> 00:01:06,530
We call these Smart Beta portfolios because they're beta portfolios that have some alpha sprinkled on top.


@@@
1
00:00:00,000 --> 00:00:09,004
Now, that we've had an overview of funds. Let's discuss two general types of funds: mutual funds and hedge funds.

2
00:00:09,005 --> 00:00:18,515
Mutual funds are available to everyday investors and have restrictions on certain strategies, such as shorting stocks or using derivatives.

3
00:00:18,515 --> 00:00:27,789
So, they're usually long only, which means fund managers buy and hold stocks that they expect will perform well in the future.

4
00:00:27,789 --> 00:00:39,954
Investors are allowed to put money into or pull money out of most mutual funds on any business day. In other words, there's no lock-up periods that prevent investors from withdrawing.

5
00:00:39,954 --> 00:00:51,870
Some popular mutual funds include the Vanguard Equity Income Fund, the T. Rowe Price Blue Chip Growth Fund, and the BlackRock Technology Opportunities Fund.


@@@
1
00:00:00,000 --> 00:00:12,285
Then, there are hedge funds. Hedge funds have fewer restrictions on their trading strategies, which allow them to take short positions and also use derivatives such as options or futures.

2
00:00:12,285 --> 00:00:19,425
Hedge funds typically take money from high-net-worth individuals or institutions such as pension funds.

3
00:00:19,425 --> 00:00:29,564
Hedge funds generally require a higher minimum investment, and may require lockup periods during which investors are not allowed to withdraw their investments.

4
00:00:29,564 --> 00:00:47,810
So, here's a question for you to ponder. Why are hedge funds called hedge funds? Is it A, hedge funds are named after hedgehogs, who play in the hedges of a rose garden or B, hedge funds are named after hedging strategies designed to limit portfolio risk?


@@@
1
00:00:00,000 --> 00:00:14,120
There are two ways that funds are evaluated on their performance, relative versus absolute returns. Relative returns refer to how the fund compares to a benchmark, which is usually an index.

2
00:00:14,119 --> 00:00:30,269
Mutual funds, both active and passive, are evaluated relative to their benchmark. Let's say for example, that an actively managed fund chooses to focus on a portfolio of stocks that are also listed in the S&amp;P 500 index.

3
00:00:30,269 --> 00:00:44,934
If the S&amp;P 500 has an annual return of two percent and the fund returns three percent, then the fund outperformed its benchmark by one percent, and we say that the relative return was one percent.

4
00:00:44,935 --> 00:00:58,750
Since this is an actively managed fund, we also refer to this relative return as the active return. For a passively managed fund, the performance is based on how closely the fund attracts index.

5
00:00:58,750 --> 00:01:09,674
For example, if the S&amp;P 500 had an annual return of two percent, and so did the passively managed fund, then the fund is meeting its investment objective.

6
00:01:09,674 --> 00:01:19,530
The relative return is zero, and since this is a passively managed fund, we say that the fund has a zero percent tracking error.


@@@
1
00:00:00,000 --> 00:00:13,394
Hedge funds are usually evaluated by absolute returns. Absolute return funds use the cash interest rate such as the US three-month Treasury rate or LIBOR as the benchmark.

2
00:00:13,394 --> 00:00:27,000
Since the cash rate is relatively small, it doesn't fluctuate much compared to the equity markets. Absolute return funds are measured by their returns themselves, not the difference between the fund and the benchmark returns.

3
00:00:27,000 --> 00:00:37,929
The average return of hedge funds in 2017 was 8.5 percent, and 5.4 percent in 2016, according to the Hedge Fund Research group.


@@@
1
00:00:00,000 --> 00:00:13,179
Hedge funds employ hedging strategies, in an attempt to deliver a market neutral returns. By market neutral, I mean that the portfolio's gains or losses are less affected by overall market movements.

2
00:00:13,179 --> 00:00:21,089
Hedging generally means entering into a transaction in order to reduce exposures to price fluctuations.

3
00:00:21,089 --> 00:00:33,640
This is done by buying derivatives such as options or futures. For instance, let's say we hold 100 shares of Daimler, the owner of car brands such as Mercedes-Benz.

4
00:00:33,640 --> 00:00:42,695
We want to ensure against losses if the stock price of Daimler drops. So, we buy 100 put options on shares in Daimler.

5
00:00:42,695 --> 00:00:50,620
As an example, each put option gives us the option to sell one Daimler share at a fixed price of 60 euros.

6
00:00:50,619 --> 00:01:01,510
So, one would be a good time to exercise that right to sell. Well, if the market price of Daimler drops below 60 euros such as 50 euros.

7
00:01:01,509 --> 00:01:09,670
We could buy Daimler stock at 50 euros per share. Use the put option to sell the same stock at 60 euros.

8
00:01:09,670 --> 00:01:20,309
We could keep that 10 euros per share as profit. This could cancel out the 10-euro per share loss that we would incur on our existing 100 shares.

9
00:01:20,310 --> 00:01:31,000
Hedging helps money managers devise investment strategies that are market neutral, which means that gains or losses are less sensitive to movements in the overall market.


@@@
1
00:00:00,000 --> 00:00:10,060
When an investor puts money into a fund, they receive shares in the fund which increase or decrease in value as the funds portfolio value goes up or down.

2
00:00:10,060 --> 00:00:24,960
The share represents a fraction of the funds fair value. To get the fair value of a fund's share, we start with the value of its investments, also called its Assets Under Management, or AUM for short.

3
00:00:24,960 --> 00:00:35,879
We take the AUM minus expenses, these expenses are the costs of running the fund such as employee salaries, transaction costs, and taxes.

4
00:00:35,880 --> 00:00:47,865
We then take the net value of the fund and divide by the number of shares in the fund. The fair value per share that we get is called Net Asset Value or NAV.


@@@
1
00:00:00,000 --> 00:00:09,780
Usually, the expenses associated with the fund are defined as a fraction of assets under management, which is called the gross expense ratio.

2
00:00:09,779 --> 00:00:17,129
Sometimes, when funds are just starting out and trying to attract investors, they give discounts to their new customers.

3
00:00:17,129 --> 00:00:24,764
So, these customers actually pay less than the gross expense ratio. This is called the net expense ratio.

4
00:00:24,765 --> 00:00:34,130
Note however, that discounts are usually not permanent. So, the gross expense ratio is a better indicator of what investors will pay in the long run.


@@@
1
00:00:00,000 --> 00:00:10,484
Mutual funds receive investor money, which they invest in a portfolio of stocks or other assets. In exchange, investors receive shares in the fund.

2
00:00:10,484 --> 00:00:18,359
For example, when a fund starts, investors may pay $100 for each share of the fund that they receive.

3
00:00:18,359 --> 00:00:32,879
There are two types of mutual funds; open-end funds and closed-end funds. Open-end mutual funds, allow investors to buy into the fund, after the fund has already started operating and investing.

4
00:00:32,880 --> 00:00:44,760
Open-end funds, also let existing investors withdraw their money from the fund directly. When people want to invest, the open-end fund creates new shares for the investors to buy.

5
00:00:44,759 --> 00:00:53,885
When investors wish to withdraw their money, they turn in their shares and the fund returns the cash value of those shares back to the investors.

6
00:00:53,884 --> 00:01:01,395
The fund also removes these shares from his total shares outstanding. Let's walk through an example.

7
00:01:01,395 --> 00:01:11,269
Let's pretend a new open-end fund initially has 10 investors and each investor buys one share of the fund at $100 per share.

8
00:01:11,269 --> 00:01:22,459
So, the fund takes this $1,000 to invest in a portfolio of stocks. We say it has $1,000 in assets under management or AUM.

9
00:01:22,459 --> 00:01:32,254
A month later, let's say the portfolio's value has increased to $1,200. So, each of the ten shares is worth $120.

10
00:01:32,254 --> 00:01:46,459
A new investor wants to buy a share of the fund. So, they pay $120 for a share and not 100. In other words, the new investor pays for the current price of the share.


@@@
1
00:00:00,000 --> 00:00:07,794
Note that open-end mutual funds must keep part of their investments in cash, in case investors wish to redeem their shares.

2
00:00:07,794 --> 00:00:18,625
For example, let's say an open-end fund has issued 10 shares and the shares are worth $100 each, one investor wishes to return their shares for cash.

3
00:00:18,625 --> 00:00:26,809
The investor gets their $100 in cash and the fund has nine shares outstanding, with investments valued at $900.

4
00:00:26,809 --> 00:00:35,129
In anticipation of potential withdrawals, the fund has kept some of its $1,000 in cash and not invested in stocks.


@@@
1
00:00:00,000 --> 00:00:06,955
Holding cash means that the portfolio cannot make use of all of its assets in the given investment strategy.

2
00:00:06,955 --> 00:00:17,969
Also, if many investors wish to redeem their investments at the same time, the fund may need to sell their existing investments to get cash and increase liquidity.

3
00:00:17,969 --> 00:00:28,774
These issues may reduce the overall potential return of the fund. To see this, imagine you invested a million dollars into an open-end mutual fund.

4
00:00:28,774 --> 00:00:35,370
The fund allocates half of this investments into a portfolio of equities, earning a 10 percent return.

5
00:00:35,369 --> 00:00:46,164
The fund keeps the other half of its investments in cash, earning two percent. What do you think will be the total rate of return on your million-dollar investment?

6
00:00:46,164 --> 00:01:02,325
Well, first of all, congratulations on having a million dollars to invest. Second of all, you'll probably guess that your rate of return will be less than the 10 percent return on the portfolio of equities because only half of your money is allocated to it.

7
00:01:02,325 --> 00:01:12,420
The return on the fund is a weighted average of the active investments and the cash. In other words, a half of 10 percent is 5.

8
00:01:12,420 --> 00:01:28,000
A half of 2 percent is 1. 5 plus 1 is 6 percent. So, the total return of the fund is 6 percent, which is less than the 10 percent return of the equities.


@@@
1
00:00:00,000 --> 00:00:12,425
Because the need to maintain cash tends to erode the total return of the open-end fund, the financial services industry created closed-end funds to address this issue.

2
00:00:12,425 --> 00:00:25,035
Closed-end mutual funds are closed ended because they accept investor money when the fund is initially created and don't create new shares or handled withdrawals afterwards.

3
00:00:25,035 --> 00:00:37,149
By making the fund closed ended, the portfolio managers no longer have to keep cash on hand and no longer have to liquidate existing positions to satisfy redemption requests.

4
00:00:37,149 --> 00:00:46,759
This also means that money managers can use all of the cash towards their investment strategies, potentially improving the overall return of the fund.

5
00:00:46,759 --> 00:00:57,670
So, what do investors do if they wish to stop investing in the fund and get their cash back? Investors can sell their shares to other investors.

6
00:00:57,670 --> 00:01:05,260
For investors, shares of closed-end funds can be bought and sold on the stock exchange just like a stock.

7
00:01:05,260 --> 00:01:11,659
This means that investors do not need to wait for the mutual fund to process their share redemption.

8
00:01:11,659 --> 00:01:26,194
However, the price of closed-end funds is determined by the supply and demand of that funds shares on an exchange, and often will not precisely reflect the fair market value of its underlying portfolio.

9
00:01:26,194 --> 00:01:35,644
In other words, closed-end funds often trade at a discount or premium to the net asset value other underlying holdings.

10
00:01:35,644 --> 00:01:44,870
In a later lesson, we'll see how exchange traded funds are designed to improve upon both open-end and closed-end funds.


@@@
1
00:00:00,000 --> 00:00:07,625
An important consideration for any investor or a portfolio manager is the cost of buying and selling stocks.

2
00:00:07,625 --> 00:00:16,689
These are called transaction costs. Transaction costs can be commissions paid to brokers or bargain-making investment banks.

3
00:00:16,690 --> 00:00:24,445
Transaction costs can also be the costs of moving the market price by trading a large block of shares.

4
00:00:24,445 --> 00:00:31,815
Large institutional investors are primarily concerned with moving the market price by making large trades.

5
00:00:31,815 --> 00:00:45,670
For example, if a fund wants to buy a stock that it considers underpriced, a large purchase of many shares will push up the market price of that stock making the original purchase decision less attractive.

6
00:00:45,670 --> 00:00:57,465
Usually, traders will break up the order into smaller pieces and make the purchases with different sellers on different exchanges in order to prevent these sellers from raising their prices.

7
00:00:57,465 --> 00:01:04,370
Portfolio managers need to consider how different strategies affect their overall transaction costs.

8
00:01:04,370 --> 00:01:19,600
For instance, if a strategy requires buying or selling based on daily price movements, this will incur more transaction costs compared to another strategy that triggers trading based on quarterly financial reports.

9
00:01:19,599 --> 00:01:27,569
When a portfolio manager decides to adjust their investment allocation, this adjustment is called portfolio rebalancing.

10
00:01:27,569 --> 00:01:40,609
When rebalancing a portfolio, a portfolio manager may decided that due to transaction costs it makes more sense to skip trades for which the transaction costs outweigh the benefits of rebalancing.

11
00:01:40,609 --> 00:01:53,070
Moreover, for large institutional investors that manage multiple funds, they can trade internally between funds that are within the institution, thereby reducing the cost of rebalancing.


@@@
1
00:00:00,000 --> 00:00:10,905
In this lesson, we learned about indices, mutual funds, and hedge funds. We learned how indices may either be market cap-weighted or price-weighted.

2
00:00:10,904 --> 00:00:21,484
We also saw an example of how to construct an index and update its value from one day to the next. We learned about mutual funds and hedge funds.

3
00:00:21,484 --> 00:00:32,005
We discussed the concept of actively managed funds versus passively managed funds. We also introduced the idea of Alpha, Beta, and Spark Beta.

4
00:00:32,005 --> 00:00:41,625
Moreover, we defined a fund's relative returns as the fund's performance relative to its benchmark, which is usually an index.

5
00:00:41,625 --> 00:00:54,185
We also learned about open-end mutual funds and closed-end mutual funds. We saw how open-end funds need to handle customer withdrawals and therefore hold cash reserves.

6
00:00:54,185 --> 00:01:07,509
We also saw how closed-end funds are designed to address this issue. We also pointed out that the market price of a closed-end fund may trade at a premium or a discount to its fair value.

7
00:01:07,510 --> 00:01:16,594
In the next lesson, you will build upon these concepts and learn how exchange-traded funds are designed to improve upon mutual funds.


@@@
1
00:00:00,000 --> 00:00:08,495
In this lesson, we'll introduce Exchange Traded Funds or ETFs, and get an understanding of how ETFs work.

2
00:00:08,494 --> 00:00:20,859
ETFs are considered a significant innovation in finance. One might say that ETFs did for the financial services industry, what smartphones did for the tech industry.

3
00:00:20,859 --> 00:00:32,899
As the introduction of ETFs led to evermore products that customers just couldn't get enough of. The world's first ETF was created by the Toronto Stock Exchange in Canada in 1990.

4
00:00:32,899 --> 00:00:43,530
The market value of ETFs in the US market grew from $100 billion in the early 2000s to $3.4 trillion by 2017.

5
00:00:43,530 --> 00:00:58,740
Globally there are about $4.5 trillion invested in ETFs. Broadly speaking, as ETFs have gained popularity due to their low fees and accessibility, they've been taking market share from traditional mutual funds.

6
00:00:58,740 --> 00:01:10,620
ETFs have lower fees because they have lower operational costs, and their transactions are structured in a way that can avoid some of the taxes that traditional mutual funds normally pay.

7
00:01:10,620 --> 00:01:21,055
The lower operational costs are usually due to passive investment management, which requires less frequent trading and therefore lower transaction costs.


@@@
1
00:00:00,000 --> 00:00:14,420
Recall that there are some shortcomings in Open-End and Closed-End Mutual funds. Open-End Mutual Funds may need to maintain parts of their assets under management in cash to let investors withdraw on any given day.

2
00:00:14,419 --> 00:00:26,945
This dilutes the funds overall performance. Also, Open-End Mutual Funds may limit the number of times that you can invest or withdraw within a certain time period, such as 90 days.

3
00:00:26,945 --> 00:00:37,734
Moreover, the price you get for withdrawing is based on the closing price at the end of that day, not the best price at the time that you initiate the transaction.

4
00:00:37,734 --> 00:00:49,129
Closed-End Mutual Funds addressed this by making their shares tradeable in the stock market. But the market value of these shares may diverge from the value of the funds portfolio.

5
00:00:49,130 --> 00:01:00,784
Is it possible to do better? Can we set up a fund that does not need to hold a reserve of cash and whose market value matches is portfolio? Yes we can.

6
00:01:00,784 --> 00:01:09,245
We can do better than mutual funds with ETFs. ETFs like mutual funds, allow us to invest in a portfolio of stocks.

7
00:01:09,245 --> 00:01:18,369
Thereby benefiting from diversification. Even better, ETFs shares can be traded on a stock exchange as if their stocks themselves.

8
00:01:18,370 --> 00:01:27,620
Plus, the market price of an ETF closely follows the value of the underlying portfolio. We'll see why later in this lesson.


@@@
1
00:00:00,000 --> 00:00:12,109
There are some common ways that investors use ETFs, and these include investments in commodities or international stocks, and also using ETFs for hedging.

2
00:00:12,109 --> 00:00:20,855
Investing in commodities is easier with ETFs. Commodities are raw materials that are interchangeable and tradable.

3
00:00:20,855 --> 00:00:33,809
By this, I mean that a commodity such as a barrel of oil can be exchanged for another barrel of oil, whereas a handmade tailored wedding dress cannot be swapped out for another wedding dress.

4
00:00:33,810 --> 00:00:44,954
Commodities include energy, such as oil and natural gas; precious metals, such as gold and silver; and agriculture, such as corn and even cows.

5
00:00:44,954 --> 00:00:58,600
If you wanted to bet that the price of gold will increase, you could buy gold from a jewelry store and hide it under your bed, then if the price of gold increased as you expected, you could sell it for a profit.


@@@
1
00:00:00,000 --> 00:00:16,429
For most investors, it's easier to buy Futures contracts that are tied to commodities. Buying a futures contract is an agreement between two parties to buy or sell an asset at a future date and at a fixed price.

2
00:00:16,429 --> 00:00:26,640
If you trade futures directly, remember to mark your calendar, because you'll need to either close the position or roll it over by the time the contract is due.

3
00:00:26,640 --> 00:00:41,805
Let me explain what it means to close the position in a futures contract. Let's say you enter into a futures contract with a farmer to buy one metric ton of cocoa, at a fixed price, at a fixed date six months from now.

4
00:00:41,804 --> 00:00:58,689
Since you're buying you are long cocoa futures. The farmer who is selling is short cocoa futures. At that six month date, you're required to pay cash and the farmer is required to send you one ton of cocoa.

5
00:00:58,689 --> 00:01:11,704
If you want to close your long position, then before the due date you should enter into a short position to sell one metric ton of cocoa at a fixed price and on the same due date.

6
00:01:11,704 --> 00:01:22,250
There's a commodities exchange that handles the paperwork. But, it's as if you paid for a ton of cocoa, then immediately sold that cocoa to someone else to get your money back.

7
00:01:22,250 --> 00:01:31,260
So, that was how to close a futures position. Rolling over contract is like renewing your gym membership for another six months.

8
00:01:31,260 --> 00:01:47,669
To roll over a futures contract, you start by canceling the contract when it's near the due date. Next, you enter into a new contract to buy cocoa at an even later date, such as six months after the original contract due date.


@@@
1
00:00:00,000 --> 00:00:06,674
If you don't cancel or roll over your futures contract by the due date, you're obligated to buy the asset.

2
00:00:06,674 --> 00:00:16,050
Whether it's a barrel of oil or a flock of sheep. So, imagine that you have bought multiple gold futures contracts, that have different due dates.

3
00:00:16,050 --> 00:00:30,329
You'll have to set up a lot of reminders on your calendar, to help you remember when to roll over or cancel each contract Fortunately, there are ETFs that track commodities by buying and rolling over futures contracts for you.

4
00:00:30,329 --> 00:00:39,600
You can buy shares of a gold ETF and hold it however long you wish and save yourself the hassle of managing individual futures contracts.


@@@
1
00:00:00,000 --> 00:00:11,044
Investing internationally is easier with ETFs. Normally, if you wanted to trade stocks in another country, you might set up a brokerage account with a firm in that country.

2
00:00:11,044 --> 00:00:18,984
Also, you would be trading when markets are open in that country's Stock Exchange, which is tough if the time zones are different from yours.

3
00:00:18,984 --> 00:00:30,015
For example, let's say you are in New York and making trading decisions for shares in Tencent or China Construction Bank which are listed in the Hong Kong Stock Exchange.

4
00:00:30,015 --> 00:00:37,229
Since the time zones are far apart, you would be spending your late evenings until early mornings managing your trades.

5
00:00:37,229 --> 00:00:49,955
What if you like having a regular sleep schedule as well as a social life? Well, financial firms create ETFs for international stocks and sell these ETF shares in the local US Stock Exchange.

6
00:00:49,954 --> 00:01:08,560
For example, BlackRock issues the iShares China Large-Cap ETF, which is called FXI for short. The FXI tracks the 50 largest stocks that are traded in the Hong Kong Stock Exchange, including Tencent and China Construction Bank.

7
00:01:08,560 --> 00:01:20,159
So, you can trade at the FXI on the New York Stock Exchange during the day, then at night you can get some well-deserved sleep or party with your friends.


@@@
1
00:00:00,000 --> 00:00:13,185
Now, let's discuss how investors can use ETFs for hedging. Recall that hedging generally means entering into a transaction in order to reduce exposure to price fluctuations.

2
00:00:13,185 --> 00:00:27,789
Let's see how hedging works with ETFs. If you have a portfolio that holds many stocks in the S&amp;P 500, you can take some short positions in an ETF that tracks the S&amp;P 500.

3
00:00:27,789 --> 00:00:41,875
The most popular is the SPDR S&amp;P 500 ETF, which is issued by State Street Global Advisors. Note that it's spelled S-P-D-R and pronounced spider.

4
00:00:41,875 --> 00:00:55,134
When the overall market moves down, your portfolio goes down with it. However, your short positions in the SPDR ETF gain at the same time limiting the impact of the market downturn.

5
00:00:55,134 --> 00:01:07,700
Note that the reverse happens when the market goes up. Your long investments will go up with a market, while your short positions in the ETF will lose reducing your net gain.

6
00:01:07,700 --> 00:01:16,910
Active fund managers may also short ETFs in order to reduce their portfolio's exposure to certain sectors, industries, or regions.

7
00:01:16,909 --> 00:01:32,799
For example, a hedge fund whose portfolio includes tech stocks may wish to short an ETF, such as the Fidelity MSCI Information Tech ETF, in order to cancel out some of the price fluctuations in the tech industry.


@@@
1
00:00:00,000 --> 00:00:11,059
Financial institutions that issue ETFs are called ETF sponsors. ETF sponsors could be banks or institutional investment firms.

2
00:00:11,060 --> 00:00:21,719
The largest ETF sponsor is BlackRock's iShares. iShares was created by Barclays' Global Investors, which in turn was acquired by BlackRock.

3
00:00:21,719 --> 00:00:34,570
As of early 2018, iShares had more than $1.75 trillion of assets under management, which made up more than one-fourth of BlackRock's total AUM.

4
00:00:34,570 --> 00:00:49,244
The other major players are Vanguard, State Street Global Advisors, Deutsche Bank, and Investgo. ETF sponsors, like mutual funds, earn fees as a percentage of assets under management.

5
00:00:49,244 --> 00:01:08,155
That is, if an ETF sponsor is managing one billion dollars in investments, while charging 0.1 percent in fees annually, then the ETF sponsor is earning 0.1 percent times one billion dollars or one million dollars per year.

6
00:01:08,155 --> 00:01:21,419
ETF fees are much smaller percentages compared to the fees charged by mutual funds and hedge funds. But the large volume of assets invested make ETFs very profitable for ETF sponsors.

7
00:01:21,420 --> 00:01:28,295
The ETF sponsor designs and maintains the ETF portfolio, so you can think of them as the fund manager.


@@@
1
00:00:00,000 --> 00:00:14,345
Now, here's where ETFs start to look different from mutual funds. Recall that with mutual funds, investors give their cash to a mutual fund and the mutual fund then invest that cash in a portfolio of stocks.

2
00:00:14,345 --> 00:00:25,214
With ETFs, the ETF sponsor only makes transactions with special partners called authorized participants or APs for short.

3
00:00:25,214 --> 00:00:40,414
Major APs include Merrill Lynch, Morgan Stanley, Goldman Sachs and Fortis Bank. An AP goes to the stock market and buys stocks that match the portfolio defined by the ETF sponsor.

4
00:00:40,414 --> 00:00:51,789
Then, the AP makes a trade with the sponsor. The sponsor creates ETF shares and gives those shares to the AP in exchange for the bundle of stocks.

5
00:00:51,789 --> 00:01:04,385
Notice that the sponsor issues ETF shares similar to what a mutual fund does. The difference is that the sponsor only deals with a set of APs who are financial institutions.

6
00:01:04,385 --> 00:01:19,349
Also, unlike a mutual fund, the ETF sponsor doesn't receive cash in exchange for its shares. Instead, the sponsor receives a bundle of stocks that matched the portfolio designed by the sponsor.

7
00:01:19,349 --> 00:01:29,840
Okay, so what do APs do with their ETF shares? Also, so far, the investors haven't been involved in any of the transactions.

8
00:01:29,840 --> 00:01:42,544
How do investors get in on some of the action and own some ETF shares for themselves? Well, the APs go to the stock exchange and sell their ETF shares on the open market.

9
00:01:42,545 --> 00:01:51,450
Investors can buy these shares and now have investments that are linked to all of the stocks that are in the ETF portfolio.

10
00:01:51,450 --> 00:02:00,314
What we've described so far is called the create process. In summary, the create process involves these steps.

11
00:02:00,314 --> 00:02:11,289
The AP buys stocks and gives them to the ETF sponsor. The ETF sponsor creates ETF shares and gives them to the AP.

12
00:02:11,289 --> 00:02:24,020
The AP then sells the ETF shares to investors. This is called the create process because it effectively creates more ETF shares that are available on the stock exchange.


@@@
1
00:00:00,000 --> 00:00:07,109
If investors hold ETF shares and want their money back, they just sell their shares on the stock exchange.

2
00:00:07,110 --> 00:00:19,739
There's no need for them to interact with ETF sponsor. For reasons we'll see soon, an AP may decide to redeem ETF shares for the original portfolio of stocks.

3
00:00:19,739 --> 00:00:27,594
This is essentially reversing the original ETF share creation process and is called, the redeem process.

4
00:00:27,594 --> 00:00:40,964
So, the AP buys ETF shares from investors in the stock market. Then, the AP trades the ETF shares with the ETF sponsor in exchange for the original stocks.

5
00:00:40,965 --> 00:00:54,884
Then, the AP sells these stocks on the stock exchange. This is called the redeem process, because the AP redeems the original basket of stocks by trading in the ETF shares.


@@@
1
00:00:00,000 --> 00:00:10,740
Recall earlier that we mentioned how ETF sponsors are more tax efficient and that their transactions are not taxed as much compared to traditional mutual funds.

2
00:00:10,740 --> 00:00:24,629
One reason is that ETF sponsors do not deal directly in cash. During ETF share creation, an ETF sponsor creates shares and receives a bundle of stocks of equal value.

3
00:00:24,629 --> 00:00:33,585
Similarly during the redemption process the sponsor gives a bundle of stocks in exchange for ETF shares of equal value.

4
00:00:33,585 --> 00:00:44,140
Since the two items being exchanged are of the same value there's no capital gain that is recorded. This is important because capital gains are taxed by the government.

5
00:00:44,140 --> 00:01:09,259
We can contrast this with an open-end mutual fund. If an investor requests to withdraw their investment the mutual fund may need to sell some stocks for cash to give to the investor the capital gains taxes on the stock sales add to the operational costs of the fund and these costs are reflected in the higher fees that the mutual fund charges investors.


@@@
1
00:00:00,000 --> 00:00:07,919
You may be wondering why an AP would bother creating or redeeming ETF shares. After all, what's in it for them?

2
00:00:07,919 --> 00:00:16,939
Do they charge a percentage fee the way an ETF sponsor does? No, but they do find other ways to make the transaction worth their while.

3
00:00:16,940 --> 00:00:26,479
This benefits investors too, because the actions of the AP helped to align ETF share prices with the stocks that they represent.

4
00:00:26,480 --> 00:00:40,594
Before we go into how this happens, let's review the concept of arbitrage. Arbitrage is the act of buying and selling assets that should be of the same value but that are priced differently in the market.

5
00:00:40,594 --> 00:00:54,429
When an investor finds a price difference between two assets that should be interchangeable, then they can buy the cheaper one and simultaneously sell the more expensive one and make a profit on the difference.


@@@
1
00:00:00,000 --> 00:00:06,195
Hi, there. Do you remember Betty from module one? She used to sell fruit at the local farmers market.

2
00:00:06,195 --> 00:00:14,445
Hi, Betty. What are you up to these days? Hi, Eddy. Well, I've been working as an investment analyst at a new hedge fund Roulant and partners.

3
00:00:14,445 --> 00:00:22,179
Well, that's cool. What brings you to the farmers market? Well, I like to check out the real-time pricing and trading that goes on in the farmer's market.

4
00:00:22,179 --> 00:00:27,510
It reminds me of the trading that goes on in the financial markets. That's an interesting connection.

5
00:00:27,510 --> 00:00:37,904
I noticed that one of the farmers here, Brock cells oranges for $1 each while another farmer Cindy sells a bag of 10 oranges for $9.

6
00:00:37,905 --> 00:00:52,310
I was wondering how you look at this pressing from the view of a hedge fund analyst. Well, I could buy 10 individual oranges from Brock at $1 each or I could buy from Cindy and pay $9, but still receive 10 oranges.

7
00:00:52,310 --> 00:00:57,740
I've also notice that there are tons of customers walking around who want to buy an orange for a dollar.

8
00:00:57,740 --> 00:01:06,370
This looks like an opportunity for arbitrage. Can you walk me through what you would do? Sure. In general, I like to buy low and sell high.

9
00:01:06,370 --> 00:01:15,015
So, I'd like to buy a bag of 10 oranges for $9 and then sell them for a dollar each. I also want to make sure that I don't get stuck holding the bag.

10
00:01:15,015 --> 00:01:21,814
How would you avoid getting stuck holding a bag of oranges? Well, first I'll try to find 10 customers who want an orange.

11
00:01:21,814 --> 00:01:29,005
At the same time I'll make an agreement with Cindy to buy the bag. So, you're buying and selling at the same time.

12
00:01:29,004 --> 00:01:37,489
Exactly, and I earned a dollar profit by using arbitrage. So, now the question is what will you do with your $1 in profit?


@@@
1
00:00:00,000 --> 00:00:09,559
Let's see how an AP might seek out arbitrage opportunities with ETFs, and how that results in stable pricing of ETF shares.

2
00:00:09,560 --> 00:00:24,524
We'll look at the iShares STOXX Europe 600 Retail ETF, which tracks eurozone retail companies. These include companies such as Inditex, which sells the Zara clothing line and is based in Spain.

3
00:00:24,524 --> 00:00:44,615
It also includes Henness and Mauritz, which sells the H&amp;M clothing line and is based in Sweden. The iShares STOXX Europe 600 Retail or UCITS for short is tradable on the Bolsa Mexicana de Valores, which is the the largest stock exchange in Mexico.

4
00:00:44,615 --> 00:00:55,559
Let's say that the investors in Mexico are very eager to invest in these European retail companies through ETFs and bid up the price of the ETF shares.

5
00:00:55,560 --> 00:01:13,230
This leads to a discrepancy between the ETF share price and the underlying prices of the stocks. We say that the ETF is trading at a premium to the net asset value or NAV because it is relatively expensive compared to the underlying stocks.

6
00:01:13,230 --> 00:01:19,974
If the ETF shares were cheaper relative to their NAV, we would say that they're trading at a discount.

7
00:01:19,974 --> 00:01:29,839
Notice that this is the type of mispricing experienced by closed-end mutual funds. The actual difference is often referred to as the basis.

8
00:01:29,840 --> 00:01:46,070
We use this term because a basis point is defined as 0.01 percent or 1 divided by 10,000. Usually, the premium or discount is small enough that its unit of measurement is a few basis points.


@@@
1
00:00:00,000 --> 00:00:08,925
Going back to the example with the UCITS ETF, the demand for the ETF has led to a premium on the ETF shares.

2
00:00:08,925 --> 00:00:28,660
An AP such as Morgan Stanley may see this price difference as an arbitrage opportunity. So, the AP triggers the create process, that is Morgan Stanley may buy shares in Inditex, H&amp;M, and all the other companies listed in the UCITS ETF.

3
00:00:28,660 --> 00:00:40,489
Then, the AP gives the bundle of stocks to the ETF sponsor, which is BlackRock's iShares. The ETF sponsor issues more ETF shares for the AP.

4
00:00:40,490 --> 00:00:49,294
The AP, Morgan Stanley, then sells the ETF shares to investors on the stock exchange, which is the Bolsa Mexicana de Valores.

5
00:00:49,295 --> 00:01:06,394
The create process injects more ETF shares into the marketplace. The large purchases of a company stocks push up the stock prices of Inditex, H&amp;M, and the other European retail stocks listed in the UCITS ETF.

6
00:01:06,394 --> 00:01:21,274
Moreover, the additional supply of ETF shares helps to bring down the price of the ETF. The AP profits from the difference between the buy price of the stocks and the sell price of the ETF shares.

7
00:01:21,275 --> 00:01:33,930
As long as there is a price discrepancy that is large enough for arbitrage opportunities, APs will buy low and sell high until the prices are aligned.


@@@
1
00:00:00,000 --> 00:00:09,574
Let's take a step back and see what we've learned. We've learned that ETFs charge smaller fees than mutual funds or hedge funds.

2
00:00:09,574 --> 00:00:23,210
Moreover, ETFs are more efficiently priced than closed-end funds because authorized participants use arbitrage to bring the ETF share prices in line with the underlying stocks.

3
00:00:23,210 --> 00:00:31,570
We hope you have gained a better understanding of how ETFs work as you'll be working on ETFs in your project for this module.


@@@
1
00:00:00,000 --> 00:00:14,879
We're going to take a minute to let our brains chill out. Learning requires active engagement of your brain and really your whole self, and research shows that giving yourself adequate time to rest and recuperate helps your new knowledge put down roots, so you can use it effectively in the future.

2
00:00:14,880 --> 00:00:28,059
Getting enough sleep, spacing your learning out throughout your weekly schedule rather than trying to do it all in one chunk, and giving yourself permission to take a break when you feel tired or frustrated can all help you maintain your motivation and your brain's learning readiness.

3
00:00:28,059 --> 00:00:35,689
Meditating is another great way to take a mental break from learning indirect the world at large. So, let's try it out, nothing to lose.

4
00:00:35,689 --> 00:00:46,399
Wherever you are right now, find a comfortable position to sit or stand in. Try to let your whole body relax, and others can be a little bit tricky especially when you've been leaning in a very active way.

5
00:00:46,399 --> 00:01:01,849
But just start by relaxing your hands and your feet, let your arms and your legs relax, let your belly relax, let your shoulders roll up back and down, close your eyes or you can pick a spot right in front of you to just stare at in a more static way.

6
00:01:01,850 --> 00:01:09,284
Then just start to let your thoughts about your learning and whatever else is going on in your life fade away.

7
00:01:09,284 --> 00:01:17,785
If it's hard to just let your brain empty itself, you can instead replace those thoughts about anything that's going on with instead thoughts about your breath.

8
00:01:17,784 --> 00:01:28,200
So, to start to tune into how your body is naturally breathing on its own, just noticing what it feels like to inhale and what it feels like to exhale.

9
00:01:28,400 --> 00:01:42,665
Maybe you're breathing more into your chest, maybe you're breathing more into your belly. Feel how your rib cage expands out in all directions as you breathe in and then contracts back to the center as you breathe out.

10
00:01:42,665 --> 00:01:49,524
You don't have to control your breath anyway, it's something that your body is just doing automatically on its own.

11
00:01:49,525 --> 00:02:04,560
Or just a couple of moments here, noticing that breath being still, being quiet, and just focusing on this emptiness instead of everything else that's going on outside of you.

12
00:02:58,250 --> 00:03:05,794
Then you can gently blink your eyes open if you close them. So, congratulations, you just meditated.

13
00:03:05,794 --> 00:03:15,139
This is something that you can use throughout your learning experience using this video or on your own, and you can take it into the rest of your life as well whenever you need a moment just to ground yourself and feel a little bit more calm.


@@@
1
00:00:00,000 --> 00:00:10,500
Welcome back. So, far in this module, you've learned about Indices and ETFs, their application in the real world and how they work on a transactional level.

2
00:00:10,500 --> 00:00:20,000
Now, we're going to talk about the baskets of stocks that underlie these instruments and you will learn how to distribute money across the stocks in the basket.

3
00:00:20,000 --> 00:00:32,600
First, we'll discuss the risk and return properties of a collection of stocks. So, let's say, you've researched your trading signals and finally come up with a list of stocks to buy.

4
00:00:32,600 --> 00:00:40,125
You know how much money you have to spend. You're ready to buy the stocks needed to construct a portfolio for an ETF.

5
00:00:40,125 --> 00:00:54,810
But, how much money should you invest in each stock? Well, putting more money in stocks that you expect to have the highest returns, seems naturally more likely to give you higher returns.

6
00:00:54,810 --> 00:01:01,984
But what if the prices of these stocks undergo the most fluctuations. That is they entail the greatest risk.

7
00:01:01,984 --> 00:01:08,644
You might have great returns for a while, but then, you could lose it all. There must be a balance here.

8
00:01:08,644 --> 00:01:23,439
How do you distribute money to not only maximize returns, but also minimize risk. That's a question that's been challenging great financial minds for years and you're about to embark on a quest in search of the solution.


@@@
1
00:00:00,000 --> 00:00:07,920
Since you've studied the stocks in your universe now for a while, you might already know which stock has been performing the best.

2
00:00:07,920 --> 00:00:15,065
Let's call it New Digital Corporation. At this point, you may want to put all your money in New Digital stock.

3
00:00:15,065 --> 00:00:23,675
But let's say you do this and New Digital stock keeps increasing in value for a while, but suddenly drops to half the price you purchased that.

4
00:00:23,675 --> 00:00:35,600
Since you used all your money to buy the stock, you just lost half your money. Ouch! What if instead you put half your money in New Digital and half your money in Big Pharma Company?

5
00:00:35,600 --> 00:00:49,445
Intuitively, this seems like it could smooth out the dips and value of your investment, because New Digital could start doing well when it launches a new tech product, while Big Pharma recovers from a failed drug trial.

6
00:00:49,445 --> 00:00:59,905
Big pharma might start doing well when it's research arm, deliver several new promising candidate drugs, while New Digital struggles during a change in management.

7
00:00:59,905 --> 00:01:11,235
What we've done here is reduced our portfolio risk by diversifying our portfolio. But why not keep spreading our money into more and more stocks?

8
00:01:11,235 --> 00:01:24,705
Can we reduce our portfolio risk indefinitely? If all the sources of risk are independent, we could in theory, reduce risk to zero by spreading our money between more and more stocks.

9
00:01:24,705 --> 00:01:33,875
This would be the case if each company is only subject to its own independent sources of risk, and there are no risks common to all companies.

10
00:01:33,875 --> 00:01:53,660
Risks specific to individual companies is called idiosyncratic risk or specific risk. However, in the real world, all companies are subject to common sources of risk that affect the entire economy such as the risks of inflation, recession, or change in interest rates or GDP.

11
00:01:53,660 --> 00:02:07,200
Risk attributable to market-wide sources is called market risk or systematic risk. Let's see if we can quantify this risk and find some guidelines for how to allocate our money.


@@@
1
00:00:00,000 --> 00:00:08,354
Let's see how this works in more detail, by constructing a portfolio that contains two stocks and then calculating its mean and variance.

2
00:00:08,354 --> 00:00:24,495
Let's put a fraction xA of our portfolio in stock A and the rest xB in stock B. We call xA and xB the weights on assets A and B in our portfolio and we know these weights xA and xB sum to one.

3
00:00:24,495 --> 00:00:38,530
To calculate the mean and variance of our portfolio, we will again need to think of future log returns as random variables, that is as quantities that can take a variety of values with different probabilities.

4
00:00:38,530 --> 00:00:48,725
You can think of returns, at a future time as being indexed by i, where i is a possible scenario or alternate timeline of the future.

5
00:00:48,725 --> 00:00:58,155
R the log return, takes different values for different scenarios and different scenarios have different probabilities of occurring.

6
00:00:58,155 --> 00:01:08,190
The expected value of R, which we referred to before as the mean or expected log return, is given by this formula.

7
00:01:08,190 --> 00:01:17,205
The total return of the portfolio in each scenario is just the weighted sum of the returns of each individual asset.

8
00:01:17,205 --> 00:01:25,335
The expected value of the portfolio return, equals the weighted sum of the individual stock's expected returns.

9
00:01:25,335 --> 00:01:40,205
To see this, we take the expected value of the portfolio returns. The summation symbol distributes across the sum, then we can pull out the weights because they are independent of scenario.


@@@
1
00:00:00,000 --> 00:00:11,355
Now, let's calculate a metric of the total risk inherent to the portfolio. We'll measure risk with volatility or more specifically with the variance, the square of the volatility.

2
00:00:11,355 --> 00:00:21,134
Let's review the formula for the variance of one asset. This is a slightly different formula for variance than we've seen before, but it's similar in spirit.

3
00:00:21,135 --> 00:00:34,625
Instead of just summing the squared difference between each observation and the mean and dividing by n, we multiply the squared difference by the probability of that value occurring and sum those.

4
00:00:34,625 --> 00:00:41,460
The formula will be the same as dividing by n, if the probability of each scenario occurring is the same.

5
00:00:41,460 --> 00:00:50,685
So, now let's calculate the variance of the portfolio. This takes just a few simple steps, but for now we will skip ahead to the answer.

6
00:00:50,685 --> 00:01:07,695
You'll find the full derivation given as text on this page. We find that the portfolio variance equals the squared of weights times each asset's variance, plus a term that includes the covariance of the assets multiplied by each weight.

7
00:01:07,694 --> 00:01:23,739
Remember that the covariance is a measure of the joint variability of two random variables. When stock A is above its average, if stock B is also above its average, they are varying together or covarying, so they have a positive covariance.

8
00:01:23,739 --> 00:01:30,905
This covariance term turns out to be very important. It basically underlies the benefit of diversification.

9
00:01:30,905 --> 00:01:38,925
Remember that the covariance is just the correlation between the two variables times each of their standard deviations.

10
00:01:38,924 --> 00:01:51,454
We know that the correlation coefficient takes values between minus one and one, where minus one means the variables are perfectly anti-correlated, and one means that they are perfectly correlated.

11
00:01:51,453 --> 00:02:03,364
Both correlation and covariance are measures of how much two variables vary together. With this knowledge, we can rewrite the portfolio variance in terms of the correlation.

12
00:02:03,364 --> 00:02:11,104
Let's see what happens to the variance of our two-asset portfolio when the correlation between stock A and stock B is one.

13
00:02:11,104 --> 00:02:23,390
In that case, the expression to the right of the equal sign is a perfect square, and the portfolio standard deviation is just a weighted average of the standard deviations of the individual stocks.

14
00:02:23,389 --> 00:02:34,229
In all other cases, the correlation coefficient is less than one. So the portfolio standard deviation is less than the weighted average of the individual standard deviations.

15
00:02:34,229 --> 00:02:53,875
How about when the correlation between stock A and B is minus one? In that case, the portfolio variance reduces to the weighted difference between the two standard deviations squared and the portfolio standard deviation becomes the absolute weighted difference between the individual standard deviations.

16
00:02:53,875 --> 00:03:01,014
In the case when the correlation is minus one, we can get a perfectly hedged portfolio by solving this equation.

17
00:03:01,014 --> 00:03:11,629
If we plug in the relation that the weights x_A and x_B sum to one, we get that x_A equals Sigma B divided by Sigma A plus Sigma B.

18
00:03:11,629 --> 00:03:24,800
These weights drive the portfolio variance to zero. However, in reality, since every asset is affected by systematic risk, the correlation between two assets will never reach minus one.


@@@
1
00:00:00,000 --> 00:00:06,540
Congratulations, on getting through the first five nodes. You're now over a third of the way through this lesson.

2
00:00:06,540 --> 00:00:13,365
Because some of the topics in this lesson are so math heavy, we wanted to create content that contain the actual formulas.

3
00:00:13,365 --> 00:00:19,830
Beta testers suggested that content delivered as text is super-helpful. Be sure to let us know what you think.

4
00:00:19,829 --> 00:00:25,989
Udacity is all about creating an environment that encourages students, but we also want to learn from you.

5
00:00:25,989 --> 00:00:32,655
We want your feedback to be included in the process. Your feedback helps us to iterate and create better lessons.

6
00:00:32,655 --> 00:00:38,380
So, if we do something that you like, let us know, and if you think we could do something better, let us know that too.


@@@
1
00:00:00,000 --> 00:00:07,465
Okay. So far you've learned about the importance of diversification and how to calculate portfolio mean and variance.

2
00:00:07,465 --> 00:00:18,900
You may wonder, what are the best ways to assign to each stock in your portfolio. Before we dive into portfolio optimization, let's take a look at the set of all possible portfolios.

3
00:00:18,900 --> 00:00:25,935
This will lead us to an important concept, the efficient frontier. Before we start, let's quickly recap.

4
00:00:25,935 --> 00:00:40,480
The expected return of a portfolio, equals the weighted sum of each stock's expected returns. The variance of a portfolio, equals the sum of the pairwise covariances weighted by the products of the weights.

5
00:00:40,480 --> 00:00:52,880
Okay. I think we're ready to go. Let's first take a look at a simple example with five stocks. Stocks A, B, C, D and E with given annual mean returns and covariances.

6
00:00:52,880 --> 00:01:00,125
One day your manager gives you $10,000 and tells you to invest in these five stocks whichever way you want.

7
00:01:00,125 --> 00:01:11,805
But taking only long not short positions. What would you do with this money? Of course, you would want to impress your manager and create a portfolio with the highest returns and the lowest risk.

8
00:01:11,805 --> 00:01:22,240
Is this possible? How would you do it? You've given the task some thought and decided to just create a few portfolios with randomly assigned weights.

9
00:01:22,240 --> 00:01:35,160
For the first portfolio, you give each stock an equal weight of 20% of the total capital. The portfolio return equals 4% and the standard deviation equals 3%.

10
00:01:35,160 --> 00:01:49,460
Is this good enough? You are not sure. So, you just keep testing different combinations by assigning a little more weight to one stock and a little less to another and then rechecking the portfolio's return and risk.

11
00:01:49,460 --> 00:02:02,540
The more simulations you do, the more interesting the problem gets. For example, both scenario one and two have the same risk, but scenario one has higher expected return.

12
00:02:02,540 --> 00:02:15,230
In this case, you will definitely prefer scenario one. Also, scenario four has the highest expected return of 0.07 but it also has the highest risk of 0.04.

13
00:02:15,230 --> 00:02:29,490
Scenario four is less attractive because it is a riskier portfolio. This is interesting; isn't it? Let's try to run thousands more scenarios and plot the expected portfolio return against portfolio volatility.

14
00:02:29,490 --> 00:02:42,230
Okay. Here is a plot representing 25,000 simulated portfolios. Each dot represents a possible risk return combination that can be generated by the basket of stocks.

15
00:02:42,230 --> 00:02:52,245
The x axis is the volatility and the y-axis is the return. What have you noticed? Let's take a look at a quick example.

16
00:02:52,245 --> 00:03:07,320
Let's take a look at these two portfolios represented by the red and yellow dots. Notice that both portfolios have the same risk of 0.035 but the red portfolio has significantly higher expected return of 0.06.

17
00:03:07,320 --> 00:03:20,440
Furthemore, the red dot outperforms all the portfolios with risk of 0.035 but there's more. Have you noticed that all the dots that lie on the upper boundary outperform the dots below it?

18
00:03:20,440 --> 00:03:28,590
This is the efficient frontier. The efficient frontier is the upper boundary of the set of possible portfolios.

19
00:03:28,590 --> 00:03:38,640
Portfolios on this boundary have the maximum achievable return for a given level of risk. Any portfolios above the frontier are unachievable.

20
00:03:38,640 --> 00:03:45,810
It's not possible to find a combination of stocks that produces a higher level of return at that same level of risk.

21
00:03:45,810 --> 00:04:00,440
On the flip side, portfolios below the frontier are certainly achievable but one might wonder why a rational investor would prefer a lower rate of return when she could achieve a higher return while taking on the same level of risk.

22
00:04:00,440 --> 00:04:12,070
The portfolio that achieves the lowest level of risk is called the minimum variance portfolio, and the portfolios on the efficient frontier are known as market portfolios.


@@@
1
00:00:00,000 --> 00:00:14,129
In the last video, you learned about the efficient frontier. You know that any point on the efficient frontier represents the portfolio that gives you the best risk return trade-off, and that these portfolios are known as market portfolios.

2
00:00:14,130 --> 00:00:27,304
But is that it? Is there a way to do better than the efficient frontier? In this video, we will be adding a risk-free asset into our portfolio and we're going to introduce something called the capital market line.

3
00:00:27,304 --> 00:00:48,905
A risk-free asset is an investment instrument that entails absolutely no risk or uncertainty. In theory, if you invest in such an instrument, you receive a guaranteed rate of return called the risk-free rate and reality entirely risk-free assets don't exist since all investments carry a certain level of risk.

4
00:00:48,905 --> 00:00:56,884
However, in practice people normally refer to the rate of return on a three month treasury bill as the risk-free rate.

5
00:00:56,884 --> 00:01:09,295
Now let's add this risk-free asset into the picture. Let's imagine we construct a new portfolio with any of the market portfolios that you prefer and the risk-free asset.

6
00:01:09,295 --> 00:01:18,335
Which market portfolio would you choose? Note that this new portfolio is a weighted sum of the risk-free asset and the market portfolio.

7
00:01:18,334 --> 00:01:32,980
For example, if you choose the green portfolio as your market portfolio and the red. as your risk-free asset, the line between them represents the potential portfolios that you can construct with the two assets.

8
00:01:32,980 --> 00:01:40,924
Now, which market portfolio would you choose so as to achieve the best return for a given level of risk?

9
00:01:40,924 --> 00:01:48,170
Well, you should be looking at the efficient frontier, the top edge of the set of possible portfolios.

10
00:01:48,170 --> 00:01:59,335
It turns out that you should choose the market portfolio that allows you to draw a straight line starting from the risk-free asset that just touches the top of the efficient frontier.

11
00:01:59,334 --> 00:02:17,039
To see why this is the best market portfolio for the job, let's take a look at lines A, B, and C. Note that line A connects the red dot which represents the risk-free asset with the green dot, which represents the tangent market portfolio on the efficient frontier.

12
00:02:17,039 --> 00:02:32,020
Line A lies above the efficient frontier and intersects the tangent portfolio. All the portfolios represented by line A have higher returns than the ones on the efficient frontier given the same risk level.

13
00:02:32,020 --> 00:02:43,945
This is not the case for lines B and C. Portfolios on this tangent line are the best achievable. They even beat the efficient frontier which consists of purely risky assets.

14
00:02:43,944 --> 00:02:54,819
This line is called the capital market line. Now what is the expected return of the portfolio consisting of the risky portfolio and the risk-free asset?

15
00:02:54,819 --> 00:03:08,824
The formula for this quantity is also the formula for the capital market line because each point on this line gives the return as a function of risk of a possible combination of the risky portfolio and the risk-free asset.

16
00:03:08,824 --> 00:03:22,129
So let's examine the graph to determine the equation. You have two points 0, rf which is the risk-free portfolio and sigma m, rm which is the market portfolio.

17
00:03:22,129 --> 00:03:37,115
The slope of a line simply equals rm minus rf divided by sigma m. This quantity the difference between the market return and the risk-free rate is called the market risk premium.

18
00:03:37,115 --> 00:03:46,160
The portfolio return is then just the risk-free rate of return as a baseline plus the slope times the portfolio volatility.

19
00:03:46,159 --> 00:03:54,335
The portfolio volatility will depend on what weights you choose to put on the risk-free asset and the market portfolio, i.e.

20
00:03:54,335 --> 00:04:09,149
where you choose to sit along the capital market line. The slope of the capital market line, which is of course the same for all points on the line is a special quantity known as the Sharpe Ratio of the market.

21
00:04:09,150 --> 00:04:20,600
We're going to discuss that more in a future lesson. In fact, if you can borrow at the risk-free rate, you can achieve points on the capital market line to the right of the tangent market portfolio.

22
00:04:20,600 --> 00:04:36,870
This amounts to setting a negative weight on the risk-free asset. That is to say, shorting it. This is why professional investors care almost only about the Sharpe ratio because they can manufacture any level of risk or return with leverage.


@@@
1
00:00:00,000 --> 00:00:07,080
In this lesson, we learned how to calculate the expected return and variance of a portfolio risky assets.

2
00:00:07,080 --> 00:00:14,809
We study the properties of all possible portfolios of risky assets, and what happens when a risk-free asset is added.

3
00:00:14,810 --> 00:00:27,679
What you learn in this lesson is essential foundational knowledge. Once you understand it, you'll be ready to learn how to obtain the best possible risk return balance from your portfolio, which is what we'll study next.


@@@
1
00:00:00,000 --> 00:00:09,449
In this lesson, you'll learn strategies for optimizing returns on your portfolio. That is to say getting the best possible combination of risk and return.

2
00:00:09,449 --> 00:00:16,820
Portfolio optimization consists of a diverse array of widely utilized and actively research techniques.

3
00:00:16,820 --> 00:00:25,559
In this and future lessons, we're going to take you from the very basics, all the way up to some of the most cutting edge methods. Let's get started.


@@@
1
00:00:00,000 --> 00:00:11,509
Previously, we studied the set of all portfolios and we know we're particularly interested in portfolios with the highest possible return given the volatility in returns.

2
00:00:11,509 --> 00:00:20,684
How do we find the highest returns then? Well, this is a type of problem known as an Optimization Problem, you may have encountered this before.

3
00:00:20,684 --> 00:00:29,780
Intuitively optimization means making something the best possible right? So, what we're trying to do is find the maximum or minimum of a function.

4
00:00:29,780 --> 00:00:38,230
In fact, we can always turn a maximization problem into a minimization one by optimizing the negative of the function.

5
00:00:38,229 --> 00:00:54,394
So, we can say that optimization involves, trying to find the minimum of a function. First, let's look at a simple example of a type you may have seen before, here we're looking at the function Y equals X minus one squared plus one.

6
00:00:54,395 --> 00:01:02,320
This is a quadratic function of one variable X so we know it looks like an upside down U shape a Parabola.

7
00:01:02,320 --> 00:01:10,054
We can see that all we have to do to find the location of that minimum is to find the value of X that makes Y the smallest.

8
00:01:10,055 --> 00:01:23,509
We will find that point by observing that the graph has a special property at that point. At every other point on the line seems to be either going down or going up, but at that point the line looks basically horizontal.

9
00:01:23,510 --> 00:01:34,534
What I'm saying is that every other point the instantaneous slope of the line is some positive or negative number, but at that minimum the instantaneous slope of the line is 0.

10
00:01:34,534 --> 00:01:45,515
That is how we will find the point. Now, let's remember that we know how to get an equation that represents the instantaneous slope of the function, the derivative.

11
00:01:45,515 --> 00:01:57,329
So, what we can do to solve for the value of X at this point mathematically, is to take the derivative of the function, set it equal to 0 and solve for X, so let's do that now.

12
00:01:57,329 --> 00:02:07,295
And check it out we got that x equals one, this is one very basic type of optimization problem that can be solved exactly.

13
00:02:07,295 --> 00:02:16,599
Later on, we'll see that there are many complexities we can add to an optimization problem and we'll require many more methods to solve them. Stay tuned.


@@@
1
00:00:00,000 --> 00:00:08,505
Earlier, we discussed a basic optimization problem. It's possible for optimization problems to have an additional wrinkle.

2
00:00:08,505 --> 00:00:20,415
Constraints can be placed on the problem to limit the region where you look for a solution. In other words, you try to find the smallest value of the function that also satisfies the constraints.

3
00:00:20,415 --> 00:00:27,750
Let's discuss how to solve an optimization problem with constraints. First, let's clarify some terminology.

4
00:00:27,750 --> 00:00:45,460
The function we are trying to optimize is called the objective or cost function. The variable we are trying to optimize, which in portfolio optimization is the vector of weights on the various assets in the portfolio, is called the optimization variable.

5
00:00:45,460 --> 00:00:54,415
Then we may have various constraints. The constraints can take the form of inequality or equality conditions.

6
00:00:54,415 --> 00:01:16,040
If there are no constraints, we say the problem is unconstrained. If the objective function is x minus one squared plus one and we apply the constraint that x must be less than or equal to zero, then the new minimum value is two because this is the smallest value of the function that also satisfies the constraint.

7
00:01:16,040 --> 00:01:26,440
A vector x is called optimal or a solution of the problem if it has the smallest objective value among all vectors that satisfy the constraints.

8
00:01:26,440 --> 00:01:35,215
The set of points for which the objective and all the constraint functions are defined is called the domain of the problem.

9
00:01:35,215 --> 00:01:48,765
A point in the domain is said to be feasible if it satisfies all the constraints. The problem is feasible if there exists at least one feasible point, and it is infeasible otherwise.

10
00:01:48,765 --> 00:01:57,465
If there are feasible points for which the objective function reaches negative infinity, we say the problem is unbounded below.

11
00:01:57,465 --> 00:02:09,125
This is the case for the function f of x equals the negative natural log of x with no constraints. A general optimization problem can be very difficult to solve.

12
00:02:09,125 --> 00:02:14,925
You can have functions of many variables and the objective function and constraints can be very complicated.

13
00:02:14,925 --> 00:02:22,380
So, instead of thinking about how to solve all types of optimization problems, let's just think about how to solve certain types.

14
00:02:22,380 --> 00:02:31,725
It turns out that there are well developed methods to solve some sub-types of problems, and they will be of great help to us as we try to optimize our portfolios.

15
00:02:31,725 --> 00:02:41,005
It turns out that a very important type of optimization problem is one for which the objective function and inequality constraints are convex.

16
00:02:41,005 --> 00:02:48,549
What does that mean? Well, it's actually very intuitive. A convex function curves upward everywhere.

17
00:02:48,549 --> 00:02:58,660
Any straight line you draw between two points on the function has to lie above the function. So, this wiggly function would not be convex.

18
00:02:58,660 --> 00:03:11,345
An important property of a convex optimization problem is that it only has one minimum. So, if you find a local minimum, you know it is the globally optimal value over the whole feasible region.

19
00:03:11,345 --> 00:03:28,825
This is actually hugely helpful. When the objective function and inequality constraints are convex and the equality constraints have the form f of x equals a times x plus b, the optimization problem is called a convex optimization problem.

20
00:03:28,825 --> 00:03:43,820
If you find a local optimum for this problem, you know it is a global optimum. There are well developed techniques for solving these problems, and the set of problems we deal with during portfolio optimization will generally be of this type.


@@@
1
00:00:00,000 --> 00:00:07,115
So far in this lesson, you've learned how to use optimization techniques to come up with optimal portfolio weights.

2
00:00:07,115 --> 00:00:19,625
Is your job done? Not really. As a portfolio manager, the next step after portfolio construction, is to constantly monitor your portfolio, and make sure it is in line with your goals.

3
00:00:19,625 --> 00:00:28,794
Over time, assets produce returns that can differ from those originally estimated. So, the amount of money invested in different assets, changes.

4
00:00:28,794 --> 00:00:36,435
Therefore, the portfolio weights, which represent the proportion of money invested in each asset, change.

5
00:00:36,435 --> 00:00:53,585
In order to make sure the portfolio maintains the desired set of weights, we need two rebalance. For example, say we started with a strategy in which we wanted to invest 50% of our portfolio in a solar energy company and 50% in a construction company.

6
00:00:53,585 --> 00:01:04,019
Let's say the solar energy company does really well and it's value appreciates such that the shares we've invested in it, now represent 70% of the value of our portfolio.

7
00:01:04,019 --> 00:01:18,769
If we wanted to return to the 50% in a solar energy company and 50% in the construction company, we'd sell some shares of the solar energy company, and buy some shares of the construction company to bring us back to 50-50.

8
00:01:18,769 --> 00:01:32,184
When you have determined your portfolio weights via optimization and later want to rebalance, you simply re-run the optimization with the same objective function and constraints, but using updated data.

9
00:01:32,185 --> 00:01:38,939
Before we discuss different rebalancing strategies, let's consider the costs incurred during rebalancing.

10
00:01:38,939 --> 00:01:47,255
Think about it, when you rebalance, you place trades. To place trades, you generally have to pay a broker commissions.

11
00:01:47,254 --> 00:01:55,935
These commissions are one type of transaction costs. Transaction costs, are probably the most important costs incurred during rebalancing.

12
00:01:55,935 --> 00:02:03,945
But, there are others, such as taxes, like capital gains taxes and the administrative costs of time and labor.

13
00:02:03,944 --> 00:02:28,654
Transaction costs can be potentially significant. In the year 2000, the Texas Permanent School Fund rebalanced its portfolio of more than 2,000 securities, 40 portfolio managers, 500 million shares, and $17.5 billion were involved not including administrative costs, the transactions themselves costs $120 million.

14
00:02:28,655 --> 00:02:37,639
Now, you can see how crazy transaction costs can be. In general, it's hard to model transaction costs directly.

15
00:02:37,639 --> 00:02:45,584
To know the transaction costs, you must know what the trade is. But the whole reason to do the optimization, is to know what the trade is.

16
00:02:45,585 --> 00:02:56,165
So, instead of trying to estimate transaction costs, we can make the assumption that transaction costs will be proportional to the magnitude of change in the holdings.

17
00:02:56,164 --> 00:03:04,664
This is called portfolio turnover and it's basically, the sum total of the changes in the weights on all the assets.

18
00:03:04,664 --> 00:03:19,040
To calculate turnover between two time periods, you would take the absolute value of the difference in weight between two time periods for each asset, and then sum these over assets.

19
00:03:19,039 --> 00:03:40,469
To get an annualized turnover, you would calculate the average turnover per rebalancing event by calculating, total turnover for all the rebalancing events in the timeframe you're considering, and dividing by the number of rebalancing events, and then multiplying by the number of rebalancing events per year.


@@@
1
00:00:00,000 --> 00:00:09,835
In the last video, we discussed the purpose of rebalancing, costs incurred during rebalancing, and we also chatted about a way to estimate those costs.

2
00:00:09,835 --> 00:00:22,269
But how do you know when to rebalance a portfolio? There are two types of events that trigger the rebalancing decision, cash flows and changes in the underlying parameters of the stock return model.

3
00:00:22,269 --> 00:00:34,945
Cash flows are movements of money into and out of a portfolio. For a portfolio of stocks, the most common forms of cash flows are dividends, capital gains and new contributions.

4
00:00:34,945 --> 00:00:51,200
For example, an investor receives dividend payments when he loans a stock. However, when he has shorted a stock, he is obligated to pay the balance of dividend payments issued to shareholders to the person who has loaned him the stock.

5
00:00:51,200 --> 00:01:06,439
Capital gains and losses are the increases or decreases in value of the assets in the portfolio, but these are unrealized until the assets are sold at which time they become realized capital gains and contribute to cash flows.

6
00:01:06,439 --> 00:01:30,769
New contributions consist of new investments in the fund or portfolio under management. The simplest way to adjust for cash flows is to buy and sell stocks using the current portfolio weights, but when some assets appreciate and some depreciate, the weights which represent the fraction of the portfolio invested in each asset change.

7
00:01:30,769 --> 00:01:41,479
A portfolio manager can take advantage of cash flows to adjust the under weighted or over weighted portfolios to return to the target asset allocation.

8
00:01:41,480 --> 00:01:55,890
Changes in model parameters do not automatically trigger rebalancing. In this case, the portfolio manager must decide whether the parameter change is large enough to warrant incurring transaction costs to rebalance.

9
00:01:55,890 --> 00:02:04,335
As a portfolio manager, you need to constantly monitor the parameters used in your portfolio and be ready to rebalance when needed.

10
00:02:04,334 --> 00:02:15,454
Parameters can change at anytime. For example, many types of corporate actions such as mergers and acquisitions or changes in management may impact the model's parameters.

11
00:02:15,455 --> 00:02:31,864
When a portfolio manager suspects that a parameter has changed, she will re-estimate the model to come up with the optimal weights, then she will decide if the portfolio should be rebalanced by weighing the benefits of rebalancing with the downsides such as transaction costs.

12
00:02:31,865 --> 00:02:43,310
The most common rebalancing strategies are based on rebalancing at set temporal frequencies, or rebalancing when portfolio weights change by a certain threshold amount.

13
00:02:43,310 --> 00:02:56,134
One strategy is to rebalance at a set temporal frequency, daily, monthly, quarterly, annually or at another frequency, regardless of the changes in the sizes of the portfolio weights.

14
00:02:56,134 --> 00:03:11,349
This temporal frequency typically comes from the original time horizon of optimization. When a portfolio manager models his portfolio, he will often optimize it using data from a chunk of time of some duration.

15
00:03:11,349 --> 00:03:21,875
For example, if a portfolio is model based on month long chunks of data, the optimal weights are only valid for a month and so rebalancing is performed monthly.

16
00:03:21,875 --> 00:03:34,449
The threshold base strategy calls for rebalancing the portfolio only when the portfolio weights have drifted from the target ones by a predetermined threshold such as 1%, 5% or 10%.

17
00:03:34,449 --> 00:03:43,400
Since rebalancing does not occur at predetermined times, it can happen as frequently as daily, but also as infrequently as yearly.

18
00:03:43,400 --> 00:03:57,560
These strategies can also be combined, such that rebalancing is performed on a scheduled basis but only when the portfolio weights have drifted from the target ones by a predetermined threshold, thus, both criteria must be fulfilled.

19
00:03:57,560 --> 00:04:05,539
On the scheduled rebalancing date, if the change in the portfolio weights is less than the threshold, rebalancing won't be performed.

20
00:04:05,539 --> 00:04:13,530
Also, if the portfolio weights drift by more than the threshold before the scheduled rebalancing date, rebalancing won't be performed.


@@@
1
00:00:00,000 --> 00:00:10,785
So far we've learned a few common ways to formulate and solve a portfolio optimization problem. But there are a number of practical limitations to these methods.

2
00:00:10,785 --> 00:00:20,704
First of all, mean returns are notoriously difficult to estimate accurately. Past returns are frequently poor estimates of future returns.

3
00:00:20,704 --> 00:00:37,250
To mitigate this problem, people frequently avoid estimating returns directly altogether. We've already introduced one possible solution which is to simply minimize the difference between your portfolio weights and another set of target weights.

4
00:00:37,250 --> 00:00:47,739
Another technique is to use an alternative quantity thought to be predictive of future returns as a substitute for actual returns value estimates.

5
00:00:47,740 --> 00:00:57,724
We're going to talk more about these methods in a future lesson. There are also problems inherent to the estimation of portfolio variance.

6
00:00:57,725 --> 00:01:08,605
The first is that variants may not be a good enough measure of risk. This is the theme that might come up whenever you rely upon variants to estimate risk.

7
00:01:08,605 --> 00:01:29,780
To understand what I mean, think about the distribution of log returns. If the distribution has a different shape on the left side and on the right, this means there's a different probability of getting very high positive returns than getting very high negative returns, but mean and standard deviation won't tell you about this asymmetry.

8
00:01:29,780 --> 00:01:37,575
For example, these two distributions have the same mean and the same standard deviation, but obviously very different shapes.

9
00:01:37,575 --> 00:01:48,760
As a quantitative trader, you really care about the shape of that negative tail because that part of the distribution tells you how likely you are to get large negative returns.

10
00:01:48,760 --> 00:01:58,459
As a workaround for this, you can formulate your optimization problem using alternative measures of risk, some of which we'll discuss in future lessons.

11
00:01:58,459 --> 00:02:14,564
Furthermore, the need to estimate an n by n covariance matrix, where n is the number of assets in your portfolio, means the problem size grows very quickly as the number of assets increases and the matrix becomes very large.

12
00:02:14,564 --> 00:02:21,460
Estimating large numbers of parameters introduces the possibility of aggregating many estimation errors.

13
00:02:21,460 --> 00:02:40,365
When the number of stocks n is of the same order of magnitude as the number of historical return data points per stock t, the total number of parameters to estimate in the covariance matrix is of the same order as the total size of the dataset, which is problematic.

14
00:02:40,365 --> 00:02:51,260
The need for a covariance matrix also means that we need to have a long enough historical stock price dataset to produce reliable estimates of covariances.

15
00:02:51,259 --> 00:02:59,215
For example, estimating a covariance matrix of 50 assets requires at least five years of daily data.

16
00:02:59,215 --> 00:03:05,784
If there are not enough daily return data, estimates of variances and covariances will not be accurate.

17
00:03:05,784 --> 00:03:20,040
But in fact any estimate is noisy. How do you handle noise? Well, instead of using raw values of quantities like means, you can utilize their ranks so that you only keep track of their relative magnitude.

18
00:03:20,039 --> 00:03:33,574
You can also simply include a term in the objective function to penalize turnover. This means that by default you are less likely to trade, therefore your results will be less reactive to noise.

19
00:03:33,574 --> 00:03:41,120
Finally, there's a method called robust optimization which takes into account confidence intervals of estimates.

20
00:03:41,120 --> 00:03:55,890
Another issue which we discussed when we mentioned rebalancing is that portfolio optimization is typically done using a chunk of data from some time period, but then needs to be performed again as time passes and things change.

21
00:03:55,889 --> 00:04:04,504
In other words, the optimized weights are really only valid as long as the estimates of the parameters fed into the problem are accurate.

22
00:04:04,504 --> 00:04:15,775
There's no built-in way to account for change over time. But the changing nature of the stock market may lead to conflicting predictions over different time horizons.

23
00:04:15,775 --> 00:04:25,060
For example, what if your analysis tells you that a return over one day horizon will be positive, but over a one month horizon it will be negative?

24
00:04:25,060 --> 00:04:34,345
In a single period optimization framework, it's not clear how to account for the different timescales while taking into account the conflicting predictions.

25
00:04:34,345 --> 00:04:43,430
One approach to account for the changing nature of the stock market over time as part of optimization is called multi-period optimization.

26
00:04:43,430 --> 00:05:00,074
In essence, this involves modeling a plan for trading over the next several periods of time using the best information available at the current time, and optimizing over this series of future trades but only ever executing the trades for the first period.

27
00:05:00,074 --> 00:05:10,210
This basically amounts to a planning exercise, the idea is simply to try to ensure that we don't place any traits now that will put us in a bad position in the future.

28
00:05:10,209 --> 00:05:31,230
In the example I just mentioned, where we have a prediction that returns over one day will be positive, returns over one month will be negative, we can see intuitively that it makes sense to go long and short to take advantage of these predictions, if transaction costs are low but to refrain from trading if transaction costs are high.

29
00:05:31,230 --> 00:05:40,470
If transaction costs are high, we may waste more money making trades than we could gain by following the predictions of positive and negative returns.

30
00:05:40,470 --> 00:05:48,344
A multi-period optimization analysis that includes transaction costs would provide an optimal solution to this problem.

31
00:05:48,345 --> 00:05:54,884
Another difficulty of portfolio optimization is the ambiguity about how to deal with transaction costs.

32
00:05:54,884 --> 00:06:03,339
Every time we make trades to rebalance the portfolio, we incur trading costs. But in general, they're difficult to precisely model.

33
00:06:03,339 --> 00:06:12,269
In order to account for them, we model them as being proportional to turn over, the sum total of changes and portfolio weights.

34
00:06:12,269 --> 00:06:23,210
But you can also include measures of turnover in the optimization problem formulation, either as a constraint, or as a term to minimize in the objective function.

35
00:06:23,209 --> 00:06:33,925
To address some of these weaknesses, factor-based models of risk are often seen as more practical but if this discussion seem rather abstract to you, just hang on.


@@@
1
00:00:00,000 --> 00:00:14,140
Hey now, you made it, and you are doing awesome. We talked about a lot in this lesson. We learned about a very important method for distributing money amongst several assets in a manner that maximizes return and minimizes risk.

2
00:00:14,140 --> 00:00:21,365
This knowledge will serve you well when you're deciding how to best invest your money. I really hope you enjoy the project.

3
00:00:21,364 --> 00:00:27,879
It's a great opportunity to cement what you've learned in memory. Looking forward to seeing you in the next module.


@@@
1
00:00:00,000 --> 00:00:05,580
Congratulations, you're more than halfway through term one. You've done amazingly well to get this far.

2
00:00:05,580 --> 00:00:15,559
You've learned the basic mechanics of stock markets, the fundamental math and vocabulary that people use to describe markets, and a few types of trading strategies based on basic hypotheses about market behavior.

3
00:00:15,560 --> 00:00:28,554
This means that you're ready for a new exciting challenge. In this project there are two parts, in the first part you'll build a smart beta portfolio, then calculate the tracking error against its index.

4
00:00:28,554 --> 00:00:37,089
In the second part, you will re-balance the portfolio using quadratic programming and calculate its turnover to evaluate the performance.

5
00:00:37,090 --> 00:00:53,594
The project will give you practical experience with the idea of smart beta, a popular type of investment strategy that you'll hear about when discussing quantitative trading, and enable you to practice the math of portfolio optimization, which is something you would do frequently as a quantitative analyst.


@@@
1
00:00:00,000 --> 00:00:11,130
Well, you made it. Made what? Made it to the, in my opinion, best part of the term. This is where it all comes to life.

2
00:00:11,130 --> 00:00:20,519
That's part of the term but they've already learned so much good stuff. They've already created their own trading strategies and they're already developing strong market intuition.

3
00:00:20,519 --> 00:00:29,548
They have an idea for how quant research works and how production quality, trading strategies work. They know about markets, volatility, outliers, indices and ETFs.

4
00:00:29,548 --> 00:00:35,549
They even know how to create optimal diverse portfolios, trading off volatility or risk with return.

5
00:00:35,549 --> 00:00:42,409
Yeah, that's true, and learning all of those things help bring them to the point where they can fully appreciate what they're doing now.

6
00:00:42,409 --> 00:01:05,090
But now we're going to put it all together and they're finally going to see the whole process end to end, from an initial hypothesis, maybe from an academic paper, we'll see a few real life examples to quantify non-hypothesis, to combining information of many types from many sources about drivers of mean returns and drivers of volatility into a single model.

7
00:01:05,090 --> 00:01:16,689
Then using optimization tools we've already introduced, to turn that into a production quality, theoretical tradable portfolio ready for a barrage of further testing.

8
00:01:16,689 --> 00:01:24,929
Well you certainly have my attention. Is this how real quants do it? Absolutely. This is how real quants do it.

9
00:01:24,930 --> 00:01:35,780
Well, that's exciting. Wait, when you mentioned ways to quantify a hypothesis, are you referring to the alphas that we discussed back when we were talking about the quantum workflow?

10
00:01:35,780 --> 00:01:45,185
Yep. That's exactly right. We're going to talk about alphas in great detail. Okay. So, students are going to hear about all of these in these lessons.

11
00:01:45,185 --> 00:01:55,009
But will they get hands-on experience to? Absolutely, your final project for this term, will give you a chance to demonstrate your understanding of this whole process.

12
00:01:55,010 --> 00:02:05,884
Frankly, I'm excited that you'll have a chance to learn about these fundamental tools. I actually don't know of anywhere else where you can find this kind of practical instruction in these methods.

13
00:02:05,885 --> 00:02:14,469
In this lesson, you'll get an inside view of how practitioners translate ideas into alpha models and then implement those alpha models in code.

14
00:02:14,469 --> 00:02:26,719
Personally, I'm also very excited that Jonathan has designed this module to give you a real practitioner's perspective of how to combine data with an idea and implement it in code.

15
00:02:26,719 --> 00:02:33,439
Yep, this is awesome. We're all super excited that you've reached this point in your journey with us and we hope you are too.


@@@
1
00:00:00,000 --> 00:00:09,165
So, what are we doing here? What is this all about? As we heard about earlier, our work in this lesson builds upon previous lessons.

2
00:00:09,164 --> 00:00:29,219
While we'll still develop trading strategies with large baskets of stocks and use optimization to find combinations of weights on stocks to seek the highest return for a given level of risk, we'll now learn how to develop models of portfolio returns and risk and add these into our optimization framework.

3
00:00:29,219 --> 00:00:40,005
Before, we always talked about the distribution of future log returns and it's mean and standard deviation like these were numbers, we can just read off of a list somewhere.

4
00:00:40,005 --> 00:00:50,975
But in general, this is not the case. Otherwise, all this would be easy. In addition, we aren't going to explicitly model returns in the way you might expect.

5
00:00:50,975 --> 00:01:03,675
The problem with explicit estimation of returns is that these estimates are very noisy and noise causes unnecessary trading which we want to avoid because trades are costly.

6
00:01:03,674 --> 00:01:14,945
Instead, we will focus on what Jonathan mentioned earlier. Drivers of mean returns and drivers of volatility and these will be the bases of our model.

7
00:01:14,944 --> 00:01:25,564
Drivers of mean returns are alpha factors and drivers of volatility or risk factors. We'll come up with a way to quantify these concepts.

8
00:01:25,564 --> 00:01:32,304
In the next few lessons, we'll learn about factors, how to create and improve them, and how they fit into the model.

9
00:01:32,305 --> 00:01:44,269
We will also explore advanced portfolio optimization using factors. We've actually seen some ideas for factors earlier in this course without explicitly calling them factors.

10
00:01:44,269 --> 00:01:59,799
For example, factors can be based on momentum, or fundamental information. When we can process the data in such a way that they have the potential to provide predictive information about the future movement of stocks, we call these factors.

11
00:01:59,799 --> 00:02:15,915
But what is a factor? A factor is a list of numerical values one assign to each stock in our stock universe that is derived to potentially be predictive of an aspect of the performance of the stocks in the future.

12
00:02:15,914 --> 00:02:27,530
In essence, factors are signals that help suggest where to place bets in a portfolio of stocks and further suggest what the relative magnitudes of those bets should be.

13
00:02:27,530 --> 00:02:38,735
Factors are the basis of quantitative portfolio management, as the search for effective trading strategies starts with a search for factors that drive stock movements.

14
00:02:38,735 --> 00:02:49,223
This lesson will introduce you to an example of a factor and show you how to standardize it so that it can be interpreted as a set of weights for a theoretical portfolio.

15
00:02:49,223 --> 00:02:56,790
After that, we'll walk you through an open source Python library, Zipline which we'll use throughout the module.

16
00:02:56,789 --> 00:03:05,634
You'll also use it in the project to create and evaluate factors encode. Zipline is a Pythonic algorithmic trading library.

17
00:03:05,634 --> 00:03:19,719
It's an event-driven system for back-testing. If you start work at a fund or a bank, it's likely that you will learn proprietary software packages that are built in-house and these will be unique to the firm that you work at.

18
00:03:19,719 --> 00:03:30,150
Learning how to use the open source packages in this program, will give you practice in learning and using custom libraries such as the ones that you may use on the job.

19
00:03:30,150 --> 00:03:39,840
By the end of this lesson, you'll have hands on practice with implementing your first factor in code which will serve as a foundation for the rest of the module.


@@@
1
00:00:00,000 --> 00:00:07,830
Let's walk through an example of a factor to learn more about what a factor looks like. We'll look at a momentum factor.

2
00:00:07,830 --> 00:00:17,359
So, let's say we have a hypothesis that the one year return of a stock gives some signal about future returns over the next few days.

3
00:00:17,359 --> 00:00:28,734
To create a factor, we get price data for stocks within the stock universe of interests and calculate the percent change of today's price from the price from one year ago.

4
00:00:28,734 --> 00:00:40,259
We repeat this for several days of data. The collection of one year returns for the set of stocks over time is what we'll refer to as a factor.

5
00:00:40,259 --> 00:00:52,590
It represents information that can potentially be useful in deciding how much weight we'll give each stock in the portfolio and whether to long or short each stock.

6
00:00:52,590 --> 00:01:01,114
Now, the question is how do we use this factor? Do we use it to try and predict how much a stock will go up the next day?

7
00:01:01,115 --> 00:01:16,039
That's quite difficult when dealing with data that has a lot of noise. What we actually want to do is to use the factor value for each stock to compare it to other stocks that are candidates to be included in a portfolio.

8
00:01:16,039 --> 00:01:31,740
So, let's say we have a portfolio of just two stocks, Apple and NVIDIA. On August 3rd, 2018, Apple's stock price reached $208 per share valuing the company at $1 trillion.

9
00:01:31,739 --> 00:01:50,260
Its price one year before was $156. So, Apple's one year return was about 33 percent. NVIDIA stock price on August 3rd, 2018 was $252, valuing the company at $152 billion.

10
00:01:50,260 --> 00:02:09,685
NVIDIA's price one year before was $166 per share. So, its annual return was about 52 percent. To summarize, Apple's factor value is 0.33 and NVIDIA's factor value is 0.52 for this one day.

11
00:02:09,685 --> 00:02:28,894
Now, what? Well, since the hypothesis of our factor is that a higher number indicates higher future returns, then according to our factor and its associated hypothesis, NVIDIA's return may be higher relative to Apple's over the next few days.

12
00:02:28,895 --> 00:02:41,280
One way to act on this signal is to put more money on the stock for which the factor has a more positive value and less money on the stock that has a less positive value.

13
00:02:41,280 --> 00:02:53,400
We could also short a stock that has a negative factor value. In this hypothetical example, we want to give a higher positive weight to NVIDIA relative to Apple.


@@@
1
00:00:00,000 --> 00:00:10,370
Now, let's see how to convert this raw calculation into a standardized factor. We want the resulting weights to satisfy two conditions.

2
00:00:10,369 --> 00:00:22,285
First, we want the sum of the weights to add up to zero. Second, we want the absolute values of the weights to sum to one.

3
00:00:22,285 --> 00:00:31,524
To satisfy the first condition, we want to find the mean or average of the list of raw factor values for all the stocks.

4
00:00:31,524 --> 00:00:43,289
Then, subtract that mean from each of the raw factor values. We say that we de-mean the factor, because we're subtracting it's mean.

5
00:00:43,289 --> 00:00:52,959
To satisfy the second condition, we want to re-scale the values. To re-scale, we divide each value by a number.

6
00:00:52,960 --> 00:01:03,545
This scalar, equals the sum of the absolute values. So, we re-scale the values by dividing each value by this scalar.

7
00:01:03,545 --> 00:01:18,030
To standardize a vector of factor values, we both de-mean and re-scale, so that the mean of this two list of numbers equals zero, and the sum of the absolute values equals one.

8
00:01:18,030 --> 00:01:31,820
We could do the de-mean or re-scale, one after the other in either order. Let's walk through an example with three stocks, each with a factor value such as a one year return.

9
00:01:31,819 --> 00:01:43,880
First, we want to de-mean the values so that they sum to zero. So, we calculate the mean by adding up the values of A, B, and C, then divide by the number of stocks.

10
00:01:43,879 --> 00:01:56,274
Then, for each stock, we get the raw factor value minus the mean. Next, we want to re-scale so that the sum of absolute values equals one.

11
00:01:56,275 --> 00:02:06,984
We can get the scalar by adding up the absolute values of the numbers. To re-scale, we divide each value by the scalar.

12
00:02:06,984 --> 00:02:21,280
This gives us the standardized values for the factor. We can verify the conditions that the sum is equal to zero and the sum of the absolute values equals one.

13
00:02:21,280 --> 00:02:35,189
Feel free to pause the video, if you want to study these steps in detail. Great. So, now you know how de-meaning and re-scaling can convert raw calculations into candidates for alpha factors.

14
00:02:35,189 --> 00:02:42,544
This is actually how we standardize factors so that we can evaluate them and compare them with other factors.

15
00:02:42,544 --> 00:03:00,764
Next, we'll look at how to interpret, why we take these steps to standardize a factor. But first, here's a random question to ponder, why were the normal distribution and the raw factor both very unhappy when they were told that they would be standardized?


@@@
1
00:00:00,000 --> 00:00:20,460
So, you just learned how to demean a factor so that the values add up to zero. But why do we do this, the short answer is that, later on, we're going to test the factor as if each value associated with each stock also determined, the stock weight in a theoretical portfolio.

2
00:00:20,460 --> 00:00:32,890
We want to make this theoretical portfolio dollar neutral, that is, the dollar amount of all long positions equals the dollar amount of all short positions.

3
00:00:32,890 --> 00:00:41,655
We want to do this to test the predictive power of the factor while excluding the influence of the overall market.

4
00:00:41,655 --> 00:00:54,799
So, why is a dollar neutral portfolio less influenced by overall market movement? Well, market movement refers to the general direction of most stocks in a country or region.

5
00:00:54,799 --> 00:01:09,170
If a portfolio was long only, it would likely move in the same direction as the market. If a portfolio was short only, it would likely move in the opposite direction of the market.

6
00:01:09,170 --> 00:01:20,160
A long short portfolio means we can take long and short positions. A dollar neutral portfolio is a special case of a long-short portfolio.

7
00:01:20,159 --> 00:01:35,815
Is the case where the value of the longs equals the values of the shorts. If a portfolio were halfway along and have short, then the market movement would have a minimal effect on the movement of this dollar neutral portfolio.

8
00:01:35,814 --> 00:01:49,469
So, a dollar neutral portfolio is set up to approximately be market neutral. Note that, an assumption we're making is that on average the Beta of each stock to the market is one.

9
00:01:49,469 --> 00:01:57,114
Remember that, Beta in the capital asset pricing model refers to how much a stock moves when the market moves.

10
00:01:57,114 --> 00:02:07,400
In general, the average Beta of stocks may not be exactly one, but when a portfolio contains thousands of stocks it's a reasonable assumption.

11
00:02:07,400 --> 00:02:15,199
So, a dollar neutral portfolio may move a bit with the market but it would ideally be close to market neutral.

12
00:02:15,199 --> 00:02:24,570
Let's now switch from talking about portfolio positions in dollar amounts to talking about portfolio positions in terms of weights.

13
00:02:24,569 --> 00:02:37,330
Let's review the meaning of portfolio weights and introduce the concept of the notional. The notional or trade book value is the gross dollar amount associated with a portfolio.

14
00:02:37,330 --> 00:02:46,104
The amount of money we'd allocate towards a stock equals its portfolio weight times the portfolio's gross notional value.

15
00:02:46,104 --> 00:02:59,275
If a portfolio notional is 100 million and the stock is given a weight of 0.01, we multiply the weight of 0.01 times 100 million which is 1 million.

16
00:02:59,275 --> 00:03:15,585
This would be the amount of money we'd allocate toward the stock. For a long or short portfolio, we can also use the weight times the notional to see the dollar value placed on that stocks position, whether it's a long position or short position.

17
00:03:15,585 --> 00:03:26,659
For example, assuming a notional of 100 million, let's say we have a stock with a weight of 0.01 this translates to a position of $1 million.

18
00:03:26,659 --> 00:03:42,564
Likewise, another stock with a weight of negative 0.01 then has a position of negative $1 million. Notice in this example, assuming that there are only two stocks in the portfolio, these positions add up to zero.

19
00:03:42,564 --> 00:03:55,639
When all the positions add up to zero, the portfolio is dollar neutral. Remember, the goal of having a dollar neutral portfolio is to make the portfolio market neutral or at least close to market neutral.


@@@
1
00:00:00,000 --> 00:00:06,845
Any theoretical dollar neutral portfolio, we technically wouldn't be using any cash to create the positions.

2
00:00:06,844 --> 00:00:21,980
In fact, sometimes, in academic papers, they call these portfolios, zero investment portfolios. In real life, there are transaction costs, margin costs for shorting positions, and differences in timing of trades.

3
00:00:21,980 --> 00:00:34,128
For the theoretical portfolio used to evaluate a factor, let's not worry about those issues. So, now, let's visualize how we convert a portfolio into a dollar neutral portfolio.

4
00:00:34,128 --> 00:00:49,359
Let's say we have some portfolio weights and want to adjust them so that they are dollar neutral. For example, let's say we have two stocks in the portfolio, the weights are both positive, so the portfolio is not dollar neutral.

5
00:00:49,359 --> 00:01:01,129
If you imagine the weights as actual weights on a balancing scale, the scale would be tilting because it's heavier on the positive side compared to the negative side.

6
00:01:01,130 --> 00:01:07,909
Notice that the relative difference between these two stocks is the weight of A minus the weight of B.

7
00:01:07,909 --> 00:01:18,954
Keep this in mind for what's coming next. If we were to shift the weights to the left towards the negative direction, we can keep going until the weights are balanced.

8
00:01:18,954 --> 00:01:29,504
Notice that the relative difference between the weights of the two stocks is still the same. So, how did we shift our weights to make the portfolio dollar neutral?

9
00:01:29,504 --> 00:01:36,214
Well, we actually took the average of the weights and then subtracted that average from each of the weights.

10
00:01:36,215 --> 00:01:46,355
Does this look familiar to you? Subtracting the mean of the weights, D means the portfolio weights. The sum of the resulting weights now equals zero.


@@@
1
00:00:00,000 --> 00:00:08,144
Okay. Now, let's get an idea for why we re-scale the weights so that the sum of the absolute values equals one.

2
00:00:08,144 --> 00:00:20,655
Our goal is for the portfolio's leverage ratio to be equal to one. So, let's first discuss the concept of leverage, and then we'll learn about what the leverage ratio is.

3
00:00:20,655 --> 00:00:31,785
Leverage is the act of borrowing in order to invest in assets. Usually, the goal when using leverage is to potentially increase the return on investment.

4
00:00:31,785 --> 00:00:39,215
Leverage is related to the word leverage as used in physics. You can think of a lever resting on a fulcrum.

5
00:00:39,215 --> 00:00:46,454
If you push the short side of the lever down by one meter, the long side will move by more than one meter.

6
00:00:46,454 --> 00:00:56,210
For a long/short portfolio, the money that is borrowed for leverage can come from borrowing cash and also from taking short positions in stocks.

7
00:00:56,210 --> 00:01:04,364
Recall that to take a short position, we borrow an asset and sell it in the market with a promise of buying it back in the future.

8
00:01:04,364 --> 00:01:11,289
We can use the cash received from these short positions in order to buy other stocks in the portfolio.

9
00:01:11,289 --> 00:01:23,104
Without leverage, if we invested in a stock, then a 10 percent change in the stock price would lead to a 10 percent change relative to our initial invested capital.

10
00:01:23,105 --> 00:01:36,135
With leverage, if we borrowed cash to double our initial investment, then a 10 percent change in the stock price will lead to a 20 percent increase relative to our initial capital.

11
00:01:36,135 --> 00:01:50,314
Let's look at an example without leverage. Pretend that we have $100,000 as initial capital. If the stock price increases by 10 percent, then the portfolio gains $10,000.

12
00:01:50,314 --> 00:01:59,145
Relative to the initial capital, the investment increased 10 percent. Now, let's look at a scenario where we use leverage.

13
00:01:59,144 --> 00:02:10,844
Start with $100,000 of initial capital. Borrow 100,000 in cash. Use the 200,000 in cash to invest in the stock.

14
00:02:10,844 --> 00:02:24,069
If the stock increases by 10 percent, then the investment increases by $20,000. Relative to the initial capital of 100,000, this is actually an increase of 20 percent.

15
00:02:24,069 --> 00:02:33,515
Since the potential investment percent gain of 20 percent is greater than the stock price gain of 10 percent, we say that this position is leveraged.


@@@
1
00:00:00,000 --> 00:00:10,155
At first glance, leverage looks awesome because if we take enough short positions, we could technically have enough cash to pay for all of our long positions.

2
00:00:10,154 --> 00:00:26,820
But does this mean that more leverage is always better? Unfortunately, the answer is no. If the stock price went down 10 percent, then our leveraged position would go down 20 percent, which is greater than the loss we'd incur without leverage.

3
00:00:26,820 --> 00:00:35,589
So, as the saying goes, there's no such thing as a free lunch. Which means that the benefits of leverage come at a cost.

4
00:00:35,590 --> 00:00:49,354
Using leverage can increase our potential gain but can also increase our potential losses. A useful measure of the amount of leverage in a portfolio is the leverage ratio.

5
00:00:49,354 --> 00:01:00,590
In terms of dollars, if we added up the magnitudes of the long and short positions, and divided that sum by the notional, this would be the leverage ratio.

6
00:01:00,590 --> 00:01:12,755
For example, if a portfolio had a notional or initial capital of $1 million and had $1 million in long positions, the leverage ratio would be one.

7
00:01:12,754 --> 00:01:24,584
If the portfolio had the same notional of 1 million but borrowed cash to go long two million positions, its leverage ratio would be two million divided by one million or two.

8
00:01:24,584 --> 00:01:38,594
If the portfolio had long positions totaling $1 million and short positions also totaling $1 million, then the leverage ratio would also be two million divided by one million, which is two.

9
00:01:38,594 --> 00:01:45,774
If the longs were two million and the shorts were two million, then the leverage ratio would be four.

10
00:01:45,775 --> 00:01:55,920
So, I hope you noticed that the more positions we take and absolute magnitude relative to the initial capital, the higher the leverage ratio.

11
00:01:55,920 --> 00:02:11,240
For theoretical portfolio, that's used for analyzing a factor, we assume a $1 notional. We add up the sum of the absolute values of the weights, then divide by the $1 notional to calculate the leverage ratio.

12
00:02:11,240 --> 00:02:29,319
Dividing by one doesn't change the sum. So, we can skip the division. For example, if we had three stocks in the portfolio with weights of negative 0.5, positive 0.3, and positive 0.2, the sum of their absolute values is one.

13
00:02:29,319 --> 00:02:44,040
So, the leverage ratio is also one. If on the other hand, the portfolio had weights of negative one, positive 0.6, and positive 0.4, then the sum of their absolute values is two.

14
00:02:44,039 --> 00:02:54,414
So, the leverage ratio is also two. Let's revisit our goal, which was to rescale portfolio weights, so that we have a leverage ratio of one.

15
00:02:54,414 --> 00:03:01,419
To do this, we can sum up the absolute values of the weights and divide each of our original weights by this sum.

16
00:03:01,419 --> 00:03:09,840
By doing this division, we are re-scaling the factor values so that the sum of their magnitudes equals one.


@@@
1
00:00:00,000 --> 00:00:09,625
So, let's revisit the steps we take to convert factor values into portfolio weights in a dollar neutral portfolio with a leverage ratio of one.

2
00:00:09,625 --> 00:00:19,525
To make the factor represent weights of a dollar neutral portfolio, de-mean the vector of values. Do this by subtracting the average from each value.

3
00:00:19,524 --> 00:00:30,184
To make the factor represent a portfolio with leverage ratio of one, re-scale the vector of values so that the sum of absolute values equals one.


@@@
1
00:00:00,000 --> 00:00:12,765
Hello and welcome. In this lesson, we will introduce you to the zipline pipeline. Zipline is an open-source algorithmic trading simulator developed by Quantopian.

2
00:00:12,765 --> 00:00:22,734
In this notebook, we will see how to create a pipeline with screens, factors, and filters on how to run them using a pipeline engine.

3
00:00:22,734 --> 00:00:34,855
So, let's get started. So, why do we need a pipeline? One reason is that on any given trading day, the entire universe of stocks consists of thousands of securities.

4
00:00:34,854 --> 00:00:44,380
Usually, you will not be interested in investing in all of them, but rather you're most likely to only select a few of them to invest.

5
00:00:44,380 --> 00:00:52,170
For example, you may only want to invest in stocks that have a 10-day average closing price of $10 or less.

6
00:00:52,170 --> 00:01:00,495
In order to avoid spending a lot of time doing data wrangling to select only the stocks you want, people often use pipelines.

7
00:01:00,494 --> 00:01:10,370
So, in general, a pipeline is a placeholder for a series of data operations used to filter and rank data according to some factors.

8
00:01:10,370 --> 00:01:18,245
Before we start building our pipeline, we will first see how we can load the stock data we're going to use into Zipline.

9
00:01:18,245 --> 00:01:28,825
Zipline uses data bundles to make it easy to use different data sources. In this notebook, we will be using stock data from quote media.

10
00:01:28,825 --> 00:01:36,695
In the Udacity workspace, you will find that the stock data from quote media has already been ingested into Zipline.

11
00:01:36,694 --> 00:01:45,195
So, we will only need to load the data. To load the data, we will use zipline's load function from the bundles class.

12
00:01:45,194 --> 00:01:53,329
In order to load a previously ingested data bundle, we must pass the name of the data bundle to the load function.

13
00:01:53,329 --> 00:02:01,579
In this case, the name of our previously ingested data bundle is eod-quotemedia, which is specified here.

14
00:02:01,659 --> 00:02:13,594
The first thing the load function does is to look for the most recent ingested data. Therefore, we must also specify the location of the previously ingested data bundle.

15
00:02:13,594 --> 00:02:21,270
This is done by setting the zipline route variable to the path where the most recent data is located.

16
00:02:21,270 --> 00:02:29,105
Before we load the data bundle, we must also register the data bundle and its corresponding ingest function.

17
00:02:29,104 --> 00:02:38,030
After we load our data, we're ready to build our first pipeline. We will now start building our pipeline step-by-step.

18
00:02:38,030 --> 00:02:46,715
We will start by building an empty pipeline with the screen. To build a pipeline, we will use Zipline's pipeline class.

19
00:02:46,715 --> 00:02:56,610
In this example, we have used a screen that selects the top 10 assets with the highest average dollar volume within a 60-day window.

20
00:02:56,610 --> 00:03:08,405
This screen acts as a filter to exclude data from our stock universe every day. The average dollar volume is a good first pass filter to avoid illiquid assets.

21
00:03:08,405 --> 00:03:16,355
This way, we can guarantee that the selected assets have enough daily trading volume to fill our orders quickly.

22
00:03:16,354 --> 00:03:29,045
It is important to note that this freshly constructed pipeline is empty. This means that it doesn't know yet how to compute anything, anyone produce any values is we ask for its output.

23
00:03:29,044 --> 00:03:38,754
The next step in building a pipeline is to add factors and filters. We will now take a look at two types of computations that can be expressed in a pipeline.

24
00:03:38,754 --> 00:03:50,504
Factors and filters. Let's take a look at factors first. In general, a factor is a function from an asset at a particular moment in time to a numerical value.

25
00:03:50,504 --> 00:04:01,430
A simple example of a factor is a most recent price of a security, because the most recent price of a security is just a number, for example, $10.

26
00:04:01,430 --> 00:04:08,340
On the other hand. A filter is a function from an asset at a particular moment in time to a boolean value.

27
00:04:08,340 --> 00:04:18,795
Boolean values are either true or false. An example of a filter is a function indicating whether a security whose price is below $5.

28
00:04:18,795 --> 00:04:33,785
This is because at any particular moment in time, this statement will either be true or false. So, the difference between factors and filters is that factors return numerical values, while filters return boolean values.

29
00:04:33,785 --> 00:04:41,069
Zipline comes with some built-in factors and filters, but also allows you to combine and create custom factors and filters.

30
00:04:41,069 --> 00:04:48,790
Before we learn how to add factor some filters to our pipeline, let's take a look at a nice feature of the pipeline class.

31
00:04:48,939 --> 00:04:59,245
A neat feature, a Zipline's Pipeline class, that it comes with the attribute show graph that allows you to render the pipeline as diagram.

32
00:04:59,245 --> 00:05:08,805
This diagram is specified using the DOT language, and consequently, we need DAG graph layout program to view the rendered image.

33
00:05:08,805 --> 00:05:16,230
In this notebook, we will use the package Graphviz to render the diagram produced by this show graph attribute.

34
00:05:16,230 --> 00:05:23,574
Let's take a look at the current diagram of our pipeline. Right now, our pipeline is empty, and it only contains a screen.

35
00:05:23,574 --> 00:05:39,894
Therefore, when we render our pipeline, we only see the diagram of the screen. We can see that our screen takes as input, the closing price and volume from the US equity pricing data set to calculate the average solar volume in a 60-day window.

36
00:05:39,894 --> 00:05:47,675
At the bottom of the diagram, we can see that the output is determined by the expression x_0 less than or equal to 10.

37
00:05:47,675 --> 00:05:58,484
This expression reflects the fact that we only selecting the top 10 assets. As we are factors and filters through our pipeline, this diagram will get more complicated.

38
00:05:58,485 --> 00:06:06,000
In this diagram, we saw that our screen takes as input prices and volume from the US equity pricing dataset.

39
00:06:06,000 --> 00:06:19,019
So, let's take a moment to talk about the datasets and data loaders. Another feature of Zipline is that it separates the actual source of the stock data from the abstract description of that dataset.

40
00:06:19,019 --> 00:06:32,839
Therefore, Zipline differentiates between the actual dataset and the loader for that dataset. For example, the loader used for the USEquity pricing dataset is the USEquityPricingLoader.

41
00:06:32,839 --> 00:06:43,810
The USEquityPricingLoader class can also be used to load, open, high, low, close volume data from other data sets, like the one from quotemedia.

42
00:06:43,810 --> 00:06:55,159
Therefore, we will set the USEquityPricingLoader as our data loader. Before we add our factors and filters, let's take a look at the raw data in our quotemedia data bundle.

43
00:06:55,160 --> 00:07:06,670
This requires a couple of steps. The first step is to build a pipeline engine. This is because, in order to execute a pipeline, Zipline employs pipeline engines.

44
00:07:06,670 --> 00:07:16,209
The SimplePipelineEngine class that we've used here associates a data loader with a trading calendar and a corresponding data bundle.

45
00:07:16,209 --> 00:07:27,394
It is important to note that the get loading parameter must be a callable function, and this is the reason we have defined this function right here, that's [inaudible] are pricing loader.

46
00:07:27,394 --> 00:07:37,290
We will also use the trading calendar used by the New York Stock Exchange. Once we have chosen our pipeline engine, we're ready to execute our pipeline.

47
00:07:37,290 --> 00:07:44,939
We can execute our pipeline by using the.run_pipeline attribute from this simple pipeline engine class.

48
00:07:44,939 --> 00:08:04,745
In this example, we will run our pipeline for a single day. We can see that the output of the pipeline is a Pandas DataFrame with a MultiIndex, where the first index level contains a trading dates, and the second index level contains the tickers for the stocks that have passed our screen.

49
00:08:04,745 --> 00:08:18,330
This tickers can be accessed and saved into a list. Once we have the tickers for the stocks that have passed or pipeline screen, we can get the historical stock data for those tickers from our data bundle.

50
00:08:18,329 --> 00:08:28,564
In order to get the historical data, we need to use Zipline's data portal class. A data portal is an interface to all the data that a Zipline simulation needs.

51
00:08:28,564 --> 00:08:37,710
Once we created the data portal like we've done here, we can get the historical data by using the get_history_window attribute.

52
00:08:37,710 --> 00:08:58,470
Here, we can see the historical data for the given start and end dates. It is important to note that when a pipeline returns a date, for example, January 7th, 2011, this includes data that will only be known prior to the market opening on that date.

53
00:08:58,470 --> 00:09:11,514
Therefore, the price shown for January 7th, 2011 is actually the closing price from the day before. Finally, let's see how we can add factors and filters to our pipeline.

54
00:09:11,514 --> 00:09:33,769
We can add both factors and filters to our pipeline using the add method from the pipeline class. The first parameter in the add method represents the factor or filter we're going to add to our pipeline, and the second parameter is a string that determines the name in the column, in the output DataFrame for that factor or filter.

55
00:09:33,769 --> 00:09:43,289
Here, we have added factor that computes the 15-day mean closing price. Now, let's render our pipeline to see what it looks like.

56
00:09:43,289 --> 00:09:59,589
We can clearly see our factor in the pipeline now. If we want our pipeline, we now see a column that contains the output of the factor, namely the 15-day mean closing price for each stock that passed our screen.

57
00:09:59,590 --> 00:10:11,985
Now, let's add a filter to our pipeline. Here, we've created a filter that returns true whenever the 15-day average closing price is above $100.

58
00:10:11,985 --> 00:10:19,464
Like I mentioned earlier, we can add this filter to our pipeline by using the.add method would use before.

59
00:10:19,465 --> 00:10:43,515
Now, let's run through our pipeline to see what it looks like. We can now see our filter in the diagram and if we run our pipeline, we can now see a column that contains the output of the filter with true for every stock that had 15-day average closing price above $100, and that passed our screen.

60
00:10:43,514 --> 00:10:52,360
That's it. Now, you know how to create a pipeline with screens, factors, and filters, and how to run them using a pipeline engine.


@@@
1
00:00:00,000 --> 00:00:11,320
Welcome to this lesson. Now that you've seen an example of a factor and how to create one from data, we'll give you a closer look at the theory behind these methods by introducing the factor model.

2
00:00:11,320 --> 00:00:16,984
This will help you understand the broader context around what you're doing when you work with factors.

3
00:00:16,984 --> 00:00:27,864
After that, we' ll walk you through some major categories of factors. These are factors such as price volume, fundamentals, sentiment, and alternative factors.


@@@
1
00:00:00,000 --> 00:00:15,019
Okay. So, we've talked about factors in the abstract and in your head you're thinking, "Okay, it's a vector where the values for each stock are proportional to some aspect of the future performance of that stock." Yes, you're right.

2
00:00:15,019 --> 00:00:22,515
When I first learned this, in some ways, it wasn't very satisfying to me. But there's a sense in which that's what you need to know.

3
00:00:22,515 --> 00:00:28,759
But I want to give you more because there is more to the story but that's where it gets more complicated.

4
00:00:28,760 --> 00:00:38,580
So stay with me and I promise it will pay dividends in your learning for the next several lessons. There is a formalism behind factors, let me tell you a little about it.

5
00:00:38,579 --> 00:00:54,615
There's such a thing in the statistical literature called a factor model. A factor model is a statistical model used to describe variability among observed correlated variables, in terms of potentially smaller number of unobserved variables called factors.

6
00:00:54,615 --> 00:01:02,285
This method is actually used in several disciplines including biology, psychology, and business, as well as finance.

7
00:01:02,284 --> 00:01:12,215
Whenever practitioners are looking to discover a smaller number of so-called latent variables that explain correlations in a set of variables of interest.

8
00:01:12,215 --> 00:01:21,765
We could apply this type of analysis to many situations, but in finance, we're typically interested in modelling the returns of several assets.

9
00:01:21,765 --> 00:01:34,340
We might notice that the returns of some assets seem to vary together. Are these companies similar in some way such that some underlying effect influences both of them in the same way?

10
00:01:34,340 --> 00:01:42,575
What if we could model the returns of a large group of stocks with a smaller set of variables which explain their common variability?

11
00:01:42,575 --> 00:01:55,315
This is what we attempt to do with factor models. The observed variables, in our case returns, are modeled as linear combinations of the factors returns plus error terms.

12
00:01:55,314 --> 00:02:11,245
So a generic linear factor model looks something like this: we model the return on a stock, bond mutual fund or something else as a linear function of returns attributed to several factors.

13
00:02:11,245 --> 00:02:26,050
The values of these factor returns are the same for all stocks. The factor exposures tell you how much the return of an individual asset should change per unit change in a factor return.

14
00:02:26,050 --> 00:02:34,789
Here, the last term, the error term, is the portion of the return on asset not related to any of the factors.

15
00:02:34,789 --> 00:02:47,730
In words, the return of any stock, i, can be decomposed into the returns of factors times the stock's exposures to those factors, plus an unexplained portion.


@@@
1
00:00:00,000 --> 00:00:12,614
We left off with the idea that the return of any stock, i, can be decomposed into the returns of factors times the stocks exposures to those factors plus an unexplained portion.

2
00:00:12,615 --> 00:00:23,420
But wait, this is looking a lot like multiple regression. What's the difference? Well, regression is one tool you might use to build a factor model.

3
00:00:23,420 --> 00:00:34,079
Also, in a factor model, the independent variables or factor returns are usually things we think are there, but have an influence that we can't measure directly.

4
00:00:34,079 --> 00:00:45,370
These are called unobserved or latent random variables. We may need to take other steps to produce these time series before we can run a multiple regression.

5
00:00:45,369 --> 00:00:52,620
For example, what if we think a company's size has something to do with the performance of its stock price?

6
00:00:52,619 --> 00:01:01,355
Maybe, smaller companies stock prices produced higher returns? Okay. That seems reasonable, so let's do a regression.

7
00:01:01,354 --> 00:01:09,625
But hang on, what data do we have? We have time series of returns for several stocks. You've seen those data before.

8
00:01:09,625 --> 00:01:18,149
Okay. So size, how do we measure that? Lets say, we're specifically thinking that market cap is the metric of interests.

9
00:01:18,150 --> 00:01:27,250
Market cap is the market value of a publicly traded companies outstanding shares. It's the share price multiplied by the number of shares outstanding.

10
00:01:27,250 --> 00:01:34,330
So, we'd have market cap for every company in our universe for every day, because it changes over time.

11
00:01:34,329 --> 00:01:50,585
That sounds like the data could be organized in a table or a matrix. But to create a single factor return time-series, we're looking for a single time-series of values that represents the effect of the size factor across stocks.

12
00:01:50,584 --> 00:02:00,754
Do you see the challenge we're facing? How do we create a single time series that represents our idea that small-cap companies generally outperform large-cap companies.

13
00:02:00,754 --> 00:02:16,484
Moreover, how do we quantify this effect on the returns of a whole set of stocks? A latent variable is something like this, something nebulous that we want to represent as a time series like a normal variable, but it's hard to directly measure.

14
00:02:16,485 --> 00:02:23,735
What we do in this case to create a single time series for our factor, is we create a theoretical portfolio.

15
00:02:23,735 --> 00:02:33,125
This portfolio long small caps and shorts large-caps every day. The time series we seek is the portfolio's daily return.

16
00:02:33,125 --> 00:02:41,930
This is how we quantify our idea that small caps should outperform large-caps and generate a single time series to represent this idea.


@@@
1
00:00:00,000 --> 00:00:10,675
There are a couple of assumptions inherent to strict factor models. The first is that the residual return is assumed to be uncorrelated with each of the factor returns.

2
00:00:10,675 --> 00:00:23,214
This does not restrict the set of possible models as much as it may seem. The factor exposures can be adjusted to achieve the condition where the residuals are uncorrelated with the factor returns.

3
00:00:23,214 --> 00:00:30,000
In fact, in simple settings using historic data, this can be achieved with multiple regression procedures.

4
00:00:30,000 --> 00:00:39,149
The key assumption of a linear factor model is that the residual for one asset's return is uncorrelated with the residual of any other asset.

5
00:00:39,149 --> 00:00:48,000
This means that the only correlations among asset total returns are those that arise due to their exposures to the factors.

6
00:00:48,000 --> 00:00:57,634
The residual component of an asset's return is assumed to be unrelated to the residual of any other asset, and hence totally specific to that asset.

7
00:00:57,634 --> 00:01:06,034
In other words, the risk associated with the residual return is specific to or idiosyncratic to the asset in question.

8
00:01:06,034 --> 00:01:14,450
This assumption makes a linear factor model powerful in the sense that it rules out many possible combinations of outcomes.

9
00:01:14,450 --> 00:01:21,305
But this power comes at a cost. The more restrictive a model is, the greater the chance that it may be wrong.

10
00:01:21,305 --> 00:01:36,390
For this reason, if using a strict factor model, it is important to try to account for the most important sources of correlations by including a sufficient number of factors, while also attempting to include only the most important ones.

11
00:01:36,390 --> 00:01:44,270
That said, there's also value to simplicity in modeling, since the goal is always to model signal and not noise.


@@@
1
00:00:00,000 --> 00:00:12,155
Okay. We talked about the assumptions inherent to factor models. Now, let's derive the covariance matrix of the returns, in terms of the factor returns, exposures, and residual term.

2
00:00:12,154 --> 00:00:23,954
We need this description of the return variants, because we want to be able to use our factor model to describe and manage the portfolio variance, and use this information for optimization.

3
00:00:23,954 --> 00:00:38,490
We'll need to use the two assumptions; one, that the residual of any asset is uncorrelated with the factor returns, and two, that the residual of any asset is uncorrelated with the residual of any other asset.

4
00:00:38,490 --> 00:00:49,350
Before we begin, we subtract from each asset random variable its mean. Such that the mean of this shifted return distribution is zero.

5
00:00:49,350 --> 00:01:00,450
We start with this equation, which is a factor model of the stock return. If there are many assets then r is a vector of observable random variables.

6
00:01:00,450 --> 00:01:12,409
What I mean by that is, r1 is a random variable that represents the return of asset one. It can take on any value with a probability determined by a probability distribution.

7
00:01:12,409 --> 00:01:27,255
R, is a vector of length equal to the number of assets. B, is a matrix of fixed that is not random coefficients, it has dimensions of assets by factors.

8
00:01:27,254 --> 00:01:37,855
F, is a vector of random variables, each of which represents the value of a factor return. Finally, this is the vector of residuals.

9
00:01:37,855 --> 00:01:44,655
Which as we said, are specific to the assets. So, there is a residual random variable for each asset.

10
00:01:44,655 --> 00:01:55,924
Now, we want to calculate the covariance matrix of the asset returns. Remember, that the covariance of two random variables can be calculated with this formula.

11
00:01:55,924 --> 00:02:03,700
But if the means of the two random variables are zero, this simplifies to the expectation value of their product.

12
00:02:03,700 --> 00:02:12,014
The expectation value of a matrix is just a matrix with the expectation value operator applied to each element.

13
00:02:12,014 --> 00:02:24,710
The matrix of products of asset returns, is just rr transpose. So, the covariance matrix can be written as the expectation value of rr transpose.

14
00:02:24,710 --> 00:02:37,954
This follows by our model for r. First, distribute the transpose to the two terms on the right. Here, we just expand out the matrix product.

15
00:02:37,955 --> 00:02:49,290
Now, distribute the transposes. We can pull the B matrix out of the expectation function, because it is not a random variable, it is fixed.

16
00:02:49,289 --> 00:02:58,609
Here, we use the fact that the residual errors and factor returns are assumed to be uncorrelated. The first assumption we discussed earlier.

17
00:02:58,610 --> 00:03:06,269
This means that the expected value of the factor returns times the errors is zero. So, the two terms in the middle becomes zero.

18
00:03:06,270 --> 00:03:20,704
We will now simplify our notation to represent the covariance matrix of the factor returns. Finally, we write down the covariance matrix of the residuals, which we know to be a diagonal matrix.

19
00:03:20,705 --> 00:03:29,485
Meaning that only the variances of the residuals may be nonzero, but the covariances between residuals of different assets are zero.

20
00:03:29,485 --> 00:03:37,854
The reason for this, is the second assumption from earlier; that the error random variables are assumed to be uncorrelated across assets.


@@@
1
00:00:00,000 --> 00:00:07,940
Okay. So, we've learned a little about factor models. So, how are they used in practice? It might not be what you expect.

2
00:00:07,940 --> 00:00:21,565
Most practitioners don't use factor models to explicitly model asset returns. Factor modeling has a long history in academic research, and some of these methods were used in the past to explicitly model returns time series.

3
00:00:21,565 --> 00:00:27,690
But when devising and testing trading strategies, the models are typically used slightly differently.

4
00:00:27,690 --> 00:00:39,655
We've seen the equations for returns and for risk in terms of factors. One key goal is that we want to be able to use these expressions in a portfolio optimization problem.

5
00:00:39,655 --> 00:00:48,120
Now, let me tell you about a couple of simplifications that quants make at this point. Let's take the matrix of factor exposures.

6
00:00:48,119 --> 00:00:56,004
As we've said, a value in this matrix represents the sensitivity of an individual asset to a specific factor return.

7
00:00:56,005 --> 00:01:06,549
A portfolio with weights x has a portfolio factor exposure of B transpose x. Now, let's introduce a new idea.

8
00:01:06,549 --> 00:01:15,079
Let's say we think there are actually two types of factors. One type of factor is predictive of the mean of the distribution of returns.

9
00:01:15,079 --> 00:01:25,983
The other type is predictive of the variance of the distribution, but not of the mean. The first describes our alpha factors, while the second describes our risk factors.

10
00:01:25,983 --> 00:01:42,245
We'd rather our portfolio be minimally exposed to risk factors. The drivers of volatility. We can try to make this happen by placing constraints on B transpose x that only apply to factors that we think are drivers of volatility.

11
00:01:42,245 --> 00:01:52,669
We do this typically in the constraints section of the optimization problem. This will specifically reduce the exposure of our portfolio to these risk factors.

12
00:01:52,670 --> 00:02:05,159
However, we don't constrain the factors that are drivers of mean returns. So, we drop them from B. Hence, you can think of B now as the risk factor loading matrix only.

13
00:02:05,159 --> 00:02:12,180
The alpha factors aren't kept in here because we're not going to constrain them the way we do with the risk factors.

14
00:02:12,180 --> 00:02:26,805
What about F and S. We think about F as the covariance matrix of factor returns that have large impact on variance across all stocks, and S as the variance that's left over.

15
00:02:26,805 --> 00:02:35,189
In fact, we define risk factors precisely as factors that have large impact on variance across all stocks.

16
00:02:35,189 --> 00:02:54,429
So, we include only the risk factors in F. Anything that's left over, like variance from the alpha factors we took out and everything we can't explain, is accounted for in S. We can use F and S together to constrain portfolio risk.

17
00:02:54,430 --> 00:03:03,454
The key takeaway here though is that, in practice, B and F explicitly contain only information about risk factors.

18
00:03:03,455 --> 00:03:15,530
S says nothing explicitly about alpha. Practitioners will usually buy F, S, and B from a commercial provider, and now we can see why that is sufficient.

19
00:03:15,530 --> 00:03:23,675
We typically don't mix our alphas and risk factors. What about the alpha factors we took out from B?

20
00:03:23,675 --> 00:03:31,540
We are left with some number of alpha factors which are values per stock per factor. What do we do with those?

21
00:03:31,539 --> 00:03:44,669
This is where the creation of the objective function and optimization comes in. We need to combine these into a single vector, which we will do in later projects, and optimize our resulting weight to these.


@@@
1
00:00:00,000 --> 00:00:10,070
In the coming series of lessons, we'll go into detail about risk factors and alpha factors. So, what's the difference, and why do we make this distinction?

2
00:00:10,070 --> 00:00:25,649
First, let's start with how factors, whether risk factors or alpha factors, are the same. The use of factors this is based on the assumption that stocks with similar characteristics may move up or down in similar ways.

3
00:00:25,649 --> 00:00:38,725
For example, stocks may exhibit similar price movements if they are also within the same sector or are similar in market cap or based in the same country or have similar fundamentals.

4
00:00:38,725 --> 00:00:47,509
Factors represent these common characteristics such as sector, market cap, or market return or the country or book-to-market ratio.

5
00:00:47,509 --> 00:01:01,330
If we model the return of a stock as the sum of the contributions from risk factors and alpha factors, then we notice that each of these factors, both alpha and risk factors, contribute to the stock return.

6
00:01:01,329 --> 00:01:13,984
In other words, each factor is adding a little bit to the movement of the stock price. However, in practice, risk factors and alpha factors are used to accomplish different goals.

7
00:01:13,984 --> 00:01:21,605
We want to use factors to help us learn something about the distribution of expected future returns.

8
00:01:21,605 --> 00:01:36,560
Factors could be predictive of the mean of the distribution of expected returns or not. They could also help to explain a significant amount of the variance of the return distribution or not.

9
00:01:36,560 --> 00:01:44,939
Factors that significantly help explain the variance of the return distribution are candidates for use as risk factors.

10
00:01:44,939 --> 00:01:59,335
We want to find a set of risk factors that explain much of the variance of a portfolio. The reason is so that we can adjust portfolio weights to reduce that variance that's caused by these risk factors.

11
00:01:59,334 --> 00:02:08,055
Reducing the movement of a portfolio that may be attributed to risk factors is how we control portfolio risk.

12
00:02:08,055 --> 00:02:25,735
One major risk factor is the market return from the capital asset pricing model. We saw earlier how adjusting portfolio weights to make them dollar neutral also helps to make a portfolio less sensitive to overall market movement thereby controlling risk.

13
00:02:25,735 --> 00:02:34,259
Factors that are significantly predictive of the mean of the return of distribution are candidates for use as alpha factors.

14
00:02:34,259 --> 00:02:43,469
We want to find alpha factors that are predictive of future price movements after we've neutralized price movements using a risk model.

15
00:02:43,469 --> 00:02:59,594
For example, if a factor signals that the mean of the expected return distribution is greater than zero, then this signal may indicate that the stock return will be positive in the future which also means it's a signal to buy.

16
00:02:59,594 --> 00:03:11,324
One example of a former alpha factor is the market cap of a stock. Small-cap stocks tend to have higher returns compared to large cap stocks.

17
00:03:11,324 --> 00:03:21,615
Factors that are neither useful in explaining the variance nor predictive or the mean of the return distribution would probably not be used as either alpha or risk factors.

18
00:03:21,615 --> 00:03:34,400
One example would be days when there are full moons. Even though a full moon may tell you when to expect werewolves or other magical creatures to appear, it probably wouldn't tell you much about which stocks to long or short.


@@@
1
00:00:00,000 --> 00:00:16,469
We generally observe different attributes among risk factors versus alpha factors. Even though factors fall along a graded incremental spectrum of these attributes, it helps to put them into two groups to get a sense of how risk factors and alpha factors differ.

2
00:00:16,469 --> 00:00:36,164
One difference is in the magnitude of the factors contribution to the stock's price movement. When we choose a set of 20 or so risk factors that explain a good portion of the variance of stocks, then taken as a whole, these risk factors tend to be major contributors to the price movement of most stocks.

3
00:00:36,164 --> 00:00:46,170
These are factors such as the market return, the country in which the company operates, the sector or line of business, and interest rates set by central bank.

4
00:00:46,170 --> 00:01:05,909
In comparison, a single alpha factor may have a smaller contribution to a stock's price movement. As an example, the book-to-market ratio multiplied by idiosyncratic volatility or the trajectory of a stock's return over time are examples of potential alpha factors.

5
00:01:05,909 --> 00:01:16,084
You will actually get an in-depth look at these two alpha factors in a later lesson. But for now just assume that we're talking about two examples of alpha factors.

6
00:01:16,084 --> 00:01:27,990
The contribution of these alpha factors may be statistically significant, but in terms of magnitude, their impact on the variance of the stock may be smaller than that of a set of common risk factors.

7
00:01:27,989 --> 00:01:36,564
This is an important reason for why we want to find a useful set of risk factors and neutralize our portfolio's exposure to them.

8
00:01:36,564 --> 00:01:46,840
If we didn't, then the portfolio's movements due to these risk factors would likely be so large as to overwhelm the effects of the alpha factors.


@@@
1
00:00:00,000 --> 00:00:06,099
Another descriptive attributes of common risk factors, is that they are well known by the investment community.

2
00:00:06,099 --> 00:00:27,460
This is not just a descriptive attribute, it also affects how risk factors drive price movements. When most of the investors in the market understand how a factor drives the price movement of a stock, then it's unlikely that anyone will gain a competitive advantage in generating higher than expected returns based on that factor.

3
00:00:27,460 --> 00:00:40,865
Why is that? Because investors who are seeking to maintain a competitive position will monitor the movements of these risk factors, and we'll adjust their portfolios in response to changes in these factors.

4
00:00:40,865 --> 00:00:51,585
Researchers have looked into how the performance of funds that use particular factors declines after those factors are widely published in academic papers.

5
00:00:51,585 --> 00:01:01,734
For example, the findings that small-cap stocks tend to have higher returns than large-cap stocks is also known as the size effect.

6
00:01:01,734 --> 00:01:17,170
Research that details this effect was first published in 1981. A mutual fund was actually created to make use of the size factor and it was called the dimensional fund advisor small company portfolio.

7
00:01:17,170 --> 00:01:31,099
In a study conducted in 2003, researchers showed how this particular funds performance after the 1981 publication, no longer generated abnormal returns based on the size factor.

8
00:01:31,099 --> 00:01:44,144
One likely explanation, is that the act of publishing the size factor and its ability to generate abnormal returns, made the size factor well-known to practitioners at investment funds.

9
00:01:44,144 --> 00:01:54,124
The practitioners then acted on this knowledge and over time, traded away this mispricing. That is one consequence of being risk factors.

10
00:01:54,123 --> 00:02:09,090
They're not likely to be used to enhance the portfolio return. In other words, risk factors may give an indication for how much the portfolio stocks bounce up and down like boats without sails, moving up and down with the ocean waves.

11
00:02:09,090 --> 00:02:22,215
However, just as a boat without sales can't drive its average position in a particular direction, an investor won't use risk factors to drive their returns in a particular direction.

12
00:02:22,215 --> 00:02:32,440
To summarize the idea, we say that risk factors are drivers of portfolio variance, but not drivers of the portfolio's mean return.


@@@
1
00:00:00,000 --> 00:00:09,615
Alpha factors eventually lose their performance as they become more publicly known among investors, and then eventually become risk factors.

2
00:00:09,615 --> 00:00:19,050
Let's look at an analogy to help frame our discussion of how this happens. Let's say I drive to work and I normally deal with a lot of traffic.

3
00:00:19,050 --> 00:00:26,390
So, I download this cool new app that tells me which lane I should choose, that will let me get to work faster.

4
00:00:26,390 --> 00:00:39,365
Since I'm one of the few people who has downloaded and used the app so far, I'm able to find lanes that have fewer cars and move through traffic a little bit faster than the other drivers.

5
00:00:39,365 --> 00:00:46,595
You can think of this app as my alpha factor, because it's helping me improve my performance on the road.

6
00:00:46,594 --> 00:00:56,049
Now let's say this app becomes really popular. So, most other drivers now use it to help them find the fastest lane.

7
00:00:56,049 --> 00:01:07,549
Now, when the app tells me to take a particular lane, many other drivers next to me also get the same signal, so they also choose that same lane.

8
00:01:07,549 --> 00:01:16,769
Many drivers actually get into the lane before or at the same time that I do, so I no longer have the ability to get ahead of the other drivers.

9
00:01:16,769 --> 00:01:23,989
Moreover, since everyone is following this app, the app is now driving the variance of these cars on the road.

10
00:01:23,989 --> 00:01:38,704
You can imagine many cars now constantly changing lanes concurrently, as a group like a school of fish, and yes, I just used a fish analogy to explain the car analogy, which in turn I'm using to explain alpha factors.

11
00:01:38,704 --> 00:01:52,129
We can now think of this driving app as a risk factor, because it is driving the movement of many cars, and therefore, the variance, but is no longer helping any of us improve our individual performance.


@@@
1
00:00:00,000 --> 00:00:16,509
We can see a similar process in the stock market. If you find an Alpha signal that suggests that a stock will have positive future returns, the assumption of future higher returns means that the price today is lower then the expected future price.

2
00:00:16,510 --> 00:00:30,190
In other words, its current market price is cheap according to the signal. If you buy it now at this price, and if your hypothesis was accurate, you'll be expecting the price to rise in the future.

3
00:00:30,190 --> 00:00:37,835
Now, what if lots of other investors also find the same signal and also decide to buy the stock now?

4
00:00:37,835 --> 00:00:46,414
The buying activity will push the stock price higher. So, by the time you get around to buying yourself, the price has already risen.

5
00:00:46,414 --> 00:00:52,354
If you buy it like everyone else, you may find that it doesn't produce the positive return you were hoping for.

6
00:00:52,354 --> 00:01:01,975
This is an example of the efficient market hypothesis, which states that assets are fairly priced based on publicly available information.

7
00:01:01,975 --> 00:01:15,424
The factor is no longer helping to drive the mean return of our investments. Instead, the factor is triggering the market price to move because it is known and being used for trade decisions by many other investors.

8
00:01:15,424 --> 00:01:23,355
So this factor is now driving the movement of the stock. Every time the factor signal changes, the market price also changes soon after.

9
00:01:23,355 --> 00:01:38,480
We now say that the factor is a risk factor. So in summary, good Alpha factors enable us to find mispricings and seek a competitive edge in the market, which drives the mean return of a portfolio.

10
00:01:38,480 --> 00:01:49,804
When Alpha factors become two well-known, then they end up triggering the market to move as a changes, which drives the variance of our portfolio's return, but no longer the mean of the return.

11
00:01:49,805 --> 00:02:03,670
So now it's a risk factor. Market participants historically have had to use their judgment and experience to decide which factors to use as Alpha factors and which factors to use as risk factors.

12
00:02:03,670 --> 00:02:12,759
Can we use AI to do this instead? Yes we can. We'll go in depth into Alpha factor combination in term two.


@@@
1
00:00:00,000 --> 00:00:09,269
Factors can be broadly categorized as either momentum or reversal factors. We've previously referred to reversal as mean reversion.

2
00:00:09,269 --> 00:00:20,335
A momentum factor suggests that an existing trend will continue. A reversal factor suggests that a trend will change and go in the other direction.

3
00:00:20,335 --> 00:00:27,129
Say we have a hypothesis that the trend of stocks one year return will continue in the same direction.

4
00:00:27,129 --> 00:00:35,585
In other words, winners keep on winning and losers keep on losing. Let's call it a one-year return momentum factor.

5
00:00:35,585 --> 00:00:47,809
When the one year return is higher for stock A compared to stock B, this indicates that we expect stock A to have a higher near-term return compared to stock B.

6
00:00:47,810 --> 00:01:03,015
Now, let's look at a possible reversal factor. Say our hypothesis is that weekly stock returns are being reverting, that is, when a stock increases over a week, we expect that it will give up some of those gains due to profit taking.

7
00:01:03,015 --> 00:01:10,814
What I mean by profit taking is that other investors may sell after recent price gains in order to lock in their profits.

8
00:01:10,814 --> 00:01:22,385
So, the weekly return for stock A is higher than the weekly return for stock B, then we expect future returns on stock A to be lower compared to stock B.

9
00:01:22,385 --> 00:01:30,050
Similarly, if the weekly return of a stock is negative, our hypothesis assumes that the stock may be oversold.

10
00:01:30,049 --> 00:01:47,430
It might trend backup as other investors try to buy the stock at a cheaper price. So, to write these calculated returns as factors, the momentum factor is just the one year return, whereas the reversal factor is the negative of the weekly return.

11
00:01:47,430 --> 00:01:58,469
Remember, that a factor is some manipulation of raw data plus a hypothesis on what that data means in terms of the relative future performance of the assets.


@@@
1
00:00:00,000 --> 00:00:12,464
One of the most commonly used data sources in quant rating is the stock price and volume. We will refer to this as price-volume, but you may also hear it referred to as technical factors.

2
00:00:12,464 --> 00:00:26,339
The raw data for price-volume can include anything that we learn from a quotes and trade feed. For example, adjusted and unadjusted prices, the open, high, low, close for each time period.

3
00:00:26,339 --> 00:00:43,659
Different time frequencies such as day, week, hour, minute, and bid-ask price quotes. There are operations that we can apply it to the price volume data in which we also defined the window length, which is how many days worth of data to use in the calculation.

4
00:00:43,659 --> 00:00:53,569
We've already seen returns, which can be daily, weekly, monthly, or yearly. Returns can also be calculated at shorter time scales such as seconds, minutes, hours.

5
00:00:53,570 --> 00:01:05,334
Returns can be calculated from the close of one day to the close of the next day. Returns can also be calculated on the open price to close price for the same day.

6
00:01:05,334 --> 00:01:18,349
For example, the open price in the morning to the close price of the afternoon on the same day. Returns can be calculated on the close price of one day to the open price of the next day.

7
00:01:18,349 --> 00:01:25,534
For example, the return from the close price of Monday evening to the open price of Tuesday morning.

8
00:01:25,534 --> 00:01:32,704
The last example is referred to as overnight returns, and we'll discuss this in detail in a later lesson.

9
00:01:32,704 --> 00:01:45,359
We can also apply operations on a distribution of returns. For instance, we can calculate the first four moments, which are called the mean, variance, skew, and kurtosis.

10
00:01:45,359 --> 00:01:53,035
The mean describes the center of the return distribution. The variance describes the spread of the distribution.

11
00:01:53,034 --> 00:01:59,465
The square root of variance, which is the standard deviation, is a common measure of return volatility.

12
00:01:59,465 --> 00:02:09,284
Skew describes the amount of asymmetry. A positive skew, means there are more extreme values in the positive side of the distribution.

13
00:02:09,284 --> 00:02:22,244
Kurtosis describes how much of the distribution occurs in the left and right tails. Stock returns distributions tend to exhibit larger tails compared to a normal distribution.

14
00:02:22,245 --> 00:02:30,799
Kurtosis is often referred to as fat tails. There's also the minimum or maximum over a certain time window.

15
00:02:30,800 --> 00:02:44,804
Note that we'll discuss skewness in a later lesson about alpha factors. So, to summarize, pretty much any statistical or time series calculation you can think of, is fair game for use in a price-volume driven factor.

16
00:02:44,805 --> 00:02:56,984
Using price and volume as a factor, has a benefit of having readily available data, since stocks trade on exchanges and many market data vendors sell trades, quotes, and bar data.

17
00:02:56,985 --> 00:03:15,625
The availability and regularity of the data source is nice to have, since other sources for factor generation such as fundamentals are updated less frequently, and still other data sources such as news, social media, or analyst reports may not always be available for the stock that we're analyzing.

18
00:03:15,625 --> 00:03:22,514
It's very helpful to have a factor that can generate a signal for every stock in the stock universe.

19
00:03:22,514 --> 00:03:30,485
This is especially helpful in quant investing, since quant investing usually involves portfolios of hundreds or thousands of stocks.

20
00:03:30,485 --> 00:03:38,900
Keep in mind that the higher frequency of price volume data often leads to more trading, and therefore higher portfolio turnover.

21
00:03:38,900 --> 00:03:48,155
Since trading decisions are based on the data, the more often the data updates and changes, the more often the portfolio is likely to require rebalancing.

22
00:03:48,155 --> 00:03:59,784
This can be good or bad and requires careful study. Faster signals means more information. More information, all other things being equal, is good.

23
00:03:59,784 --> 00:04:11,810
However, more rebalancing means more trades and more transaction costs. So, strategy that is based on frequently updated data generally has more transaction costs.

24
00:04:11,810 --> 00:04:22,600
As a researcher, a key thing you need to determine is if the higher information content of the signal, more than offsets, the likely higher transaction costs.


@@@
1
00:00:00,000 --> 00:00:09,609
Volume-based factors usually include price information, to categorize the volume in a given time period as a net buy or net sell.

2
00:00:09,609 --> 00:00:18,585
At first glance, the idea of net buying or net selling, may appear counterintuitive when you recall, that every transaction involves a buyer and a seller.

3
00:00:18,585 --> 00:00:33,844
Some examples will help to clarify what we mean by this. A volume factor may track whether the total trading volume of a stock is higher than normal, and use that to determine whether recent price movements are significant or not.

4
00:00:33,844 --> 00:00:45,744
For example, sometimes stock exchanges are open on or around a major holiday, and since fewer investors are participating in trading, volume may be lower on these days.

5
00:00:45,744 --> 00:00:53,719
So investors will take the low volume into consideration, when deciding whether price movements are significant or not.

6
00:00:53,719 --> 00:01:09,594
Volume can help add more context to a price-based factor, such as a moment factor. For instance, unusually high volumes that accompanies a major price movement, may be a sign that the momentum signal is more significant than usual.

7
00:01:09,594 --> 00:01:22,384
One could say that the price-based momentum factor, is conditioned on the volume information. One piece of volume data that is independent of price is called short interest.

8
00:01:22,385 --> 00:01:37,444
Short interest tracks the quantity of a stock shares, which are held short. When a trader shorts a stock, they borrow the stock from its owner and sell it, with a hope of buying the stock back at a later date when it's cheaper.

9
00:01:37,444 --> 00:01:51,740
If instead, the stock price increases, the short seller will realize mark to market losses. Mark to market is an accounting concept, that assess the value of a health asset at its market price.

10
00:01:51,739 --> 00:02:01,369
So mark to market losses, are unrealized losses that are tracked in accounting, even if the asset isn't sold to realize those losses.

11
00:02:01,370 --> 00:02:11,885
This can become expensive if the stock price keeps rising, so short-sellers may decide to cut their losses, by buying the stock at the market price, and closing out their position.

12
00:02:11,884 --> 00:02:22,060
This is called a short squeeze. If this sounds a bit complicated, consider the mirror image of a short squeeze, which is a long squeeze.

13
00:02:22,060 --> 00:02:32,045
If you bought stock in a retail company, and watch the stock price decline day after day, you might have decided at some point to cut your losses and sell the stock.

14
00:02:32,044 --> 00:02:46,415
This long squeeze is the mirror image of a short squeeze. When a short squeeze occurs due to an upward price movement, the additional stock buying that it triggers, tends to add more upward momentum to the stock price.

15
00:02:46,414 --> 00:02:58,010
So if recent short interests is higher than usual, this might be considered a buy signal. Since an increase in price, may trigger a domino effect of additional increases.

16
00:02:58,009 --> 00:03:06,575
This could motivate a nice Alpha factor. Find stocks that have high price momentum, and are highly shorted.

17
00:03:06,574 --> 00:03:14,360
In this case, short interest data could act as a conditioning information, to make a momentum signal stronger.


@@@
1
00:00:00,000 --> 00:00:10,140
The other commonly used data in factor construction are called fundamentals. These are measures derived from financial statements such as price to earnings ratios.

2
00:00:10,140 --> 00:00:22,695
Price volume factors and fundamental factors are the most commonly used factors for quant trading, and it's possible for some active fund managers to rely primarily on these two types of factors.

3
00:00:22,695 --> 00:00:33,219
A practical difference with fundamental factors compared to technical factors is that financial statement data is usually updated on a quarterly basis, not daily.

4
00:00:33,219 --> 00:00:43,560
This means that if a fund is trading at a daily frequency, they use the most recent fundamental data for the three months until it's refreshed in the next quarter.

5
00:00:43,560 --> 00:00:51,920
Fundamental factors allow for higher capacity, which means that one can potentially put more capital on that particular trade.

6
00:00:51,920 --> 00:01:00,409
This is in part due to lower turnover, as we would only update our portfolio when new financial statements arrive every three months.

7
00:01:00,409 --> 00:01:09,494
Lower turnover generally means lower transaction costs, which means we can put more capital behind a strategy based on fundamental factors.

8
00:01:09,495 --> 00:01:20,825
One factor you've seen before is the market cap, which is a measure of the size of a company. Generally, small cap companies have historically outperformed large cap companies.


@@@
1
00:00:00,000 --> 00:00:14,339
Many fundamental factors are different variations of the price to earnings ratio. Note that [inaudible] typically use the data measured as earnings per share divided by price instead of price divided by earnings.

2
00:00:14,339 --> 00:00:24,875
Why? Earnings could be zero or close to zero. So, the data using price divided by earnings may end up with numbers close to infinity or even null values.

3
00:00:24,875 --> 00:00:43,820
Using earnings divided by price helps to reduce these kinds of data issues. The book to price ratio may be a good alternative to earnings divided by price because earnings may be negative while book to price ratios remain positive as long as the company's net asset value is positive.

4
00:00:43,820 --> 00:00:53,820
Other variations of fundamental ratios try to remove accounting choices from the metric by looking at direct cash flows instead of earnings.

5
00:00:53,820 --> 00:01:07,144
Example of these include cash flow, earnings before interest, tax depreciation, and amortization. You don't need to dive into accounting for this lesson, but it helps to get a sense of how earnings are different from cash flow.

6
00:01:07,144 --> 00:01:19,875
Cash flow is the amount of cash flowing into and out of a company. The cash flow can be a more volatile metric than earnings, but it's also more difficult for company executives to manipulate.

7
00:01:19,875 --> 00:01:26,495
Cash flow maybe more volatile due to large upfront capital costs such as buying expensive equipment.

8
00:01:26,495 --> 00:01:33,500
Accounting smooths out the earnings by spreading out the cost of these purchases over the lifetime of the asset.

9
00:01:33,500 --> 00:01:45,995
So, earning numbers are more smooth and less jumpy compared to cash flow. The earnings numbers come from the interpretations of accounting rules and there can be some judgment in applying these accounting rules.

10
00:01:45,995 --> 00:02:01,390
This is not the case with cash. With cash, it's either there or it isn't. Since accountants decide how to spread out the initial purchase over time, there is a human element in earnings that may add noise to the data.


@@@
1
00:00:00,000 --> 00:00:10,080
The previous factors that we've seen, such as price and volume, book-to-market, and analyst forecasts, tend to occur at regular intervals, whether it's daily or quarterly.

2
00:00:10,080 --> 00:00:17,885
Another class of factors are based on events that may not be scheduled and may have a significant impact on the stock price.

3
00:00:17,885 --> 00:00:31,929
For example, when the Deepwater Horizon oil rig exploded in the Gulf of Mexico in 2010, British Petroleum saw its share price drop from $60 to under $30 per share over the next 40 days.

4
00:00:31,929 --> 00:00:44,024
In 2016, when Microsoft announced its intention to acquire LinkedIn, the share price of LinkedIn jumped almost 50 percent on that day, while Microsoft shares decreased by three percent.

5
00:00:44,024 --> 00:00:53,160
Events can be either macro level events such as natural disasters, major changes in government, or changes in interest rates by central bank.

6
00:00:53,159 --> 00:01:00,439
Events can also be corporate events such as mergers and acquisitions, spinoffs, and new product announcements.

7
00:01:00,439 --> 00:01:08,135
Still other common events are index ads or index deletions and even weight changes for major indices.

8
00:01:08,135 --> 00:01:19,520
Many kinds of events are one-off and rare events, but some, such as earnings releases, earnings guidance, and product announcements, can be expected with some regularity.


@@@
1
00:00:00,000 --> 00:00:10,140
Index adds can drive buying of the stock that was added to a major index, while index deletions can drive selling of that stock. So, why is that?

2
00:00:10,140 --> 00:00:17,554
You'll recall, that an index is not a fund. So, what causes this buying and selling when an index changes?

3
00:00:17,554 --> 00:00:26,894
Recall that funds and ETFs, particularly passive funds that track a major index, want to create portfolios that match that index.

4
00:00:26,894 --> 00:00:48,369
So any changes in the index requires a change in the funds portfolio. Even Spark Beta or actively managed funds may consider adjusting their portfolios after an index add or deletion is announced since they may use that particular index as their benchmark, and the stock universe for the portfolio may depend upon what's in the benchmark.

5
00:00:48,369 --> 00:00:57,524
For example, when Monsanto was acquired in June of 2018, the S&amp;P 500 removed Monsanto and added Twitter.

6
00:00:57,524 --> 00:01:08,834
Twitter's stock increased by about five percent after the announcement as funds and ETFs that track the S&amp;P 500 bought Twitter's stock in order to match the index.

7
00:01:08,834 --> 00:01:19,535
We could follow the news for index adds and buy the stock anticipating that other funds who need to buy that stock will add momentum to the price increase in the short term.

8
00:01:19,534 --> 00:01:29,125
Similarly, index deletions could be a signal to sell or share the stock since funds will also sell their holdings to match the index that they're tracking.

9
00:01:29,125 --> 00:01:40,479
As always, you need to use care and judgment here as it is likely that this information is very well known and therefore might not be a strong Alpha factor.


@@@
1
00:00:00,000 --> 00:00:08,509
Event driven strategies are often combined with fundamental and sentiment factors which help us interpret the significance of the event.

2
00:00:08,509 --> 00:00:21,765
For example, companies typically will release their earnings once every three months. The dates are scheduled in advance so there are opportunities to make trading decisions both before and after the event.

3
00:00:21,765 --> 00:00:36,875
For pre-event trading, we could use analyst sentiment and fundamentals to decide if we expect the earnings release to exceed or fall short of expectations, then we can buy or short before the event.

4
00:00:36,875 --> 00:00:49,430
For example, after analyzing financial statements, we may think that the news or analyst sentiment is overly positive and hypothesize that the actual earnings will be less than what's expected.

5
00:00:49,429 --> 00:01:00,795
So this would be a signal to short or sell. For post-event trading, we could determine whether the actual earnings release was better or worse than expected.

6
00:01:00,795 --> 00:01:15,344
This is also referred to as an earnings surprise. For example, if the actual earnings were above what was expected based on analyst's ratings and fundamentals, then this could potentially be a buy signal.

7
00:01:15,344 --> 00:01:27,564
We could also combine this with price movements immediately following the earnings release to decide whether other investors considered the event to be positive or negative for the stock price.

8
00:01:27,564 --> 00:01:36,454
In other words, we could treat the price movements immediately following an earnings announcement as a measure of investor sentiment towards the stock.

9
00:01:36,454 --> 00:01:45,515
To summarize, events in combination with price, volume, sentiment and fundamentals may inform a strategy.

10
00:01:45,515 --> 00:01:54,390
One commonly known strategy, which doesn't work so well anymore because it is very well known, is the post earnings announcement drift.

11
00:01:54,390 --> 00:02:01,420
When firms announced earnings, if earnings are above or below expectations, they're referred to as earning surprises.

12
00:02:01,420 --> 00:02:09,299
If we assume an efficient market, we would expect the market price to adjust quickly and stabilize after an earning surprise.

13
00:02:09,299 --> 00:02:22,859
However, in reality, is often possible that prices continued drifting in the same direction, upward for a positive surprise or downward for negative surprise for about two months after the event.

14
00:02:22,860 --> 00:02:37,159
Even though this factor is well-known and therefore are not as likely to be used to see abnormal returns, it is instructive to study this phenomenon to get some intuition of a market mechanics and behavioral psychology in the markets.


@@@
1
00:00:00,000 --> 00:00:07,035
Analysts factors are derived from the published reports of sell-side research analysts at investment banks.

2
00:00:07,035 --> 00:00:16,254
Their reports include ratings such as buy, hold, or sell. They also include earnings estimates or price targets for the stock.

3
00:00:16,254 --> 00:00:25,615
Sell-side analysts may be assigned to a few companies within the same industry, and may even meet with the management of the companies that they evaluate.

4
00:00:25,614 --> 00:00:35,204
This research is intended for buy-side investment firms such as mutual funds and hedge funds, who execute their trades with these sell-side investment banks.

5
00:00:35,204 --> 00:00:44,599
Since many large investment firms value the opinions or research analysts, their published analysis can have significant effects on stock trading.

6
00:00:44,600 --> 00:00:55,698
If you read financial news and the news mentioned that a particular stock has a buy or hold rating, this is likely an average rating of multiple research analysts.

7
00:00:55,698 --> 00:01:15,605
We may also refer to analyst ratings as a kind of sentiment factor because each research analysts aggregates their information about the company's financials, it's growth prospects as well as its current price, and then summarizes this into the analysts overall conclusion or sentiment about that stock.

8
00:01:15,605 --> 00:01:31,114
Investment banks choose their own rating scale. For example, one has buy, neutral, or sell. Another has overweight, equal weight, underweight, or more volatile.

9
00:01:31,114 --> 00:01:41,400
Yet another has trading by, recommended list, market out performer, market performer and market underperformer.

10
00:01:41,400 --> 00:01:52,519
Sell-side ratings are usually aggregated to create a consensus sentiment about a particular stock. It's also useful to focus on when analysts change their ratings.

11
00:01:52,519 --> 00:02:03,824
For example, if multiple analysts start changing their ratings of a stock from hold to sell, we might expect investors who follow analyst ratings to start selling that stock.

12
00:02:03,825 --> 00:02:16,349
Analyst ratings may experience a heard mentality. It's difficult for a single analyst to publish a sell rating when most other analysts in the industry are publishing a buy rating.

13
00:02:16,349 --> 00:02:26,599
In other words, it's safer to be wrong when everyone else is also wrong, but more costly for an analyst to be wrong when everyone else is right.

14
00:02:26,599 --> 00:02:37,389
So instead of taking a simple average of analyst ratings, it may be useful to focus on the most reputable star analysts and give their ratings more weight.

15
00:02:37,389 --> 00:02:47,055
One way to combine analyst sentiment into a trading signal is to aggregate all the ratings that have changed over a certain period such as the past three months.

16
00:02:47,055 --> 00:02:56,884
If we count all the ratings upgrades and subtract all the ratings downgrades, then we have a number that is either positive, negative or 0.

17
00:02:56,884 --> 00:03:03,289
A positive number represents a positive sentiment, whereas a negative number is a negative sentiment.

18
00:03:03,289 --> 00:03:14,599
To rescale this sentiment between negative 1 and 1, we could divide this number by the total number of analysts that we're tracking, whether they upgraded, downgraded or cut their rating the same.


@@@
1
00:00:00,000 --> 00:00:08,144
In the history of finance, there has been a continued search to find new sources of information and to derive meaning from them.

2
00:00:08,144 --> 00:00:24,984
For instance, before the 1980s, stock traders used to get their financial data from newspapers. However, stock exchanges began to use computers to automate their record keeping and Bloomberg terminals were invented to deliver financial data directly to trading desks.

3
00:00:24,984 --> 00:00:32,984
Similarly, finance professionals had been collecting and analyzing data that goes beyond financial statements and stock quotes.

4
00:00:32,984 --> 00:00:43,594
These include data collected on the Internet such as social media or online reviews. This also includes satellite images, consumer transactions, and mobile app usage.

5
00:00:43,594 --> 00:00:56,899
Any data that are relatively new sources for generating trading signals such as data not including price volume, financial statements, or analysts reports are referred to as alternative data.


@@@
1
00:00:00,000 --> 00:00:16,274
If you are starting out in the field of finance and you ask any experienced portfolio manager, investment banker or research analyst for advice on how to improve your skills, it's very likely that one of their suggestions would be to read financial news.

2
00:00:16,274 --> 00:00:24,375
Most people working in financial services read financial and business news regularly in order to stay in touch with the markets.

3
00:00:24,375 --> 00:00:36,405
Watch for trends and look out for unexpected events that may affect their portfolios. The rise of social media has created another source that we can use to measure the pulse of the markets.

4
00:00:36,405 --> 00:00:49,560
For example, negative tweets about accompany whether thereby investors, or customers, may provide a lead for investors signaling that it may be a good idea to follow up and analyze that stock in more detail.

5
00:00:49,560 --> 00:00:58,210
Companies that receive lots of positive attention from both news, media and social media may have a higher chance of being over bought.

6
00:00:58,210 --> 00:01:09,885
As much of the general public hear as positive mentions about a particular company, they may also jump in to buy the stock hoping to ride the wave of a stock that appears to just keep going up.

7
00:01:09,885 --> 00:01:19,504
This may push the stock to a price that's above what's supported by fundamentals. At which point the stock may be a potential candidate to short sell.

8
00:01:19,504 --> 00:01:28,195
Notice how we don't have to assume that every news article or a social media post is the definitive truth about the health of a company.

9
00:01:28,194 --> 00:01:36,635
This data is really measuring the sentiment of people whether they're investors, customers, or public figures with lots of followers.

10
00:01:36,635 --> 00:01:46,355
Financial analysts have been applying natural language processing or NLP on news and social media to analyze public sentiment towards companies.

11
00:01:46,355 --> 00:01:54,950
In other words, they're trying to turn raw text into a signal about whether people have a positive or negative view of a stock's future.

12
00:01:54,950 --> 00:02:05,699
This is similar to tracking the sentiment of financial research analysts except with news and social media there isn't always a clearly labeled buy, hold or sell rating.

13
00:02:05,700 --> 00:02:15,590
NLP can be used to estimate how likely a news article blog post or social media posts can be categorized as buy, hold or sell.

14
00:02:15,590 --> 00:02:26,800
In term two of this program, you will learn how to use NLP and deep learning to analyze social media data and apply these skills in order to generate Alpha factors.


@@@
1
00:00:00,000 --> 00:00:16,140
Advanced natural language processing techniques are being used to enhance fundamental analysis. Fundamental analysis is the kind of work done by traditional investment analysts who work for financial institutions, like mutual funds and brokerage firms.

2
00:00:16,140 --> 00:00:36,104
Much of the text-based analysis that fundamental researchers perform on companies, whether it's reading financial news, reading quarterly earnings reports, listening to earnings calls, or reading forms submitted to regulatory agencies, this work can be enhanced with natural language processing.

3
00:00:36,104 --> 00:00:48,274
For example, a human analyst usually tracks 10 to 20 companies. Computers could act as a first-pass filter sifting through the same data source for thousands of companies.

4
00:00:48,274 --> 00:01:01,979
The automated methods could filter, categorize, and label the information. This could help fundamental analysts decide which companies or information sources to prioritize for further analysis.

5
00:01:01,979 --> 00:01:10,999
So, the pre-processing performed with NLP could support and enhance the existing workflow of fundamental research analysts.

6
00:01:10,998 --> 00:01:17,560
One can also look for insights in the required paperwork that companies send to government regulators.

7
00:01:17,560 --> 00:01:28,975
In the United States, the Securities Exchange Commission or SEC receives 10-K forms from companies once a year and three 10-Q forms per year.

8
00:01:28,974 --> 00:01:38,789
The 10-K details the companies view of its business, its past financial results for the year, as well as its current or potential future business risks.

9
00:01:38,790 --> 00:01:54,364
One potential use of these 10-Ks is to track how a company's business evolves over time. For instance, one study used natural language processing to analyze Amazon's 10-Ks from the years 2007 to 2016.

10
00:01:54,364 --> 00:02:02,144
The researchers attempted to determine which sectors the company most closely resembled based on its 10-K reports.

11
00:02:02,144 --> 00:02:09,664
In 2007, the company's description of its business most resembled the software and hardware sectors.

12
00:02:09,664 --> 00:02:20,360
Over time, the company's description based on it's 10-K forms resembled more of the retail and software sector and to some extent the media sector.

13
00:02:20,360 --> 00:02:31,919
You may be wondering how we could make use of this information. The sector that companies closely resemble are factors that help explain the price movement of their stocks.

14
00:02:31,919 --> 00:02:40,664
You could imagine that retail stocks may be correlated to some extent, and technology companies may also exhibit similar price movements.

15
00:02:40,664 --> 00:02:50,974
In a later lesson, we'll learn how the sector is a particularly important factor and we'll learn about adjusting a portfolio to be sector neutral.

16
00:02:50,974 --> 00:03:09,250
Another application of natural language processing on 10-K forms is sentiment analysis. You may be familiar with sentiment analysis on movie reviews or restaurant reviews, usually to categorize user-generated text as positive, neutral, or negative.

17
00:03:09,250 --> 00:03:28,805
Sentiment analysis can be used with other categories as well. For instance, one could apply sentiment analysis to estimate how much little risk a company's 10-K describes or how much uncertainty the company faces from competitors, customers, or suppliers.

18
00:03:28,805 --> 00:03:39,375
One could imagine creating factors based on positive outlook, negative outlook, little risk, business uncertainty, or any number of categories.

19
00:03:39,375 --> 00:03:50,344
These labels could also be used to create Alpha factors. In fact, you will be applying natural language processing on corporate documents in term two of this program.

20
00:03:50,344 --> 00:03:57,844
For now, we want to first build-up a comfortable foundation in factor risk modeling and Alpha factor research.

21
00:03:57,844 --> 00:04:06,280
This will give you the context to understand where NLP and deep learning techniques can fit into the quad workflow.


@@@
1
00:00:00,000 --> 00:00:12,500
Alternative data could be generated by scraping social media profiles of companies, tracking building permits or hospital purchases, and analyzing satellite imagery to name a few examples.

2
00:00:12,500 --> 00:00:21,280
Construction companies file permit requests with local governments when installing components of a house and also when upgrading components.

3
00:00:21,280 --> 00:00:31,309
This information could be tracked to better understand the housing market. Some hospitals provide information about purchases of medical equipment and supplies.

4
00:00:31,309 --> 00:00:38,065
This could also be used to assess the sales of companies that supply medical equipment to these hospitals.

5
00:00:38,064 --> 00:01:02,980
These days, companies have public profiles in major social media platforms. Information about their employee count, job postings, employee ratings of the company, store locations, number of social media followers, number of active users of their mobile apps, number of check-ins by customers, and so on, these can be tracked over time to gain insights into a company.

6
00:01:02,979 --> 00:01:11,450
An interesting hypothesis is that companies, which are struggling with reduced sales, may be more likely to increase marketing on social media.

7
00:01:11,450 --> 00:01:28,814
So, it's possible to look for recent increased marketing efforts as a potential alpha factor. Satellite images can also be used to get a high level overview of parking lots, mining sites, oil rigs, shipping ports, and construction zones.

8
00:01:28,814 --> 00:01:45,144
A retailer's customer base can be estimated from counting how many cars visit their retail stores. Empty parking lots likely suggest fewer customers, full parking lots suggest more customers and potentially more sales as well.

9
00:01:45,144 --> 00:01:57,055
In fact, the parking lot data generated by a startup, Orbital Insight, closely tracked the downward trend of JC Penney's stock price from 2012 to 2017.

10
00:01:57,055 --> 00:02:07,055
Crude oil storage can be tracked by analyzing satellite and drone images. Now, you may be wondering why oil storage is relevant if you're trading equities.

11
00:02:07,055 --> 00:02:15,015
Well, crude oil storage is tracked closely in order for oil traders to set prices on a barrel of oil.

12
00:02:15,014 --> 00:02:23,324
Storage of commodities in general gives an indication of how much is being produced relative to how much is being consumed.

13
00:02:23,324 --> 00:02:36,840
Prices are based on both supply and demand. Storage levels and changes in storage levels provide a signal about both supply and demand, and therefore a signal about the price.

14
00:02:36,840 --> 00:02:50,450
Price changes of oil trickles down to impact the operation costs of any businesses that rely on petroleum, jet fuel, gasoline, diesel, heating oil, and other products that are derived from crude oil.

15
00:02:50,449 --> 00:03:06,955
These include airlines, trucking, shipping, and utility companies to name a few. In the US, oil traders closely monitor the weekly oil and natural gas storage reports issued by the Energy Information Administration or EIA.

16
00:03:06,955 --> 00:03:20,120
The EIA surveys crude oil stockholders, which may use floating roof tanks, fixed roof tanks, underground caverns, trains, trucks, and pipelines to store crude oil.

17
00:03:20,120 --> 00:03:27,810
Some companies have also looked into tracking oil pipeline flows in order to estimate changes in crude oil storage levels.

18
00:03:27,810 --> 00:03:39,860
With aerial footage, it's possible to track floating roof tank storage levels. Floating roof tanks are cylinders with a roof that floats on top of the stored oil.

19
00:03:39,860 --> 00:03:50,039
Computer imaging can track shadows to estimate the height of these roofs and subsequently estimate the volume of crude oil stored in a floating roof tank.

20
00:03:50,039 --> 00:03:59,039
Orbital Insight produces energy storage estimates based on his analysis of this image data for the US, OPEC, and China.


@@@
1
00:00:00,000 --> 00:00:11,835
Hey, you made it. You've learned quite a lot in this lesson, from the principles of factor models to the covariance matrix of factor returns, to the application of factor models and quant finance.

2
00:00:11,835 --> 00:00:24,445
Yes. Congrats on building up your understanding of factors. You've also learned about the distinction between alpha factors and risk factors and have an overview of various data sources from which to derive factors.

3
00:00:24,445 --> 00:00:33,679
This foundation will help you in the upcoming lessons where you will learn about risk factor models, alpha factors, and advanced portfolio optimization.


@@@
1
00:00:00,000 --> 00:00:07,640
Hi there. So far, you've been introduced to the concept of a factor and the framework of the multifactor model.

2
00:00:07,639 --> 00:00:16,625
We've also touched upon how in practice we defined some factors as risk factors and other factors as alpha factors.

3
00:00:16,625 --> 00:00:32,515
In this lesson, we will focus on risk factors and the fundamentals of the risk factor model. Risk factor models are used to describe the volatility or risk of assets such as stocks based on the movements of the risk factors.

4
00:00:32,515 --> 00:00:43,280
The goal of a risk factor model is to measure, control and possibly neutralize a portfolio's exposure to major sources of risk.

5
00:00:43,280 --> 00:00:51,234
In order to achieve this goal, we need a way to model the portfolio's risk in terms of the common risk factors.

6
00:00:51,234 --> 00:01:01,630
To model a portfolio's risk, we want to model individual asset variances and pairwise asset covariances in terms of these risk factors.

7
00:01:01,630 --> 00:01:28,409
To model asset variance and covariance in terms of risk factors, we need a couple pieces of information; the variances and covariances of risk factors, the factor exposures which measure how exposed each asset is to each risk factor, and the specific variance of each asset that is not explained by the risk factors.

8
00:01:28,409 --> 00:01:39,590
To calculate the variances and covariances of the risk factors, we also need to calculate the returns of the factors which are called the factor returns.

9
00:01:39,590 --> 00:01:50,914
We'll also make use of the asset returns which will help us either estimate the factor exposures or the factor returns depending on the type of risk model.

10
00:01:50,915 --> 00:02:03,350
We can then use these pieces of information to estimate the specific variants of each asset. To start off, we also should decide what factors we'll use in our model.

11
00:02:03,349 --> 00:02:12,229
By the way, don't worry if these terms don't look familiar yet, you'll see a couple of ways of getting all the pieces that go into a risk model.

12
00:02:12,229 --> 00:02:19,735
So along the way, you'll see what these words mean. We'll begin with the motivation for using risk factor models.

13
00:02:19,735 --> 00:02:27,995
Factor models have some benefits over modelling portfolio risk directly from the covariances of its individual assets.

14
00:02:27,995 --> 00:02:47,185
Next, we will cover the framework for the risk factor model. The risk factor model attempts to explain the variance of a portfolio as the sum of two parts; the variance that can be attributed to the common risk factors plus variance that is not explained by those risk factors.

15
00:02:47,185 --> 00:02:56,975
I just want to point out that risk factor modeling combined with alpha factor modeling are both crucial to successful portfolio optimization.

16
00:02:56,974 --> 00:03:07,125
Just as a car needs both a brake pedal and an accelerator or a good basketball team needs both a good defense and also a good offense.

17
00:03:07,125 --> 00:03:17,879
An effectively optimized portfolio needs to both control risk using risk factors, while seeking higher than expected returns using alpha factors.


@@@
1
00:00:00,000 --> 00:00:09,350
We've previously seen how volatility is a common measure of risk. Variance is just the volatility squared.

2
00:00:09,349 --> 00:00:23,964
If we were to estimate the variance of a portfolio of two stocks, we would calculate this as the weighted sum of the variance of the returns of each stock as well as the covariance between the returns of these two stocks.

3
00:00:23,964 --> 00:00:43,615
If we were looking at the US stock market, we may have 9,000 stocks in the stock universe. Recall that we can organize these variances and covariances into a covariance matrix, which contains the pairwise covariances of each stock with itself and with all the other stocks in the portfolio.

4
00:00:43,615 --> 00:00:57,440
This means we have a covariance matrix that has 9,000 rows and 9,000 columns. Multiplying 9,000 by 9,000 results in 81 million elements.

5
00:00:57,439 --> 00:01:07,030
However, since the covariance matrix is symmetric, we will have about 14.5 million unique values to fill in the matrix.

6
00:01:07,030 --> 00:01:19,765
That's still too many values to estimate and maintain. Notice that this issue of having too many values to estimate is a general problem referred to as the Curse of Dimensionality.

7
00:01:19,765 --> 00:01:32,155
In one dimension, there are a couple of thousand stock returns to estimate. In two dimensions, to fill in the two dimensional matrix, we end up with a couple million values to estimate.

8
00:01:32,155 --> 00:01:39,984
This process of calculating the covariance matrix of assets is called a historical measure of a portfolio's risk.

9
00:01:39,984 --> 00:01:51,069
It becomes difficult to do when there are many stocks in the portfolio. The challenge with estimating the covariance matrix of assets motivates the need for a different approach.


@@@
1
00:00:00,000 --> 00:00:11,205
To get us ready to look at the risk factor model, let's review the factor model of returns. Note that some of this may look familiar from an earlier lesson, which is great.

2
00:00:11,205 --> 00:00:17,489
Our team is really excited about what you'll be able to do by the time you get to this next project.

3
00:00:17,489 --> 00:00:28,670
But we also recognize that it's a pretty steep climb to reach the top of the mountain of knowledge. So, we're trying to guide you up the mountain as gently as possible.

4
00:00:28,670 --> 00:00:46,155
Let's begin by looking at a factor model of return. For a single stock and a single factor, the return of a stock can be modeled as the sum of two parts: the factor contribution to return plus the specific return.

5
00:00:46,155 --> 00:01:04,635
The contribution of the factors is also called the common return. The common return is defined as the stock's exposure to that factor times the rate of return of the factor is the part of the stock's return that can be explained by the factor.

6
00:01:04,635 --> 00:01:16,204
The stock's exposure to the factor has a couple commonly used names: factor exposure, factor loading, factor sensitivity, or factor beta.

7
00:01:16,204 --> 00:01:26,080
We'll refer to this as the factor exposure. The rate of return of the factor is the percent change of a particular factor.

8
00:01:26,079 --> 00:01:38,905
We'll call this the factor return. An example of a factor that we've seen before is the market return, as approximated by an index such as the S&amp;P 500.

9
00:01:38,905 --> 00:01:48,460
The other contributor to the stock return is the specific return which is the return of the stock that is not explained by the factors in the model.

10
00:01:48,459 --> 00:01:58,090
This may also be referred to as the idiosyncratic return or idiosyncratic shock. We'll call this the specific return.

11
00:01:58,090 --> 00:02:07,375
So, to summarize, the stock return equals the factor exposure times the factor return plus the specific return.

12
00:02:07,375 --> 00:02:20,789
A model can have multiple factors contributing to the return of the stock. So, for instance, the return can be modeled as the contribution of one factor plus the contribution of a second factor.

13
00:02:20,789 --> 00:02:29,000
As before, the specific return represents the part of the stock return that isn't explained by the chosen factors.

14
00:02:29,000 --> 00:02:39,524
In general, we can choose any number of factors in order to model stock's return. We can refer to this as a multi-factor model of returns.

15
00:02:39,525 --> 00:02:48,769
Cool. So, that was how we model the return of a single stock, but as you may guess, we're going to be more interested in a portfolio of stocks.


@@@
1
00:00:00,000 --> 00:00:08,205
Already know, let's add one more layer of complexity, by using a factor model to describe a portfolio of stocks.

2
00:00:08,205 --> 00:00:20,728
A portfolio's return can also be modeled as a sum of two parts, the returns that can be explained by a set of factors, and the returns that are specific to each stock within the portfolio.

3
00:00:20,728 --> 00:00:39,375
Let's look at a single factors contribution to the portfolio. Assuming we had the factor exposures of each stock within the portfolio, and we also had the weight of each stock in the portfolio, how do we get the portfolios factor exposure?

4
00:00:39,375 --> 00:00:48,055
Well, a portfolio's exposure to a factor is a weighted average of the exposures from his individual stocks.

5
00:00:48,054 --> 00:00:59,229
For each stock in the portfolio, we multiply the stocks portfolio weight, by that stocks exposure to that factor, and we sum up all these weighted Betas.

6
00:00:59,229 --> 00:01:08,885
This process may look familiar to you. We do the same with portfolio returns, by calculating a weighted average of the individual stock returns.

7
00:01:08,885 --> 00:01:16,844
The difference here is that we're calculating the portfolio's factor exposure, instead of the portfolio's returns.

8
00:01:16,844 --> 00:01:28,724
So, now we can get the portfolio's exposure to one factor, exposure to a second factor, and exposures to each factor in the model.

9
00:01:28,724 --> 00:01:37,170
If we have each factors contribution to the portfolio return, we can sum up the contributions of all the factors.

10
00:01:37,170 --> 00:01:49,519
The remaining part of the portfolio's return is the specific return. Can you think of how to get the portfolios specific return, if we have the specific return of each stock?

11
00:01:49,519 --> 00:02:07,295
The portfolio's specific return is the weighted sum of the specific return of each stock. So, we take each stock's weight in the portfolio, multiplied by the stock's specific return, and add this up for all stocks in the portfolio.

12
00:02:07,295 --> 00:02:16,425
This weighted sum is the portfolio's specific return. This is the portfolio's return, modeled in terms of factors.

13
00:02:16,425 --> 00:02:32,439
The point is to explain most of the portfolio return, using a set of common factors. Ideally, the chosen factors would explain more of the portfolio return, and the specific return would explain less.

14
00:02:32,439 --> 00:02:43,930
From here, we'll get the portfolio's variance, modelled in terms of these common factors. We'll look at how to model portfolio variance for the rest of this lesson.


@@@
1
00:00:00,000 --> 00:00:15,689
We can also model the portfolio's variance as a function of factors. The portfolio's variance can be broken up into two parts, the risk contribution of the factors, and the specific risk that is not due to the factors.

2
00:00:15,689 --> 00:00:30,105
Note that is similar to what we saw earlier with the factor model of returns. The end goal is to become familiar with this formula, which models portfolio variance in terms of common factors.

3
00:00:30,105 --> 00:00:44,950
These variables represent matrices. The point of these matrix multiplications is to add up the individual stock variances and pairwise covariances, to get the variance of the entire portfolio.

4
00:00:44,950 --> 00:00:54,754
Also, to remember the various pieces and to make the math easier to digest, I'll use ice cream as an analogy to describe the variables.

5
00:00:54,755 --> 00:01:05,629
Here's the covariance matrix of factors, we can think of this as the main ingredients for making the ice cream, such as milk and sugar.

6
00:01:05,629 --> 00:01:15,620
Here is the matrix of factor exposures, as well as its transpose, which we can think of as measuring spoons for the ingredients.

7
00:01:15,620 --> 00:01:30,474
Here's the matrix of specific variances, also known as idiosyncratic variances. Which we can think of as ingredients that are specific to each ice cream, such as pecans or chocolate chips.

8
00:01:30,474 --> 00:01:44,600
Finally, there is the matrix of portfolio weights, and its transpose. We can think of these as ice cream scoops, which help us choose how much of each flavor of ice cream to put in our bowl.


@@@
1
00:00:00,000 --> 00:00:08,830
Let's begin with our matrix notation, and consider a portfolio with two stocks, in a model that uses two factors.

2
00:00:08,830 --> 00:00:17,509
The variance of the portfolio is the weighted sum of the variances and pairwise covariances of its stocks.

3
00:00:17,510 --> 00:00:25,164
We can think of each stock as a different flavor of ice cream, such as butter pecan or mint chocolate chip.

4
00:00:25,164 --> 00:00:34,575
We'll start with the first stock. Its return is modeled as a linear combination of the factors plus the specific return.

5
00:00:34,575 --> 00:00:43,053
These factors are used in the model to describe all the stocks. So, in the context of risk modeling we call these common factors.

6
00:00:43,054 --> 00:00:56,914
Risk factors, or common risk factors. Let's take the variance of both sides of the equation. This becomes the variance of each factor plus twice the covariance of the factors.

7
00:00:56,914 --> 00:01:05,569
Plus the variance of the specific return. The model assumption is that the specific return is not correlated with the factors.

8
00:01:05,569 --> 00:01:14,890
So, this specific return doesn't show up in any covariance operator. You can pause the video to study these formulas as needed.

9
00:01:15,049 --> 00:01:23,129
For constant like the factor exposure, we can move it to the outside of the variance operator if we square it.

10
00:01:23,129 --> 00:01:37,480
Similarly, we can move the factor exposures to the outside of the covariance operators. The sum of the first three terms containing the factors is the systematic variance.

11
00:01:37,829 --> 00:01:53,734
The last term is the specific variance also called the idiosyncratic variance. If we think of this first stock as butter pecan ice cream, then the systematic variance describes the main ingredients such as sugar and milk.

12
00:01:53,734 --> 00:02:08,104
While the specific variance describes the pecans. Okay. That was the variance of one stock. Let's look at the second stock, and see how these two mixed together to create a bowl of ice cream.


@@@
1
00:00:00,000 --> 00:00:09,609
If you haven't looked at variance operators in a while, it may not be obvious why we're able to take the constant factor exposure out by squaring it.

2
00:00:09,609 --> 00:00:21,405
Just a reminder of why we'd square the constant when putting it outside of the variance operator. Remember, that variance takes the square of each observations difference from the mean.

3
00:00:21,405 --> 00:00:32,115
For example, let's pretend there are just two data points for factor one with equal probabilities, then the variance of factor one looks like this.

4
00:00:32,115 --> 00:00:43,505
If we take the variance of a constant times the variable, then it looks like this. Notice that this constant shows up next to every variable.

5
00:00:43,505 --> 00:01:00,649
If we take the constant out one step at a time, then we'll eventually factor out that constant from the squared terms, which means we'll also apply the square operator on this constant, then we'll factor out the squared constant a bit more.

6
00:01:00,649 --> 00:01:10,355
Notice that the variance of a constant times a random variable is the same as the squared constant times the variance of that variable.

7
00:01:10,355 --> 00:01:19,920
You can pause the video here if you want to look a bit longer. So, that's why we can take the constant out of variance operator by squaring it.


@@@
1
00:00:00,000 --> 00:00:07,200
Recall that I'm referring to the first stock as butter pecan ice cream. The second stock can be modeled in a similar way.

2
00:00:07,200 --> 00:00:17,425
We can think of it as another flavor of ice cream. Like mint chocolate chip. Similar to before, the second stocks variance can be modeled as two parts.

3
00:00:17,425 --> 00:00:25,910
The part that is explained by the common risk factors, and the part that is not. I'd like to point out something very cool.

4
00:00:25,910 --> 00:00:35,274
Which is that all the values that you see inside a variance operator or covariance operator contain just the risk factor as one in two.

5
00:00:35,274 --> 00:00:50,215
Which we've nicknamed sugar and milk. The variance and covariance functions don't need to include information that is specific to the stocks because we can take the factor exposures outside of the variance and covariance operators.

6
00:00:50,215 --> 00:00:59,175
This is very helpful because as you'll see later on, it greatly simplifies the number of pairwise covariances that we'll need to estimate.

7
00:00:59,174 --> 00:01:08,209
So, now that we have models that describe the individual variances of the two stocks, what is the pairwise covariance of these two stocks?

8
00:01:08,230 --> 00:01:15,715
To get the covariance of the two stocks, we substitute their returns with the factor models of their returns.

9
00:01:15,715 --> 00:01:25,094
Now, we have a covariance of two expressions and each expression is some weighted sum of two factors and it's specific return.

10
00:01:25,094 --> 00:01:37,159
To help keep track of these formulas, notice that for the first stock, factor 1's contribution is labeled with white sugar, while factor 2's contribution is labeled with plain milk.

11
00:01:37,159 --> 00:01:46,260
Also, for stock 2, factor 1's contribution is brown sugar, while factor 2's contribution is chocolate milk.

12
00:01:46,420 --> 00:01:56,569
Our next step is to apply the pairwise covariance operator on each pair of terms. We see that each stock is the sum of three terms.

13
00:01:56,569 --> 00:02:05,019
This would end up with three times three or nine covariances. You can pause the video for a bit to study this.

14
00:02:05,549 --> 00:02:17,060
By definition, the specific returns are uncorrelated with the factors. So, the five pairwise covariances that include the specific returns will become zero.

15
00:02:17,060 --> 00:02:26,079
In other words, wherever we see a covariance that contains pecan or chocolate chips, we'll assume the covariance is zero.

16
00:02:26,080 --> 00:02:34,370
The remaining four covariance pairs that include only factors may still be non-zero. So, we'll keep these.

17
00:02:34,370 --> 00:02:49,380
These are the four pairwase covariances written out. For example, here's the covariance between the contribution of factor 1 to stock 1, and the contribution of factor 1 to stock 2.

18
00:02:49,379 --> 00:03:01,360
The images here are to help you keep track of the notation. You can see the four pairs of covariances among the white sugar, brown sugar, plain milk, and chocolate milk.


@@@
1
00:00:00,000 --> 00:00:07,905
Let's clean up the formulas a bit. We can move the constant factor exposures outside of the covariance operators.

2
00:00:07,905 --> 00:00:17,875
Also, the covariance of a factor with itself is also called the variance of that factor. So, the expressions now look like this.

3
00:00:17,875 --> 00:00:28,509
So, to summarize, we can express the pairwise covariance of two stocks as a sum of four terms, written in terms of the two factors.

4
00:00:28,820 --> 00:00:38,488
Now we have the variance of the first stock, the variance of the second stock, and their covariances written in terms of the two factors.

5
00:00:38,488 --> 00:00:45,304
So, that's pretty great, because we actually have the pieces we need to fill in a covariance matrix of assets.

6
00:00:45,304 --> 00:00:53,534
Recall that another way to get these four values is to use a time series of the stocks to calculate the variances and covariances.

7
00:00:53,534 --> 00:01:05,340
But as we mentioned earlier, this approach doesn't scale well when handling thousands of stocks. So, instead, we have modeled the covariance matrix of assets in terms of the common risk factors.

8
00:01:05,340 --> 00:01:13,310
We can plug the formulas for the variances and covariances into the four elements of this covariance matrix of assets.

9
00:01:13,310 --> 00:01:20,530
It becomes a bit hard to read. So, let's use these images of ice cream to help us keep track of the different pieces.


@@@
1
00:00:00,000 --> 00:00:08,850
Coming up, we'll cover major flavors of the risk model. We'll start with a risk model that uses a single risk factor, the market return.

2
00:00:08,849 --> 00:00:16,839
This is the capital asset pricing model which you've learned earlier. Then, we'll build on this to learn about the Fama French three factor model.

3
00:00:16,839 --> 00:00:26,489
The Fama French model which was proposed by Eugene Fama and Kenneth French is the basis for the movement towards the multifactor model of assets.

4
00:00:26,489 --> 00:00:36,159
In fact, many papers that study or propose potential alpha factors, can trace some of their methodology to the Fama-French three factor model.

5
00:00:36,159 --> 00:00:50,015
The cap m and Fama French three factor model are both examples of time series risk models. We'll then introduce a cross-sectional risk model and see how they're different from time series risk models.

6
00:00:50,015 --> 00:00:59,034
In a later lesson, we'll also introduce a model of Leighton risk factors using a kind of unsupervised machine learning principle component analysis.

7
00:00:59,034 --> 00:01:06,635
In quanta investing, you may come across all three types of risk models, times series, cross sectional or PCA.

8
00:01:06,635 --> 00:01:17,040
So, it helps to understand each one. You will use the PCA risk model in this next project and we'll use one of the time series models in term two of this program.


@@@
1
00:00:00,000 --> 00:00:08,070
So, you've seen the structure of the risk factor model and how it attempts to explain a portfolio's risk in terms of the common risk factors.

2
00:00:08,070 --> 00:00:17,204
Let's see how this works with a factor that we've seen before, the market return which was introduced when discussing the capital asset pricing model.

3
00:00:17,204 --> 00:00:27,949
Our goal is to use the market return as a single factor in our Risk Factor Model and then fill in the values that we need to calculate the portfolio variance.

4
00:00:27,949 --> 00:00:40,799
First, there is the covariance matrix of factors. Since there's only one factor, it has one value which is the variance of the market's excess return above the risk-free rate.

5
00:00:40,990 --> 00:00:54,989
To make our example more manageable, we'll work with just two stocks. For the matrix of factor exposures, we want the factor exposure of each stock to the single factor, which in our model, is the market return.

6
00:00:54,990 --> 00:01:04,604
So the factor exposure matrix is a column vector with two rows, and the transpose is a row vector with two columns.

7
00:01:04,605 --> 00:01:16,219
We'll also fill in the matrix of specific returns for each stock. Let's see how we get the values that we'll put into the matrices of this risk model.

8
00:01:16,219 --> 00:01:27,474
For the variance of the market factor, we can collect a time series of an index. For instance, the S and P 500 if the stock universe is focused on the US stock market.

9
00:01:27,474 --> 00:01:37,530
We can choose a time window of four weeks, eight weeks, or 12 weeks. Then calculating the variance of this time series gives an estimate for this value.

10
00:01:37,530 --> 00:01:45,634
There's a bit of a judgement call as to what time window to use. Too short of a sample, and the data is mostly noise.

11
00:01:45,635 --> 00:01:52,950
Too long of a sample, and the earlier data points may no longer be useful for estimating the current variance of the market.

12
00:01:52,950 --> 00:02:01,420
Also, the time window used for analysis scales with a holding period and the frequency at which we expect to trade.

13
00:02:01,420 --> 00:02:13,584
So, for daily trading, we may try different windows of several weeks. For strategies with longer holding periods, we may use a longer time window such as one, two, or three years.


@@@
1
00:00:00,000 --> 00:00:08,804
To estimate the other variables, we'll also collect a time series of data for each stock and make use of the capital asset pricing model.

2
00:00:08,804 --> 00:00:25,959
Remember that the capital asset pricing model assumes that the stock's excess return above the risk-free rate can be modeled as its exposure to the market's excess return above the risk-free rate multiplied by the market's excess return above the risk-free rate.

3
00:00:27,480 --> 00:00:36,530
To make this discussion less wordy, I'll just say market's excess return when referring to the market excess return above the risk-free rate.

4
00:00:36,530 --> 00:00:43,575
I'll also just say stock's excess return when referring to the stock's excess return above the risk-free rate.

5
00:00:43,575 --> 00:00:54,625
Also, notice that the market exposure, which is what we've been calling this Beta variable when discussing the CAPM, is called a factor exposure when we're discussing factor models.

6
00:00:54,625 --> 00:01:01,375
The term factor exposure is a more general term that describes any factor including the market factor.

7
00:01:01,375 --> 00:01:11,120
We're going to demonstrate one way to estimate factor exposures using regression. But keep in mind that factor exposure estimation is not a settled science.

8
00:01:11,120 --> 00:01:26,554
So, there are also other approaches to estimating a stock's exposure to a particular factor. We'll get a time series of the market's excess return, a time series of the first stock's excess return and run them through a regression.

9
00:01:26,555 --> 00:01:42,729
The coefficient Beta is an estimate of the first stock's exposure to this one factor. We can do the same with a time series of the second stock's returns and estimate the factor exposure of the second stock to this one factor.


@@@
1
00:00:00,000 --> 00:00:15,690
We can also use the results of the regression to estimate a time series for the specific return. For each time period, the specific return is the residual from taking the actual minus estimated excess return of the stock.

2
00:00:15,689 --> 00:00:31,559
The actual return is data that we just used to input into the regression. The estimated return is calculated by using the market return and the regression estimates for the factor exposure and the intercept term.

3
00:00:31,559 --> 00:00:41,259
If we calculate the difference between actual and estimated returns for multiple time periods, this is an estimate for the specific return time series.

4
00:00:41,259 --> 00:00:55,325
The variance of the specific return can be used in the risk model. Similarly, we can also calculate the specific return of the second stock, then calculate the variance of that specific return.

5
00:00:55,325 --> 00:01:03,575
Note that calculating specific variance is also not a settled science. The time window chosen affects the result.

6
00:01:03,575 --> 00:01:13,900
This difference between actual and estimated return, which we often refer to as the residual return, is what we're calling the specific return in this context.

7
00:01:13,900 --> 00:01:24,515
If you're working in quant finance, you may hear an expression like, the returns that are not explained by the model or the returns that are not explained by the factors.

8
00:01:24,515 --> 00:01:39,929
Since the goal of risk factor models is to explain where the returns of assets came from by breaking them down into well-defined components or factors, anything left over that is unexplained is represented by the residual.


@@@
1
00:00:00,000 --> 00:00:07,985
If we look again at our risk model, we can see that we've got estimates for each of the variables in several of these matrices.

2
00:00:07,985 --> 00:00:21,710
In particular, we are able to fill in the matrix of factor variances and covariances, the matrix of factor exposures and the matrix of specific variances.

3
00:00:21,710 --> 00:00:33,344
The product of factor exposures to factor variances and covariances plus specific variances is essentially the covariance matrix of the stocks.

4
00:00:33,344 --> 00:00:43,730
The stock weights in the matrices at the left and right end of this expression can be multiplied to this covariance matrix of assets to get the portfolio variance.

5
00:00:43,729 --> 00:01:02,890
In a later lesson, you'll learn about choosing optimal portfolio weights. But for now, you can think of that inner core, the covariance matrix of assets, as the plug-and-play component that we can insert into various portfolios that contained a subset of the assets contained in that matrix.


@@@
1
00:00:00,000 --> 00:00:06,710
Now, we'll build upon the previous risk model by including two additional factors for a total of three.

2
00:00:06,710 --> 00:00:14,160
This is the famous Fama-French model, which can be considered the model that started the movement towards multi-factor models.

3
00:00:14,160 --> 00:00:24,070
The original three-factor Fama-French model includes the market return, and adds two additional factors, which the other's called size and value.

4
00:00:24,070 --> 00:00:33,114
Let's look at these two additional factors, and how the data for them are generated. The first additional factor we will look at is size.

5
00:00:33,115 --> 00:00:43,939
The size of a company is measured by its market cap. There's been evidence that shows, that small-cap companies have higher average returns compared to large-cap companies.

6
00:00:43,939 --> 00:01:04,719
In other words, if we can determine that company ABC is a small-cap stock, valued at one billion dollars, and company XYZ, is a large cap stock valued at $10 billion, then we could expect that company ABC will have higher risk adjusted returns, compared to XYZ.

7
00:01:04,719 --> 00:01:13,679
The question is how much more return can we expect for company ABC for being a fraction of the size of XYZ?

8
00:01:13,680 --> 00:01:27,905
Remember that we're trying to fill in the factor risk model. The factor covariance matrix uses the time series of factor returns for the three factors: market, size and value.

9
00:01:27,905 --> 00:01:40,210
Well, if you had evidence, that small cap stocks had higher than average returns, than large-cap stocks, what would you do as a rational investor to make use of this knowledge?

10
00:01:40,370 --> 00:01:59,075
One way that seems to make sense is to buy small-cap stocks, and to short large-cap stocks. If we had a portfolio in which you bought small-cap stocks and shorted large-cap stocks, we could track the portfolio's value over time so we'd have a time series.

11
00:01:59,075 --> 00:02:08,134
We could also calculate the daily return on this portfolio. The return of this portfolio is the return of the size factor.

12
00:02:08,134 --> 00:02:21,909
Let's think about what this portfolio tells us. If the portfolio's return is positive, it means that on that day, favoring small cap stocks had a positive effect on the portfolio's return.

13
00:02:21,909 --> 00:02:31,555
If the portfolio's return is negative, then it means that on that day, favoring small cap stocks had a negative effect on the portfolio's return.

14
00:02:31,555 --> 00:02:41,180
So that's pretty cool. We have a way of quantifying whether a smaller size actually adds to positive or negative returns on each day.


@@@
1
00:00:00,000 --> 00:00:10,980
Notice that while we calculated our returns for one particular portfolio, if we chose different stocks, we could create another portfolio to represent the size factor.

2
00:00:10,980 --> 00:00:18,734
If we decided to buy in short twice as much, this could potentially be another portfolio representing the size factor.

3
00:00:18,734 --> 00:00:38,320
How do we generalize this to other portfolios that buy small-cap stocks in short large-cap stocks? Well, rather than trying to create an infinite number of long-short portfolios, we can use a single standard portfolio and then see how other portfolios co-vary with it.

4
00:00:38,320 --> 00:00:46,924
In other words, portfolios that are just like the standard portfolio would move up or down in the same percent as the standard portfolio.

5
00:00:46,924 --> 00:00:56,204
Some other portfolio that may not have such an extreme bet on the market cap of stocks may move a fraction as much as the standard portfolio.

6
00:00:56,204 --> 00:01:05,859
In fact, we don't have to limit ourselves to portfolios. We could check to see how much a single stock moves in relation to this long-short portfolio.

7
00:01:05,859 --> 00:01:25,199
If we perform the regression in which the independent variable was the return of this size portfolio and the stock return was the dependent variable, then the beta coefficient of that regression would give us a measure of how exposed the stock is to the movements of this size-based portfolio.

8
00:01:25,198 --> 00:01:37,950
Let's look at what Fama and French did to create the size factor. Start with an estimation universe, which is a set of stocks that are representative of a region or countries stock market.

9
00:01:37,950 --> 00:01:46,930
Sort the stocks by market cap. The top 90 percent by market cap are put into a portfolio called "Big".

10
00:01:46,930 --> 00:02:08,384
The bottom 10 percent by market cap are put into a portfolio called "Small". Calculate the equal weighted return of the Small portfolio, and the equal weighted return of the Big portfolio, then take the difference, the return of small minus the return of big.

11
00:02:08,384 --> 00:02:19,609
This difference gives us a measure of the return that can be attributed to investing based on the hypothesis that small-cap stocks tend to outperform large-cap stocks.

12
00:02:19,610 --> 00:02:28,604
We can think of this in terms of a long-short portfolio which buys the small cap stocks and shorts and equal dollar amount of the large-cap stocks.

13
00:02:28,604 --> 00:02:41,289
The daily returns of this long-short portfolio are the factor returns of this size factor. So, we now have a factor time series that we can use in a factor model.

14
00:02:41,289 --> 00:02:50,870
Note that in the Fama-French notation, this size factor is written with the acronym SMB. Can you guess what that stands for?


@@@
1
00:00:00,000 --> 00:00:13,665
Now let's look at the third factor, the value factor. There is research to suggest that stocks that have a high book value, relative to their market price, tends to outperform stocks with lower book value.

2
00:00:13,665 --> 00:00:22,710
Put another way, stocks that are cheap relative to their fundamentals, tend to outperform stocks that are expensive, relative to their fundamentals.

3
00:00:22,710 --> 00:00:36,365
Stocks with high fundamental valuations, relative to market price, are often called value stocks. Stocks with high market price, relative to their fundamental valuations, are called growth stocks.

4
00:00:36,365 --> 00:00:50,530
Remember from the previous discussion of the size factor that we can create a theoretical long/short portfolio in order to express the hypothesis that value indicates an additional boost and returns relative to growth.

5
00:00:50,530 --> 00:01:10,100
To do this, first, sort the stocks in the estimation universe by their book-to-market value. Stocks with high book-to-market values, those that are above the 70th percentile, are grouped into a portfolio called value or high, where high refers to high book-to-market value.

6
00:01:10,100 --> 00:01:22,655
Stocks with low book-to-market values, below the 30th percentile, are placed into a portfolio called growth or low, where low refers to low book-to-market value.

7
00:01:22,655 --> 00:01:38,450
Note that these percentile cutoffs are the ones used in the Fama-French model. Next, calculate the equal weighted return of the value portfolio and equal weighted return of the growth portfolio.

8
00:01:38,450 --> 00:01:47,935
The long/short portfolio can be constructed by buying the value portfolio and shorting an equal dollar amount of the growth portfolio.

9
00:01:47,935 --> 00:01:57,425
The daily returns of this long/short portfolio gives us the factor return of the value factor, which we can now use in a factor model.

10
00:01:57,425 --> 00:02:12,000
In the Fama-French model, the value factor is written as HML. This stands for High Minus Low, where High refers to the value portfolio and Low refers to the growth portfolio.


@@@
1
00:00:00,000 --> 00:00:09,910
So, now we've seen how Fama and French used theoretical portfolios to create factor returns, time series that represents size and value.

2
00:00:09,910 --> 00:00:19,335
What Fama and French did when combining these factors was to define theoretical portfolios that were based on both size and value.

3
00:00:19,335 --> 00:00:31,079
Let's see what I mean by that, first sort by market cap, define the group of small stocks and also define the group of big stocks.

4
00:00:31,079 --> 00:00:43,795
Then, for the small group, sort by book to market value. Divide this small group into three groups: value, neutral and growth.

5
00:00:43,795 --> 00:00:57,424
Do the same for the big group. Sort the stocks by market cap. Sort them by book to market value, then divide them into three groups: value, neutral and growth.

6
00:00:57,424 --> 00:01:09,430
Now, we'll effectively have six portfolios which are six building blocks, with which we'll use to construct the long short portfolios representing the size and value factors.

7
00:01:09,430 --> 00:01:19,459
The small minus big size factor is the sum of the three small portfolios minus the sum of the three big portfolios.

8
00:01:19,459 --> 00:01:28,549
The high minus low value factor is the sum of two value portfolios minus the sum of two growth portfolios.

9
00:01:28,549 --> 00:01:45,775
Notice that the two neutral portfolios aren't used in this formula. Okay, one more thing. Notice that small minus big was constructed using three building blocks for the long position and three building blocks for the short position.

10
00:01:45,775 --> 00:01:59,810
However, high minus low was built using two building blocks for the loan and two for the short. Since we plan to use both factors in a multiple regression, we want to even out the effects of each factor.

11
00:01:59,810 --> 00:02:10,379
So, we'll divide the small minus big by three and divide the high minus low by two. You can pause here to study these formulas.


@@@
1
00:00:00,000 --> 00:00:07,205
Now, let's see how we can use the Fama French three-factor model to fill in the values we need for the risk model.

2
00:00:07,205 --> 00:00:24,154
We have three factors, so we'll fill in the covariance matrix of factors that is three by three. Notice that the first row has the market factor in all three columns, the second row has the size factor, and the third row has the value factor.

3
00:00:24,155 --> 00:00:38,064
We want time series of three factor returns; market, size, and value. The market factor can be the excess return of a market index such as the S&amp;P 500.

4
00:00:38,064 --> 00:00:46,380
The size factor is the return of a theoretical portfolio that long, small stocks and shorts, big stocks.

5
00:00:46,380 --> 00:01:01,424
So, it's going along the three small portfolios and shorting the three big portfolios. The value factor is the return of the theoretical portfolio that longs value stocks and shorts growth stocks.

6
00:01:01,424 --> 00:01:11,829
So, it's going long two value portfolios and shorting two growth portfolios. You can pause here to study these formulas before continuing.

7
00:01:11,829 --> 00:01:23,154
Now, let's look at the matrix of factor exposures. We'll assume just two stocks. Each stock has a factor exposure for each of the three factors.

8
00:01:23,155 --> 00:01:38,069
The transpose of the factor exposure matrix looks similar, just turned on its side. To estimate factor exposures for the first stock, we can use regression, this time a multiple regression.

9
00:01:38,069 --> 00:01:47,129
For the first stock, the regression looks like this. The independent variables are the factor returns for the market, size, and value.

10
00:01:47,129 --> 00:01:58,260
The dependent variable is the return of the first stock. The same procedure can be used to estimate the factor exposure for stock two.

11
00:01:58,260 --> 00:02:06,184
The specific variance matrix holds the variance for each stock that is not attributable to these factors.

12
00:02:06,185 --> 00:02:20,530
For the specific variants of the first stock, let's first get a time series of the specific return. The specific return for each day is the actual return of the stock minus is estimated return.

13
00:02:20,530 --> 00:02:28,675
The estimated return is calculated using the factor returns and factor exposures that we calculated earlier.

14
00:02:28,675 --> 00:02:39,895
We can do the same for the second stock. Taking the variance of the specific returns gives us the values that we can put in the matrix of specific variances.

15
00:02:39,895 --> 00:02:53,924
You can pause here to study the formulas a bit. So, we did it. We have all the pieces that go into the factor model of portfolio variance and we estimated these values using the Fama French three-factor model.

16
00:02:53,925 --> 00:03:05,469
Just a reminder that this approach is called a Time Series Risk Model. Next, we'll look at another common type of risk model, which is the cross-sectional risk model.


@@@
1
00:00:00,000 --> 00:00:09,919
What we've seen so far are called time series risk models. Most practitioners tend to buy commercial risk models that are built and maintained by other companies.

2
00:00:09,919 --> 00:00:16,035
These commercial risk models tend to be a different type of risk model, a cross-sectional risk model.

3
00:00:16,035 --> 00:00:29,085
The use of cross-sectional risk models is quite common within the investment industry. So, even though it is unlikely that you will have to implement a cross-sectional risk model from scratch, it is good to have a sense for how they work.

4
00:00:29,085 --> 00:00:41,240
So, what is a cross-sectional risk model, and how is it different from time series risk model? Well, let us first see how a cross-section differs from a time series.

5
00:00:41,240 --> 00:00:55,185
If we have lots of data on a single stock over time, this is a time series. If we have lots of data on many stocks in a single time period, such as one day then that is a cross-section.

6
00:00:55,185 --> 00:01:05,780
For risk models, a cross-sectional factor model also differs from a time series factor model in terms of what is known and what is estimated by a regression.

7
00:01:05,780 --> 00:01:18,579
Let us look at the factor model has a bunch of factor exposures and factor returns. When we use a time series model, the factor returns were calculated first using theoretical portfolios.

8
00:01:18,579 --> 00:01:27,484
Then, given the factor returns data and the stock returns, the factor exposures were estimated using regression.

9
00:01:27,484 --> 00:01:39,034
With the cross-sectional factor model we would actually calculate the factor exposures, the betas before doing a regression, then the factor returns are estimated using regression.

10
00:01:39,034 --> 00:01:49,039
This may sound pretty confusing at first, I have to admit, it was confusing to me after hearing it for the first time, and also second time and the third time.


@@@
1
00:00:00,000 --> 00:00:09,365
Let's step back and look at an equation z equals x times y. If we knew both z and x, then we can solve for y.

2
00:00:09,365 --> 00:00:21,550
Similarly, if we knew both z and y, then we can solve for x. In other words, if we know any two of the three variables in this equation, we can solve for the one that's missing.

3
00:00:21,550 --> 00:00:31,899
So, now let's revisit the factor model, keeping in mind that we can think of the stock return, the factor exposure, and the factor returned as three variables.

4
00:00:31,899 --> 00:00:38,925
If we can calculate two of these, then we can use regression to estimate the third variable that's missing.

5
00:00:38,924 --> 00:00:46,969
In the case of a time series risk model, like the Fama-French example, we get the stock return in factor returns first.

6
00:00:46,969 --> 00:00:57,744
So, we use regression to estimate the factor exposure Beta. For a cross-sectional risk model, we get the stock return and factor exposure Betas first.

7
00:00:57,744 --> 00:01:19,255
Then we use regression to estimate the factor return. Again, this might take some getting used to, because when we look at regression formulas, we're probably used to seeing a letter y for the dependent variable, a letter x for the independent variable, and a Greek letter Beta to represent the coefficient that we're estimating.

8
00:01:19,254 --> 00:01:32,469
Instead, we can shake things up and let our brains be more flexible. So, we'll think about what data is available as inputs for the regression and what variables we don't have, and we'll estimate with regression.


@@@
1
00:00:00,000 --> 00:00:12,190
Let's look at a few examples factors we can use with this cross-sectional risk model. There are categorical factors such as the country in which the company is based or the sector that it is a part of.

2
00:00:12,189 --> 00:00:18,875
Intuitively, we can imagine that the country of a company may affect its stock price movement in some way.

3
00:00:18,875 --> 00:00:32,609
For instance, if a country's government added or removed regulations that impacted only countries that operated in that country then the impact would be concentrated on companies that are exposed to that country.

4
00:00:32,609 --> 00:00:39,265
Similarly, it makes sense that companies sector is a useful indicator about its stock price movements.

5
00:00:39,265 --> 00:00:58,015
For instance, if oil prices increased sharply, we can imagine this having a positive impact on the stock price of most companies that operate in the oil and gas production sector but have a negative impact on businesses that buy oil and gas products such as airlines or trucking companies.

6
00:00:58,015 --> 00:01:05,609
What you may notice about categorical variables such as country or sector is that we can't sort categorical variables.

7
00:01:05,609 --> 00:01:20,469
Sorting only makes sense if we have numerical data. Recall that if we were to try using a time series risk model, our first step would be to create a theoretical portfolio whose daily returns represent the factor returns.

8
00:01:20,469 --> 00:01:30,060
But to construct the theoretical portfolio representing the factor, we first need to sort the values so that we can divide them into separate quantile groups.

9
00:01:30,060 --> 00:01:39,505
This isn't possible with categorical variables like a country or sector. Let's talk about how we normally handle categorical variables.

10
00:01:39,504 --> 00:01:56,370
You may have heard of dummy variables or one-hot encoding. If the country category has 12 unique countries for instance, then we could expand the single country variable into 12 separate variables each representing one country.

11
00:01:56,370 --> 00:02:06,949
So, if a stock where in the USA, it would have a value of one assigned to the USA variable and zero for all other country variables.

12
00:02:06,950 --> 00:02:19,270
When we think in terms of the factor model, the value of one or zero that is assigned to each country variable is the factor exposure for that stock to that country.


@@@
1
00:00:00,000 --> 00:00:09,365
Let's focus on just one country variable, such as the USA country variable and see how we can estimate the values that will plug into the risk model.

2
00:00:09,365 --> 00:00:17,615
For the factor exposure matrix and its transpose, we want each stock's factor exposure to the USA country factor.

3
00:00:17,614 --> 00:00:24,625
This can be set to one, if the company is based in the USA and zero if it's based in another country.

4
00:00:24,625 --> 00:00:33,414
You could also imagine choosing decimal values between zero and one based on how much of the company's revenue is generated in the USA.

5
00:00:33,414 --> 00:00:43,685
In any case, the point is that we can estimate the values for the factor exposures. Next, we want to fill in the co-variance matrix of factor returns.

6
00:00:43,685 --> 00:00:54,449
So, we want to obtain a time series of factor returns. To make it easier to follow, I'm going to simplify our model so that it contains only one factor.

7
00:00:54,450 --> 00:01:02,403
So, the factor co-variance matrix contains just a single variants for the factor return of country USA.

8
00:01:02,404 --> 00:01:12,450
Also, the matrix of factor exposures has just one column for the single factor. The number of rows equals the number of stocks.

9
00:01:12,450 --> 00:01:20,304
We want to get the factor return, time series of the country USA, so that we can calculate a variance from it.

10
00:01:20,305 --> 00:01:29,359
The journey to estimating a time series starts with a single step. So, lets first estimate the factor return for a single time period.

11
00:01:29,359 --> 00:01:37,869
Once we estimate the factor return for one time period, we can repeat the process to estimate factor returns for more time periods.

12
00:01:37,870 --> 00:01:49,923
Okay. We're going to estimate the factor return for a single day. So, we take a cross section by collecting data points for a single day but across multiple stocks.

13
00:01:49,924 --> 00:02:01,135
Each stock has a factor exposure associated with it. The factor exposure is one, if the company is 100 percent exposed to country USA.

14
00:02:01,135 --> 00:02:14,705
The factor exposure is zero, if the stock is completely not exposed to the country USA. It's somewhere in between, if the stock is partly exposed to USA, but also to other countries.

15
00:02:14,705 --> 00:02:25,900
We'll also have the return for each stock on that time period. So, given the stock return and factor exposure, we want to estimate the factor return.

16
00:02:25,900 --> 00:02:38,949
We have several data points, one for each stock in the stock universe. So, we can use a regression model to fit a line through the plot of stock return versus factor exposure betas.

17
00:02:38,949 --> 00:02:56,055
The slope of that line is the estimate for the factor return for that one time period. We can interpret this estimate of factor return as the amount of return that can be attributed to being exposed to the risk factor of country USA.

18
00:02:56,055 --> 00:03:06,765
So, that was one factor return for a single time period using a single cross-section of data. Yeah. We've now estimated one point.

19
00:03:06,764 --> 00:03:18,259
Remember that we want to find factor returns over time. If we repeat this cross-sectional regression for multiple time periods, we'd get a time series for the factor return.

20
00:03:18,259 --> 00:03:27,810
That's great. If we calculate the variance of the factor return time series, we can plug it into the co-variance matrix of factor returns.


@@@
1
00:00:00,000 --> 00:00:13,324
Now that we have the matrix of factor variances and covariances, we also want to get values to fill in the matrix of specific variances which contain the variances of the specific returns for each stock.

2
00:00:13,324 --> 00:00:19,929
If there are two stocks in the stock universe, then there are two specific risk variances to calculate.

3
00:00:19,929 --> 00:00:30,960
Let's look at the specific risk variants of the first stock. The data to input into the variance is a time series of the specific returns for the stock.

4
00:00:30,960 --> 00:00:42,349
The specific return is the residual or the difference from taking the stocks actual return minus the stocks estimated return using the chosen risk factors.

5
00:00:42,350 --> 00:00:51,270
So for each time period, we have the actual stock return and can calculate the estimated return based on the chosen risk factors.

6
00:00:51,270 --> 00:00:58,859
We can get the estimated return of the stock at each time period by inputting values that we've already calculated.

7
00:00:58,859 --> 00:01:15,829
The estimated return of the stock for a single time period is a stock's factor exposure to country USA, multiplied by the factor return of USA for time t, plus the intercept term for time t. This is great.

8
00:01:15,829 --> 00:01:27,625
Now we have one data point for the specific return of stock one. We can do the same thing for multiple time periods and get a time series for the specific return of the stock.

9
00:01:27,625 --> 00:01:34,655
The variance of this time series is the estimate that we can use in the matrix of specific variances.


@@@
1
00:00:00,000 --> 00:00:13,265
Fundamental factors can also be used in the cross-sectional risk factor model. In fact we'll discuss two that we saw in the Fama-French time series model, the book to market value and market cap.

2
00:00:13,265 --> 00:00:20,234
This is an interesting example because we can see that there are two approaches to working with the same factor.

3
00:00:20,234 --> 00:00:29,250
There's the method we saw using a time series risk factor model, and now we'll see how to do this with the cross-sectional risk factor model.

4
00:00:29,250 --> 00:00:39,920
Just to revisit our goal, we're going to fill in the values to calculate the portfolio variance. For starters, let's see what data we already have to work with.

5
00:00:39,920 --> 00:00:48,150
For each stock, we have a book to market value, which may be updated once every three months if the company is based in the US.

6
00:00:48,149 --> 00:00:56,405
We also have a market cap, which may be updated daily. These values can be set as the factor exposures.

7
00:00:56,405 --> 00:01:06,829
Notice that in the Fama-French time series model, the book to market and market cap, were used in theoretical long-short portfolios to set the factor returns.

8
00:01:06,829 --> 00:01:14,864
However, for the cross-sectional model, these values are used directly as the factor exposures instead.

9
00:01:14,864 --> 00:01:24,225
We can also obtain the stock returns, and we want to estimate the factor returns using regression. So, we'll take the following steps.

10
00:01:24,224 --> 00:01:33,295
For all the stocks in the selected stock universe, obtain each stock's factor exposures, which are the book to market value and market cap.

11
00:01:33,295 --> 00:01:43,820
Also get each stock's return for one time period. Next, request the stock return against the factor exposures.

12
00:01:43,819 --> 00:01:57,205
The regression estimates the factor return for each of the two factors. Since this factor return is estimated based on all the stocks in the stock universe, it is general enough to apply to all those stocks.

13
00:01:57,204 --> 00:02:06,590
Notice that in practice, even if the stock universe is 9,000 stocks, we may use a subset of those stocks to perform the regression.

14
00:02:06,590 --> 00:02:15,760
This subset is called the estimation universe because it's the stock universe that is used for estimating these model parameters.

15
00:02:15,759 --> 00:02:23,974
To get a time series of factor returns, repeat this process of performing a multiple regression for each day.

16
00:02:23,974 --> 00:02:34,250
Once we obtain a time series of factor returns, we can calculate factor variances and covariances to fill in the covariance matrix of factors.

17
00:02:34,250 --> 00:02:45,340
To fill in the matrix of specific variances, we can calculate the specific return from the stock return minus the estimated stock return using the chosen factors.

18
00:02:45,340 --> 00:02:54,400
The variance of the specific return time series for each stock is what we'll use to fill in the matrix of specific variances.


@@@
1
00:00:00,000 --> 00:00:11,044
It's important to be familiar with the cross-sectional approach as it's the one that most commercial risk models are based, and commercial risk models are widely used by institutional investors.

2
00:00:11,044 --> 00:00:34,855
In academic research, it's common to see the time series approach, whereas in industry, practitioners usually either purchase a commercial risk model such as the ones built and maintained by MSCI Barra, Axioma, or Northfield, or they may use a third approach, principal component analysis, which we'll learn about in the next lesson.

3
00:00:34,854 --> 00:00:53,504
One challenge of constructing a risk model on your own is that you need to decide for yourself which risk factors to use in the model, and don't have an easy way to enforce the assumption that the factors are independent of each other, or that the specific returns of the stocks are independent of each other as well.

4
00:00:53,505 --> 00:01:02,369
Commercial risk models can be purchased off the shelf so that institutional investors can focus their attention on researching Alpha factors.

5
00:01:02,369 --> 00:01:21,039
The machine-learning approach with PCA, which we'll learn in the next lesson, is nice because it derives latent factors which are by definition explaining the most variance in the distribution of returns while also enforcing that each latent factor is independent of all the others.


@@@
1
00:00:00,000 --> 00:00:12,705
By now, you've seen a few important types of factor models. You know that the goal of factor models is to model roughly things you think have a similar underlying effect on your variables of interest.

2
00:00:12,705 --> 00:00:20,625
You want to represent a variable or variables in terms of several important underlying variables or factors.

3
00:00:20,625 --> 00:00:33,740
In our case, we are trying to represent a large number of similar variables that returns time series of several financial assets in terms of a smaller number of common underlying factors.

4
00:00:33,740 --> 00:00:40,949
There's another way to do this that relies on the machine learning method, principal components analysis or PCA.

5
00:00:40,950 --> 00:00:54,219
PCA is a technique you can use to represent your data set in terms of hidden latent features or dimensions, and potentially reduce the number of dimensions of your data set by dropping the least informative dimensions.

6
00:00:54,219 --> 00:01:06,944
We'll show you what we mean by that. If you've been reading about machine learning or artificial intelligence, you may have heard of this method before because it's used widely across all types of industries and fields.

7
00:01:06,944 --> 00:01:17,644
What we're going to do now is refresh the math you'll need to understand PCA and then explain in detail how this method is typically used for risk modeling and finance.


@@@
1
00:00:00,000 --> 00:00:09,839
Okay, we're doing math now, so let's start with something super simple; a vector. Oh my gosh you're thinking, a vector dah, I've seen this a million times.

2
00:00:09,839 --> 00:00:18,925
Well, I just want to remind you that there are a couple of different ways of thinking about a vector, that we will transition between when discussing analysis methods.

3
00:00:18,925 --> 00:00:28,714
First, a vector is a collection of numbers, as in you might make a bunch of measurements and put them into a column in a spreadsheet application and think of that as a vector.

4
00:00:28,714 --> 00:00:34,690
For example, you could think of the returns of several companies stocks on the same day as a vector.

5
00:00:34,689 --> 00:00:40,615
But the other way to think of a vector, which you may also be familiar with, is as a direction in space.

6
00:00:40,615 --> 00:00:52,320
Remember like a 3 D vector is also an arrow pointing from the origin to a point in 3 D space. These are both valid views and we'll need both to get to where we're going.

7
00:00:52,320 --> 00:00:59,409
I want you to be comfortable with the idea that we'll need to switch back and forth between them, as we move through this content.


@@@
1
00:00:00,000 --> 00:00:09,199
PCA, amounts to finding a new and special basis for your dataset. But hang on, what does it mean to find a new basis?

2
00:00:09,199 --> 00:00:17,660
I'm guessing that you've heard about and even done this yourself before. But before we move on, let's just briefly review exactly what this means.

3
00:00:17,660 --> 00:00:28,750
Let's return to thinking about vectors as directions in space, and start in the 2D plane. You know that we can represent a point living in the 2D plane with two coordinates.

4
00:00:28,750 --> 00:00:40,109
The coordinates tell you how to get to the point from the origin. For example, the vector 1,2 means go one unit to the right, and two up to get to this point.

5
00:00:40,109 --> 00:00:49,464
So, we see that we can decompose the overall movement represented by the vector into two movements; one to the right, and one up.

6
00:00:49,465 --> 00:01:00,639
Once we know the directions to go, we can specify how far to go in each direction. Towards this goal, let's write down two vectors, i hat and j hat.

7
00:01:00,640 --> 00:01:12,295
The first one, we write as 1,0 which means move one unit to the right. The second one, we write as 0,1 which represents moving one unit up.

8
00:01:12,295 --> 00:01:25,969
Our original vector is made up of one i hat and two j hats. We can write it as 1 times 1,0 plus 2 times 0,1 or 1,2.

9
00:01:25,969 --> 00:01:37,200
In other words, our vector can be written as a linear combination of i hat and j hat. We call i hat and j hat a set of basis vectors for the 2D plane.

10
00:01:37,200 --> 00:01:53,459
A set of vectors is a basis for a space, if no vector in the set can be written as a linear combination of the others, and any vector in the space can be written as a linear combination of vectors in the set.

11
00:01:53,459 --> 00:02:08,120
If we want to express this using matrix multiplication, we could write it this way. Now, we see that the number in the top position of this vector means how much of i hat, or how much of the first basis vector.

12
00:02:08,120 --> 00:02:19,880
While the number in this bottom position means how much of the second basis vector. So, 1,2 is how we write our vector in the i hat, j hat basis.

13
00:02:19,879 --> 00:02:29,060
Although we use i hat and j hat most of the time, there are other bases we can choose for the 2D plane, because there are different ways to get to our point.

14
00:02:29,060 --> 00:02:40,689
We could shoot out over here, and then go back over here. Remember, that our choice of basis also effectively sets up a coordinate grid for us to represent our vectors in.

15
00:02:40,689 --> 00:02:48,955
We are used to seeing a grid made up of square boxes, the usual X, Y coordinate system, with the basis vectors i hat and j hat.

16
00:02:48,955 --> 00:02:57,580
But if we use these red and blue vectors as our basis vectors, a more sensible grid would be made up of these parallelograms.

17
00:02:57,580 --> 00:03:14,554
Thus, we can represent our original vector with a different combination of basis vectors. We can think of this as another language for writing down our original vector, because we see that we can write down a new set of instructions for creating the original vector.

18
00:03:14,555 --> 00:03:24,530
Now, all we need is a way to translate between the two languages, the original language, where we use i hat j hat, and this one with the blue and red vectors.


@@@
1
00:00:00,000 --> 00:00:08,460
Let's take a close look at these red and blue vectors. In fact, let's try looking at them in the original i hat j hat basis.

2
00:00:08,460 --> 00:00:18,135
In the original basis, the first vector can be written (1,0.5). That is to say it can be created with one i hat and a half a j hat.

3
00:00:18,135 --> 00:00:30,420
The second vector is (-1,1) so it can be created with minus one i hat and one j hat. Here, we've learned how to write the vectors of the new basis in the language of the old basis.

4
00:00:30,420 --> 00:00:42,449
So, we effectively know how to translate between them. Now, let's see how we can use this information to take a vector expressed in one basis and find out how to express it in another basis.

5
00:00:42,450 --> 00:00:51,309
We know that we need two copies of the blue vector and one of the red vector to represent our original vector, let's write that down.

6
00:00:51,310 --> 00:01:03,804
When we multiply it out, we see that we recover our vector as written in the i hat, j hat basis, which makes sense since we know that these vectors are also written in the i hat j hat basis.

7
00:01:03,804 --> 00:01:21,693
So, now we can see how translation between the two languages works. We need to write the new basis vectors in the language of the old basis, then the formula for building the vector in the language of the new basis gives us our vector in the language of the old basis.

8
00:01:21,694 --> 00:01:34,879
Conversely, if we were starting with (1,2) in our original language of i hat, j hat and we wanted to translate to this new basis, we'd have to find out how to write i hat and j hat in the new basis.

9
00:01:34,879 --> 00:01:56,854
Then, we'd multiply it by that matrix and outward pop our original vector in the new basis. It turns out that the matrix that tells you how to translate from the language of the old basis to the language of the new basis is just the inverse of the matrix that tells you how to translate from the language of the new basis to the language of the old basis.

10
00:01:56,855 --> 00:02:06,294
This makes sense because translating from the old basis to the new basis and back should have the effect of doing nothing and give you back what you started with.


@@@
1
00:00:00,000 --> 00:00:13,705
Okay. Let's get back to what we really want to talk about, PCA. In a nutshell, what is PCA? PCA is a series of calculations that gives us a new and special basis for our data.

2
00:00:13,705 --> 00:00:24,609
Why is it special? Well, the first dimension is the dimension along which the data points are the most spread out, we say they have the most variance along this dimension.

3
00:00:24,609 --> 00:00:46,259
What do we mean exactly? We mean that, if we start with a dataset and we consider a new axis in the 2D plane represented by this line through the origin, then we find the coordinates of our points along this new axis by projecting them by the shortest path to the new axis.

4
00:00:46,810 --> 00:01:20,019
Consider the variance or spread of this set of coordinates along the line. When we do PCA or trying to choose our new axis in such a way that the new coordinates are as spread out as possible or have maximum variance, it turns out that the choice of line, where the coordinates are most spread out, is also the choice of line that minimizes the perpendicular distance of each coordinate to the line.

5
00:01:20,019 --> 00:01:30,105
We say that the basis minimizes reconstruction error. Maximizing variance and minimizing reconstruction error go hand in hand.

6
00:01:30,105 --> 00:01:44,474
The squared distance from the origin to the projection, plus the squared distance from the projection to the point, equals the squared distance from the origin to the point, this is just the Pythagorean theorem.

7
00:01:44,474 --> 00:02:07,909
So, when you change the orientation of the line, if one increases the other must decrease. The orientation of the line chosen by PCA is the one that maximizes the squared distances along the line for all points and simultaneously minimizes the squared perpendicular distances to the line for all points.

8
00:02:07,909 --> 00:02:17,770
This is how we find the first basis direction. The next basis direction must be perpendicular or orthogonal to the first.

9
00:02:17,770 --> 00:02:24,989
In our little example, there is only one choice for this dimension, because we are working in a two-dimensional space.

10
00:02:24,990 --> 00:02:45,985
But if we were working in a higher-dimensional space, the requirement for the next basis direction would be that it be orthogonal to the first and also maximize the variance of the points along that dimension and so on until we have as many new dimensions as we had dimensions to start with.


@@@
1
00:00:00,000 --> 00:00:06,685
Okay. So, what I told you is that we want to maximize the variance of the data in the new basis direction.

2
00:00:06,684 --> 00:00:14,035
Now I want to show you exactly how what I showed you in pictures translates into symbols that we write down.

3
00:00:14,035 --> 00:00:33,365
I don't think this is as essential for you to know, because you already have the core idea. But I want to show this to you, so that if you read about PCA in books or online, you'll be able to follow what's written, and because for myself I like to go through these things step-by-step to make sure I really understand exactly what's going on.

4
00:00:33,365 --> 00:00:43,305
I think this is important for being able to make inferences about the results of mathematical calculations, and for being able to compare among techniques that do similar things.

5
00:00:43,304 --> 00:00:50,220
I want to be able to afford to you the opportunity to understand on a deep level to. So, let's get started.

6
00:00:50,219 --> 00:00:57,085
What we're going to do here is go through what we already described in a bit more detail, in a bit more rigorously.

7
00:00:57,085 --> 00:01:08,454
So, let's start with something I left out earlier. The very first thing we need to do when we do PCA is make sure the data are centered around zero. What do we mean by that?

8
00:01:08,454 --> 00:01:17,605
Let's say the dataset we started with was over here in the 2D plane. The coordinates of this point are a and b.

9
00:01:17,605 --> 00:01:25,609
Here's the mean of all the points x coordinates, and here's the mean of all the ports y coordinates.

10
00:01:25,609 --> 00:01:36,099
When we begin PCA, if these means are non-zero, we want to subtract the mean in each dimension so that we get a new dataset centered around zero.

11
00:01:36,099 --> 00:01:43,834
First, we subtract the x mean from each data point. Then we subtract the y mean from each data point.

12
00:01:43,834 --> 00:01:51,269
Now the points are centered around zero,. This is called mean centering or mean normalizing the data.


@@@
1
00:00:00,000 --> 00:00:12,464
Now for simplicity, let's look at just one of these mean centered data points. We're looking for a new basis for the data and we want to write down the coordinates of this point in a new basis.

2
00:00:12,464 --> 00:00:25,164
This is the same thing we discussed earlier, we are looking for two new basis vectors for the 2D plane which give us an alternate way of describing our original points location.

3
00:00:25,164 --> 00:00:35,170
Okay. Let's say we've run PCA and it has spit out the new basis directions, these are called Principal Components PCs or just components.

4
00:00:35,170 --> 00:00:47,534
They are the directions of the new basis as written in the language of the old basis. It's required by the algorithm that these have length 1 and be perpendicular to each other.

5
00:00:47,534 --> 00:00:58,685
So, let's show them slightly smaller. We'll consider them to be length 1 now. Now, let's find our points' coordinates in the new basis.

6
00:00:58,685 --> 00:01:16,670
Let's do one coordinate at a time. We'll start with the red basis vector. Let's write down some names, let's call the vector pointing to our original data point X and the red basis vector W. Notice that what we've formed here is a right triangle.

7
00:01:16,670 --> 00:01:31,079
If X is the length of the X vector and Theta is the angle between the X vector and the W vector and you remember your trigonometry, you'll see that this distance is X cosine Theta.

8
00:01:31,079 --> 00:01:43,244
Now, let's remember the formula for the dot product. X.W is the length of X times the length of W times the cosine of the angle between them, Theta.

9
00:01:43,245 --> 00:02:05,545
So this length equals X.W divided by the length of W. This quantity is the projection of our original point onto the new basis direction, and it turns out it's exactly the quantity we're looking to maximize over all the data points when we pick our new basis direction.

10
00:02:05,545 --> 00:02:23,400
In fact, when I say maximize the variance, I actually mean that if we have all these dot products for all the X vectors pointing to each of our data points, we want the variance of this new set of numbers to be as large as possible.


@@@
1
00:00:00,000 --> 00:00:09,515
Variance, okay we know the formula for that. It's just the sum of the squared deviations from the mean, divided by n minus one.

2
00:00:09,515 --> 00:00:18,204
But if the mean of the original data coordinates is zero, so is the mean of their projections onto the new direction.

3
00:00:18,204 --> 00:00:33,269
So, we have that this term I call mu is zero. So, we're trying to maximize this thing. Now, I'm going to show you how to write this in a more condensed fashion, that you will see if you're reading about PCA.

4
00:00:33,270 --> 00:00:42,950
First, let's notice that if each of these quantities is a number and a vector, the variance above is just the squared length of the vector.

5
00:00:42,950 --> 00:00:52,000
Now let's look at the thing inside the double bars. If you don't follow this step, try writing it down for yourself and multiplying it out.

6
00:00:52,000 --> 00:01:02,439
But see that we can write this as a matrix where x_1 is the first row, x_2 is the second row and x_3 is the third row times the w vector.

7
00:01:02,439 --> 00:01:10,465
Remember that this w is just the length of the w vector, so it's just a number. I'll just pull it out over here.

8
00:01:10,465 --> 00:01:27,295
Now, let's write this in a more condensed form. See that the matrix on the left is just our data matrix where the rows are the observations, so the number of rows is the number of data points and the columns are the features or dimensions.

9
00:01:27,295 --> 00:01:41,570
The vector on the right is just our w vector. Another way you might see this written is like this. Be sure to go through these steps on your own if you find you're a little fuzzy on them.

10
00:01:41,569 --> 00:01:56,530
But now we've arrived at where we wanted to. This last quantity is called a Rayleigh quotient. This is exactly the quantity we try to maximize in PCA, when we're looking for w; the direction of the first dimension.


@@@
1
00:00:00,130 --> 00:00:22,445
Let's summarize. What I'm trying to tell you is that the goal of maximizing these links, the projections of our data onto the first dimension can be written this way, where this thing in here is the variance of this set of projections of the X vectors onto the W direction, up to a constant factor.

2
00:00:22,445 --> 00:00:42,095
Then, we choose W so as to maximize that variance. This is how we choose the first principle component, what we would do to find the next principle component is, subtract the component of each data vector in the direction of the first principle component.

3
00:00:42,094 --> 00:00:50,875
Remember that when you subtract a vector from another vector, you line up the first vector's tail with a second vector's tip.

4
00:00:50,875 --> 00:00:59,344
Then, we'd get a new set of data vectors that live in the space orthogonal to the first principle component.

5
00:00:59,344 --> 00:01:06,894
In our example here, this new space is only a straight line orthogonal to the first principle component.

6
00:01:06,894 --> 00:01:22,084
If we were working in three-dimensions, and we found the direction of the first principal component, we'd find the component of each data point along that direction, and subtract that from the data point.

7
00:01:22,084 --> 00:01:29,810
These new data vectors would all live in the 2D plane perpendicular to the first principle component.

8
00:01:29,810 --> 00:01:37,799
Then we'd apply the same procedure to find the next principle component, until we had one for each dimension.


@@@
1
00:00:00,000 --> 00:00:10,300
Great. So, we found this new and special set of basis vectors, which are the principal components. What now? Well, let's look at what we have.

2
00:00:10,300 --> 00:00:18,655
We have these new basis dimensions. What are the significance of these? How do we interpret them? What do they represent?

3
00:00:18,655 --> 00:00:28,859
Well, we've been talking about vectors in geometric space so far. So, we picture the principal components as directions in space, and they are that.

4
00:00:28,859 --> 00:00:39,375
But when we have a data table, the dimensions have another meaning as well. Each dimension represents a feature in a dataset, one type of measurement.

5
00:00:39,375 --> 00:00:46,420
For example, if we're talking about stock returns, each dimension represents the returns of an individual company.

6
00:00:46,420 --> 00:01:03,019
The new basis dimensions, the PCs, are combinations of the original dimensions. Therefore, they have funny units, some amount of returns of company A plus some other amount of returns of company B.

7
00:01:03,020 --> 00:01:13,355
These new units may or may not correspond to any quantity that makes sense in the real world. We say they may or may not be interpretable.

8
00:01:13,355 --> 00:01:24,000
Whether or not we think the new basis dimensions represent some real-world quantity and whether or not we care depends on the problem we're trying to solve.


@@@
1
00:00:00,000 --> 00:00:07,800
So, you have a sense of what the PCs are. Now, I have an important thing to tell you about how we use them.

2
00:00:07,799 --> 00:00:23,590
In fact, we frequently don't use all the PCs. Instead, we decide to use some fraction of them starting with the first, which we think explain most of the variance, and I'm going to explain what I mean by that in a moment.

3
00:00:23,589 --> 00:00:44,574
So, for example, with the 2D dataset I'm showing you here, if we decide to only use the first PC, instead of an x-coordinate and a y-coordinate for each data point, we just use a single coordinate that represents how far along the first basis dimension the point falls.

4
00:00:44,575 --> 00:00:55,354
This is a lower dimensional representation of the same dataset and it makes the most sense if the data approximately fall along a line to begin with.

5
00:00:55,354 --> 00:01:05,789
In this new representation, we lose the information about how far the original data points lie in the direction perpendicular to the first PC.

6
00:01:05,790 --> 00:01:17,775
However, if those distances are small, we don't lose much information. But if we have many dimensions to begin with, how do we decide how many PCs to use?

7
00:01:17,775 --> 00:01:30,135
Well, it can depend on the application, but one way is by calculating how much variance each of the PCs account for and by dropping those that account for the least variance.

8
00:01:30,135 --> 00:01:41,435
That seems reasonable, but how do we implement that quantitatively? Well, we know that the variance along each dimension is an important quantity in PCA.

9
00:01:41,435 --> 00:01:50,699
It turns out that the total variance is the same in the original basis as it is in the new basis. Let me explain what I mean by that.

10
00:01:50,700 --> 00:02:05,614
Let's start with the three data points we had before, where we've already mean centered the data. The variance along the original horizontal dimension is the sum of the squares of these lengths.

11
00:02:05,614 --> 00:02:25,175
The variance along the original vertical dimension is the sum of the squares of these lengths. But because of the Pythagorean theorem, the sum of the squares of all of these lengths equals the sum of the squares of the distances from the origin to each data point.

12
00:02:25,175 --> 00:02:47,464
When we find the new PC basis, we can calculate the variances along the new dimensions. But since each dimension is still orthogonal to every other dimension, we still have that the sum of the variances in each dimension equals the squared distance to each data point from the origin.

13
00:02:47,465 --> 00:02:59,555
Since the sum of the variance of all the data points is determined by their distance from the center, the sum of total variance is the same, regardless of which basis you choose.

14
00:02:59,555 --> 00:03:08,259
This quantity, the sum of the squares of the distances to each data point from the origin, is called the total variance of the data.

15
00:03:08,259 --> 00:03:19,484
As you have already seen, each principal component, that is, each new dimension for the data, is associated with some fraction of the total variance.

16
00:03:19,485 --> 00:03:38,870
The first PC is associated with the most, the second with the next most, and so on down the line. So, in order to decide how many PCs to keep, we might look at the variance of the data along each dimension and drop the dimensions along which the data vary the least.

17
00:03:38,870 --> 00:03:56,530
It is in this sense that PCA is used as a dimensional reduction algorithm. When we drop the dimensions that capture less of the spread of the data, we have lost some information, but retained most of the spread and thus most of the information.


@@@
1
00:00:00,000 --> 00:00:07,674
Hello and welcome. In this notebook, we will learn how to use principal component analysis for dimensionality reduction.

2
00:00:07,674 --> 00:00:19,295
Dimensionality reduction, is one of the main applications of PCA. In the previous lessons, you've already learned how PCA works and about eigenvectors and eigenvalues.

3
00:00:19,295 --> 00:00:28,690
In this notebook, we will see how to apply PCA to a small dataset. Let's begin by understanding what dimensionality reduction is all about.

4
00:00:28,690 --> 00:00:38,479
Let's suppose we had some two dimensional data that looks like this. We can see that most of the data points lie close to a straight line.

5
00:00:38,479 --> 00:00:48,324
We can also see that most of the variation in the data occurs along this direction, but there's not much variation along this direction.

6
00:00:48,325 --> 00:00:57,420
This means we can explain most of the variation of the data by only looking at how the data points are distributed along this straight line.

7
00:00:57,420 --> 00:01:06,445
Therefore, we could reduce this two-dimensional data to one-dimensional data by projecting all these data points onto this straight line.

8
00:01:06,444 --> 00:01:19,085
By projecting the data onto a straight line, we can actually reduce the number of variables needed to describe the data because you only need one number to specify a data point's position along a straight line.

9
00:01:19,084 --> 00:01:30,089
Therefore, the two variables that describe the original data can be replaced by a new variable that actually encodes this linear relationship.

10
00:01:30,090 --> 00:01:43,795
It is important to note that the new variable is just an abstract tool that allows us to express this data in a more compact form, and may or may not be interpreted as a real-world quantity.

11
00:01:43,795 --> 00:01:51,140
Now, let's see how we can do this in code. For simplicity, we will use a small two-dimensional dataset.

12
00:01:51,140 --> 00:01:57,270
In a later notebook, you'll get a chance to apply what you learned in this notebook to real stock data.

13
00:01:57,269 --> 00:02:07,140
We will start by creating some randomly correlated data. In this code, you can choose the range of your data and the amount of correlation.

14
00:02:07,140 --> 00:02:17,905
The code outputs a plot with the data points and the amount of correlation. In this case, we chose our data range to be between 10 and 80.

15
00:02:17,905 --> 00:02:34,889
Therefore, the datapoints range between 10 and 80 in both the x and y axis. Remember, a correlation of zero means no correlation at all, and a correlation of one means complete correlation.

16
00:02:34,889 --> 00:02:46,574
You can vary the amount of correlation and create the data that you like. Once you have created your data, the next step in PCA is to center your data around zero.

17
00:02:46,574 --> 00:03:00,665
It is also customary to normalize your data. This is known as mean normalization. While centering the data is necessary, normalizing the data is optional, and this is what I have done here.

18
00:03:00,664 --> 00:03:11,140
Mean normalization not only centers your data around zero, but also distributes your data evenly, in a small interval around zero.

19
00:03:11,139 --> 00:03:21,920
As you can see here, the data is no longer in the range between 10 and 80. But after normalization, the data is now distributed between minus three and three.

20
00:03:21,919 --> 00:03:33,204
This will help your algorithm converge faster. With our data centered, we're ready to perform PCA. To do this, we will use a package called Scikit-learn.

21
00:03:33,205 --> 00:03:49,080
Scikit-learn's PCA class allows us to easily implement PCA on data. The first thing we need to do is to create a PCA object with a given set of parameters including the number of principal components we want to use.

22
00:03:49,080 --> 00:03:59,884
We'll start by using two components because we want to visualize them later. Here, we can see the parameters that the PCA algorithm is going to use.

23
00:03:59,884 --> 00:04:13,379
The next step is to pass the data to the PC object using the fit method. A quick note. In Scikit-Learn, the PCA algorithm automatically centers the data for you.

24
00:04:13,379 --> 00:04:20,930
So, you could pass the original dataset to the fit method instead of the normalized data as we have done here.

25
00:04:20,930 --> 00:04:32,634
Once we fit the data, we can use the attributes of the PCA class to see the eigenvectors also known as the principle components, and its eigenvalues.

26
00:04:32,634 --> 00:04:46,780
One important attribute of the PCA class is the explained variance ratio. The explained variance ratio gives us the percentage of variance explained by each of the principal components.

27
00:04:46,779 --> 00:04:56,545
In general, the principle components with the largest eigenvalues explain the majority of the variance, and this is usually the ones that we want to keep.

28
00:04:56,545 --> 00:05:06,845
For example, here we can see that the first principle component explains 94 percent of the variance, and has the largest eigenvalue.

29
00:05:06,845 --> 00:05:17,924
Now that we have the principle components, we can visualize them. Here, we see the data with the first principle component, and the second principle component.

30
00:05:17,925 --> 00:05:25,324
We can see that the first principal component lies along the direction in which the data varies the most.

31
00:05:25,324 --> 00:05:36,754
One question that you will frequently face is how many principal components you should use. For example, suppose you had a dataset with 1,000 dimensions.

32
00:05:36,754 --> 00:05:43,955
Should you reduce this dataset to 500 dimensions or could you do better and reduce it to 100 dimensions?

33
00:05:43,954 --> 00:05:52,069
Usually, the number of principal components is chosen depending on how much of the variance of the original data you want to retain.

34
00:05:52,069 --> 00:06:00,024
For example, you may want to retain 90 percent of the variance, or you may only want to retain 50 percent of the variance.

35
00:06:00,024 --> 00:06:11,074
You can use the explained variance ratio attribute of the PCA class to determine the number of components you need to keep to retain a given amount of variance.

36
00:06:11,074 --> 00:06:22,214
For example, if you wanted to retain 90 percent of the variance, you can add up the elements in the explained variance ratio array until the desired value is reached.

37
00:06:22,214 --> 00:06:31,765
The number of elements you had to add up to reach the desired value determines the number of principal components needed to retain that level of variance.

38
00:06:31,764 --> 00:06:40,904
For example, if you had to add up five elements to retain 90 percent of the variance, then you will need five principal components.

39
00:06:40,904 --> 00:06:48,935
Now, that we have seen what all the principal components look like, we will now use PCA to perform dimensionality reduction.

40
00:06:48,935 --> 00:06:57,214
Since the data we're using in this simple example only has two dimensions, the best we can do, is to reduce it to one dimension.

41
00:06:57,214 --> 00:07:15,450
So, now choose the number of principle components in our PCA algorithm to be equal to one. Once we ran the PC algorithm with only one component, we can see what the transform data looks like by using the transform method of the PCA class.

42
00:07:15,449 --> 00:07:25,709
In this simple case, the transform method projects the data onto the first principal component so we will just get a straight line.

43
00:07:25,709 --> 00:07:37,839
When working with higher dimensional data, the transform method will project the data onto the lower-dimensional surface determined by the number of principal components you used in your algorithm.


@@@
1
00:00:00,000 --> 00:00:10,914
So, let's return to the main subject of this lesson, which is factor models of risk. So, how do we use PCA to create a factor model of risk?

2
00:00:10,914 --> 00:00:29,304
What are our data? They are a set of time series of stock returns for many, many companies. Our main motivations for using PCA are to reduce the dimensionality of these data and also find a representation of them that captures a maximum amount of their variance.

3
00:00:29,304 --> 00:00:38,160
We'll use this representation or model of risk later when we seek to minimize risk as part of an optimization problem.

4
00:00:38,159 --> 00:00:45,255
We're going to be talking about a lot of matrix multiplications here, so I'm going to try to always write down the dimensions.

5
00:00:45,255 --> 00:00:51,365
I think keeping track of the dimensions will help you keep track of what's going on. It's certainly helps me.

6
00:00:51,365 --> 00:01:14,830
When we use a factor model, remember, we have a representation of the returns that looks like this. In order to produce a factor model of risk using PCA, we need to look at what the PCA algorithm gives us and map its outputs to each of these matrices; the factor exposures, factor returns, and the idiosyncratic risk matrix.

7
00:01:14,829 --> 00:01:25,754
So, let's look at what these symbols represent in terms of matrices. The returns matrix has dimensions of number of companies by number of time points.

8
00:01:25,754 --> 00:01:39,790
The matrix of factor exposures has dimensions, number of companies by number of factors. The matrix of factor returns has dimensions of number of factors by number of time points.

9
00:01:39,790 --> 00:01:52,224
Finally, the matrix of specific risk has dimensions number of companies by number of time points. Let's take a look at what we get from running the PCA algorithm.

10
00:01:52,224 --> 00:02:09,409
Remember that PCA finds a new basis for the data. If we keep all the PCs, then multiplying the representation of the data in the PC language by the matrix of PCs completely recreates the data in the original basis.

11
00:02:09,409 --> 00:02:19,990
On the other hand, if we drop some of the PCs then multiplying the data matrix in the new basis by the matrix of PCs almost recreates the data.

12
00:02:19,990 --> 00:02:28,705
This is the compressed representation of the data. If we add in what's leftover, then we have a complete representation of the data.

13
00:02:28,705 --> 00:02:40,000
Now, this is looking a lot like the factor representation we discussed a little bit earlier. In fact, we're just going to use this as our factor model.


@@@
1
00:00:00,000 --> 00:00:09,059
The matrix that describes the coordinates of the principal components in the original basis is the matrix of factor exposures.

2
00:00:09,060 --> 00:00:19,175
The matrix of data transformed into the new language is the matrix of factor returns. How do we calculate the factor returns?

3
00:00:19,175 --> 00:00:32,950
Well, we multiply the original data by the inverse of the factor exposures matrix. Since we required this matrix to be orthonormal, its inverse is just its transpose.

4
00:00:32,950 --> 00:00:40,439
Remember that for the risk model, we need to calculate the factor covariance matrix. How do we do that?

5
00:00:40,439 --> 00:00:49,445
Well, we have the matrix of factors, we just calculate their covariance the same way we always calculate a covariance matrix.

6
00:00:49,445 --> 00:01:01,659
Remember that since the factors are projections of the data onto the PCs, they are orthogonal. This means that the factor covariance matrix is a diagonal matrix.

7
00:01:01,659 --> 00:01:13,084
This is true for PCA, but not necessarily true for other types of factor models. Finally, remember that the factors are returns time series.

8
00:01:13,084 --> 00:01:25,629
They don't necessarily have direct interpretations in the physical world, but you can think of them as returns time series for some kind of latent or unknown driver of return variance.

9
00:01:25,629 --> 00:01:36,395
If the returns time series are daily returns, then when we calculate their variances, we have estimates of the factor returns variances on a daily basis.

10
00:01:36,394 --> 00:01:50,454
Typically, we annualize these by multiplying the whole matrix by an annualization factor. Remember, we don't need the square root of the factor because our numbers here are variances not standard deviations.

11
00:01:50,454 --> 00:02:03,435
So, if we were going from daily to annualized, we multiply by 252. Lastly, we need to calculate the idiosyncratic or specific risk matrix.

12
00:02:03,435 --> 00:02:11,799
We know that what's left over from our model is the original returns data minus the part we represent with the PCs.

13
00:02:11,800 --> 00:02:23,234
To calculate the specific risk matrix, we calculate the covariance matrix of the residuals, and set the off-diagonal elements to zero.

14
00:02:23,235 --> 00:02:31,674
Since we set the off-diagonal elements to zero, we now do not have a perfect recreation of the covariance matrix of the original data.

15
00:02:31,675 --> 00:02:42,060
But this is reasonable because we try to capture as much of the correlation in the data as possible with the principal components that we decided to keep.


@@@
1
00:00:00,000 --> 00:00:13,259
Well, I hope you're excited because you have just learned a whole stack of things. You've learned about a really cool and commonly used algorithm, and also about one way it's used in finance to model risk.

2
00:00:13,259 --> 00:00:22,274
Now, hold on to your hats because next we're going to talk about possibly the most exciting part of this whole series of lessons, Alpha factors.


@@@
1
00:00:00,000 --> 00:00:11,714
Welcome to the overview of alpha factors. This is the extra exciting part where you finally get to start turning hypotheses into code and testing that code against data.

2
00:00:11,714 --> 00:00:20,679
Alpha factors are tools operating on data that give us signals about how stocks may perform relative to one another.

3
00:00:20,679 --> 00:00:38,659
You've heard a little about alpha factors already and how they differ from risk factors. Remember that alpha factors are hopefully predictive of future mean returns, while risk factors impart information about commonality of return variance.

4
00:00:38,659 --> 00:00:45,799
The search for alpha factors is essentially the search for deviations from the efficient market hypothesis.

5
00:00:45,799 --> 00:00:57,109
The efficient market hypothesis has different forms, but generally it means that all available information is reflected in the current price of an asset, which suggests that assets are priced fairly.

6
00:00:57,109 --> 00:01:03,795
When searching for exceptions to the efficient market hypothesis, we're looking for mispricing in the market.

7
00:01:03,795 --> 00:01:11,135
We learned about arbitrage before, which is the simultaneous buying and selling of a perfect substitute to make a profit.

8
00:01:11,135 --> 00:01:30,185
In the context of this lesson, we'll also use the term arbitrage. But here, it refers to buying an asset that may be underpriced or shorting one that may be overpriced, where the expected return is in excess of what it should be for the risk we bear.

9
00:01:30,185 --> 00:01:38,689
In this lesson, we'll start by discussing academic research as a source for hypotheses and for alpha factor generation.

10
00:01:38,689 --> 00:01:52,564
We'll cover techniques for processing a raw alpha factor including methods, such as sector neutralization, ranking, Z-scoring, smoothing, and conditioning factors on other factors.

11
00:01:52,564 --> 00:02:06,040
Each of these techniques is important in the process of turning the numbers in our alpha vectors into signals that represent whether to buy or short each stock in a portfolio and by how much.

12
00:02:06,040 --> 00:02:23,650
That is to say, these techniques are relevant to the goals that we have for real-world portfolios. We'll also cover techniques for evaluating factors, such as the sharpe ratio, information coefficient, information ratio, and turnover analysis.

13
00:02:23,650 --> 00:02:35,335
These will help you decide whether your alphas provide enough return relative to risk, and whether they will cause too much trading to likely be profitable in the real world.

14
00:02:35,335 --> 00:02:48,394
The topics we discuss here will prepare you for the following lesson, in which we extract alpha factor ideas from academic research papers, and then implement those ideas in code.

15
00:02:48,395 --> 00:02:57,584
This is exactly the type of work that quants do in the real world. In this lesson, we step into the typical life of a quant researcher.

16
00:02:57,585 --> 00:03:05,000
You'll see how quants generate alpha factor ideas that may make their way into real hedge fund portfolios.


@@@
1
00:00:00,000 --> 00:00:11,714
Since we just talked about risk factors, let's talk about how both the risk and alpha factors help us create successful portfolios and how they're different.

2
00:00:11,714 --> 00:00:31,789
We use risk factors in risk factor models to help us model the movement of our stocks prices due to common publicly known factors that may explain price movements in a broad cross-section of stocks defined as systematic returns and systematic volatility.

3
00:00:31,789 --> 00:00:52,034
Since these risk factors may significantly impact our portfolio volatility without providing appropriate compensation in the form of returns, our goal is to neutralize the portfolio's exposure to these risk factors, so that their remaining movements of stocks in our portfolio can be attributable to our alpha factors.

4
00:00:52,034 --> 00:00:59,524
We can use an analogy where factors are sounds that you hear when eating lunch at a busy restaurant.

5
00:00:59,524 --> 00:01:08,790
Risk factors are like the background noise, like the conversations of all the patrons, the noise from the servers and the kitchen noises.

6
00:01:08,790 --> 00:01:21,319
The alpha factors are like the soft voices of your friends who are sitting with you for lunch. If we don't neutralize the background noise, it may drown out the sound of your friends voices.

7
00:01:21,319 --> 00:01:35,200
Similarly, alpha factors have important information that may be overwhelmed by the movements of risk factors such as the movements of the market or movements of a sector.

8
00:01:35,200 --> 00:01:43,469
We want to isolate the effect of alpha factors. To do that, we eliminate the effect of risk factors.


@@@
1
00:00:00,000 --> 00:00:21,429
Note that the word alpha is used to mean different things within the world of finance. In the context of factor models, we'll refer to an alpha model as an algorithm that transforms input data into a list of numbers, one number for each stock under consideration per time step.

2
00:00:21,429 --> 00:00:31,734
A higher positive number means we want to put more money on that stock law. A negative number means we want to short the stock.

3
00:00:31,734 --> 00:00:49,375
An alpha vector refers to this list of numbers for a single time period, such as a day. Each number in the alpha vector is proportional to the amount of money we want to allocate towards each stock, according to the alpha model.

4
00:00:49,375 --> 00:01:00,859
The alpha vector at each time step is standardized, so that it has mean zero and the sum of the absolute values adds up to one.

5
00:01:00,859 --> 00:01:12,870
We'll use the term alpha value to refer to a single number in the alpha vector. So, it's a number assigned to a single stock for a single time period, such as a day.

6
00:01:12,870 --> 00:01:26,115
We'll refer to an alpha factor as a time series of alpha vectors. So, it's the set of alpha vectors over multiple time periods, such as multiple days.

7
00:01:26,114 --> 00:01:36,839
A raw alpha factor is the initial output of the alpha model, excluding additional processing that is done to improve the alpha.

8
00:01:36,840 --> 00:01:47,015
We'll use the term raw alpha factor to distinguish from an alpha factor that has been processed to improve its signal, and make it easier to work with.

9
00:01:47,015 --> 00:01:55,304
Note that within the finance industry, there is no single consistent way that practitioners use the term alpha or alpha factor.

10
00:01:55,305 --> 00:02:05,054
Though, our use is the most common. We'll use these definitions to help us identify which step in our alpha generation we're referring to.

11
00:02:05,055 --> 00:02:15,215
We'll also be talking about our stock universe, or simply our universe. This refers to the set of stocks which we are considering in our portfolio at each time step.


@@@
1
00:00:00,000 --> 00:00:18,149
You may be wondering how to come up with alpha factors and trading strategies. Some sources of inspiration, maybe reading financial news, observing the markets for curious behavior, studying the methods of famous investors both discretionary and want or talking to industry practitioners.

2
00:00:18,149 --> 00:00:26,975
We'll explore academic research papers as a source for alpha factor ideas through the remainder of this lesson and in the next lesson.

3
00:00:26,975 --> 00:00:37,480
It's important to understand a key point before we do this. We should not expect to get strong production-ready alphas ''as is'' from academic papers.

4
00:00:37,479 --> 00:00:47,784
Why not? Because the very act of publication which makes the paper publicly available also diffuses this information into the market.

5
00:00:47,784 --> 00:00:56,879
As participants adopt the trading strategy, the strength of the effect is diffused. Why then do we look at academic papers at all?

6
00:00:56,880 --> 00:01:11,974
For four reasons. First, these papers can spur idea generation for us to create derivative works. Second, the work conserve as baseline factors to compare with our own alpha generation.

7
00:01:11,974 --> 00:01:19,825
Third, we can learn how professional full-time researchers approach problems and validate their findings.

8
00:01:19,825 --> 00:01:34,805
Fourth, perhaps we can learn about new sources of data or novel ways to work with data. Academic research can be published in general interests economic journals, investment focused journals, or open source journals.

9
00:01:34,805 --> 00:01:42,659
General interest journals and investment focused journals both require peer review and require a paid subscription.

10
00:01:42,659 --> 00:01:50,795
Open source repositories usually include working papers that have not yet been peer reviewed and are also free.

11
00:01:50,795 --> 00:02:07,004
Open source repositories for research papers include, SSRN and the archive. One benefit to using open source journals is that there's a broader selection of papers and their ideas more recent, since peer review is a process that takes time.

12
00:02:07,004 --> 00:02:18,379
It helps to figure out which other papers cite a particular article, to see how other researchers attempted to replicate critique or improve upon the original idea.

13
00:02:18,379 --> 00:02:25,180
Once you've found some papers of interests, scan the abstract and find out if the data use is accessible to you.

14
00:02:25,180 --> 00:02:35,620
Focus on papers for which the data is accessible so that you may try to replicate their work. Check that the methodology could potentially be practical.

15
00:02:35,620 --> 00:02:43,389
Academic papers almost never include real-world constraints like transaction costs or liquidity directly.

16
00:02:43,389 --> 00:02:49,974
You need to make a judgment quickly if you think adding these constraints will completely invalidate the ideas presented.

17
00:02:49,974 --> 00:02:59,465
Some papers include these aspects in a roundabout way. They conducted Turnover Analysis or they choose a stock universe that includes more illiquid stocks.

18
00:02:59,465 --> 00:03:11,449
Some examples of impractical methods may include choosing a large universe of say 8,000 stocks, in which most of the factor returns are derived from a subset of the small and micro cap stocks.

19
00:03:11,449 --> 00:03:20,180
Remember that small and micro cap stocks are less liquid and factors that rely on illiquid stocks may be difficult to implement in practice.

20
00:03:20,180 --> 00:03:31,244
Note that the process of finding academic papers, studying papers, implementing the papers and then evaluating them is a very important skill to have as a quantitative researcher.

21
00:03:31,245 --> 00:03:40,200
We hope to walk you through some of these examples, so that you will eventually have the confidence to conduct your own research after this course.


@@@
1
00:00:00,000 --> 00:00:15,510
Okay. Let's say we've read a paper, extracted some ideas, and have come up with an Alpha factor. Now, let's talk about some of the common early transformations we might do to that factor in order to move it closer towards our goal.

2
00:00:15,509 --> 00:00:31,195
A vector of numbers that represents the weights we'd use to create a portfolio. These transformations help to enhance, improve, and prepare our Alpha factors so that they may be used during portfolio optimization to choose actual portfolio weights.

3
00:00:31,195 --> 00:00:39,515
When we actually implement traits for our portfolio, we want that portfolio to be neutral to common risk factors.

4
00:00:39,515 --> 00:00:50,480
We should not wait however for the optimization step to think about common risk as we do not want to rely exclusively on optimization to make this happen.

5
00:00:50,479 --> 00:01:03,094
Rather, it's best to consider obvious common risks even at the Alpha research stage. The most significant common factor risks are market risk and sector risk.

6
00:01:03,094 --> 00:01:16,025
We control market risks explicitly by the definition of an Alpha factor. The sum of the values is zero implying zero exposure to the market.

7
00:01:16,025 --> 00:01:24,655
Note that an important assumption that we'll make is that on average the Betas or exposure of the stocks to the market are all one.

8
00:01:24,655 --> 00:01:35,905
So even though in reality the regression Beta of stock ABC to the market may be 1.2 or the Beta of stock XYZ maybe 0.8.

9
00:01:35,905 --> 00:01:45,740
When dealing with stock universes of hundreds or thousands of stocks, we often can take the simplifying assumption that they all have the same Beta of one.

10
00:01:45,739 --> 00:02:03,939
To get the values to sum to zero, we subtract the mean from each Alpha value in the vector. When the value sums to zero, the theoretical portfolio is said to be dollar neutral, and so general market movements that affect all stocks are cancelled out.

11
00:02:03,939 --> 00:02:13,599
This may not precisely eliminate all market risk as market Betas may not all be equal to one, but this is often a fine assumption at this stage.


@@@
1
00:00:00,000 --> 00:00:20,019
So now that we've controlled for market risk, how can we control sector risk at this point. We often want our alpha factor to be sector neutral so that it's not exposed to overall movements within a sector and this means that we want our short weights in the sector to be equal in magnitude to our long waits in the sector.

2
00:00:20,019 --> 00:00:29,115
We achieve this by calculating the average of all weights in the sector then subtracting the sector mean from each weight within that sector.

3
00:00:29,114 --> 00:00:42,454
Again, will make the simplifying assumption that each stock's Beta to its sector is equal to 1. Let's look at a simple example with only two stocks in the tech sector; Apple an Alphabet.

4
00:00:42,454 --> 00:00:51,304
To make our alpha factor neutral to the tech sector, we could go along one of these stocks and short the other with the weight of the same magnitude.

5
00:00:51,304 --> 00:01:05,394
If we think about our raw Alpha factor, let's say that Apple's raw Alpha value is 0.33 and Alphabet's are raw alpha value is 2.31.

6
00:01:05,394 --> 00:01:22,909
The average is 1.32 and we subtract this mean from each stock's raw-value. So Apple's sector neutral Alpha value is now negative 0.99 and Alphabets is positive 0.99.

7
00:01:22,909 --> 00:01:31,759
So by subtracting the industry average, we're able to convert this raw Alpha factor into an Alpha factor that is sector neutral.

8
00:01:31,760 --> 00:01:41,165
Again this is because the sum of the negative weights is equal to the magnitude of the sum of the positive weights in each sector.

9
00:01:41,165 --> 00:01:48,870
If the entire sector were to move up or down, the sector neutral portfolio would be unlikely affected by this movement.

10
00:01:48,870 --> 00:01:56,465
A portfolio would likely consist of multiple sectors. So we'd repeat the sector neutralization for each sector.

11
00:01:56,465 --> 00:02:09,280
Note that we can neutralize the Alpha vector by market and also by sector. So in practice, we could first neutralized by the market and then take that re-scaled vector and neutralize it by sector.


@@@
1
00:00:00,000 --> 00:00:16,365
You may notice, that if the amount we invest in each stock of our portfolio is tied to the Alpha value that we get from daily data, then we would be constantly buying and selling every day in order to follow the signal faithfully.

2
00:00:16,364 --> 00:00:24,245
In other words, as the Alpha vector changes every day, we'd have to adjust our portfolio weights every day also.

3
00:00:24,245 --> 00:00:32,530
We also have to address what happens to our Alpha vector when we encounter outliers, or extreme values in the data.

4
00:00:32,530 --> 00:00:46,520
If we have had a large increase in the office signal for one stock, then a sharp decrease the next day, this would effectively tell us to buy a lot of that stock and then sell a lot of that stock the next day.

5
00:00:46,520 --> 00:01:01,489
This may or may not be warranted. In the real world, trading costs money. So, we want to be very confident that if we go to make a trade and bear that cost, that it is indeed warranted.

6
00:01:01,490 --> 00:01:13,578
Some ways to keep extreme values from leading to unnecessarily large trades, are by clipping very large and small values at for example the 95th percentile and the fifth percentile.

7
00:01:13,578 --> 00:01:24,415
This process is called winsorizing. Here's an example of winsorizing in Alpha vector which has Alpha values for each stock, for a single day.

8
00:01:24,415 --> 00:01:32,328
For any number that exceeds the 95th percentile, we've replaced that outlier with a number at the 95th percentile.

9
00:01:32,328 --> 00:01:39,435
Also for any values that are below the fifth percentile, we replace those with a number at the fifth percentile.

10
00:01:39,435 --> 00:01:50,175
Again, this is called winsorizing. We can also deal with outliers, by setting a maximum magnitude allowed waits for any single stock.

11
00:01:50,174 --> 00:02:08,490
Note that we would handle outliers for each Alpha vector which may be updated each day. Even when we've dealt with outliers, there's still the issue of whether it makes sense to buy and sell based on the signal if their relative magnitudes for the Alpha values don't change.

12
00:02:08,490 --> 00:02:20,870
Let's again take the example of Apple and Alphabet. One day, Apple's Alpha value is 0.33 and Alphabet's value is 0.31.

13
00:02:20,870 --> 00:02:30,745
What if on the next day, Apple's Alpha value increased by 0.01 and Alphabet's Alpha value also increased by 0.01.

14
00:02:30,745 --> 00:02:37,705
If we translated these directly into portfolio weights, the weights would change slightly from day one today two.

15
00:02:37,705 --> 00:02:44,814
But the important thing to notice is that we'd still be putting more money on Apple relative to Alphabet.

16
00:02:44,814 --> 00:03:04,564
So, maybe we wouldn't actually want to change our positions at all. Often what we want to do, is have a more robust version of the signal which is able to withstand outliers, handle noise in the data, and also keep us from making potentially excessive traits.


@@@
1
00:00:00,000 --> 00:00:08,634
Ranking is a broadly useful method in statistics to make calculations more robust and less sensitive to noise.

2
00:00:08,634 --> 00:00:17,615
So, how do we use ranking here? If we have just two stocks in our portfolio, we would sort them by the office signal in descending order.

3
00:00:17,614 --> 00:00:41,433
So, for instance, let's say we had a Raw Alpha values for Apple, Alphabet, and IBM. If we sorted their values and gave a rank of one to the lowest value, two for the next highest value, and three for the highest value; we've converted an Alpha Vector of decimals into an Alpha Vector of ranks.

4
00:00:41,433 --> 00:00:52,394
Note that we use descending order to preserve the property that the highest numerical Alpha value is proportional to the stock we think will rise the most.

5
00:00:52,395 --> 00:01:16,355
Now, let's see how ranking helps us deal with outliers. If Apple's Alpha value surged from 0.33 to 0.50 from day one to day two, and the Alpha Values of the other two also increased by 0.01 to 0.02, then the relative order or ranks of the three stocks wouldn't change.

6
00:01:16,355 --> 00:01:24,815
When this gets translated into portfolio weights, it means that we wouldn't be expected to make a trade based on these ranks.

7
00:01:24,814 --> 00:01:40,094
We can also see how ranking helps us deal with noise. If Apple's signal increases by one percent from day one to day two, and Alphabet signal also increases by one percent, their relative values don't change.

8
00:01:40,094 --> 00:01:47,984
So, the rank of Apple stays at two on both days and the rank of Alphabet stays at one for both days.

9
00:01:47,984 --> 00:01:58,870
Again, ranking is a broadly useful method for making data analysis more robust. We'll see ranking used again later in this lesson.


@@@
1
00:00:00,000 --> 00:00:07,945
It's common for researchers to normalize alpha factors by subtracting the mean and dividing by the standard deviation.

2
00:00:07,945 --> 00:00:19,479
We can also refer to this process as Z-scoring because the result is a Z-score. Z-scoring helps to standardize the data, so that it has a consistent range and distribution.

3
00:00:19,480 --> 00:00:37,250
This makes it easier to compare and combine more than one alpha factor. For instance, if we assume a somewhat Gaussian distribution, then about 95 percent of the values in a Z-scored distribution would be roughly between negative two and positive two.

4
00:00:37,250 --> 00:00:44,539
Notice that a ranking also has a similar benefit of setting different alpha vectors on the same scale.

5
00:00:44,539 --> 00:00:59,104
Whatever the original raw alpha values are for two different alpha factors when we convert to ranks, if we had a universe of 100 stocks, then the ranking for any alpha would be the integers 1-100.

6
00:00:59,104 --> 00:01:13,519
Note that Z-scoring has an additional benefit that you wouldn't get from ranking. Z-scoring sets alpha vectors on a comparable scale even when we're dealing with differently sized stock universes.

7
00:01:13,519 --> 00:01:36,335
For instance, certain alpha factors may be generated on different stock universes. So, if we had one alpha factor calculated on a universe of 100 stocks and another alpha factor calculated on 500 stocks, the rankings for the first would range from 1-100, while the rankings for the second would range from 1-500.

8
00:01:36,334 --> 00:01:44,585
Converting the ranks into a Z-score would rescale the two alpha vectors so that they could be comparable and more readily combined.

9
00:01:44,584 --> 00:01:54,670
Recall that ranking still makes our alpha vectors more robust against outliers and noisy data, and Z-scoring does not provide the same benefit.

10
00:01:54,670 --> 00:02:03,294
So, a valid approach would be to use ranking only when you know that all your alpha vectors are using the same stock universe.

11
00:02:03,295 --> 00:02:12,920
It's also useful to apply ranking and then Z-scoring, if you think that some of your alpha vectors may be generated on different stock universes.

12
00:02:12,919 --> 00:02:26,325
One thing to remember is that when you work in a large asset management company, you may be one of many alpha researchers, and there may be other team members, portfolio managers, who use your alpha vectors.

13
00:02:26,324 --> 00:02:35,719
In this case, the portfolio managers want to get standardized outputs from all the quants. Z-scoring is one way to achieve this.

14
00:02:35,719 --> 00:02:45,530
Academic research often uses Z-scoring, as many papers are based on the work of Fama and French, who designed one of the first multi-factor models.

15
00:02:45,530 --> 00:02:54,200
Since this paper has influenced many subsequent papers in the study of factors, it's likely you'll see Z-scoring in academic papers.


@@@
1
00:00:00,000 --> 00:00:08,664
Financial data is noisy, and sometimes the data we're working with is sparse. For instance, it might have many missing values.

2
00:00:08,664 --> 00:00:16,870
We can apply smoothing techniques across the time dimension to help make the factor more robust to noise and sparse data.

3
00:00:16,870 --> 00:00:24,325
Fundamental data is also sparse in nature, since it is only updated in the US, for example, once every three months.

4
00:00:24,324 --> 00:00:35,359
If we wish to use in a higher frequency, such as daily frequency, we should copy the most recent data over to each new day until the next new data point arrives three months later.

5
00:00:35,359 --> 00:00:46,795
We could also apply smoothing so that the daily data points incorporate some weighted averaging of not only the most recent update, but also the value from previous quarters.

6
00:00:46,795 --> 00:00:53,604
To make our Alpha factor more robust to noise and sparse data, we can apply a rolling window average.

7
00:00:53,604 --> 00:01:07,590
A rolling window is a kind of smoothing technique. A variation of a rolling window average is a weighted average, where the most recent Alpha may be given more weight and prior Alpha values are included but given less weight.

8
00:01:07,590 --> 00:01:24,200
This is called linear decay. For example, if we choose a window length of two and took the weighted average of the most recent Alpha values and the prior days Alpha values, we could give the newest value a weight of two and the earlier value a weight of one.

9
00:01:24,200 --> 00:01:35,329
In general, for a window length of T, we would give the most recent Alpha value a weight of T. Then for the earlier alpha values, we'd give them smaller weights.

10
00:01:35,329 --> 00:01:47,730
This technique can be very effective. In fact, I've seen several cases where the application of a smoothing operator both increase the Sharpe ratio and decrease the turnover.

11
00:01:47,730 --> 00:02:02,530
By the way, we'll cover the Sharpe ratio and a proxy for measuring turnover later in this lesson. These are both important evaluation metrics that help us find out if an Alpha factor that we create has the potential to enhance a portfolio's performance.


@@@
1
00:00:00,000 --> 00:00:13,425
Once we've calculated an Alpha factor, we can calculate some evaluation metrics, that let us compare it to other Alpha factors and get a sense for how it might perform when used to design a real-world portfolio.

2
00:00:13,425 --> 00:00:23,369
These metrics include factor returns, sharpe ratio, information coefficient, information ratio, quantile analysis, and turnover analysis.

3
00:00:23,370 --> 00:00:33,484
We evaluate Alpha factors to help us decide whether we'll use them in creating a combined Alpha factor, which we'll then use within portfolio optimization.

4
00:00:33,484 --> 00:00:49,310
One useful construct, is the return of the factor, which is called the factor return. The idea is that this is the return that a theoretical portfolio designed to capitalize on the arbitrage idea presented by the factor would produce.

5
00:00:49,310 --> 00:00:58,969
You can think of this as a way to directly measure the returns your portfolio would have if their weights were determined purely by the Alpha factor.

6
00:00:58,969 --> 00:01:10,790
To calculate the factor return we create a theoretical portfolio in which the weights for each stock on each day are set equal to the Alpha value for each stock on that day.

7
00:01:10,790 --> 00:01:22,969
So, every day we use the prior day's known data to calculate an Alpha vector, which we would standardize to have mean zero, and the sum of absolute values equal to one.

8
00:01:22,969 --> 00:01:39,254
We use the Alpha value of each stock i, as the weight for that stock on the current day T. At the end of that day we check the returns of each stock i, for that day T, which we'll call the single day return.

9
00:01:39,254 --> 00:01:49,399
Then, we can calculate the weighted average of the returns, using the Alpha values as weights. This gives us the factor return for a single day.

10
00:01:49,400 --> 00:01:59,855
We can repeat this for multiple days for a window of a year or more. This time series of daily portfolio returns is the factor return.

11
00:01:59,855 --> 00:02:20,509
Note that we'll use historical data to calculate the factor return. So, the example of calculating this one day at a time, is to help you see that we're simulating daily portfolio decisions that are only based on information that existed at the time of the theoretical trade, and these portfolio decisions are refreshed every day.

12
00:02:20,509 --> 00:02:36,800
Notice that the factor return depends upon the stock universe and time window of our theoretical portfolio, but it's a useful way to compare various Alpha factors to each other if we choose the same universe and time window.


@@@
1
00:00:00,000 --> 00:00:07,390
One important thing to note is that when we say "universe", we really mean Universe Construction Rule.

2
00:00:07,389 --> 00:00:14,929
Meaning, we cannot pick a static list of stocks and just use those for all time periods historically.

3
00:00:14,929 --> 00:00:25,269
If we chose a static list of stocks that exists today, or that have data for the time window of interests, that would include look-ahead bias.

4
00:00:25,269 --> 00:00:33,649
Look-ahead bias occurs when we simulate a trade on historical data using information that wouldn't be known at that time.

5
00:00:33,649 --> 00:00:43,894
A particular kind of look-ahead bias is survivorship bias. Survivorship bias occurs when we have the benefit today of choosing stocks for the past.

6
00:00:43,895 --> 00:00:59,268
In order to have an unbiased sample for our stock universe, we would not want to exclude companies that, for example, went bankrupt or were acquired because in the past, we would not know about those future events.

7
00:00:59,268 --> 00:01:13,555
Instead, we have to use a rule that creates the universe as it would have been for each day. One way to do this is to use a commercial index provider and use the components of an index as they were historically.

8
00:01:13,555 --> 00:01:22,770
That's actually what we do here. We're using the S&amp;P 500 index. The S&amp;P 500 is an index that's managed by Standard and Poor's.

9
00:01:22,769 --> 00:01:41,840
They managed index, adds, and deletes. By choosing our stock universe to include the stocks when they're in the index and exclude them when they are no longer in the index, we can more faithfully simulate a portfolio that would have existed in that point in time, thereby reducing look-ahead bias.


@@@
1
00:00:00,000 --> 00:00:10,255
Let's get back to the interpretation of the factor return. When initially learning about alpha factors, the magnitude of the return can create some confusion.

2
00:00:10,255 --> 00:00:25,254
In the alphas we work with below the annual returns range from 1%-4% or so. Let's be honest, that seems really small, is that what institutions really want from their researchers?

3
00:00:25,254 --> 00:00:37,409
The key to placing these returns and context is to understand the concept of the return denominator in research and the actual leverage likely to be used in production trading.

4
00:00:37,409 --> 00:00:57,060
Let's first define return denominator, when we normalize an alpha vector one of the steps is to sum up the absolute values of the alpha values, then divide each value by the sum in order for the normalized vector to have the sum of absolute values equal to one.

5
00:00:57,060 --> 00:01:10,870
The sum of the absolute magnitudes of the alpha values is the denominator that I'm referring to. Leverage is the act of borrowing money in order to take positions in assets like stocks.

6
00:01:10,870 --> 00:01:22,504
The leverage ratio is the sum of the absolute value of all positions long and short divided by whatever capital we actually devote to supporting those positions.

7
00:01:22,504 --> 00:01:44,613
This concept will likely become more clear when we study backtesting in term two, until then, think about it this way, a trading strategy produces dollars of gains and losses, to convert those dollars in terms of returns or in return space, we need to divide by some denominator, which in this case is called the capital denominator.

8
00:01:44,614 --> 00:01:53,474
At the research stage, we ignore this and simply assume one dollar of capital to one dollar of longs and shorts.

9
00:01:53,474 --> 00:02:02,894
When we increase leverage by putting less than one dollar of capital against one dollar of positions, the leverage ratio increases.

10
00:02:02,894 --> 00:02:16,459
A leverage ratio of one means we apply one dollar of capital for one dollar of positions. A leverage ratio of four means, we apply 25 cents of capital for one dollar of positions.

11
00:02:16,460 --> 00:02:25,610
In research, we are looking at the return of the normalized alpha factor, the denominator is one, which implies a leverage ratio of one.

12
00:02:25,610 --> 00:02:39,939
In a real trading, we would combine several factors and then apply leverage, depending on the final portfolio, an institution might apply leverage of anywhere from two times to six times.

13
00:02:39,939 --> 00:02:53,104
In the context of that leverage the magnitude of the alpha factor returns start to make sense and likely look comparable to the published returns observable from hedge fund return indices.

14
00:02:53,104 --> 00:03:03,655
This insight should also help explain why I continually refer to implementing this style of trading as inside an asset management firm.

15
00:03:03,655 --> 00:03:15,929
It would be very unlikely that an individual, could obtain the necessary leverage and low-cost market access to allow him or her to trade in this style, in an individual brokerage account.


@@@
1
00:00:00,000 --> 00:00:07,810
The sharpe ratio, sometimes referred to as the risk adjusted return, is a key metric for evaluating alpha factors.

2
00:00:07,809 --> 00:00:32,270
This is the ratio of the daily factor return divided by the daily standard deviation of the return. As an example, if we had three years worth of daily data, we could calculate the average daily return over those three years and divide by the sample standard deviation of those daily returns, and then analyse that by multiplying by the square root of the number of trading days in a year.

3
00:00:32,270 --> 00:00:40,990
If we have two factors that have the same universe and similar turnover profiles, we will prefer the one with the higher sharpe ratio.

4
00:00:40,990 --> 00:00:56,165
Sharpe ratio is the key metric that institutional asset managers are judged on. Note that the Sharpe ratio, like many evaluation metrics for alpha factors, help us to compare the relative performance of alpha vectors.

5
00:00:56,164 --> 00:01:11,519
The sharpe ratio is key, not the magnitude of factor returns. Because as we learned in the immediately preceding section, we can amplify or dampen returns by the use of leverage.


@@@
1
00:00:00,000 --> 00:00:15,704
A useful evaluation metric is the rank information coefficient, often referred to as rank IC. The rank IC tells us whether the ranks of our alpha values are correlated with the ranks of the future returns.

2
00:00:15,705 --> 00:00:28,379
In other words, if the alpha factor suggested that we bet more on stock ABC and less on stock XYZ, was the future return of ABC relatively high?

3
00:00:28,379 --> 00:00:43,174
Was a future return of stock XYZ relatively low? If the future performance of the assets matched the expectations that was suggested by the alpha factor, then the information coefficient would be higher.

4
00:00:43,174 --> 00:00:51,484
Otherwise, it would be lower and possibly negative. For some insight, let's first look at this without ranking.

5
00:00:51,484 --> 00:01:15,560
Let's pretend there are just two stocks in our universe, ABC and XYZ. Stock ABC has a high positive alpha value and XYZ has a very negative alpha value as calculated before time, t. Between time T to time T plus one, the return of stock ABC is positive and XYZ's return is negative.

6
00:01:15,560 --> 00:01:29,355
These alpha values appear to be correlated with the forward asset returns. Note here, when we say asset return, we're referring to the return of each stock for each time period.

7
00:01:29,355 --> 00:01:45,105
We also specify that the asset return is a forward return. We say, forward asset return to specify that the return is in the future or forward in time compared to when the alpha value was calculated.

8
00:01:45,105 --> 00:01:57,150
So, if the alpha values are calculated before time t, then the forward asset return is calculated with data that occurs later from time t to time t plus one.

9
00:01:57,150 --> 00:02:04,545
To make our evaluation more robust, we want to use ranks instead of the original alpha values and returns.

10
00:02:04,545 --> 00:02:13,060
Stock ABC is alpha value before time t is high and positive, while stock XYZ is alpha value, is negative.

11
00:02:13,060 --> 00:02:28,384
So, ABC has a higher rank of two and XYZ has a lower rank of one. The forward asset return of ABC from time t to time t plus one is higher compared to XYZ.

12
00:02:28,384 --> 00:02:37,280
So, the rank of ABC's forward asset return is two, and the rank of XYZ's forward asset return is one.

13
00:02:37,280 --> 00:02:47,210
If the ranks of the alpha values and the ranks of the forward asset returns are highly correlated, the rank IC metric would be close to one.

14
00:02:47,210 --> 00:02:57,930
In this example, since the ranks of the two alpha values are equal to the ranks of the two forward asset returns, then the rank IC in this example is one.


@@@
1
00:00:00,000 --> 00:00:09,285
More formally, here are the steps we'll take to calculate the Rank IC. For each time period, rank the raw Alpha factor.

2
00:00:09,285 --> 00:00:18,940
For instance, with a stock universe of three stocks, the lowest Alpha value has a rank of 1, and the highest Alpha value has a rank of 3.

3
00:00:18,940 --> 00:00:33,075
Also, calculate the forward asset returns and rank them. The stock with the lowest forward return has a rank of 1, and the stock with the highest forward return has a rank of 3.

4
00:00:33,075 --> 00:00:41,670
The correlation of ranked values is called the Spearman rank correlation to distinguish it from the Pearson correlation.

5
00:00:41,670 --> 00:00:54,340
Let's clarify these two terms. The Pearson correlation is what you're probably familiar with, is the covariance of two variables which is then re-scaled using the standard deviations of the two variables.

6
00:00:54,340 --> 00:01:05,650
The denominator makes the correlation range from negative 1 to 1. The Pearson Correlation is also the square root of the R-squared in a linear regression.

7
00:01:05,650 --> 00:01:13,784
Recall that the R-squared represents the proportion of variance in one variable that is explained by the second variable.

8
00:01:13,784 --> 00:01:25,859
The Spearman rank correlation is the same as the Pearson correlation, except the variables x and y are converted to ranks before calculating the covariance and standard deviations.

9
00:01:25,859 --> 00:01:35,629
To get the rank IC, calculate the correlation between the ranked Alpha vector and the ranked forward asset returns for a single time period.

10
00:01:35,629 --> 00:01:46,870
Repeat this over multiple time periods to get a time series. You may be wondering why we use the Spearman rank correlation as opposed to the Pearson correlation.

11
00:01:46,870 --> 00:01:56,019
That is why do we use ranking? The answer is because we don't care about being wrong in the right direction.

12
00:01:56,019 --> 00:02:14,314
Let's say stock ABC is our top Alpha value in a universe of 2,000 stocks. Our alpha value will likely be a small positive value, then imagine that ABC outperforms all the other stocks not by a small amount, but by a very large amount.

13
00:02:14,314 --> 00:02:22,615
The Pearson correlation would calculate a lower score effectively penalizing us because we didn't get the relative magnitude correct.

14
00:02:22,615 --> 00:02:36,869
On the other hand, the Spearman rank correlation would not be affected by this. We prefer to use ranking to evaluate our performance because what matters is that our Alpha is still profitable as we had hoped.


@@@
1
00:00:00,000 --> 00:00:19,399
A metric that is a special application of the Sharpe ratio is the information ratio. Recall that we are separating known factors as risk factors, which drive the variants of stocks and Alpha factors which drive portfolio returns after neutralizing to those risk factors.

2
00:00:19,399 --> 00:00:32,725
The return driven by risk factors is the systematic return, whereas the return driven by the Alpha factors after taking out the effect of risk factors, well, this is the specific return.

3
00:00:32,725 --> 00:00:45,079
The specific return is also referred to as the residual return because it is the residual amount of return that remains after taking out the systematic return.

4
00:00:45,079 --> 00:00:58,144
The information ratio is the Sharpe ratio applied to the specific return. So, it's the average specific return divided by its standard deviation annualized.

5
00:00:58,145 --> 00:01:07,855
The information ratio can be interpreted as the Sharpe ratio that measures the performance that the fund manager contributes to the portfolio.

6
00:01:07,855 --> 00:01:20,229
Note that in the context of what we're doing in this lesson, that is Alpha modeling, we are creating a market and common factor neutral portfolio.

7
00:01:20,230 --> 00:01:33,704
Note that in this case, the Sharpe ratio equals the information ratio. We simply introduced the term information ratio here because you will see it often in academic literature.

8
00:01:33,704 --> 00:01:43,000
As I stated previously, the most important metric in evaluating an Alpha factor and ultimately a trading strategy or fund is the Sharpe ratio.

9
00:01:43,000 --> 00:01:54,614
So, some good advice might be, okay, just create Alpha strategies with high Sharpe ratios. That advice though is pretty useless because it's descriptive not proscriptive.

10
00:01:54,614 --> 00:02:05,004
In other words, it tells us what we want to achieve, but it says nothing about how to get there. So, how do we actually create high Sharpe Ratio strategies?

11
00:02:05,004 --> 00:02:25,715
Well, the fundamental law of active management gives us some insight. From a philosophical standpoint, perhaps the most important thing you will learn in this program is the following relationship; IR equals IC times the square root of B.

12
00:02:25,715 --> 00:02:33,939
This comes from the seminal work a Richard Grinold, which is aptly titled, the Fundamental Law of Active Management.

13
00:02:33,939 --> 00:02:43,250
This is further detailed in the subsequent book by both Richard Grinold and Ronald Khan titled, Active Portfolio Management.

14
00:02:43,250 --> 00:02:54,764
By the way, this formula IR equals IC times the square root of B sometimes referred to as the E equals MC squared of finance.

15
00:02:54,764 --> 00:03:02,439
I just want to make a point about this formula. This result is based on some simplifying assumptions and should be thought of as a guide.

16
00:03:02,439 --> 00:03:09,800
It would be more correct to say that IR is proportional to IC times the square root of B as opposed to equal.

17
00:03:09,800 --> 00:03:21,385
This is not a formula we will use directly. Meaning, we don't actually use it to make calculations. Rather, we use this identity to direct how we structure our investment process.

18
00:03:21,384 --> 00:03:29,614
Also, don't worry if the relationship between IR, IC, and B doesn't look obvious to you as it's not supposed to be.

19
00:03:29,615 --> 00:03:36,395
If you're interested in the derivation, you can check out the original paper or the book by Grinold and Kahn.

20
00:03:36,395 --> 00:03:44,799
I haven't defined what IR, IC, and B mean in the context of this relationship. But stay tuned, it's coming up.


@@@
1
00:00:00,000 --> 00:00:20,075
The breadth is the number of independent trading opportunities annualized. The key insight on this is the word "independent." For example, if we are long 30 oil services stocks and short 30 semiconductor stocks for a year, how many independent bets is that?

2
00:00:20,074 --> 00:00:44,179
The answer is one. Not 60, one. Why? Because we expect the stock holdings in each sector are diversified to such an extent that we are likely not going to get any material impact from any one stock, and we have simply one bet that the oil service sector will outperform the semiconductor sector.

3
00:00:44,179 --> 00:00:54,145
The key takeaway here is that in order to maximize the number of independent bets in our portfolio, we need to remove exposure to common risk factors.

4
00:00:54,145 --> 00:01:07,420
Again, risk factors are factors which indicate which stocks have strong commonality. If we hold positions with strong commonality, then we are not holding independent positions.

5
00:01:07,420 --> 00:01:22,349
At the Alpha research stage, this is precisely why we often demean by sector. One way to improve our Sharpe ratio is to increase the probability that our Alpha factor can accurately choose the correct stocks.

6
00:01:22,349 --> 00:01:31,740
This is measured by the information coefficient. We should always strive to do this but recognize that this is indeed very challenging.

7
00:01:31,739 --> 00:01:40,670
The other way to improve performance is to increase the number of independent bets we make. This is the breadth.

8
00:01:40,670 --> 00:02:02,255
This is the key advantage quants have over everyone else; the ability to achieve high breadth. In general, a reason why institutional investors take a cross-sectional approach of trading multiple stocks is to maximize breadth and improve their performance which can be measured as the information ratio.


@@@
1
00:00:00,000 --> 00:00:08,289
So far, we've looked at evaluation metrics that assume unlimited liquidity in zero transaction costs.

2
00:00:08,289 --> 00:00:18,390
In other words, we've assumed that we can buy or short the stocks when we want and at the market price without cost or impact to that marketplace.

3
00:00:18,390 --> 00:00:26,575
We also saw that, increasing the number of stocks and the number of trades can improve one evaluation metric, the information ratio.

4
00:00:26,574 --> 00:00:34,804
However, when we include real-world constraints and costs, there are reasons why more trades aren't always better.

5
00:00:34,804 --> 00:00:49,984
When trading in the real market, there are constraints due to both liquidity and transaction costs. Liquidity of a stock refers to how likely one can buy or sell the stock when they want to in the amount that they want to.

6
00:00:49,984 --> 00:01:01,310
A proxy for liquidity is the bid-ask spread. The bid-ask spread is the difference between the price that an investor can buy versus sell immediately in the market.

7
00:01:01,310 --> 00:01:11,600
High liquid stocks that trade in high amounts every day have many shares that are accessible to the public and can be borrowed for short positions.

8
00:01:11,599 --> 00:01:25,074
Examples would be large market cap stocks in highly developed stock markets like the US. The bid-ask spreads in liquid stocks maybe around three to five basis points.

9
00:01:25,075 --> 00:01:41,165
For example, if the price at which one can sell a stock is $100 and the price at which one can buy a stock is 100.05, then the bid-ask spread is said to be five basis points.

10
00:01:41,165 --> 00:01:54,519
Low liquidity stocks have fewer accessible shares or are traded on a less active exchange. You may also refer to low liquidity as highly illiquid.

11
00:01:54,519 --> 00:02:09,305
Bid-ask spreads for less liquid stocks maybe 20 to 30 basis points. Remember, a decent Alpha factor might have an annual return of four percent or 400 basis points.


@@@
1
00:00:00,000 --> 00:00:08,085
Another real-world constraint is the total transaction cost, the implicit and explicit costs of making trades.

2
00:00:08,085 --> 00:00:17,129
Transaction costs add up and are often a substantial contributor to the success or lack of success of a trading strategy.

3
00:00:17,129 --> 00:00:32,089
Most people immediately think of commissions, the explicit cost, as the transaction cost. However, for an institutional market participant, the commission is usually by far the smallest component of the true transaction cost.

4
00:00:32,090 --> 00:00:40,979
The biggest component, the implicit component, is what is called the market impact. Institutions need to make large trades.

5
00:00:40,979 --> 00:00:53,170
Market impact is the effect that a market participant has when he or she buys or sells an asset. It is the extent to which buying or selling moves the price against the buyer or seller.

6
00:00:53,170 --> 00:01:13,390
More buying will move the market price upward. More selling will move the market price downward. For instance, if a fund is trying to sell a large block of shares to an investment bank, then the larger the block of shares, the more risk the investment bank is taking in holding the opposite side of the trade.

7
00:01:13,390 --> 00:01:20,984
So, the investment bank would ask for a better deal in the form of a lower price, which would then diminish the profit of the fund.

8
00:01:20,984 --> 00:01:30,284
To avoid adversely moving the market price, they will likely slice the trade into smaller blocks. Then we'll trade those blocks over time.

9
00:01:30,284 --> 00:01:37,670
Taking more time to trade all blocks also entails the risk that the market price will move over that time.

10
00:01:37,670 --> 00:01:51,400
So to summarize, real-world transaction costs can diminish the performance of an alpha factor. So, we want some way to evaluate an alpha factors potential impact on a portfolio's total transaction cost.


@@@
1
00:00:00,000 --> 00:00:11,500
Since liquidity and transaction costs are dependent upon market conditions at the time of a trade, it's difficult to simulate actual transaction costs when evaluating an Alpha factor.

2
00:00:11,500 --> 00:00:30,135
So a useful proxy for these real-world constraints is to measure turnover. Generally, factors that are updated quarterly such as fundamental factors, have lower turnover compared to factors that are updated daily such as those based on price and volume data.

3
00:00:30,135 --> 00:00:43,349
Turnover measures what fraction of the portfolio's total value gets traded in a time period. For example, let's say a portfolio is valued at $100 million.

4
00:00:43,350 --> 00:00:49,475
On a single day, let's imagine one million dollars are brought and another one million dollars are sold.

5
00:00:49,475 --> 00:01:07,359
So, the turnover in this case is two million dollars divided by 100 million or two percent. Since we're out the Alpha research stage and not dealing with an actual portfolio, turnover can be defined in terms of the change in portfolio weights.

6
00:01:07,359 --> 00:01:21,229
We calculate turnover by taking the difference between weights for each stock between yesterday and today, then we sum up the absolute values of the weights for all stocks in the portfolio.

7
00:01:21,230 --> 00:01:27,859
Recall that when constructing an Alpha factor, we're likely to convert raw factor values into ranks.


@@@
1
00:00:00,000 --> 00:00:11,449
Another way to measure turnover, is called the factor rank autocorrelation. The factor rank autocorrelation is close to one, when the ranking of stocks doesn't change much from day to day.

2
00:00:11,449 --> 00:00:18,240
To give you an illustration, let's imagine our universe consists of just two stocks, apple and alphabet.

3
00:00:18,239 --> 00:00:32,090
If over several days, the rank of apple, based on its alpha signal is always two, and the rank of alphabet is always one, then the previous ranks of each stock are perfectly correlated with the current day's ranks.

4
00:00:32,090 --> 00:00:53,064
In other words, the ranks are highly correlated or the factor rank autocorrelation is close to one. When the ranks of stocks don't change much from day to day, this also means that the weights of our theoretical portfolio don't change much either, which means less trading, and therefore less turnover.

5
00:00:53,064 --> 00:01:04,329
To calculate the factor rank autocorrelation for a single time period, we'll get the ranked alpha vector for the previous day, and also the ranked alpha vector for the current day.

6
00:01:04,329 --> 00:01:11,469
Then we calculate the Spearman rank correlation between the prior days and current days ranked alpha vectors.

7
00:01:11,469 --> 00:01:19,759
We can repeat this for multiple days and a time window, so that we have a time series of factor rank autocorrelations.

8
00:01:19,760 --> 00:01:34,459
A high factor rank autocorrelation is a proxy to indicate that the turnover is lower. A low, or even negative factor rank autocorrelation, would then indicate high turnover.

9
00:01:34,459 --> 00:01:43,484
Let's look at how we would use a turnover measure, such as factor rank autocorrelation in the context of other evaluation metrics.

10
00:01:43,484 --> 00:01:55,805
If two alpha factors have similar sharpe ratios, similar quintile performance, and similar factor returns, we'll always prefer the one with lower turnover.

11
00:01:55,805 --> 00:02:07,200
If an alpha factor has high sharpe ratio, but very high turnover, we may need to consider whether the alpha factor would survive backtesting, paper trading, and real trading.

12
00:02:07,200 --> 00:02:18,369
As a reminder of why lower turnover helps us, it makes it more possible for us to execute trades if the stocks are liquid and can reduce our transaction costs.

13
00:02:18,370 --> 00:02:33,689
Managing turnover is a delicate balance. On one hand, we want high turnover because that means alpha factor is taking advantage of new information, and making more trades which likely implies higher breath.

14
00:02:33,689 --> 00:02:41,935
However, on the other hand, trading costs money and excessive turnover could mean the alpha is simply capturing noise.

15
00:02:41,935 --> 00:02:52,864
Over time, the more you work in alpha research, the more you will develop intuition about this. Evaluating alpha factors is both an art and a science.

16
00:02:52,865 --> 00:03:15,419
Keep in mind that factor rank autocorrelation is different than rank IC. Even though these metrics both use ranking and correlation, rank IC is measuring whether an alpha factor is correlated with forward returns, while factor rank autocorrelation is measuring how stable the ranked alpha vectors are from day to day.


@@@
1
00:00:00,000 --> 00:00:09,865
We saw earlier that the rank IC can tell us overall how well an Alpha vector's predictions align with the subsequent stock returns for the next period.

2
00:00:09,865 --> 00:00:21,535
You may be wondering if it's possible to drill down deeper to look at the Alpha values assigned each stock to see which subset of stocks actually contribute most or least to the factor return of the portfolio.

3
00:00:21,535 --> 00:00:33,285
Ideally, if our Alpha model and hypothesis were accurate, the stock with the highest Alpha value for that day would also have the highest positive return the next day.

4
00:00:33,284 --> 00:00:47,379
Similarly, the stock with the lowest Alpha value for that day would also have the lowest return in the next day, which would be good for our theoretical portfolio since we would be shorting that stock.

5
00:00:47,380 --> 00:01:04,719
Remember, that we are working with perhaps hundreds or even thousands of stocks in our universe. So, a good middle ground is to divide our Alpha vector into quantiles and analyze their returns and volatility within those quantiles.

6
00:01:04,719 --> 00:01:17,799
An ideal Alpha model would be one where the group of stocks containing the highest Alpha values for that day would also have the highest average return, and possibly, the highest risk adjusted to return.

7
00:01:17,799 --> 00:01:25,585
Similarly, the group of stocks containing the lowest Alpha values would ideally have the lowest returns.

8
00:01:25,584 --> 00:01:38,344
For example, if we have 25 stocks in our universe and wanted divide them up each into five groups of equal size, we would be using five quantiles also called quintiles.

9
00:01:38,344 --> 00:01:47,259
For each day, we'll sort the stocks by their Alpha value. The five highest values go in the fifth group, which are above the fourth quintile.

10
00:01:47,260 --> 00:02:01,390
The stocks with the lowest five values go in the first group, which are below the first quintile. Similarly, we fill groups 2, 3, 4, with five stocks each based on their Alpha value.

11
00:02:01,390 --> 00:02:09,804
We can keep track of the individual returns within each of the five groups over a time window such as one, three, or five years.

12
00:02:09,805 --> 00:02:20,944
Then we can calculate the mean return within each group as well as the standard deviation. We can call what we just did quantile analysis or quantile performance.


@@@
1
00:00:00,000 --> 00:00:12,129
Here's an example of an ideal Alpha factors quintile performance. The fifth group containing the highest Alpha values would ideally give the highest returns.

2
00:00:12,130 --> 00:00:27,234
There would also ideally be a smooth progression, down the fourth, third, second, and first groups. That is the fourth group would have lower returns than the fifth group, but higher than the other groups.

3
00:00:27,234 --> 00:00:34,245
The first group would ideally have the lowest returns, and the second group would have the second lowest returns.

4
00:00:34,244 --> 00:00:48,600
It's common for the tails to show predictive power; that is group five may have higher returns, group one low returns, but groups two, three, and four have returns not that much distinguishable from zero.

5
00:00:48,600 --> 00:00:55,725
In these scenarios, the Alpha model's performance is more reliant on a smaller subset of the Alpha vector.

6
00:00:55,725 --> 00:01:03,119
In other words, on any given day, the factor return is due to the model being right on a subset of the stocks.

7
00:01:03,119 --> 00:01:14,694
Similarly, it's also possible for the tails to show less predictive power and for most of the factor return to be attributed to the middle groups; two, three, and four.

8
00:01:14,694 --> 00:01:27,484
Again, this means that a subset of the entire Alpha vector appears to get it right on any given day, and it's usually less correct when the Alpha number is either very high or very low.

9
00:01:27,484 --> 00:01:36,924
You must diagnose these situations very carefully. In most cases you want to see a monotonic relationship.

10
00:01:36,924 --> 00:01:50,359
Each successive quantile produces a higher return than the previous one. In general, if you don't see this it could indicate that your factor is not robust and invalid.

11
00:01:50,359 --> 00:02:03,710
However, there are exceptions and you should think about this before you run the quintile analysis. One example is the Sloan's accrual anomaly which you can read about on the link below.

12
00:02:03,709 --> 00:02:14,669
Whenever the factor return relies on a smaller subset of the Alpha vector having predictive power, then there's higher risk associated with using that Alpha.


@@@
1
00:00:00,000 --> 00:00:11,155
As practitioners, we were reading academic research papers for insights. So, it's helpful to know how academic papers and practitioners view quantiles differently.

2
00:00:11,154 --> 00:00:19,520
Academic papers tend to take raw alpha signals, which are not ranked, and split them into quantiles only.

3
00:00:19,519 --> 00:00:39,330
Their analysis usually focuses on the tails. That is, the highest and lowest quantiles. The reason that academic researchers are focused on this, is that they want to detect broadly applicable market phenomenon, and are not necessarily interested in how to implement a trading strategy.

4
00:00:39,329 --> 00:00:55,149
Because of this, you may notice that certain academic research will find an effect in just one tail such as the lowest quintile, while they find less of an effect in the other tail quantile or even in the other quantiles in between.

5
00:00:55,149 --> 00:01:02,774
Note that this means that the abnormal returns that they detect in such a paper, may only work on a subset of the stock universe.

6
00:01:02,774 --> 00:01:10,774
Practitioners have the goal of implementing a trading strategy that can be applied to all the assets in their universe.

7
00:01:10,775 --> 00:01:20,179
This means that with quintile analysis, practitioners care about how the factor performs across all quantile groups, not just one or two tails.

8
00:01:20,180 --> 00:01:25,870
This is because practitioners want to see how the factor influences all stocks and there trading universe.

9
00:01:25,870 --> 00:01:42,120
So, even if a research study found that an alpha factor only worked in the subset of the quantile distribution, the practitioner will view this as a graded spectrum in order to generate an alpha number for every stock in their universe.


@@@
1
00:00:00,000 --> 00:00:10,355
Recall that we prefer to control for risk within the Alpha factors, rather than waiting for their risk model optimizer to do it completely for us.

2
00:00:10,355 --> 00:00:28,675
This is why we make the Alpha vector dollar neutral and often also sector neutral. The reason we wish to control for risk early on within the Alpha vector itself is so that after the optimization, the portfolio weights are not too different from the original Alpha vector.

3
00:00:28,675 --> 00:00:34,850
It's natural for you to wonder why is it an issue at the portfolio weights are nothing like the original Alpha vector?

4
00:00:34,850 --> 00:00:43,899
To understand this, remember that we're evaluating each Alpha factor in isolation of other Alpha vectors.

5
00:00:43,899 --> 00:00:50,579
We evaluate the Alpha factor to see if it may be a good candidate to help improve the performance of a portfolio.

6
00:00:50,579 --> 00:01:07,789
If after optimization, the risk model significantly changes the weights for stock so that they no longer follow the weights chosen by the Alpha vector, then we can't expect the performance of the Alpha to carry over into the portfolio.

7
00:01:07,790 --> 00:01:30,339
As the example, imagine if an Alpha vector gives most positive weight to stock ABC. But after the risk model neutralizes various common risks, the final portfolio weights end up shorting stock ABC, then the signal from the Alpha vector which essentially says, "Buy stock ABC" is no longer being followed.

8
00:01:30,340 --> 00:01:40,250
This means that the performance that we originally hoped to gain from using this Alpha vector may not carry over or transfer to the portfolio itself.

9
00:01:40,250 --> 00:01:58,655
Remember that the Alpha vector is an expression of expected returns. If the portfolio weights after every optimization are very different from the original Alpha factors, then the portfolio itself may not be capturing the information expressed by those Alpha vectors.

10
00:01:58,655 --> 00:02:07,750
To see how closely the optimized portfolio weights still match the original Alpha vector, we can calculate something called the transfer coefficient.

11
00:02:07,750 --> 00:02:14,580
The transfer coefficient measures how closely the optimized portfolio weights match the original Alpha vector.

12
00:02:14,580 --> 00:02:22,109
To get the transfer coefficient, we need to use portfolio optimization that includes a risk factor model.

13
00:02:22,110 --> 00:02:32,680
We will learn about the portfolio optimization step in a later lesson. For now, let's just assume that we have a portfolio optimizer that outputs portfolio weights.

14
00:02:32,680 --> 00:02:39,490
We start with our Alpha vector, pass it through the portfolio optimization and get portfolio weights.

15
00:02:39,490 --> 00:02:47,235
The transfer coefficient is the correlation between the Alpha vector and the resulting portfolio weights.

16
00:02:47,235 --> 00:02:55,849
A high transfer coefficient is a good sign that the Alpha vector has survived the portfolio optimizers risk neutralization.

17
00:02:55,849 --> 00:03:06,300
The transfer coefficient is also a useful measure for another desired attribute of an Alpha factor independence from risk factors.

18
00:03:06,300 --> 00:03:21,750
Ideally, we would like to find Alpha factors that are independent of the common risk factors. If Alpha factors were too correlated with the risk factors, then they would also be neutralized by the risk model.


@@@
1
00:00:00,000 --> 00:00:13,484
In some alpha factor papers that we'll discuss in the next lesson, the academic research discusses findings that are specific to one direction or one tail end of a particular attribute.

2
00:00:13,484 --> 00:00:24,500
A practitioner's approach is to think about extending the interpretation of the academic conclusion, so that it can be more readily applied to general situations.

3
00:00:24,500 --> 00:00:36,695
For example, in an alpha factor that we'll discuss, the curvature of a stock's price path over time has some predictive value of its future expected performance.

4
00:00:36,695 --> 00:00:50,675
Even though the paper itself doesn't discuss linear trajectories, a practitioner would want to find some meaningful hypothesis about straight lines and how they compare to convex or concave functions.

5
00:00:50,674 --> 00:01:02,150
In other words, when a straight line is compared to a convex curve, the line is relatively more concave than the convex curve.

6
00:01:02,149 --> 00:01:17,599
By thinking how an academic finding can be applied more broadly to a full range of a particular attribute, a practitioner has a better chance of creating an alpha factor that can more generally be applied to the entire stock universe.


@@@
1
00:00:00,000 --> 00:00:10,794
So far, we've been looking at simple individual alpha factors. But there are times when we can create factors that are conditioned on another factor.

2
00:00:10,794 --> 00:00:36,700
We can create a conditional factor by multiplying two factors together. For example, if factor A gives an alpha value for a single stock, and factor B gives a different alpha value for that same stock, then we can create a conditioned, alpha value for that stock by multiplying those two separate alpha values together.

3
00:00:36,700 --> 00:00:46,919
Early on in my career, it was the case that some fairly simple, at least as thought of in hindsight today, factors could be profitable in live trading.

4
00:00:46,920 --> 00:00:56,525
Markets though have become much more competitive as time has marched on and as data, tools, and knowledge have become more available.

5
00:00:56,524 --> 00:01:06,685
Today, it's likely that you need to find conditionals to make really good alpha factors as such, this section is critical.

6
00:01:06,685 --> 00:01:19,459
So, how do we interpret what this multiplication is doing? We'll first look at a common practice in academic papers, which we'll refer to as quantiles of quantiles.

7
00:01:19,459 --> 00:01:29,510
If a research paper is looking at the effects of one factor condition on another, they would divide the data into quantiles for one factor.

8
00:01:29,510 --> 00:01:43,299
Let's say we're using 10 decile groups, then within each decile group they would split the stocks into another 10 decile groups based on the second factor.

9
00:01:43,299 --> 00:01:58,195
So, now we have 100 quantile groups conditioned on two factors. Because we have 10 deciles based on one factor and each decile group has 10 deciles based on the second factor.

10
00:01:58,194 --> 00:02:18,844
Now, let's see what happens if we create a condition factor based on two factors. Let's say we have a stock universe of 100 stocks and we'll convert signal A and signal B into scores from one to 10 for each signal, then we'll calculate the conditional factor as signal A times signal B.

11
00:02:18,844 --> 00:02:36,754
Let's pause a moment to think what multiplication means visually. If you remember learning your multiplication tables as a kid, you might remember seeing a rectangular grid with a column of numbers 1-10 along the left edge and the numbers 1-10 above the top.

12
00:02:36,754 --> 00:02:43,189
Multiplying the two numbers for each row and column allows us to fill all the boxes inside the multiplication table.

13
00:02:43,189 --> 00:02:55,310
This is exactly what we're doing with our two factors. As you can see, the product of A times B, ranges from a value of one to a value of a hundred.

14
00:02:55,310 --> 00:03:02,724
Notice that, not all the values in between one and 100 are represented, as there are some numbers that occur twice.

15
00:03:02,724 --> 00:03:12,000
However, overall, you can see that there is a range of values between one and a hundred. Does this look familiar to you?

16
00:03:12,000 --> 00:03:20,360
When we discuss the process of calculating deciles nested inside deciles, we also ended up with 100 groups.

17
00:03:20,360 --> 00:03:35,395
Even though multiplication of the two factors does not give the exact same groupings as we would get from nested quantiles, they have the same goal of creating a composite factor that is conditioned on the value of both the individual factors.

18
00:03:35,395 --> 00:03:42,319
We will seek some conditional factors in detail when we analyze some research papers to generate factor ideas.

19
00:03:42,319 --> 00:03:56,115
When you read ideas or hypotheses like, stocks that go up on high volume are likely to revert, or momentum is more pronounced on stocks with low volume, you need to use conditionals.


@@@
1
00:00:00,000 --> 00:00:15,064
Congratulations on making it through this lesson. We reviewed some foundational building blocks that you will see again and use when we translate academic papers into alpha ideas and then implement those ideas in code.

2
00:00:15,064 --> 00:00:25,515
But let's just remember that, even after testing, if your alphas look promising, this is really only the first stage of their lives so to speak.

3
00:00:25,515 --> 00:00:33,619
Lets step back a bit and see how the search for alpha factors fits into the big picture of the quant workflow.

4
00:00:33,619 --> 00:00:42,195
We first propose and generate alpha factors, then evaluate them to find those that might show some promise.

5
00:00:42,195 --> 00:00:52,689
Then we perform out-of-sample testing of the alpha factors using historical data that wasn't used to construct the alpha vector.

6
00:00:52,689 --> 00:01:08,295
If that looks promising, then we would conduct paper trading in which we don't use real money, but we follow the factor as if we're making theoretical trays over time on newly arriving live market data for some period.

7
00:01:08,295 --> 00:01:16,004
If that showed promise, then we would put the alpha into production in a real portfolio with real money.

8
00:01:16,004 --> 00:01:24,595
The alpha at that stage would be blended with other alphas and the final alpha vector would pass through a portfolio optimizer.

9
00:01:24,594 --> 00:01:35,949
We would likely start by giving that alpha factor a small weight in the combined vector, and given more weight if it performed and improved portfolio performance.

10
00:01:35,950 --> 00:01:45,545
We would monitor the alpha factor over time knowing that at some point, the factor's usefulness will erode because we're trading in a competitive market.

11
00:01:45,545 --> 00:01:53,840
Then, we would remove the alpha factor and go back to the beginning to search for new promising alpha factors.

12
00:01:53,840 --> 00:02:06,924
Note that when I say "we" in those steps, after the first one, the "we" could be a human portfolio manager or it could be an artificially intelligent agent and AI.

13
00:02:06,924 --> 00:02:23,945
Even in the first step, we can utilize AI techniques to help us recover both these AI applications. The application for alpha generation and the AI application for portfolio construction in term two of the API for trading nano degree program.

14
00:02:23,944 --> 00:02:35,840
We will cover later stages of this process in future lessons. We will cover backtesting, out-of-sample testing and other methods to avoid overfitting.

15
00:02:35,840 --> 00:02:59,270
It can't be overstated how important out-of-sample testing is in practice. If our goal is to seek above average performance in real production trading with real money and real world constraints, then it is critically important to avoid overfitting to pass data to the point that our alpha factors cannot generalize to current market conditions.

16
00:02:59,270 --> 00:03:14,585
Even within the academic scientific community, both in finance and in the general sciences, there are serious discussions about overfitting and about validating the results of research.

17
00:03:14,585 --> 00:03:27,165
The rest of this lesson will focus on generating and evaluating alpha factors. But I do hope you keep in mind that it's perfectly normal if you feel the need to go back and review the concepts covered in this lesson.

18
00:03:27,164 --> 00:03:38,360
I know there was quite a lot of material, but I'm also very excited for you as you're learning the techniques and the type of thinking that real quant researchers deploy in their search for alpha.


@@@
1
00:00:01,189 --> 00:00:08,919
Well, hello there. Thanks for joining me. I'm just going to pop in here and share some thoughts about reading research papers.

2
00:00:08,919 --> 00:00:15,585
I remember when I got started doing academic work, reading a research paper felt like a daunting task.

3
00:00:15,585 --> 00:00:25,844
I understood that reading a research paper meant sitting down with a cup of tea and reading the paper like I would a book from cover to cover, trying to absorb every word.

4
00:00:25,844 --> 00:00:34,655
When I accomplished this successfully, it took me several hours. When I failed, I would leave off in the middle, often not remembering what I learned at the beginning.

5
00:00:34,655 --> 00:00:44,989
It must be said that being able to read and learn from a research paper quickly comes with experience, but also comes with experience in the specific field you're reading about.

6
00:00:44,990 --> 00:01:01,130
This is because research projects in each very specific niche field are highly interdependent. When you see that one paper is basically using the same method as another, and you already understand that method, you can skip that entire method section.

7
00:01:01,130 --> 00:01:13,234
When you see that a paper references another paper for some idea, and you've already read that other paper, because it's a very influential or so-called classic paper in the field, you'll understand the reference.

8
00:01:13,234 --> 00:01:22,969
There's no easy, quick way to gain this knowledge that I know of. It's one reason that developing expertise in a field is a long, hard one process.

9
00:01:22,969 --> 00:01:30,594
But there are some things to know about research papers and some strategies you can take to make the process a little easier.

10
00:01:30,594 --> 00:01:42,924
I think the most important thing to remember is that you don't have to read everything. Research papers are designed in such a way as to show you, the reader, where to find different types of information.

11
00:01:42,924 --> 00:01:55,690
Perhaps the second most important thing to remember is that research papers are written by people. They are not infallible, they're not gospel, and they are not perfect at communicating what they aim to communicate.

12
00:01:55,689 --> 00:02:11,060
What I mean to say is that if you don't understand something you're reading, it may be that you lack the background to understand it, but it may also be that the paper's author wrote it in a way that they understand it, but not in a way that anyone else can understand it.

13
00:02:11,060 --> 00:02:27,349
Often, the author is writing to an audience that already knows the field, not to a new comer. Instead of banging your head against the wall, so to speak, or rereading the same section many times, you may have to go elsewhere for a better understanding of the topic.


@@@
1
00:00:00,000 --> 00:00:06,189
Let's talk about the structure. Do you remember grade school science fairs and learning about the scientific method?

2
00:00:06,190 --> 00:00:18,050
Well guess what, what you learned actually applies to the real world. Papers are actually structured with title, abstract, methods, results, conclusions, and discussion sections.

3
00:00:18,050 --> 00:00:29,320
Let's start at the beginning. The very first thing that you'll read is the paper's title. You've probably picked out the paper because the title indicates that the topic has to do with something you care about.

4
00:00:29,320 --> 00:00:37,890
Sometimes, it has an important keyword. The second thing that you should pay attention to is the list of authors. Why does this matter?

5
00:00:37,890 --> 00:00:46,590
Well, it tells you something about the provenance of the ideas within. Usually, the first author is the main author, the person who did most of the work.

6
00:00:46,590 --> 00:00:53,780
But sometimes there are multiple main authors, indicated with asterisks. Usually, the last author is this person's adviser.

7
00:00:53,780 --> 00:01:01,399
Typically, a more senior person in the field. The ideas in the paper maybe offshoots of this more senior person's other ideas.

8
00:01:01,399 --> 00:01:13,390
So, they may be related to this person's older papers. When there are only one or two authors, it could be that this person or people are already established in this field and they worked on this project on their own.

9
00:01:13,390 --> 00:01:22,974
Sometimes the list of authors is huge. This means that many teams of people typically in different places around the world had to cooperate to accomplish this work.

10
00:01:22,974 --> 00:01:31,484
Okay. The third thing you'll read is the abstract. This is a short summary of the paper. Sometimes this is as far as you get.

11
00:01:31,484 --> 00:01:42,239
The abstract should communicate basically the question or problem the author tried to address, what the author did, the main important finding, and what they think their result means.

12
00:01:42,239 --> 00:01:56,565
That is how it addresses the original question or problem. If you read the abstract and the paper is about something different than what you thought it was, or doesn't indicate that they found something meaningful, you can stop right there.

13
00:01:56,564 --> 00:02:05,555
Sometimes the abstract looks interesting, but doesn't give you enough information. If you get past the abstract, you come to the introduction.

14
00:02:05,555 --> 00:02:13,655
In this part, the author is supposed to give some background about the field, and explain why they decided to study the topic of the paper.

15
00:02:13,655 --> 00:02:21,500
It's supposed to help you follow the flow of ideas that led to the question, which in turn led to the idea of their paper.

16
00:02:21,500 --> 00:02:26,990
Sometimes the author does a great job at this, and you can really learn something about the field in this part.

17
00:02:26,990 --> 00:02:36,409
Sometimes the introduction is short and not very helpful. Often, in this section, the paper will cite other papers that may be useful to look at to.

18
00:02:36,409 --> 00:02:46,575
The next section is usually the methods section. Do you have to go through this line by line? No. In fact, you can skip this entire section.

19
00:02:46,574 --> 00:02:52,865
You might skip ahead to the results and conclusions to know what they actually found and come back to this section later.

20
00:02:52,865 --> 00:03:04,545
You might end up reading this, if you want to know that they were careful to control for some important confounding variable which you would care about if you were trying to decide for yourself, if their results are meaningful.

21
00:03:04,544 --> 00:03:13,330
Or you might read this, if you want to replicate what they did in whole or in part. Next, is usually the result section.

22
00:03:13,330 --> 00:03:20,560
This section should communicate the main measurements the authors made. Ideally, everything is well labeled and clear.

23
00:03:20,560 --> 00:03:26,230
But sometimes it's not, and you have to skip back to the methods to find out what some measurement is.

24
00:03:26,229 --> 00:03:36,880
Finally, the conclusions or discussion should communicate why the results are important, what they mean, and how they relate to the original question or problem.


@@@
1
00:00:00,000 --> 00:00:06,960
Another thing to remember, is that journals themselves impose their own editorial influence on papers too.

2
00:00:06,960 --> 00:00:15,390
They look for papers of certain types. They ask for papers to be structured in certain ways. They impose word counts and style guides.

3
00:00:15,390 --> 00:00:22,964
Often within a field, certain journals are known to be the most competitive to get your work published in and the most high profile.

4
00:00:22,964 --> 00:00:34,969
Typically these journals have the largest most diverse readership. So, a paper must be of sufficient importance and broad interests to be published there and be circulated to the most eyeballs.

5
00:00:34,969 --> 00:00:47,245
The other important thing I wanted to tell you is that, if what you are trying to do is understand a whole field and not just an individual experiment, or project, what you may want is a review paper.

6
00:00:47,244 --> 00:00:55,219
This is a paper that has a different purpose. It's meant to be a summary, or discussion of several papers, or ideas on the same topic.

7
00:00:55,219 --> 00:01:06,634
These are like gold for newcomers to a field. It's not always clear where and how to find them. Sometimes a journal that normally publishes regular papers puts out an issue of reviews.

8
00:01:06,635 --> 00:01:13,489
Other times you'll find helpful reviews and journals that only publish reviews. For example, the journal annual reviews.

9
00:01:13,489 --> 00:01:22,164
A final thing to know, when you are reading research papers is that, yes a paper is a piece of communication or documentation about a study.

10
00:01:22,165 --> 00:01:34,629
But it is also an assertion that science has progressed, that knowledge has advanced, in other words it's an assertion that the work documented in the study means something important about the world.

11
00:01:34,629 --> 00:01:41,765
When a paper is complete enough to be sent to a journal, it usually means that many people think the work is valid and meaningful.

12
00:01:41,765 --> 00:01:50,880
The lead author, her colleagues, her adviser, and potentially other people in the research community who the person showed the work to or discuss it with.

13
00:01:50,879 --> 00:01:57,760
But then when a journal editor takes interest in a paper, the work is usually sent out to other people who do similar work.

14
00:01:57,760 --> 00:02:05,209
These people often raise very important concerns about the validity or meaning of the work that nobody else has mentioned yet.

15
00:02:05,209 --> 00:02:12,430
Their assertions can block publication of the paper if not addressed. This is the peer review process.

16
00:02:12,430 --> 00:02:20,715
Finally, even after a paper is published, sometimes information comes out that cast the information in the paper in a new light.

17
00:02:20,715 --> 00:02:34,745
For example, sometimes other researchers try to do the same thing and get different results. This isn't to say that you should necessarily always doubt the authors assertions, but neither should you believe them wholesale.

18
00:02:34,745 --> 00:02:54,275
The reality is far more nuanced than that. The vast majority of the time, they did what they say they did but there may be a caveat about how they did it such that what they did is not as meaningful as they think, this is just to say that the process of establishing knowledge is long and has many twists and turns.

19
00:02:54,275 --> 00:03:07,880
Remember that when you read a paper, you're somewhere in the middle of that process which is also rather exciting because when you're reading papers, you maybe learning about things that are so new that very few people in the world have heard of them yet.

20
00:03:07,879 --> 00:03:18,729
Getting back to the actual reading, sometimes it is important that you read an entire paper. For example, if you're doing a presentation on it and people are going to ask you detailed questions.

21
00:03:18,729 --> 00:03:23,955
But more often there's certain information you are looking to take from a paper and that's all you need.

22
00:03:23,955 --> 00:03:31,929
I think the most important thing to remember in practice is to keep up your momentum. Don't get stuck on a tiny detail that you don't understand.


@@@
1
00:00:00,000 --> 00:00:09,169
Hi, welcome to the lesson where we'll take a deeper look at four academic papers and see how we can code up Alpha factors from them.

2
00:00:09,169 --> 00:00:17,259
In building the material for this lesson, I browsed dozens of academic papers and chose some of these to implement.

3
00:00:17,260 --> 00:00:30,270
Note, that I didn't choose these papers particularly because they produced great Alphas, rather, I chose them to give a diversity of style and show important techniques and building blocks for making Alphas.

4
00:00:30,269 --> 00:00:39,750
In fact, some of the performance is disappointing in the period we test, but the concepts that you'll learn are key to the work that you may do as a quant.

5
00:00:39,750 --> 00:00:52,964
I'll discuss each of these papers and we'll see how each can be implemented in code. I will also comment on instances where I take a practitioner's approach to interpreting or generalizing ideas from the academic research.

6
00:00:52,965 --> 00:01:11,734
In reading these papers and learning to implement Alpha factors based on their ideas, you will learn about Alpha factor building blocks that we'll refer to as overnight returns, accelerated and decelerated gains or losses, positive skew, and idiosyncratic volatility.

7
00:01:11,734 --> 00:01:25,700
You'll gain insights into how psychology and market mechanics shape the hypotheses upon which these factors were built, and also see different ways in which a factor can condition the effect of another factor.

8
00:01:25,700 --> 00:01:34,670
I hope that you can develop a good habit of reading academic research, proposing, and implementing, and then evaluating potential Alpha factors.

9
00:01:34,670 --> 00:01:41,059
These skills and experiences will serve you well in preparing you for roles in quantitative finance.


@@@
1
00:00:00,000 --> 00:00:12,129
Let's read the abstract of the paper, Overnight Returns and Firm Specific Investor Sentiment. We can see that the authors tried to analyze something called overnight returns.

2
00:00:12,130 --> 00:00:27,794
If we skip ahead to Section 2 titled sample, variable definitions, and descriptive statistics, we can see that overnight returns are defined as the percent change from the close price of the previous day to the opening price of the next day.

3
00:00:27,795 --> 00:00:42,219
The overnight returns are also called closed open returns. They also mentioned investor sentiment. If we jump to the introduction, we see that overnight returns may be used as a proxy for firm-level investor sentiment.

4
00:00:42,219 --> 00:00:51,265
In the context of the paper, firm level sentiment appears to refer to whether investors have a positive or negative view of a stock's future price.

5
00:00:51,265 --> 00:01:02,300
It also helps to check the paper for signs of either a momentum or mean-reversion factor. It mentions short-term overnight return persistence.

6
00:01:02,299 --> 00:01:13,700
When we read the word persistence, we can interpret this as referring to some kind of momentum. The paper also says that stocks with high overnight returns underperform.

7
00:01:13,700 --> 00:01:24,065
We can interpret this as a kind of mean reversion. After reading the abstract, it's okay to quickly scan the paper for a preview of major sections.

8
00:01:24,064 --> 00:01:35,070
It's common for papers to have an introduction, methodology, results, and conclusion section. It's also okay to read the paper out of order.

9
00:01:35,069 --> 00:01:44,800
For example, you can start with the introduction, jump to the conclusion, then work your way through the methodology and results.


@@@
1
00:00:00,000 --> 00:00:12,804
From reading the introduction, we see that the authors refer to investor sentiment as the positive or negative views of investors, especially individuals who tend to cluster their orders around a market open.

2
00:00:12,804 --> 00:00:21,105
The authors also define overnight returns as the close to open returns and describe the following hypothesis.

3
00:00:21,105 --> 00:00:28,384
Individual investors may notice attention getting events and may then choose to trade on those events.

4
00:00:28,385 --> 00:00:35,765
Since many of them have day jobs, they may place trades aftermarket clothes that would be executed the following morning.

5
00:00:35,765 --> 00:00:45,069
The hypothesis continues to assume that these overnight trades may be subject to mean-reversion by the middle of that second day.

6
00:00:45,070 --> 00:00:54,299
Based on this, you may start thinking about how this can be useful as an alpha factor. Let's say, a stock's overnight returns are high.

7
00:00:54,299 --> 00:01:08,109
So, that morning, you see an increase in a stock's open price relative to the previous day's close. According to this hypothesis, the stock is overbought and we may expect a correction in a form of a drop in the price.

8
00:01:08,109 --> 00:01:16,844
So, if you calculate the overnight return of the stock, you can short the stock that same morning when the overnight return is high.

9
00:01:16,844 --> 00:01:26,229
However, that's not the effect that we will look to capture here. There are ideas later in the paper that we will use to construct an alpha factor.

10
00:01:26,230 --> 00:01:33,780
Also, in the introduction, the authors state that they test for short-run persistence in overnight returns.

11
00:01:33,780 --> 00:01:53,045
This indicates some momentum in the overnight returns over a short term window. If we continue reading in section two titled sample, variable definitions, descriptive statistics, we can see that they're interested in a window of one week or five trading days.

12
00:01:53,045 --> 00:02:02,879
They calculate the average daily close to open return over a week as a measure of the persistence or momentum in sentiments.

13
00:02:02,879 --> 00:02:12,650
Again, based on this information, you can think about how this weekly average momentum factor might be used in a theoretical portfolio.

14
00:02:12,650 --> 00:02:27,044
If according to the hypothesis, the weekly average close to open return for a stock is high and positive, we might want to buy more shares of that stock with the assumption that the positive returns will continue in the short run.

15
00:02:27,044 --> 00:02:38,385
Finally, the abstract also identifies a third possible factor. With the hypothesis that in the longer term, stocks with high overnight returns under perform.

16
00:02:38,384 --> 00:03:02,250
In other words, in a longer time window, stocks exhibit mean reversion. If we go to section five titled longer-term returns, the hypothesis based on other papers is that stocks that are more attractive to speculators and therefore see more near term positive returns may end up under performing over the next 12 months.


@@@
1
00:00:00,000 --> 00:00:12,245
The paper identifies its dataset as the Center for Research and Security Prices, which is often abbreviated as CRSP and can be referred to as CRISP.

2
00:00:12,244 --> 00:00:21,208
This is a fairly well-known pricing dataset. And if you check out their website, you can see that the data is available via subscription.

3
00:00:21,208 --> 00:00:34,774
We don't use that data precisely, but we are using a pricing dataset which is very similar. It's also helpful to check what stock universe the authors think are most relevant to the factors they are analyzing.

4
00:00:34,774 --> 00:00:47,260
Even though a factor would ideally be applicable and useful for a broad range of stocks, it's also possible that certain factors only generate meaningful signals for a certain subgroup of stocks.

5
00:00:47,259 --> 00:00:57,969
In the abstract, the authors mentioned that mean reversion or momentum factors that they're evaluating are more pronounced for hard to value firms.

6
00:00:57,969 --> 00:01:13,329
If we look through the paper for mention of harder to value firms, we see how they define them in section four titled, Short-term Overnight Return Persistence, Hard to Value Firms and Institutional Share Holdings.

7
00:01:13,329 --> 00:01:29,614
According to other papers they cite, harder to value firms are more volatile, have a smaller market cap, are newer companies, currently less profitable, and considered high growth companies as exhibited by higher price to earnings ratios.

8
00:01:29,614 --> 00:01:42,969
Remember that I said when you see and, think conditional factor. If you wanted to implement these specific ideas, you would do so via the mechanics of a conditional factor.

9
00:01:42,969 --> 00:01:58,189
We don't do that in this exercise however. The authors provide some context for why harder to value firms may be more likely to exhibit overbuying at market open or weekly momentum of closed open returns.

10
00:01:58,189 --> 00:02:07,614
The hypothesis is that individual investors may rely more on sentiment when the fundamentals of the company are more difficult to measure.

11
00:02:07,614 --> 00:02:21,004
This sentiment is expressed in the recent closed open returns of the stock. We can also scan the paper for useful methodology, which we can apply more broadly to any factors that we're evaluating.

12
00:02:21,003 --> 00:02:39,444
For instance, in section two, the authors define the weekly overnight return on the stock. If you haven't read this section yet, you may imagine taking a window of five days daily returns, adding them up to get the weekly cumulative close to open return.

13
00:02:39,444 --> 00:02:49,409
The authors do something slightly different. They take the average of the daily returns over five trading days, then they multiply by five.

14
00:02:49,409 --> 00:03:03,520
Can you think of how this is different or why they may do this? Notice that if we had missing data, then simply adding up the daily closed open returns over a five-day window, may not make each week comparable.

15
00:03:03,520 --> 00:03:10,405
If a certain week is missing data for one day, then we would only have four days worth of returns to add up.

16
00:03:10,405 --> 00:03:21,319
By taking the daily average and then multiplying by five, we can better deal with missing data and help make each week's cumulative return more comparable to other weeks.


@@@
1
00:00:00,000 --> 00:00:11,224
In section three titled Weekly Overnight Returns, the authors check if the weekly close-to-open returns persist one, two, three, or four weeks into the future.

2
00:00:11,224 --> 00:00:22,739
The authors make use of quantiles to analyze the data. They sort the weekly overnight returns and then divide these into deciles or 10 quantiles.

3
00:00:22,739 --> 00:00:39,159
If we jump to table two, titled Short-Run Persistence of Weekly Overnight Returns and Subsequent Weeks' Total Return, we can see that decile one contains the lowest overnight returns, while decile 10 contains the highest overnight returns.

4
00:00:39,159 --> 00:00:47,935
In the other columns, we can see that decile one has low overnight returns one, two, three, and four weeks later.

5
00:00:47,935 --> 00:01:02,719
Similarly, decile 10 has higher positive returns in its subsequent weeks. This is an indication that weekly overnight returns may persist for up to four weeks into the future.

6
00:01:02,719 --> 00:01:17,040
We now have some ideas for potential factors and how to implement them. First, we'll want to calculate overnight returns, which are the returns from the closed price of the previous evening to the open price of the following morning.

7
00:01:17,040 --> 00:01:36,274
Next, we'll aggregate overnight returns by week to get weekly overnight returns. Based on the analysis of this paper, the hypothesis we'll use for this factor is that higher weekly returns for a stock indicate higher subsequent returns in the near future.

8
00:01:36,275 --> 00:01:46,320
In other words, our strategy will be to overweight stocks that have high weekly overnight returns and to underweight stocks that have low weekly returns.


@@@
1
00:00:00,000 --> 00:00:10,059
Here's a question for you. If two stocks begin and end at the same point over a period of time, does it matter how they got there?

2
00:00:10,060 --> 00:00:22,800
If you ever heard the fable about the tortoise and the hare, then you might remember the saying, "slow and steady wins the race." Maybe that idea is relevant in trading.

3
00:00:22,800 --> 00:00:31,375
For example, let's pretend the return of two stocks. Let's call them stock tortoise and stock rabbit.

4
00:00:31,375 --> 00:00:45,695
Over the same year are both plus 20 percent. If we created a simple momentum factor based on the one year return, this would give us the same signal for both stock tortoise and stock rabbit.

5
00:00:45,695 --> 00:00:58,000
What if we were to look at the trajectory of both stock tortoise and stock rabbit and notice that the path they each took to arrive at the plus 20 percent was rather different?

6
00:00:58,000 --> 00:01:08,269
For example, let's say stock tortoise has a linear trajectory, where it incrementally increases throughout the year at a steady pace.

7
00:01:08,269 --> 00:01:21,670
Let's say that stock rabbit increases at a higher rate, say plus 40 percent earlier in that year, but then drops by 50 percent later in the year.

8
00:01:21,670 --> 00:01:36,480
Even though the tortoise and the rabbit reached the same point at the end of one year, both plus 20 percent, which one might you think will do better in the next few days, weeks, or months?


@@@
1
00:00:00,000 --> 00:00:14,535
The paper titled, The Formation Process of Winners and Losers in Momentum Investing, discusses how the trajectory of a stock is an indicator for whether its momentum is accelerated or decelerated.

2
00:00:14,535 --> 00:00:30,185
When a stock is recently showing higher returns, this paper refers to these as accelerated gains. When a stock is recently showing minimal positive returns, the paper refers to these as decelerated gains.

3
00:00:30,184 --> 00:00:47,635
If you look at a stock price trajectory of an accelerated gain, the shape will be convex. In case you can't remember what a convex curve looks like, the plot of y equals x squared is an example of a convex function.

4
00:00:47,634 --> 00:01:00,775
Essentially, the trajectory is showing an accelerated increase. On the other hand, a stock price trajectory of a decelerated gain will have a concave shape.

5
00:01:00,774 --> 00:01:09,224
As a reminder of what concave means, a plot of y equals the square root of x is an example of a concave function.

6
00:01:09,224 --> 00:01:20,375
The trajectory is showing a decelerated increase. We can also describe accelerated losses and decelerated losses as concave or convex.

7
00:01:20,375 --> 00:01:29,155
An accelerated loss has a concave shape. You can imagine the plot of y equals negative x squared as a concave function.

8
00:01:29,155 --> 00:01:41,450
A decelerated loss has a convex shape. You can imagine the plot of y equals negative square root of x as a convex shape.

9
00:01:41,450 --> 00:01:52,385
So far, we've described four general shapes. What we want to point out is which type of trajectory may be preferred for going long or going short.

10
00:01:52,385 --> 00:02:02,464
We'd say that an accelerated gain may have higher future returns compared to a decelerated gain, if the starting and end points are the same.

11
00:02:02,465 --> 00:02:17,329
So, we would prefer to go long the accelerated gain, a bit more than the decelerated gain. We would also say that an accelerated loss is predictive of more negative returns compared to a decelerated loss.

12
00:02:17,330 --> 00:02:24,079
So, all else being equal, we would prefer to short the accelerated loss more than the decelerated loss.

13
00:02:24,080 --> 00:02:34,430
An important point that might not be obvious to see in the paper is that we can view the convexity or concavity of a curve in a relative terms.

14
00:02:34,430 --> 00:02:50,194
In other words, compared to a convex curve, a straight line is relatively more concave. Likewise, compared to a concave curve, a straight line is relatively more convex.

15
00:02:50,194 --> 00:03:01,700
You may recall with the example of the tortoise stock and the rabbit stock. The tortoise stock trajectory look like a straight line, whereas the rabbits stock was concave.

16
00:03:01,699 --> 00:03:15,139
So, relative to the rabbit stocks trajectory, the tortoise stocks trajectory was more convex and also more predictive of higher future returns under this hypothesis.


@@@
1
00:00:00,000 --> 00:00:11,019
Now that we have a sense for how accelerated or decelerated gains and losses look like visually, how do we represent their relative convexity or concavity in numbers?

2
00:00:11,019 --> 00:00:22,429
One way to approximate a curve, is with a formula that looks like y equals x squared. Or more accurately, y equals ax plus bx squared.

3
00:00:22,429 --> 00:00:33,129
The authors use t as the independent variable name representing time, such as the number of days from the start of the stocks trajectory.

4
00:00:33,130 --> 00:00:46,955
They also use beta as the coefficient for t, and gamma as the coefficient for t squared. To make the discussion easier to follow, let's just give descriptive names to the coefficients.

5
00:00:46,954 --> 00:00:54,770
The coefficient for t will be called gain, and the coefficient for t squared will be called accelerate.

6
00:00:54,770 --> 00:01:02,920
You'll see why we chose these names in a minute. Let's see what a positive or negative direction coefficient might look like.

7
00:01:02,920 --> 00:01:14,269
If we just ignore the t squared term, a positive gain coefficient, will make a line that slopes upward which could represent a stock price that is gaining.

8
00:01:14,269 --> 00:01:23,995
Conversely, a negative gain coefficient, makes a line that slopes downward which could represent a stock price that is losing.

9
00:01:23,995 --> 00:01:45,289
Next, we can look at the accelerate coefficient and how it affects the shape of the curve. If the gain coefficient is positive, and the accelerate coefficient is also positive, both the t term and the t squared term are pointing up and to the right, so the curve is convex and looks like an accelerated gain.

10
00:01:45,290 --> 00:01:59,500
If the gain coefficient is still positive but the accelerate coefficient is negative, the t term is being counteracted by the t squared term, so the curve looks like a decelerated gain.

11
00:01:59,500 --> 00:02:15,250
Let's look at two more cases when the gain coefficient is negative. If the accelerate coefficient is also negative, then both terms are pushing the trajectory downward, so the curve looks like an accelerated loss.

12
00:02:15,250 --> 00:02:28,040
Lastly, if the gain coefficient is negative but the accelerate coefficient is positive, the two terms counteract each other and the curve looks like a decelerated loss.

13
00:02:28,039 --> 00:02:41,270
Okay, now for the fun part. Based on whether the gain coefficient and the accelerated coefficient are positive or negative, can you decide which scenarios you would rather go long, or short?


@@@
1
00:00:00,000 --> 00:00:12,320
Going back to the question I asked earlier, based on whether the gain coefficient and the accelerate coefficient are positive or negative, can you decide which scenarios we would rather go long or short?

2
00:00:12,320 --> 00:00:21,735
We'll take a look at how the coefficients of the stock price approximation formula can inform our decision as to which stock we should go long or short.

3
00:00:21,734 --> 00:00:31,079
This in turn allows us to create an alpha factor based on these two coefficients. Let's take an example of two stocks, A and B.

4
00:00:31,079 --> 00:00:45,230
Stock A has a gain coefficient of 10 and an accelerate coefficient of two. Stock B also has a gain coefficient of 10 and accelerate coefficient of negative two.

5
00:00:45,229 --> 00:01:04,045
If you were going long both stocks, which one would you put more weight on? Since stock A has the trajectory of an accelerated gain, whereas stock B looks like a decelerated gain, we would put a larger long wait on stock A, the accelerated gain.

6
00:01:04,045 --> 00:01:15,679
Let's look at another example with two downward trending stocks. Let's say stock C has a gain coefficient of negative 10 and accelerate coefficient of two.

7
00:01:15,680 --> 00:01:23,844
Stock D has a gain coefficient of negative 10, as well as an accelerate coefficient of negative two.

8
00:01:23,844 --> 00:01:39,320
If you were to go short both stocks, which one would you put a larger short position on? In this case, stock C has the trajectory of a decelerated loss, and stock D has the look of an accelerated loss.

9
00:01:39,319 --> 00:01:51,724
So, you'd prefer to put a larger shortwave on stock D, the accelerated loss. Now, you may have noticed something interesting with the signs of the gain and the accelerate coefficients.

10
00:01:51,724 --> 00:02:00,859
When the product of gain times accelerate is positive and large, the signal generally means to take a larger long position.

11
00:02:00,859 --> 00:02:21,265
When both are negative, the signal means to take a larger short position. If we were to convert the gain and accelerate coefficients into ranks then multiply them together, the product of the gain times accelerate would represent an alpha factor.

12
00:02:21,264 --> 00:02:31,835
When the rank of gain and the rank of accelerate are both small, the product is small, and this is a signal to take a larger short position.

13
00:02:31,835 --> 00:02:42,270
If the rank of gain and the rank of accelerate are both large, then the product is also large, and this is a signal to take a larger long wait.

14
00:02:42,270 --> 00:02:56,375
This is our first conditional factor. The key idea is that when you see and think conditional. Here, we have momentum and convexity.

15
00:02:56,375 --> 00:03:07,515
So, to summarize, we can use a multiple regression where the independent variables are time and time squared, and the dependent variable is the stock price.

16
00:03:07,514 --> 00:03:23,199
This regression gives us estimates for the coefficients gain and accelerate. We create our factor by first converting gain and accelerate into ranks, and then multiplying the ranks together to create a joint factor.


@@@
1
00:00:00,000 --> 00:00:08,300
The next alpha factor that I'd like to share with you is based on the paper titled, Expected Skewness and Momentum.

2
00:00:08,300 --> 00:00:20,160
Before we get into the formal definition of skewness, I'd like to give you a hypothetical example that hopefully will guide your understanding as we discuss the conditional skewness and momentum vector.

3
00:00:20,160 --> 00:00:35,130
Why conditional? What word do you see in the title? "And", as a practitioner, it's often helpful to think of the alpha factors in terms of market mechanics and or behavioral psychology.

4
00:00:35,130 --> 00:00:42,115
If you look at which stocks are in the news, some stocks get more of press coverage and social media posts than others.

5
00:00:42,115 --> 00:00:50,225
Let's hypothesize that stocks that tend to get higher than average media attention tend to also become overpriced.

6
00:00:50,225 --> 00:01:04,064
What might cause that mispricing? When individual investors are flooded with news about a high flying company, there's a natural fear of missing out or "Fomo" as the kids are calling it these days.

7
00:01:04,064 --> 00:01:25,254
This sets up investors for a situation referred to as attentional bias. Attentional bias is a phenomenon where people focus on some aspect at the expense of focusing on the big picture and are more likely to notice evidence that supports their prior existing beliefs.

8
00:01:25,254 --> 00:01:39,099
So, when a stock that is getting lots of news coverage has a high one day return in the past month, this may trigger individual investors to see this as a sign that the stock will keep going up.

9
00:01:39,099 --> 00:01:46,164
Their fear of missing out may kick in, so they jump in and buy as well pushing the stock further up.

10
00:01:46,165 --> 00:02:04,269
Since this increase may not be based on fundamentals, the market as a whole both institutional investors and individual investors may realize that the stock is over bought and sell, thus starting to push the stock price back down to earth.

11
00:02:04,269 --> 00:02:20,629
If this is indeed true, then the maximum one day return in a historical window can be used as a reversal factor which can also be combined with momentum to decide that the stock maybe over bought or oversold.

12
00:02:20,629 --> 00:02:30,474
The maximum one day return over a month is the way that we can measure skewness of returns. We'll explain skew in the next section.

13
00:02:30,474 --> 00:02:43,775
Before we go to the next section though, let's pause and talk generally about alpha factors again. Many of you might be skeptical of this approach as many newcomers to call financer.

14
00:02:43,775 --> 00:02:55,655
Surely in the context of this specific factor, you can think of a few occasions in recent memory where the idea behind this factor was wrong, maybe spectacularly wrong.

15
00:02:55,655 --> 00:03:17,444
Meaning you can surely think of ideas where a stock went up a lot and then continued going up. Alpha factors though are trying to capture a mispricing, often imperceptible to humans across one, many stocks two, on a relative basis, and three, persistent across time.

16
00:03:17,444 --> 00:03:38,849
We are not trying to get very high conviction in any one specific stock. As we discussed in the section about the fundamental law of active management, we can expect that our skill on any one stock will be low, even imperceptibly difficult to distinguish from noise by a human.

17
00:03:38,849 --> 00:03:54,270
Skill just has to be marginally better than 50-50 at the single trait level, and then, we can apply it across many stocks to get a complete alpha factor that hopefully exhibits good Sharpe ratio.


@@@
1
00:00:00,000 --> 00:00:09,759
I hinted previously, that we could use skewness as a signal that stocks may be over bought in the short-term and may revert back soon afterwards.

2
00:00:09,759 --> 00:00:23,710
To help you understand skewness, I'll first introduce a visual notion of skew. Then the common academic definition of skew, followed by the proxy for skewness that we will use when creating this Alpha factor.

3
00:00:23,710 --> 00:00:33,619
Skewness refers to the asymmetry in a distribution. When a distribution is negatively skewed, it has a longer tail on the left.

4
00:00:33,619 --> 00:00:44,545
When the distribution is positively skewed, it has a longer tail on the right. Skewness also means that the mean of a distribution is different than its medium.

5
00:00:44,545 --> 00:00:55,280
Negative skew, indicates that the mean is to the left of the median. You can imagine the small portion of extreme values on the left, pulling the mean to the left.

6
00:00:55,280 --> 00:01:04,924
While the median stays closer to the majority of observations. Similarly, a positive skew indicates that the mean is to the right of the median.

7
00:01:04,924 --> 00:01:14,420
You can imagine that the extreme values to the right, pull the mean to the right as well while the median stays closer to the majority of observations.

8
00:01:14,420 --> 00:01:22,129
If you learned about skewness in school, you may recall the concept of the first, second, third, and fourth moments of a distribution.

9
00:01:22,129 --> 00:01:29,194
The first moment is commonly known as the mean or average. The second moment is known as the variance.

10
00:01:29,194 --> 00:01:39,225
The third moment is skewness which we are discussing here. The fourth moment is kurtosis which measures the tails of the distribution.

11
00:01:39,224 --> 00:01:50,125
Stock returns generally exhibit excess kurtosis or fat tails compared to the normal distribution. For now though, we'll focus on skewness.

12
00:01:50,125 --> 00:02:01,710
So, those were the visual and traditional interpretations of skewness. Now, let's circle back to the definition of skewness used in this paper, and what we will implement in the Alpha factor.

13
00:02:01,709 --> 00:02:16,995
A useful proxy for skewness is the maximum daily return of a stock over the past 20 trading days. This definition is useful in part, because of how it captures how individual investors may perceive skewness.

14
00:02:16,995 --> 00:02:27,869
It's possible to imagine that an investor may look at a recent stock high of a company as an indicator, upon which they will base their decision to buy or sell the stock.


@@@
1
00:00:00,000 --> 00:00:08,384
Let's walk through four hypothetical examples and assume that skewness is a reversal factor, which is another word for mean reversion.

2
00:00:08,384 --> 00:00:16,140
We'll be more specific and say that the maximum daily return over the trailing month is our measure of skew.

3
00:00:16,140 --> 00:00:29,265
Let's also assume that the return over the trailing year is a momentum vector. Notice that it's more precise to call our definition of skew as a measure of positive skew.

4
00:00:29,265 --> 00:00:42,284
We aren't defining negative skew with a minimum over the trailing month. Even though this is also a valid approach, for simplicity, we'll just be measuring positive skew.

5
00:00:42,284 --> 00:00:55,234
Remember, the Alpha vector is a relative ranking exercise. In our first scenario, let's say momentum is positive and skew is positive.

6
00:00:55,234 --> 00:01:08,564
This is the scenario that we saw in the introduction to this factor. We can think of the positive skew as having a dampening effect on the positive momentum of the stock.

7
00:01:08,564 --> 00:01:20,624
The paper calls this weakened momentum. For our second scenario, let's say momentum is still positive, but the skew is less positive.

8
00:01:20,623 --> 00:01:30,789
The less positive skew may indicate that future returns maybe stronger than if there were a positive skew.

9
00:01:30,790 --> 00:01:40,459
The paper calls this enhanced momentum. For our third scenario, we'll look at negative momentum and positive skew.

10
00:01:40,459 --> 00:01:54,235
The paper describes this scenario by suggesting a phenomenon where investors are overly optimistic about a downward trending stock when the stock bounces backup temporarily.

11
00:01:54,234 --> 00:02:06,060
The hypothesis continues to suggest that when the stock rebounds positively, individual investors may see this as a sign of recovery so they may buy as well.

12
00:02:06,060 --> 00:02:16,879
To conclude this scenario, as fundamental still put downward pressure on the stock, the momentum then reasserts itself and the stock continues to slide downward.

13
00:02:16,879 --> 00:02:29,275
This scenario is also referred to as enhanced momentum and is the primary scenario for which the researchers found more significant effect of skewness on momentum.

14
00:02:29,275 --> 00:02:42,800
For our fourth scenario, we'll look at negative momentum and less positive skew. The less positive skew has a dampening effect on the negative momentum, so it can be referred to as weakened momentum.

15
00:02:42,800 --> 00:02:51,320
Now that you've seen how momentum and skew interact, can you think of how these can be combined into a conditional factor?


@@@
1
00:00:00,000 --> 00:00:13,695
Let's see how we can create a conditional factor based on momentum and skew. We first might want to convert both momentum and skew to ranks, since ranks are generally more stable and more robust to outliers.

2
00:00:13,695 --> 00:00:22,155
Notice though, that skew is a reversal or mean-reversion signal. So, we want to rank the skew in reverse order.

3
00:00:22,155 --> 00:00:31,670
Think about this in the context of our definition of enhanced momentum, and work out for yourself a few examples of why this works.

4
00:00:31,670 --> 00:00:47,009
That is, if our stock universe had 100 stocks, then the stock with the smallest skew would have the rank of 100, whereas the stock with the largest skew would have a rank of one.

5
00:00:47,009 --> 00:00:59,049
The momentum factor would be ranked as usual, where the most negative return is given a rank of one, and the most positive return has a rank of 100.

6
00:00:59,049 --> 00:01:18,199
If we multiply the rank of momentum times the reverse rank of skew, we will have the joint factor. The smallest alpha values would translate into the largest short positions, and the largest alpha values would translate into the largest long positions.


@@@
1
00:00:00,000 --> 00:00:21,390
The last alpha factor that we'll construct is based on the paper titled "Arbitrage asymmetry and the idiosyncratic volatility puzzle." This paper uses a lot of the concepts that we've learned, including a unique example of using some of the output of a risk model as part of alpha factor construction.

2
00:00:21,390 --> 00:00:30,568
Personally, I enjoy this example because we combined so much of what we have learned in addition to some more ideas from behavioral finance.

3
00:00:30,568 --> 00:00:38,385
Based on this paper, we'll construct a conditional factor, a fundamental factor enhanced by a volatility factor.

4
00:00:38,384 --> 00:00:45,344
Let's start with some background information that will help you interpret the meaning of the two factors that we'll combine.

5
00:00:45,344 --> 00:00:56,919
We'll first remind ourselves of how arbitrage supports efficient markets. Next, we'll look at volatility and how volatility may limit arbitrage activities.

6
00:00:56,920 --> 00:01:06,170
Then we'll look at the idiosyncratic portion of volatility and why this might be more useful as a factor than the total volatility.

7
00:01:06,170 --> 00:01:20,259
Then we'll refresh the meaning of a value factor, which is a type of fundamental factor. Lastly, we'll combine the value factor and idiosyncratic volatility into a joined factor.

8
00:01:20,260 --> 00:01:28,480
We'll discuss generalizing what we've learned so that you can use your creativity and create related factors of your own.


@@@
1
00:00:00,000 --> 00:00:14,460
You may recall that overcharge is a process that seeks profits due to mis-pricing of assets. In other words, overcharge looks for inefficiencies in the market in order to profit from these inefficiencies.

2
00:00:14,460 --> 00:00:28,839
Moreover, the act of overcharge actually reduces the mis-pricing of assets, and the individual acts of overcharge by market participants as a whole add up to support a more efficient market.

3
00:00:28,839 --> 00:00:35,284
When we think of profit seeking activities in their role in maintaining efficient pricing in markets.

4
00:00:35,284 --> 00:00:43,519
It can also include buying stocks that are considered under-priced or shorting stocks that are considered over-priced.

5
00:00:43,520 --> 00:00:52,155
In other words, overcharge here is that the instantaneous buying and selling of perfect substitute assets.

6
00:00:52,155 --> 00:01:09,965
Here, we simply mean buying assets with an expected return greater than that warranted by the risk taken, and similarly shorting assets with an expected negative return that is less than that warranted by the risk taken.

7
00:01:09,965 --> 00:01:25,529
Even though only taking a long or only taking a short position is not technically overcharge, when market participants seek to profit from mis-priced assets, their actions help to reduce that mis-pricing and make markets more efficient.


@@@
1
00:00:00,000 --> 00:00:13,959
Not all arbitrage opportunities are created equal. If you found two similar stocks with similar mispricing, all else being equal, you'd prefer to act on the stock that is less risky.

2
00:00:13,960 --> 00:00:25,800
For example, let's imagine we have identified through a model in which we have high confidence two stocks, A and B, which are each undervalued by 10 percent.

3
00:00:25,800 --> 00:00:46,550
However, stock B typically fluctuates two times as much as stock A. If we're limited to our capital deployment, which is how much cash we can invest, and we can only choose one stock, we would likely choose to buy stock A since our expected Sharpe ratio will be much higher.

4
00:00:46,549 --> 00:00:55,340
Remember, in neither case are we guaranteed to make money, we just have a model in which we have high confidence.

5
00:00:55,340 --> 00:01:13,584
Now take this example and think of the universal effect. All savvy arbitragers like us, would prefer stock A, and the opportunity in stock A might get arbitraged away before we even have a chance to buy the stock.

6
00:01:13,584 --> 00:01:26,800
If that's true, then in fact, we may be better off trying to capture the discount in stock B as there will be less competition to capture the opportunity presented there.

7
00:01:26,799 --> 00:01:37,475
That's the gist of this paper. The risk that acting on an arbitraged opportunity may actually yield a loss is called arbitrage risk.

8
00:01:37,474 --> 00:01:47,599
As you may have guessed from the example, one source of arbitrage risk is the volatility of the stock, defined as the standard deviation of returns.


@@@
1
00:00:00,000 --> 00:00:10,589
Remember from our prior discussions, especially the one about risk models, the returns can be broken up into a systematic and idiosyncratic component.

2
00:00:10,589 --> 00:00:25,150
Likewise, the volatility of returns can be broken up into systematic and idiosyncratic components. Idiosyncratic risk maybe a more helpful indicator of arbitrage risk. Why is this?

3
00:00:25,149 --> 00:00:36,905
Because market participants who pursue strategies to capture relative mispricings will seek to eliminate common factor risks leaving them bearing idiosyncratic risks only.

4
00:00:36,905 --> 00:00:45,229
As such, when we think about the risks to arbitrage, we likely should consider only the idiosyncratic risks.

5
00:00:45,229 --> 00:00:52,585
Recall that the systematic component of a stock's risk can be attributed to movements in major risk factors.

6
00:00:52,585 --> 00:01:01,060
For example, using the CAPM model, a part of a stock's movement can be attributed to the movement of the market as a whole.

7
00:01:01,060 --> 00:01:10,090
The rest of the stock's return that cannot be attributed to major risk factors is the idiosyncratic component of its return.

8
00:01:10,090 --> 00:01:17,210
One way to isolate the idiosyncratic return, is by fitting a multiple regression using the risk factors.

9
00:01:17,209 --> 00:01:26,990
In this research paper, the authors use the Fama-French model. The model gives a prediction of the return due to the model factors.

10
00:01:26,989 --> 00:01:35,870
Since the model factors do not fully explain the stock's returns, there will be a difference between the actual and modeled returns.

11
00:01:35,870 --> 00:01:48,659
This difference is called the residual. If we take the standard deviation of the residual return, we get the idiosyncratic volatility which is also called the idiosyncratic risk.

12
00:01:48,659 --> 00:02:00,150
In short, the idiosyncratic volatility or iVol is the volatility that is specific to the stock, and not explained by the risk factors.

13
00:02:00,150 --> 00:02:16,400
This is useful for us because when we want to measure a stock's arbitrage risk, what we're interested in is the specific or idiosyncratic risk of that stock, and not the systematic risk that exists broadly among all stocks.


@@@
1
00:00:00,000 --> 00:00:10,109
The concept of value in factor modeling is one of the two most widely known concepts, the other being the concept of momentum.

2
00:00:10,109 --> 00:00:26,754
The significance of value can be traced to the work of Ben Graham and David Dodd, who wrote about value investing, which seeks to estimate accompanies intrinsic value than by companies that are priced below that intrinsic value.

3
00:00:26,754 --> 00:00:41,200
Some well known followers of Graham in value investing are Warren Buffett and Charlie Munger. These types of investors are referred to as fundamental investors are also discretionary investors.

4
00:00:41,200 --> 00:00:51,810
Given the apparent success of value investing by discretionary investors, quants have worked to replicate the idea by creating quantitative measures of value.

5
00:00:51,810 --> 00:01:00,789
Since I mentioned quants being inspired by discretionary investors, I want to tell you about an idea I think is very misunderstood.

6
00:01:00,789 --> 00:01:29,334
This is something I am very passionate about, most quants, those who are new to quant finance and even famous and very experienced quants often think and proclaim loudly and even impolitely that fundamental investors trade by gut instinct and have no process, whereas quants are the standard bearers of the scientific method and they are the only ones with a repeatable process.

7
00:01:29,334 --> 00:01:44,664
This is absolutely wrong, I have worked with some of the best fundamental investors in the world in addition to some of the best quants and both these groups, have had irrational well-articulated, repeatable process.

8
00:01:44,665 --> 00:01:58,159
The key difference though between the two are: one, the kind of inputs they use; two, how they process that information; and three, the shape of the portfolios that emerge.

9
00:01:58,159 --> 00:02:12,274
Historically, fundamental investors have relied on detective work together inputs. This work includes reading corporate filings, talking to research analysts, and reading analyst reports.

10
00:02:12,275 --> 00:02:20,224
Let's think the company conference calls, talking to executives at the companies, and even visiting companies on-site.

11
00:02:20,224 --> 00:02:27,835
Fundamental investors then synthesize all this information into a variant view, which is a non-consensus view.

12
00:02:27,835 --> 00:02:38,809
The shape of portfolios differ between fundamental investors and quant investors. When I refer to shape, I mean the number of positions and also the concentration of those position.

13
00:02:38,810 --> 00:02:48,585
Think of the shape function in the Python library pandas. Great fundamental investors have high IC, the measure of skill.

14
00:02:48,585 --> 00:03:01,340
But, because of the style and depth of their research process, they can only achieve low breadth because it's only humanly possible to research so many companies with that much depth.

15
00:03:01,340 --> 00:03:12,134
But, make no mistake, these investors use repeatable inputs, follow a repeatable process and produce high IC bets.

16
00:03:12,134 --> 00:03:24,129
Quants on the other hand, have historically used structured data like price and volume data, financial statement data, quantified a research analyst ratings, and the like.

17
00:03:24,129 --> 00:03:32,880
As discussed, quants typically have lower IC, but more than make up for that with much higher breadth.


@@@
1
00:00:00,000 --> 00:00:09,155
One of the most exciting developments in investing in the last couple of years is the convergence of the quants and fundamental processes.

2
00:00:09,154 --> 00:00:16,425
This is called quantamental investing in the popular press, as a combination of the words quant and fundamental.

3
00:00:16,425 --> 00:00:28,635
But don't let that goofy name fool you. It's more than just a fad or a marketing term. This quantamental convergence of fundamental and quant is significant and growing.

4
00:00:28,635 --> 00:00:44,714
The idea is that the advent of AI methods, and the exponential growth of data generated by the real economy are now allowing quants to use inputs that have historically been exclusively in the domain of fundamental investors.

5
00:00:44,715 --> 00:01:02,150
On the other hand, the growing acceptance of the value of data science, and the observable successive quants, has led fundamental investors to embrace data-driven research methods and some aspects of the quant workflow, like the use of risk models.

6
00:01:02,149 --> 00:01:21,210
This is an exciting time. Let me give you a concrete example of this convergence. A part of the investment process that has been historically solely in the domain of fundamental researchers is text analysis, listening to conference calls, reading corporate filings, news, et cetera.

7
00:01:21,209 --> 00:01:28,395
There are now effective computational methods to get intelligence from this text information in a systematic way.

8
00:01:28,394 --> 00:01:36,409
Note that in term two, we will cover traditional natural language processing techniques, as well as deep learning for NLP.


@@@
1
00:00:00,000 --> 00:00:14,154
Fundamentals such as price to earnings ratio and price to book ratios have often been used in alpha factors that represent the ratio of market price to intrinsic value.

2
00:00:14,154 --> 00:00:21,885
We could use these fundamental factors by themselves to indicate whether a stock is underpriced or overpriced.

3
00:00:21,885 --> 00:00:32,115
One thing to be aware of though when dealing with fundamentals is that the data change much less frequently than price.

4
00:00:32,115 --> 00:00:39,910
Fundamental values come from financial statements which, in the United States, are only updated quarterly.

5
00:00:39,909 --> 00:00:51,200
This means that the turnover for fundamental Alphas will be likely much lower than it is for price data-driven Alphas.

6
00:00:51,200 --> 00:01:00,789
Recall that this can be a positive because lower turnover means less trading cost but it also could be a negative.

7
00:01:00,789 --> 00:01:16,655
We won't be as responsive as the data does not arrive frequently. As a general rule, fundamental Alphas have high capacity but have lower Sharpe ratio than compared to their price data-driven cousins.

8
00:01:16,655 --> 00:01:29,075
Some firms embrace this as it allows them to deploy much larger assets for clients. Other firms though can be wary of using fundamentally driven Alphas.

9
00:01:29,075 --> 00:01:46,969
One middle ground is to use fundamental data as a conditioning factor along with price-driven data or to use a fundamental factor with low weight in a combined Alpha vector that includes more responsive Alphas.

10
00:01:46,969 --> 00:01:57,115
In this case, since higher idiosyncratic volatility can indicate instances of higher mispricing, we can combine iVol with a fundamental factor.

11
00:01:57,114 --> 00:02:06,140
We can think of iVol as a conditioning factor since by itself it doesn't indicate whether to go in the long or short direction.

12
00:02:06,140 --> 00:02:14,390
This is an important idea, the conditional information doesn't need to be an alpha factor on its own.

13
00:02:14,389 --> 00:02:26,199
The conditioning iVol factor can potentially be used to enhance the signal of any other factor. Perhaps you can try this out in your own work.


@@@
1
00:00:00,000 --> 00:00:14,900
Now that we've walked through risk factor models and Alpha factors, I hope you'll start to see how you can try out different variations that deviate from the specific methods you see in academic papers.

2
00:00:14,900 --> 00:00:29,875
For instance, instead of using the Fama French model to extract idiosyncratic volatility, you may try other risk factor models that you learned about in this module, such as a risk factor model based on principle component analysis.

3
00:00:29,875 --> 00:00:38,540
Moreover, you can try pairing iVol with other fundamental factors or really any other Alpha factors as well.

4
00:00:38,539 --> 00:00:54,399
If you can develop a good habit of reading academic research, proposing and implementing then evaluating potential Alpha vectors, these experiences will serve you well in preparing for roles in quantitative finance.


@@@
1
00:00:00,170 --> 00:00:15,610
You did it. This lesson was pretty advanced, but also very meaningful in that we practice reading academic research, thinking about Alpha ideas based on the papers, and then coding up these factors.

2
00:00:15,609 --> 00:00:25,335
I want to emphasize that the value of this lesson isn't that I'm giving you four Alpha factors that will help you become an instant millionaire.

3
00:00:25,335 --> 00:00:35,780
The value that I hope you'll derive from this lesson is an understanding of the process and thinking that quants go through to generate Alphas every day on the job.

4
00:00:35,780 --> 00:00:44,719
Remember, we're teaching you how to fish for Alpha factors instead of just giving you the Alpha factors.

5
00:00:44,719 --> 00:01:04,560
As the saying goes, "Give your students an Alpha factor and they'll have a good Alpha factor for a day or at least until their Alpha factor turns into a common risk factor." Teach your students how to fish for their own Alpha factors and they'll hopefully have good Alpha factors for a lifetime.


@@@
1
00:00:00,000 --> 00:00:09,445
You have come so far. Just think of all the amazing things we've told you in the past few lessons, I bet your head is spinning in a good way.

2
00:00:09,445 --> 00:00:17,880
We've been talking about factors, how to think about them, how to find them, how to code them up, how to make them robust, and how to evaluate them.

3
00:00:17,879 --> 00:00:26,070
We've told you about risk factors and about the fun ones, alpha factors. But in this lesson, the rubber really hits the road.

4
00:00:26,070 --> 00:00:37,419
Now we want to find the optimal weights for our portfolio. So, we're going to plug our Alpha and risk factors into the formalism we discussed earlier in the course for portfolio optimization.

5
00:00:37,420 --> 00:00:44,599
This lesson is going to be critical for finishing the project. I can't wait to tell you about it, so, let's get started.


@@@
1
00:00:00,000 --> 00:00:18,869
Our goal is to set up a portfolio optimization problem using our alpha factors and risk model. In practice, it's possible that we are doing this in order to design a portfolio from scratch, but it's also possible that we are trying to guide the evolution of an existing portfolio.

2
00:00:18,870 --> 00:00:35,384
So, there may already be a portfolio in production with capital invested in a universe of assets and portfolio weights on the assets from a previous optimization, which may have evolved as the asset values appreciated or depreciated.

3
00:00:35,384 --> 00:00:53,844
So, how do we set up the optimization with our new or updated alpha factors and updated data? Thinking back to what we learned before about portfolio optimization, we know we want to do something like maximize return and limit risk as measured by variance.

4
00:00:53,844 --> 00:01:06,219
This is already great intuition for how to set up the problem. Can you guess where our alpha factors and risk model would go in the problem formulation following this intuition?

5
00:01:06,219 --> 00:01:25,040
Let's make this explicit starting with the alpha factors. Let's say we have just one alpha factor, we know that on a given day, our alpha factor is a vector of values, one value per stock that is hopefully predictive of the future mean return of each stock.

6
00:01:25,040 --> 00:01:32,090
But now we want a quantity in the objective function that represents the predicted portfolio return.

7
00:01:32,090 --> 00:01:46,875
Somehow, we need to sum the alpha values over the portfolio. Can you see what we want here? To calculate the portfolio alpha, we just take the dot product of the alpha with a vector of portfolio weights.


@@@
1
00:00:00,000 --> 00:00:14,150
Okay, now where does the risk model come in? Well, we discussed previously that if we put the quantity representing the mean in the objective, one option is to place a constraint on the portfolio variance.

2
00:00:14,150 --> 00:00:28,454
We already know how to calculate the portfolio covariance matrix using a risk model. All we do to get the portfolio variance is the same thing we did back when we were calculating the portfolio variance before.

3
00:00:28,454 --> 00:00:36,975
We write the portfolio variance as the weight vector transpose times the covariance matrix times the weight vector.

4
00:00:36,975 --> 00:00:46,744
That's great, so let's limit portfolio risk. We'll say it has to be below some value. We're already most of the way there.

5
00:00:46,744 --> 00:00:56,039
You might have one very reasonable question at this point which is, how do we know what value to set as our limit on portfolio variance?

6
00:00:56,039 --> 00:01:04,980
Well, this value represents the tolerable variance of your portfolio returns per time period over which you run the optimization.

7
00:01:04,980 --> 00:01:13,280
As a portfolio manager, this is usually given to you, but where does it come from? Well, it represents a business decision.

8
00:01:13,280 --> 00:01:19,359
Remember that the investment portfolio we're designing represents a product, which is sold to investors.

9
00:01:19,359 --> 00:01:25,594
When investors are shopping around, they typically compare products with different risk return characteristics.

10
00:01:25,594 --> 00:01:35,354
What numbers are reasonable? The general stale volatility can be observed from the market itself, for example, from an index like the S&amp;P 500.

11
00:01:35,355 --> 00:01:45,445
The annualized standard deviation of the S&amp;P 500 over 100 years has been 12 percent or so. So, people benchmark their risk against that.

12
00:01:45,444 --> 00:01:52,719
If you look at indices that measure hedge fund industry performance, you see volatility of around four to five percent.

13
00:01:52,719 --> 00:02:02,035
So, if I'm running a hedge fund, there's business risk associated with setting my product's portfolio risk to something dramatically different from those numbers.

14
00:02:02,034 --> 00:02:09,370
Over time, there's been evolutionary convergence to the levels of risk, that makes sense given the market for these products.

15
00:02:09,370 --> 00:02:18,884
If you work in a really big firm, and you're managing a small portfolio, you will likely receive this number as a mandate, such as you can't lose more than 10 percent.

16
00:02:18,884 --> 00:02:27,359
So, you can calibrate your risk so that you are comfortable with where you are currently in relation to that maximum loss threshold.


@@@
1
00:00:00,000 --> 00:00:09,695
As of right now, we have this as our optimization objective function. But there's one other term that it's a good idea to add to it at this point.

2
00:00:09,695 --> 00:00:17,949
This is what's called a regularization term. Here we use the L2 norm of the portfolio weights as the regularization term.

3
00:00:17,949 --> 00:00:27,149
In other types of problems, you might use a different term. What this do? Well, as you'll recall, the L2 norm of a vector is its length.

4
00:00:27,149 --> 00:00:38,460
So, in this case, the term is basically the length of the vector of portfolio weights. This entire quantity will get very large if the weight on any single asset gets large.

5
00:00:38,460 --> 00:00:54,740
So, the effect is to penalize this behavior and enforce the spreading of weight amongst assets. The parameter here controls the balance between maximizing the portfolio alpha and enforcing the spreading of the weights across assets.

6
00:00:54,740 --> 00:01:05,144
To further clarify this, let's imagine two possible extreme states of the world. In the first, you feel maximally confident in your alpha factors.

7
00:01:05,144 --> 00:01:20,534
You know for sure which stocks are going to go up and down in the future and by how much. In this situation you'd put all your long money in the stocks that will go up the most and all your short money in the stocks that will go down the most.

8
00:01:20,534 --> 00:01:30,084
In the second extreme state, you are absolutely sure that you know which stocks are going to go up and which are going to go down, but you don't know by how much.

9
00:01:30,084 --> 00:01:43,950
You only know the sign of the change. So, in this case, the optimal portfolio is equal weight on all the stocks you take long positions on, and equal weight on all the stocks you take short positions on.

10
00:01:43,950 --> 00:01:53,035
You can think of the regularization parameter as a dial that you can use to tune between these two extreme versions of your prior knowledge of the world.

11
00:01:53,034 --> 00:02:02,820
This is the Bayesian interpretation of this technique. If you set the parameter equal to zero, that's the state where you have full confidence in the alpha vector.

12
00:02:02,819 --> 00:02:12,280
As the parameter approaches infinity, the output weights approach equal weighting. So, you can think of the regularization parameter as your conviction dial.


@@@
1
00:00:00,000 --> 00:00:11,565
But we're not quite done. As you know, we can impose lots of different constraints on the problem. These usually have to do with real-world constraints on the way we want our portfolio to behave.

2
00:00:11,564 --> 00:00:18,044
Sometimes they just make sure our results aren't really wacky. Let's discuss some common constraints.

3
00:00:18,044 --> 00:00:25,154
First off, we need to think about whether our portfolio is long only or whether we are allowed to take short positions.

4
00:00:25,155 --> 00:00:36,609
We'd usually know this from limitations in our trading environment. If we needed to enforce the constraint that our portfolio be long only, we'd require that each of our portfolio weights to be positive.

5
00:00:36,609 --> 00:00:46,630
If we can take long or short positions, we'd place no such constraint. It's very common to require that our portfolio be market neutral.

6
00:00:46,630 --> 00:00:53,380
What do we mean by that? In effect, it means that across the board, we short as much capital as we long.

7
00:00:53,380 --> 00:01:08,115
This means that we lose as much as we gain from movements that affect the entire market. In other words, when a portfolio is designed to be market neutral, the portfolio's returns are hopefully less affected by movement in the entire market.

8
00:01:08,114 --> 00:01:22,795
We mentioned making the portfolio market neutral when we discussed alpha factors. The difference here is that we place this as a constraint on the portfolio weights themselves, so that the constraint will be enforced by the optimization.

9
00:01:22,795 --> 00:01:35,679
Hedge funds usually require that the sum of the weights be zero. However, an alternative is to control the balance of long to short positions by requiring that the sum of the weights remain in some range.


@@@
1
00:00:00,000 --> 00:00:18,410
Another important constraint is the leverage constraints. With this constraint, we limit the leverage ratio which, as you recall, is the sum of the absolute dollar value of all positions, long and short, divided by whatever actual capital we devote to supporting those positions.

2
00:00:18,410 --> 00:00:33,210
In our portfolio optimization problem, we work with portfolio weights which are percentages of total invested capital, so the leverage ratio is just the sum of the absolute value of the weights, long and short.

3
00:00:33,210 --> 00:00:43,465
Again, we discussed this when we discussed alpha factors, but the basic idea here is that when you short stocks, you borrow the stocks and sell them on the open market.

4
00:00:43,465 --> 00:00:54,674
You can use the cash you get to take long positions on other stocks. In general, any investment position is said to be leveraged if it is financed by a debt position.

5
00:00:54,674 --> 00:01:06,609
Whether it's financed by borrowed cash or borrowed stocks. However, it would be much more risky to take very large short positions in order to finance very large long positions.

6
00:01:06,609 --> 00:01:21,974
These are large bets and shorting can be complicated. If the leverage ratio is allowed to be greater than one, it means the total magnitude of all year long and short positions is greater than the amount of initial capital that you have to work with.

7
00:01:21,974 --> 00:01:32,400
This means you're borrowing money either by shorting more stocks or by borrowing cash. You have to borrow from somebody and they'll have a maximum there willing to lend to you.

8
00:01:32,400 --> 00:01:42,200
So, you place a constraint on your leverage ratio which is the sum of the absolute values of all your positions to control the scale of this borrowing.

9
00:01:42,200 --> 00:01:59,759
This will keep you from ending up in a very extreme leverage position. If you're using an objective function like this and you are not using a risk constraint like this, you need a constraint like the leverage constraint to keep your weights from growing to infinity.

10
00:01:59,760 --> 00:02:09,349
You can try to control leverage by just controlling risk, but the leverage constraint is usually a hard constraint that will also be necessary to enforce.


@@@
1
00:00:00,000 --> 00:00:07,370
Another common type of constraint limits the extent to which you are exposed to any individual factor.

2
00:00:07,370 --> 00:00:19,380
This is where you might impose specific limitations on your exposure to common risk model factors. Individual sectors, momentum, value, or company size.

3
00:00:19,379 --> 00:00:34,914
Remember that the factor exposure matrix has dimensions of assets and factors. When you multiply the transposed factor exposure matrix by the weight vector, which has dimensions of assets, you get a vector that has dimensions of factors.

4
00:00:34,914 --> 00:00:47,344
Thus, this product is still a vector. When we impose a constraint like this, where this is a single scalar, it means that the constraint applies to each element of the vector.

5
00:00:47,344 --> 00:00:54,940
That is, we apply the same constraint for every factor. Again, where do we get these constraint numbers from?

6
00:00:54,939 --> 00:01:09,805
They come from business and risk mandates. Finally, it's common to place some ultimate constraints on the individual weights themselves just to ensure that you don't end up in very extreme positions in individual assets.

7
00:01:09,805 --> 00:01:19,879
Again, remember that the weight vector is a vector with dimensions of assets, so placing a constraint like this applies to each asset weight in the vector.

8
00:01:19,879 --> 00:01:27,625
You can think of this constraint as a kind of insurance. In theory, the risk model should help limit portfolio risk.

9
00:01:27,625 --> 00:01:38,974
But in case there's a problem with a risk model, setting limits on how much weight to put on any single stock also helps to protect the portfolio from concentrated risk exposure.

10
00:01:38,974 --> 00:01:50,659
This constraint is often impose so that the hedge fund can clearly communicate to investors that there is an absolute limit on the extent to which the portfolio is invested in any one asset.


@@@
1
00:00:00,000 --> 00:00:06,974
Let's think for a moment why what we've come up with now is superior to the way we were doing optimization before.

2
00:00:06,974 --> 00:00:14,570
Remember how before we had this large covariance matrix of assets that we were using to estimate portfolio variance.

3
00:00:14,570 --> 00:00:21,765
Well, now we have a smaller number of factors. Common commercial risk models have around 70 factors.

4
00:00:21,765 --> 00:00:34,685
So, this matrix is now a 70 by 70 matrix, as opposed to the potentially several thousand by several thousand matrix we would have had if we were using a covariance matrix of the assets.

5
00:00:34,685 --> 00:00:42,634
There are literally just fewer elements in this matrix. This means that we have reduced the number of quantities we are trying to estimate.

6
00:00:42,634 --> 00:00:54,189
How many fewer elements are there? Well, let's count them. Let's say the dimension of our asset covariance matrix is n. Remember that covariance matrices are symmetric.

7
00:00:54,189 --> 00:01:08,194
There are n elements on the diagonal. If we look at the remaining elements, we see we have a matrix of dimension n minus 1 by n. But each element is in this matrix exactly twice.

8
00:01:08,194 --> 00:01:22,165
So, the number of unique elements is n times n minus 1 divided by 2. So, we have n elements along the diagonal and a half of n times n minus 1 number of elements that are not along the diagonal.

9
00:01:22,165 --> 00:01:37,660
We'll rearrange the formula a bit to get n times n plus 1 all divided by 2. So, in total, we have n times n plus 1 divided by 2 quantities to estimate.

10
00:01:37,659 --> 00:01:49,260
Let's say n is 3,000. This is then around 4.5 million quantities. If n is instead 70, then this is more like 2.5 thousand quantities.

11
00:01:49,260 --> 00:01:56,094
There's a big difference in scale there. There are many fewer opportunities to introduce estimation error.

12
00:01:56,094 --> 00:02:04,460
The fact that we are now estimating many fewer parameters is a good thing and an important reason why we use factor models of risk.

13
00:02:04,459 --> 00:02:13,500
Another thing to remember is that each of the elements in this matrix is an estimate of variance or covariance of random variables.

14
00:02:13,500 --> 00:02:56,555
In practice, we estimate variances and covariances using time series of data. If you have n assets, and you want to estimate the covariance matrix of those n assets, then the number of days of return that you need, t, has to be greater than n, and ideally much greater than n. There's more we need to talk about to explain why the number of data points needs to be much greater than the number of variables n. But one problem with insufficient data is that there won't be enough observations to accurately estimate all of the variances and covariances, which means that the covariance matrix, based on the sample, would be significantly different from the population covariance matrix.

15
00:02:56,555 --> 00:03:03,890
Also, PCA, using such a covariance matrix, would not be able to produce meaningful principal components.

16
00:03:03,889 --> 00:03:14,789
If you have 3,000 securities, you need at least 3,000 days of data, which is about 12 years. But we also know that variance and covariance probably change over time.

17
00:03:14,789 --> 00:03:26,814
So, does using 12 years of data to predict the variance for the next month even make sense? Another reason people use the risk factor model formulation of the covariance matrix is that you get around that problem.


@@@
1
00:00:00,000 --> 00:00:09,535
Remember how earlier we talked about the possibility of setting yourself a problem that doesn't have a solution because there is no answer that satisfies all the constraints?

2
00:00:09,535 --> 00:00:20,998
Well, that can happen in practice. When you're working with two or three dimensional problems where you can graph the functions and observe what's going on, it might be easier to see when the problem becomes infeasible.

3
00:00:20,998 --> 00:00:27,429
However, in practice you're working in a high-dimensional space where it's hard to visualize what's going on.

4
00:00:27,429 --> 00:00:38,359
In general learning what makes the problem infeasible comes with experience. So, much of this might sound rather abstract, but we want to have a short discussion here to give you an intuition.

5
00:00:38,359 --> 00:00:51,679
One possible situation where this might arise, is if you fix the weight on a particular stock. For example, sometimes for some reason the compliance group at your company may restrict trading company wide on a particular stock.

6
00:00:51,679 --> 00:01:01,549
So, there may be some stock that you can't trade or there may be a particular stock that's difficult to short, so again, you'd like to constrain the weight on that stock.

7
00:01:01,549 --> 00:01:10,614
However, if you put a hard constraint on the weight on that particular stock, this might cause you to reach an infeasible solution in your optimization problem.

8
00:01:10,614 --> 00:01:19,385
Let's just say first that it's often dangerous to put equality constraints in an optimization problem because the problem can get infeasible very easily.

9
00:01:19,385 --> 00:01:30,295
Remember that inequality constraint is a much stronger condition than an inequality constraint. It's the difference between requiring that they answer lie on a line or below a line, for example.

10
00:01:30,295 --> 00:01:39,719
Infeasibility is an issue with your constraints. One possible solution is to move one or more constraints to penalty terms in the objective function.

11
00:01:39,719 --> 00:01:49,874
This changes the problem from placing a hard limit on some quantity to seeking to minimize that quantity, but balanced against the other terms in the objective.

12
00:01:49,873 --> 00:02:00,230
So, what are general rules of thumb or guidelines for avoiding these situations in practice? In production you're inheriting a live portfolio.

13
00:02:00,230 --> 00:02:13,405
Your job in optimization is to transition that portfolio to a new state using new data, new Alpha factors, and keeping in mind that market movements have caused the old weights to change slightly.

14
00:02:13,405 --> 00:02:23,534
The first thing to check is whether the starting portfolio produces an infeasible problem or not. In general start with fewer terms in the objective function.

15
00:02:23,534 --> 00:02:34,054
If that problem becomes infeasible, try to figure out which constraints are the ones causing the infeasibility, see if you can change or override any of those constraints.


@@@
1
00:00:00,000 --> 00:00:10,254
As you already know, transaction costs are a hugely important thing to think about. Can you do anything to mitigate transaction costs and the optimization problem itself?

2
00:00:10,255 --> 00:00:19,714
Well, you know you want to minimize transaction costs. Can you just plop some quantity that measures transaction costs somehow into the objective function?

3
00:00:19,714 --> 00:00:28,359
That sounds like a seemingly satisfying solution. The problem with this is that in practice, it's extremely difficult to know how to represent those costs.

4
00:00:28,359 --> 00:00:36,009
To know the transaction costs, you must know what the trade is, but the whole reason to do the optimization is to know what the traders.

5
00:00:36,009 --> 00:00:47,065
In general, transaction cost is a function of trade size. But the problem is even an infinitesimally small trade will incur non-trivial transaction costs.

6
00:00:47,064 --> 00:00:57,549
That is, transaction cost is a discontinuous function of trade size. This is because there are lower bounds on bid ask spreads and Trade Commission's.

7
00:00:57,549 --> 00:01:07,789
This means that a transaction cost term can't be included in the objective function easily. One possibility is to impose a turnover constraint.

8
00:01:07,790 --> 00:01:15,670
Again, simply limiting the net change in weight on each asset relative to the weights in the previous portfolio.

9
00:01:15,670 --> 00:01:24,739
In general, turnover is directly proportional to transaction cost. So, you might limit turnover to say 20 percent.

10
00:01:24,739 --> 00:01:43,315
The problem with doing this is you can easily get to an infeasible situation. It's very possible that you can have a situation where through market movements, the portfolio gets pushed into a certain scenario where to have all the constraints satisfied, the turnover constraint must be violated.

11
00:01:43,314 --> 00:01:59,854
For example, market movements may have pushed the size of a stock's position beyond the maximum allowed by the individual weight constraint, but the trade required to reduce that weight may be more than allowed by the turnover constraint.

12
00:01:59,855 --> 00:02:13,689
There are two potential solutions. The first is to put that turnover constraint into a penalty term in the objective function so that instead of a hard cap, you have a scaling term that penalizes turnover.

13
00:02:13,689 --> 00:02:30,000
The second solution is to put the whole optimization problem inside a loop. You can impose a hard turnover constraint, and if the problem is infeasible, you can progressively relax the turnover constraint and rerun the optimization until the problem becomes feasible.


@@@
1
00:00:00,000 --> 00:00:13,995
Why will two portfolios employing the same underlying strategy run on the same as at universe during the same period yield a different portfolio if started at different points in time?

2
00:00:13,994 --> 00:00:21,175
The reason is that transitions between different sets of portfolio weights can be more or less costly.

3
00:00:21,175 --> 00:00:36,625
Consider two portfolios trying to make the same transition to the same ideal portfolio. A starting portfolio close to this portfolio might be directed here, but one starting from farther away might be directed elsewhere.

4
00:00:36,625 --> 00:00:52,930
For example; say I start my strategy on January 1 and the signal says, go long Apple 10 percent. So, I enter this 10 percent long position, then later on June 1, the ideal position is to go short Apple 10 percent.

5
00:00:52,929 --> 00:01:06,629
If I've imposed a turnover constraint or penalty in the problem, the optimizer may not conclude that it's actually ideal to enter this 10 percent short position because it's too far away from the position I'm already in.

6
00:01:06,629 --> 00:01:15,674
However, if my way on Apple was previously zero, it may tell me that the best next step is to go 10 percent short on Apple.

7
00:01:15,674 --> 00:01:24,054
This could even happen for portfolio started at the same time but holding different amounts of capital if they have a liquidity constraint.

8
00:01:24,055 --> 00:01:31,890
A liquidity constraint means that we restrict our positions to be below some percent of each asset's daily trade volume.

9
00:01:31,890 --> 00:01:39,734
If two portfolios have different capital amounts than the smaller capital portfolio is less constrained by a liquidity constraint.

10
00:01:39,734 --> 00:01:54,429
For example, if you have some small stock that trades one million dollars per day, and you have a small portfolio that wants to take a 10 percent or 100k position in that stock, which is 10 percent of its volume, that would be okay.

11
00:01:54,430 --> 00:02:10,500
But if you had a $100 million portfolio and the same Alpha vector, then that portfolio would want to take a $10 million position in the stock, which would be 10 times its daily volume, and so you'd only be able to take a very tiny position.

12
00:02:10,500 --> 00:02:18,300
So, ultimately, due to these trading constraints, the final portfolio weights of these portfolios would be very different.

13
00:02:18,300 --> 00:02:26,284
In practice, you may see this effect if you run the same strategy for different clients. Their portfolios may net different returns.

14
00:02:26,284 --> 00:02:41,074
This is the surprising but very common feature. It also affects backtesting. We've talked about backtesting only a little, but you'll recall that it involves rigorously simulating, trading a portfolio by testing on historical data.

15
00:02:41,074 --> 00:02:50,819
This type of simulation involves a lot of data and a lot of steps. You may want to speed it up by running parts of the computation in parallel across many computers.

16
00:02:50,819 --> 00:03:01,875
However, this is difficult because each step of the computation where we calculate what the portfolio would be at each point in time is in practice dependent on the previous stage.

17
00:03:01,875 --> 00:03:14,770
We call this path dependency. However, for portfolios of the same size over long periods of time, like years, you would expect the paths of the portfolios to rejoin each other.

18
00:03:14,770 --> 00:03:29,005
People have attempted creative solutions to this difficulty. They may run simulations of 18-month windows of time that have six months of overlap on different machines with the whole simulation spanning 10 years.


@@@
1
00:00:00,000 --> 00:00:10,759
Let's take a moment to have a final discussion about what is going on here. We started with our alpha vector, which is a vector of numbers that we think will be proportional to future returns.

2
00:00:10,759 --> 00:00:19,710
We want to maximize our alpha times our weights and minimize risk as modeled by our risk model, and we use optimization to achieve this.

3
00:00:19,710 --> 00:00:27,030
We also apply several other constraints during optimization, such as a constraint that we are market neutral, for example.

4
00:00:27,030 --> 00:00:37,695
But if we apply the market neutral constraint now, why did we subtract the mean from each individual alpha to make them market neutral back when we were calculating our alphas?

5
00:00:37,695 --> 00:00:44,715
Isn't this just duplicating alpha? Why not just wait for the optimizer to make the portfolio market neutral?

6
00:00:44,715 --> 00:00:54,515
We are using optimization so that we can control risk, but it creates a challenge when the optimization has a significant effect on the alpha vector.

7
00:00:54,515 --> 00:01:13,334
If your resulting portfolio is massively different than your alpha vector, then all your research and evaluation of your alpha factors up to that point might be invalidated if the final portfolio weights given by the optimizer don't look anything like the original alpha factor.

8
00:01:13,334 --> 00:01:28,425
So, you don't want the optimization to change your alpha vector too much. If you find some alphas and the optimization yields a very different portfolio than what you had when you started, then it's hard to figure out how to adjust the parameters.

9
00:01:28,424 --> 00:01:39,415
It can be hard to know whether to adjust the risk model or the constraints and by how much. Your alpha research is your pure expression of expected return.

10
00:01:39,415 --> 00:01:51,344
Any deviation from that is a sub-optimal deviation. But you're willing to accept that to trade off against risk, and you're willing to change the alpha vector to optimize on risk.

11
00:01:51,344 --> 00:01:59,510
But that only goes so far. If you end up with a portfolio that is completely far away from the alpha vector, then this isn't helpful either.

12
00:01:59,510 --> 00:02:08,754
You've evaluated the alpha factors and have some sense of how good they are, but the portfolio may no longer be following the signals of those alpha factors.

13
00:02:08,754 --> 00:02:16,700
So, what that means from a practical perspective is you want to introduce risk control as early in the process as possible.

14
00:02:16,699 --> 00:02:23,775
If you know that you'll use your alpha in a portfolio that is sector neutral, then you should make the alpha factor sector neutral.

15
00:02:23,775 --> 00:02:34,599
That way, if the alpha factor already looks good given the sector neutral weights, it has a better chance of translating well from theoretical alpha to resulting portfolio.


@@@
1
00:00:00,000 --> 00:00:08,384
Wow. Just wow. I am really proud of you. You made it through factor models and all the portfolio optimization content.

2
00:00:08,384 --> 00:00:15,750
This is tough stuff, no bones about it. This is a huge accomplishment on your part, and I congratulate you.

3
00:00:15,750 --> 00:00:21,564
Maybe, take a few minutes and think back to everything you have learned since you started this Nanodegree.

4
00:00:21,565 --> 00:00:48,475
Back in the beginning, we were talking about stock prices, adjusting for corporate actions, Panda's time series operations but since then, you've created several trading strategies, learned a number of modeling techniques, learned about portfolio optimization, and now, finally, you're seeing how to put it all together into a real professional level institutional trading strategy, drawing in novel ideas for Alphas.

5
00:00:48,475 --> 00:01:00,519
This is a huge progression. Onwards and upwards, time to put your knowledge to work. This project is truly top notch and I think you'll learn a lot by getting through it.


@@@
1
00:00:00,000 --> 00:00:13,625
Now that you've learned a lot of the theory behind alpha and risk factors, and had a chance to work through some of the important ideas and exercises, you have the opportunity to put all these ideas into practice in your final project of term one.

2
00:00:13,625 --> 00:00:22,765
Personally, it's my favorite project of the term in part, because I designed it. In this project, we code up and test several alpha factors.

3
00:00:22,765 --> 00:00:29,989
First, we evaluate the factors to see which ones may be promising candidates to put together into a combined alpha factor.

4
00:00:29,989 --> 00:00:35,905
We examine them initially to check whether the alphas have predictive power in the cross-section of stocks.

5
00:00:35,905 --> 00:00:42,670
We calculate their Sharpe ratios, and we tried to get a sense of how much trading they would incur by a turnover analysis.

6
00:00:42,670 --> 00:01:00,810
Then we throw the alphas into an optimizer in order to calculate optimal portfolio weights by maximizing our predicted return via our alphas and simultaneously attempting to neutralize exposure to common risk factors, which are sources of return variance.

7
00:01:00,810 --> 00:01:08,734
I created this project as a fully production-ready prototype, so that you could see how industry practitioners do this in the real world.

8
00:01:08,734 --> 00:01:21,025
It was especially important to me that you see how ideas for alphas turned into code and how both alpha and risk factor models played distinct and equally important roles in portfolio construction.

9
00:01:21,025 --> 00:01:30,170
I also utilized industry-ready Python libraries that make common operations simpler and faster, so you have a chance to get familiar with these packages.


@@@
1
00:00:00,000 --> 00:00:09,949
Hi there. Congratulations again on completing term one of the AI for Trading nanodegree program. You've built a solid foundation in quant research over these last few months.

2
00:00:09,949 --> 00:00:19,454
In term two, you'll build upon this foundation with an emphasis on advanced techniques using natural language processing and deep neural nets.

3
00:00:19,454 --> 00:00:33,755
You'll use these AI methods to process alternative data and generate trading signals. After, you'll learn and practice back testing, which is a crucial step in research and which is not often taught in academic or online courses.

4
00:00:33,755 --> 00:00:43,320
Gaining concrete experience in Thoreau backtesting is what will enable you to find actionable signals and differentiate the real signals from the noise.

5
00:00:43,320 --> 00:01:00,019
Finally, you'll also learn advanced techniques for Alpha combination using machine learning. Alpha combination is also a key step in the quant workflow, as real-world portfolios, use a combination of signals from multiple Alphas.

6
00:01:00,020 --> 00:01:07,795
Deciding not only which Alphas to include but how to combine them is a key skill set to have as a quant researcher.


@@@
1
00:00:00,000 --> 00:00:13,525
Welcome to Natural Language Processing. Language is an important medium for human communication. It allows us to convey information, express our ideas, and give instructions to others.

2
00:00:13,525 --> 00:00:24,184
Some philosophers argue that it enables us to form complex thoughts and reason about them. It may turn out to be a critical component of human intelligence.

3
00:00:24,184 --> 00:00:32,420
Now consider the various artificial systems we interact with every day, phones, cars, websites, coffee machines.

4
00:00:32,420 --> 00:00:41,810
It's natural to expect them to be able to process and understand human language, right? Yet, computers are still lagging behind.

5
00:00:41,810 --> 00:00:50,240
No doubt, we have made some incredible progress in the field of natural language processing, but there is still a long way to go.

6
00:00:50,240 --> 00:01:09,609
And that's what makes this an exciting and dynamic area of study. In this lesson you will not only get to know more about the applications and challenges in NLP, you will learn how to design an intelligent application that uses NLP techniques and deploy it on a scalable platform.


@@@
1
00:00:00,000 --> 00:00:12,750
What makes it so hard for computers to understand us? One drawback of human languages, or feature depending on how you look at it, is the lack of a precisely defined structure.

2
00:00:12,750 --> 00:00:19,879
To understand how that makes things difficult let's first take a look at some languages that are more structured.

3
00:00:19,879 --> 00:00:30,809
Mathematics, for instance, uses a structured language. When I write y equals 2x plus 5 there is no ambiguity in what I want to convey.

4
00:00:30,809 --> 00:00:40,679
I'm saying that the variable y is related to the variable x as two times x plus five. Formal logic also uses a structure language.

5
00:00:40,679 --> 00:00:58,535
For example, consider the expression parent(x,y) and parent(x,z) implies sibling(y, z). This statement is asserting that if x is a parent of y and x is a parent of z, then y and z are siblings.

6
00:00:58,534 --> 00:01:06,359
A set of structure languages that may be more familiar to you are scripting and programming languages.

7
00:01:06,358 --> 00:01:22,805
Consider this SQL statement. SELECT name, email FROM users WHERE name LIKE A%. We are asking the database to return the names and e-mail addresses of all users whose names begin with an A.

8
00:01:22,805 --> 00:01:30,000
These languages are designed to be as unambiguous as possible and are suitable for computers to process.


@@@
1
00:00:00,000 --> 00:00:09,134
Structured languages are easy to parse and understand for computers because they are defined by a strict set of rules or grammar.

2
00:00:09,134 --> 00:00:18,510
There are standard forms of expressing such grammars and algorithms, that can parse properly formed statements to understand exactly what is meant.

3
00:00:18,510 --> 00:00:26,683
When a statement doesn't match the prescribed grammar, a typical computer doesn't try to guess the meaning, it simply gives up.


@@@
1
00:00:00,000 --> 00:00:15,734
The languages we use to communicate with each other also have defined grammatical rules. And indeed, in some situations we use simple structured sentences but for the most part human discourse is complex and unstructured.

2
00:00:15,734 --> 00:00:24,210
Despite that, we seem to be really good at understanding each other and even ambiguities are welcome to a certain extent.

3
00:00:24,210 --> 00:00:41,780
So, what can computers do to make sense of unstructured text? Here are some preliminary ideas. Computers can do some level of processing with words and phrases, trying to identify key words, parts of speech, named entities, dates, quantities, etc.

4
00:00:41,780 --> 00:00:49,630
Using this information they can also try to parse sentences, at least ones that are relatively more structured.

5
00:00:49,630 --> 00:01:07,394
This can help extract the relevant parts of statements, questions, or instructions. At a higher level computers can analyze documents to find frequent and rare words, assess the overall tone or sentiment being expressed, and even cluster or group similar documents together.

6
00:01:07,394 --> 00:01:17,000
You can imagine that building on top of these ideas, computers can do a whole lot with unstructured text even if they cannot understand it like us.


@@@
1
00:00:00,000 --> 00:00:12,300
So what is stopping computers from becoming as capable as humans in understanding natural language? Part of the problem lies in the variability and complexity of our sentences.

2
00:00:12,300 --> 00:00:30,059
Consider this excerpt from a movie review. "I was lured to see this on the promise of a smart witty slice of old fashioned fun and intrigue. I was conned. " Although it starts with some potentially positive words it turns out to be a strongly negative review.

3
00:00:30,059 --> 00:00:37,839
Sentences like this might be somewhat entertaining for us but computers tend to make mistakes when trying to analyze them.

4
00:00:37,840 --> 00:00:52,164
But there is a bigger challenge that makes NLP harder than you think. Take a look at this sentence. "The sofa didn't fit through the door because it was too narrow." What does "it" refer to?

5
00:00:52,164 --> 00:01:05,819
Clearly "it" refers to the door. Now consider a slight variation of this sentence. "The sofa didn't fit through the door because it was too wide." What does "it" refer to in this case?

6
00:01:05,819 --> 00:01:20,230
Here it's the sofa. Think about it. To understand the proper meaning or semantics of the sentence you implicitly applied your knowledge about the physical world, that wide things don't fit through narrow things.

7
00:01:20,230 --> 00:01:34,000
You may have experienced a similar situation before. You can imagine that there are countless other scenarios in which some knowledge or context is indispensable for correctly understanding what is being said.


@@@
1
00:00:00,000 --> 00:00:09,845
Natural language processing is one of the fastest growing fields in the world. NLP Is making its way into a number of products and services that we use every day.

2
00:00:09,845 --> 00:00:32,265
Let's begin with an overview of how to design an end-to-end NLP pipeline. Not that kind of pipeline; a natural language processing pipeline, where you start with raw text, in whatever form it is available, process it, extract relevant features, and build models to accomplish various NLP tasks.

3
00:00:32,265 --> 00:00:41,094
Now that I think about it, that is kind of like refining crude oil. Anyways, you'll learn how these different stages in the pipeline depend on each other.

4
00:00:41,094 --> 00:00:47,489
You'll also learn how to make design decisions, how to choose existing libraries, and tools to perform each step.


@@@
1
00:00:00,000 --> 00:00:09,975
Let's look at a common NLP pipeline. It consists of three stages, text processing, feature extraction and modeling.

2
00:00:09,974 --> 00:00:26,714
Each stage transforms text in some way and produces a result that the next stage needs. For example, the goal of text processing is to take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.

3
00:00:26,714 --> 00:00:38,535
Similarly, the next stage needs to extract and produce feature representations that are appropriate for that type of model you're planning to use and the NLP task you're trying to accomplish.

4
00:00:38,534 --> 00:00:53,094
When you're building such a pipeline, your workflow may not be perfectly linear. Let's say, you spend some time implementing text processing functions, then make some simple feature extractors, and then design a baseline statistical model.

5
00:00:53,094 --> 00:01:02,909
But then, maybe you are not happy with the results. So you go back and rethink what features you need, and that in turn, can make you change your processing routines.

6
00:01:02,909 --> 00:01:11,000
Keep in mind that this is a very simplified view of natural language processing. Your application may require additional steps.


@@@
1
00:00:00,000 --> 00:00:08,550
Let's take a closer look at text processing. The first question that comes to mind is, why do we need to process text?

2
00:00:08,550 --> 00:00:16,125
Why can we not feed it in directly? To understand that, think about where we get this text to begin with.

3
00:00:16,125 --> 00:00:28,489
Websites are a common source of textual information. Here's a portion of a sample web page from Wikipedia and the corresponding HTML markup, which serves as our raw input.

4
00:00:28,489 --> 00:00:38,984
For the purpose of natural language processing, you would typically want to get rid of all or most of the HTML tags, and retain only plain text.

5
00:00:38,984 --> 00:00:50,440
You can also remove or set aside any URLs or other items not relevant to your task. The Web is probably the most common and fastest growing source of textual content.

6
00:00:50,439 --> 00:01:02,679
But you may also need to consume PDFs, Word documents or other file formats. Or your raw input may even come from a speech recognition system or from a book scan using OCR.

7
00:01:02,679 --> 00:01:17,185
Some knowledge of the source medium can help you properly handle the input. In the end, your goal is to extract plain text that is free of any source specific markers or constructs that are not relevant to your task.

8
00:01:17,185 --> 00:01:26,840
Once you have obtained plain text, some further processing may be necessary. For instance, capitalization doesn't usually change the meaning of a word.

9
00:01:26,840 --> 00:01:35,799
We can convert all the words to the same case so that they're not treated differently. Punctuation marks that we use to indicate pauses, etc.

10
00:01:35,799 --> 00:01:43,350
can also be removed. Some common words in a language often help provide structure, but don't add much meaning.

11
00:01:43,349 --> 00:01:56,000
For example, a, and, the, of, are, and so on. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.


@@@
1
00:00:00,000 --> 00:00:07,700
Okay. We now have clean normalized text. Can we feed this into a statistical or a machine learning model?

2
00:00:07,700 --> 00:00:19,115
Not quite. Let's see why. Text data is represented on modern computers using an encoding such as ASCII or Unicode that maps every character to a number.

3
00:00:19,114 --> 00:00:27,445
Computer store and transmit these values as binary, zeros and ones. These numbers also have an implicit ordering.

4
00:00:27,445 --> 00:00:43,814
65 is less than 66 which is less than 67. But does that mean A is less than B, and B is less and C? No. In fact, that would be an incorrect assumption to make and might mislead our natural language processing algorithms.

5
00:00:43,814 --> 00:00:54,689
Moreover, individual characters don't carry much meaning at all. It is words that we should be concerned with, but computers don't have a standard representation for words.

6
00:00:54,689 --> 00:01:04,409
Yes, internally they are just sequences of ASCII or Unicode values but they don't quite capture the meanings or relationships between words.

7
00:01:04,409 --> 00:01:14,580
Compare this with how an image is represented in computer memory. Each pixel value contains the relative intensity of light at that spot in the image.

8
00:01:14,579 --> 00:01:23,420
For a color image, we keep one value per primary color; red, green, and blue. These values carry relevant information.

9
00:01:23,420 --> 00:01:32,099
Two pixels with similar values are perceptually similar. Therefore, it makes sense to directly use pixel values in a numerical model.

10
00:01:32,099 --> 00:01:39,974
Yes, some feature engineering may be necessary such as edge detection or filtering, but pixels are a good starting point.

11
00:01:39,974 --> 00:01:49,019
So the question is, how do we come up with a similar representation for text data that we can use as features for modeling?

12
00:01:49,019 --> 00:01:56,099
The answer again depends on what kind of model you're using and what task you're trying to accomplish.

13
00:01:56,099 --> 00:02:06,660
If you want to use a graph based model to extract insights, you may want to represent your words as symbolic nodes with relationships between them like WordNet.

14
00:02:06,659 --> 00:02:16,349
For statistical models however, you need some sort of numerical representation. Even then, you have to think about the end goal.

15
00:02:16,349 --> 00:02:28,229
If you're trying to perform a document level task, such as spam detection or sentiment analysis, you may want to use a per document representations such as bag-of-words or doc2vec.

16
00:02:28,229 --> 00:02:39,979
If you want to work with individual words and phrases such as for text generation or machine translation, you'll need a word level representation such as word2vec or glove.

17
00:02:39,979 --> 00:02:48,000
There are many ways of representing textual information, and it is only through practice that you can learn what you need for each problem.


@@@
1
00:00:00,000 --> 00:00:20,009
The final stage in this process is what I like to call modeling. This includes designing a model, usually a statistical or a machine learning model, fitting its parameters to training data using an optimization procedure, and then using it to make predictions about unseen data.

2
00:00:20,010 --> 00:00:26,984
The nice thing about working with numerical features is that it allows you to utilize pretty much any machine learning model.

3
00:00:26,984 --> 00:00:34,229
This includes support vector machines, decision trees, neural networks, or any custom model of your choice.

4
00:00:34,229 --> 00:00:41,234
You could even combine multiple models to get better performance. How you utilize the model is up to you.

5
00:00:41,234 --> 00:00:51,274
You can deploy it as a web-based application, package it up into a handy mobile app, integrate it with other products, services, and so on.


@@@
1
00:00:00,000 --> 00:00:07,330
In this lesson, you'll learn how to read text data from different sources and prepare it for feature extraction.

2
00:00:07,330 --> 00:00:18,750
You'll begin by cleaning it to remove irrelevant items, such as HTML tags. You will then normalize text by converting it into all lowercase, removing punctuations and extra spaces.

3
00:00:18,750 --> 00:00:25,920
Next, you will split the text into words or tokens and remove words that are too common, also known as stop words.

4
00:00:25,920 --> 00:00:35,424
Finally, you will learn how to identify different parts of speech, named entities, and convert words into canonical forms using stemming and lemmatization.

5
00:00:35,424 --> 00:00:45,450
After going through all these processing steps, your text may look very different, but it captures the essence of what was being conveyed in a form that is easier to work with.


@@@
1
00:00:00,000 --> 00:00:07,890
The processing stage begins with reading text data. Depending on your application, that can be from one of several sources.

2
00:00:07,889 --> 00:00:17,684
The simplest source is a plain text file on your local machine. We can read it in using Python's built in file input mechanism.

3
00:00:17,684 --> 00:00:28,349
Text data may also be included as part of a larger database or table. Here, we have a CSV file containing information about some news articles.

4
00:00:28,349 --> 00:00:38,984
We can read this in using pandas very easily. Pandas includes several useful string manipulation methods that can be applied to an entire column at once.

5
00:00:38,984 --> 00:00:50,210
For instance, converting all values to lowercase. Sometimes, you may have to fetch data from an online resource, such as a web service or API.

6
00:00:50,210 --> 00:01:02,684
In this example, we use the requests library in Python to obtain a quote of the day from a simple API, but you could also obtain tweets, reviews, comments, whatever you would like to analyze.

7
00:01:02,685 --> 00:01:11,280
Most APIs return JSON or XML data, so you need to be aware of the structure in order to pull out the fields that you need.

8
00:01:11,280 --> 00:01:18,000
Many data sets you will encounter have likely been fetched and prepared by someone else using a similar procedure.


@@@
1
00:00:00,000 --> 00:00:08,509
Text data, especially from online sources, is almost never clean. Let's look at the Udacity course catalog as an example.

2
00:00:08,509 --> 00:00:14,835
Say you want to extract the title and description of each course or Nanodegree. Sounds simple, right?

3
00:00:14,835 --> 00:00:21,865
Let's jump into Python and give it a shot. You can follow along by downloading and launching the text processing notebook.

4
00:00:21,864 --> 00:00:33,420
We can fetch the web page like any other online resource using the requests library. It looks like we got back the entire HTML source.

5
00:00:33,420 --> 00:00:39,719
This is what the browser needs to render the web page. But most of this is useless for our purposes.

6
00:00:39,719 --> 00:00:47,884
We need a way to extract all the plain text as visible on the website. How about using regular expressions?

7
00:00:47,884 --> 00:00:56,915
Let's define a pattern to match all HTML tags and remove them by replacing with a blank string. Okay, that did something.

8
00:00:56,914 --> 00:01:06,859
We can see that the page title has been extracted successfully, but there is a lot of JavaScript and a number of other items that we don't need.

9
00:01:06,859 --> 00:01:13,754
In fact, this regular expression somehow didn't match some tags. Maybe they were nested inside other tags.

10
00:01:13,754 --> 00:01:21,019
Maybe we need to account for tags spread across lines. Anyway, this doesn't seem like the best approach for this job.

11
00:01:21,019 --> 00:01:28,079
What we really need is a way to parse the HTML, just like a web browser, and pull out the relevant elements.

12
00:01:28,079 --> 00:01:49,099
Introducing BeautifulSoup. It's a nice Python library meant to do exactly that. You just pass in the raw web page text which in this case contains HTML to create a soup object, and then you can extract the plain text, leaving behind any HTML tags using a symbol called to the get-text method.

13
00:01:49,099 --> 00:01:58,185
This takes care of nested tags, tags that are broken across lines, and a multitude of other edge cases that make HTML parsing a pain.

14
00:01:58,185 --> 00:02:10,960
It also forgives some small errors in HTML just like browsers, making it more robust. Let's see. That's better.

15
00:02:10,960 --> 00:02:23,620
I don't see any HTML tags, but there are still a bunch of JavaScript and a lot of spaces. What else can we do? Let's take a look at how the HTML source is structured.

16
00:02:23,620 --> 00:02:34,159
The easiest way to do this is to right click on an element of your choice, here, this course title, and choose Inspect or View Page Source.

17
00:02:34,159 --> 00:02:43,125
Now look at where the title is placed and what is the most distinct way of finding it in the HTML documentary.

18
00:02:43,125 --> 00:02:50,950
Here, we have a parent div with a class of course summary card. That sounds promising. Let's use it.

19
00:02:50,949 --> 00:02:59,034
BeautifulSoup is actually very powerful. It enables you to walk the tree or dorm in many different ways.

20
00:02:59,034 --> 00:03:09,534
Here we are asking the library to find all divs with a class of course summary card. The result returned is a list of all such divs in the document.

21
00:03:09,534 --> 00:03:23,710
Let's store this in a variable and look at one of the divs. Okay. Scrolling through this, I see that the title is stored in this a-tag which is contained in this H3 tag.

22
00:03:23,710 --> 00:03:36,209
How do we extract this title? One way to get to it is using a CSS selector. And now we can fetch the plain text content just like we did before.

23
00:03:36,210 --> 00:03:51,300
Great. One last thing. Let's strip out the extra whitespace from both ends. There you go. Now, let's look back at the HTML to see how we can grab the description text. There it is.

24
00:03:51,300 --> 00:04:04,699
It's a div with an attribute called data-course-short-summary, but no value or any other attribute. Again, there is a way to select such tags using CSS.

25
00:04:04,699 --> 00:04:15,944
Specify the tag name, here, div, followed by the attribute name in square brackets. Looks good. Let's extract the text and clean it up.

26
00:04:15,944 --> 00:04:24,500
All right. We can now repeat this over all course summaries. To do this, we can use a simple for loop.

27
00:04:24,500 --> 00:04:35,079
Looks spot on to me. Let's store this data so we can use it later. Here, we are simply keeping the data in a list called courses.

28
00:04:35,079 --> 00:04:42,550
What we did just now is called scraping a web page. Although it sounds a little violent, trust me, it's not.

29
00:04:42,550 --> 00:04:54,634
In fact, scraping is very common. Google News is a prime example. It pulls out the title and first sentence or two from news articles and displays them.

30
00:04:54,634 --> 00:05:06,245
Google probably uses a combination of rules and machine learning to identify what portion of the HTML contains the title and the beginning of the article text that it can use as a preview.

31
00:05:06,245 --> 00:05:17,920
It works great most of the time, but sometimes, it does fail. Here, for this article on quantum entanglement, the preview doesn't seem to match the title at all.

32
00:05:17,920 --> 00:05:32,979
It looks more like a caption for this image. What likely happened is that the caption was the first piece of text on the web page and Google's algorithm picked that up as if it was part of the main article.

33
00:05:32,980 --> 00:05:40,145
This just goes to show how seemingly routine tasks and text processing are still not solved all the way.

34
00:05:40,144 --> 00:05:48,910
Okay, let's look back at what we just achieved. We started by fetching a single web page, the Udacity course catalog.

35
00:05:48,910 --> 00:06:03,189
Then we tried a couple of methods to remove HTML tags. We finally settled on using BeautifulSoup to parse the entire HTML source, find all course summaries, and extract the title and description for each course.

36
00:06:03,189 --> 00:06:15,819
And then we saved them all in a list. Depending on what you're planning to do next, you may continue to treat these chunks as part of a single document or consider each to be a separate document.

37
00:06:15,819 --> 00:06:24,000
The latter is useful, for instance, if you want to group related courses. The problem then reduces to document clustering.


@@@
1
00:00:00,000 --> 00:00:10,650
Plain text is great but it's still human language with all its variations and bells and whistles. Next, we'll try to reduce some of that complexity.

2
00:00:10,650 --> 00:00:17,325
In the English language, the starting letter of the first word in any sentence is usually capitalized.

3
00:00:17,324 --> 00:00:34,769
All caps are sometimes used for emphasis and for stylistic reasons. While this is convenient for a human reader from the standpoint of a machine learning algorithm, it does not make sense to differentiate between Car, car, and CAR, they all mean the same thing.

4
00:00:34,770 --> 00:00:45,844
Therefore, we usually convert every letter in our text to a common case, usually lowercase, so that each word is represented by a unique token.

5
00:00:45,844 --> 00:00:56,234
Here's some sample text, a review for the movie, The Second Renaissance, a story about intelligent robots that get into a fight with humans over their rights.

6
00:00:56,234 --> 00:01:09,334
Yup, the way we treat robots these days. Anyway, if we have the reviews stored in a variable called text, converting it to lowercase is a simple call to the lore method in Python.

7
00:01:09,334 --> 00:01:33,669
Here's what it looks like after a conversion. Note all the letters that were changed. Other languages may or may not have a case equivalent but similar principles may apply depending on your NLP task, you may want to remove special characters like periods, question marks, and exclamation points from the text and only keep letters of the alphabet and maybe numbers.

8
00:01:33,670 --> 00:01:45,724
This is especially useful when we are looking at text documents as a whole in applications like document classification and clustering where the low level details do not matter a lot.

9
00:01:45,724 --> 00:01:58,505
Here, we can use a regular expression that matches everything that is not a lowercase A to Z, uppercase A is Z, or digits zero to nine, and replaces them with a space.

10
00:01:58,504 --> 00:02:05,694
This approach avoids having to specify all punctuation characters, but you can use other regular expressions as well.

11
00:02:05,694 --> 00:02:19,080
Lowercase conversion and punctuation removal are the two most common text normalization steps. Whether you need to apply them and at what stage depends on your end goal and the way you design your pipeline.


@@@
1
00:00:00,000 --> 00:00:08,375
Token is a fancy term for a symbol. Usually, one that holds some meaning and is not typically split up any further.

2
00:00:08,375 --> 00:00:19,634
In case of natural language processing, our tokens are usually individual words. So tokenization is simply splitting each sentence into a sequence of words.

3
00:00:19,635 --> 00:00:34,580
The simplest way to do this is using the split method which returns a list of words. Note that it splits on whitespace characters by default, which includes regular spaces but also tabs, new lines, et cetera.

4
00:00:34,579 --> 00:00:41,375
It's also smart about ignoring two or more whitespace characters in a sequence, so it doesn't return blank strings.

5
00:00:41,375 --> 00:00:58,490
But you can control all this using optional parameters. So far, we've only been using Python's built-in functionality, but some of these operations are much easier to perform using a library like NLTK, which stands for natural language toolkit.

6
00:00:58,490 --> 00:01:07,189
The most common approach for splitting up texting NLTK is to use the word tokenized function from nltk.tokenize.

7
00:01:07,189 --> 00:01:15,360
This performs the same task as split but is a little smarter. Try passing in some raw text that has not been normalized.

8
00:01:15,359 --> 00:01:27,645
You'll notice that the punctuations are treated differently based on their position. Here, the period after the title Doctor has been retained along with Dr as a single token.

9
00:01:27,644 --> 00:01:35,269
As you can imagine, NLTK is using some rules or patterns to decide what to do with each punctuation.

10
00:01:35,269 --> 00:01:45,289
Sometimes, you may need to split text into sentences. For instance, if you want to translate it. You can achieve this with NLTK using sent tokenize.

11
00:01:45,290 --> 00:02:07,120
Then you can split each sentence into words if needed. NLTK provide several other tokenizers, including a regular expression base tokenizer that you can use to remove punctuation and perform tokenization in a single step, and also a tweet tokenizer that is aware of twitter handles, hash tags, and emoticons.


@@@
1
00:00:00,000 --> 00:00:09,300
Stop words are uninformative words like, is, our, the, in, at, et cetera that do not add a lot of meaning to a sentence.

2
00:00:09,300 --> 00:00:20,130
They are typically very commonly occurring words, and we may want to remove them to reduce the vocabulary we have to deal with and hence the complexity of later procedures.

3
00:00:20,129 --> 00:00:28,414
Notice that even without our and the in the sentence above, we can still infer it's positive sentiment toward dogs.

4
00:00:28,414 --> 00:00:39,554
You can see for yourself which words NLTK considers to be stop words in English. Note that this is based on a specific corpus or collection of text.

5
00:00:39,554 --> 00:00:48,914
Different corpora may have different stop words. Also, a word maybe a stop word in one application, but a useful word in another.

6
00:00:48,914 --> 00:00:56,839
To remove stop words from a piece of text, you can use a Python list comprehension with a filtering condition.

7
00:00:56,840 --> 00:01:11,159
Here, we apply stop word removal to the movie review after normalizing and tokenizing it. The result is a little hard to read, but notice how it has helped reduce the size of the input, at the same time important words have been retained.


@@@
1
00:00:00,000 --> 00:00:13,134
Remember parts of speech from school? Nouns, pronouns, verbs, adverbs, et cetera. Identifying how words are being used in a sentence can help us better understand what is being said.

2
00:00:13,134 --> 00:00:22,395
It can also point out relationships between words and recognize cross references. NLTK, again, makes things pretty easy for us.

3
00:00:22,394 --> 00:00:31,524
You can pass in tokens or words to the POS tag function which returns a tag for each word identifying different parts of speech.

4
00:00:31,524 --> 00:00:39,394
Notice how it has correctly labelled the first utterance of "lie" as a verb, while marking the second one as a noun.

5
00:00:39,395 --> 00:00:50,304
Refer to the NLTK documentation for more details on what each tag means. One of the cool applications of part of speech tagging is parsing sentences.

6
00:00:50,304 --> 00:01:02,819
Here's an example from the NLTK book that uses a custom grammar to parse an ambiguous sentence. Notice how the parser returns both interpretations that are valid.

7
00:01:02,820 --> 00:01:18,969
It is much easier to see the difference when we visualize the parse trees. I shot an elephant in my pajamas, versus, I shot an elephant and the elephant was in my pajamas.


@@@
1
00:00:00,000 --> 00:00:13,609
Named entities are typically noun phrases that refer to some specific object, person, or place. You can use the ne_chunk function to label named entities in text.

2
00:00:13,609 --> 00:00:30,830
Note that you have to first tokenize and tag parts of speech. This is a very simple example, but notice how the different entity types are also recognized: person, organization, and GPE, which stands for geopolitical entity.

3
00:00:30,829 --> 00:00:44,139
Also note how it identified the two words, Udacity and Inc, together as a single entity. Out in the wild, performance is not always great but training on a large corpus definitely helps.

4
00:00:44,140 --> 00:00:51,250
Named entity recognition is often used to index and search for news articles, for example, on companies of interest.


@@@
1
00:00:00,000 --> 00:00:07,980
In order to further simplify text data, let's look at some ways to normalize different variations and modifications of words.

2
00:00:07,980 --> 00:00:19,280
Stemming is the process of reducing a word to its stem or root form. For instance, branching, branched, branches et cetera, can all be reduced to branch.

3
00:00:19,280 --> 00:00:32,743
After all, they conveyed the idea of something separating into multiple paths or branches. Again, this helps reduce complexity while retaining the essence of meaning that is carried by words.

4
00:00:32,743 --> 00:00:40,524
Stemming is meant to be a fast and crude operation carried out by applying very simple search and replace style rules.

5
00:00:40,524 --> 00:00:49,135
For example, the suffixes 'ing' and 'ed' can be dropped off, 'ies' can be replaced by 'y' et cetera.

6
00:00:49,134 --> 00:00:58,274
This may result in stem words that are not complete words, but that's okay, as long as all forms of that word are reduced to the same stem.

7
00:00:58,274 --> 00:01:12,632
Thus, capturing the common underlying idea. NLTK has a few different stemmers for you to choose from, including PorterStemmer that we use here, SnowballStemmer, and other language-specific stemmers.

8
00:01:12,632 --> 00:01:26,295
You simply need to pass in one word at a time. Note that here, we have already removed stop words. Some of the conversions are actually pretty good, like started, reduced to start.

9
00:01:26,295 --> 00:01:46,454
Others, like people, losing the 'e' at the end are a result of applying very simplistic rules. Lemmatization is another technique used to reduce words to a normalized form, but in this case, the transformation actually uses a dictionary to map different variants of a word back to its root.

10
00:01:46,454 --> 00:01:54,439
With this approach, we are able to reduce non-trivial inflections such as is, was, were, back to the root 'be'.

11
00:01:54,439 --> 00:02:03,409
The default lemmatizer in NLTK uses the Wordnet database to reduce words to the root form. Let's try it out.

12
00:02:03,409 --> 00:02:12,240
Just like in stemming, you initialize an instance of WordNetLemmatizer and pass in individual words to its lemmatize method.

13
00:02:12,240 --> 00:02:21,640
What happened here? It seems that only the word ones got reduced to one, all the others are unchanged.

14
00:02:21,639 --> 00:02:31,110
If you read the words carefully, you'll see that ones is the only plural noun here. In fact, that's exactly why it got transformed.

15
00:02:31,110 --> 00:02:38,495
A lemmatizer needs to know or make an assumption about the part of speech for each word it's trying to transform.

16
00:02:38,495 --> 00:02:46,569
In this case, WordNetLemmatizer defaults to nouns, but we can override that by specifying the PoS parameter.

17
00:02:46,569 --> 00:03:00,299
Let's pass in 'v' for verbs. This time, the two verb forms 'boring' and 'started' got converted. Great. Note that there are other verbs, but they are already in the root form.

18
00:03:00,300 --> 00:03:10,439
Also, note how we passed in the output from the previous noun lemmatization step. This way of chaining procedures is very common. Let's recap.

19
00:03:10,439 --> 00:03:18,334
As we saw in the previous examples, stemming sometimes results in stems that are not complete words in English.

20
00:03:18,335 --> 00:03:30,570
Lemmatization is similar to stemming with one difference, the final form is also a meaningful word. That said, stemming does not need a dictionary like lemmatization does.

21
00:03:30,569 --> 00:03:37,000
So depending on the constraints you have, stemming maybe a less memory intensive option for you to consider.


@@@
1
00:00:00,000 --> 00:00:07,275
We have covered a number of text processing steps. Let's summarize what a typical workflow looks like.

2
00:00:07,275 --> 00:00:19,559
Starting with a plain text sentence, you first normalize it by converting to lowercase and removing punctuation, and then you split it up into words using a tokenizer.

3
00:00:19,559 --> 00:00:34,134
Next, you can remove stop words to reduce the vocabulary you have to deal with. Depending on your application, you may then choose to apply a combination of stemming and lemmatization to reduce words to the root or stem form.

4
00:00:34,134 --> 00:00:47,000
It is common to apply both, lemmatization first, and then stemming. This procedure converts a natural language sentence into a sequence of normalized tokens which you can use for further analysis.


@@@
1
00:00:00,000 --> 00:00:08,009
Once we have our text ready in a clean and normalized form, we need to transform it into features that can be used for modeling.

2
00:00:08,009 --> 00:00:15,155
For instance, treating each document like a bag of words allows us to compute some simple statistics that characterize it.

3
00:00:15,154 --> 00:00:25,050
These statistics can be improved by assigning appropriate weights towards using a TF-IDF Scheme. This enables a more accurate comparison between documents.

4
00:00:25,050 --> 00:00:36,240
For certain applications, we may need to find numerical representations of individual words, and for that, we can use word embeddings, which are a very efficient and powerful method.

5
00:00:36,240 --> 00:00:42,000
In this lesson, you will learn all these techniques for extracting relevant features from text data.


@@@
1
00:00:00,000 --> 00:00:11,294
The first feature representation we'll look at is called Bag of Words. The Bag of Words model treats each document as an un-ordered collection or bag of words.

2
00:00:11,294 --> 00:00:23,250
Here, a document is the unit of text that you want to analyze. For instance, if you want to compare essays submitted by students to check for plagiarism, each essay would be a document.

3
00:00:23,250 --> 00:00:40,380
If you want to analyze the sentiment conveyed by tweets, then each tweet would be a document. To obtain a bag of words from a piece of raw text, you need to simply apply appropriate text processing steps: cleaning, normalizing, splitting into words, stemming, lemmatization, et cetera.

4
00:00:40,380 --> 00:00:49,905
And then treat the resulting tokens as an un-ordered collection or set. So, each document in your data set will produce a set of words.

5
00:00:49,905 --> 00:00:58,309
But keeping these as separate sets is very inefficient. They're of different sizes, may contain different words, and are hard to compare.

6
00:00:58,310 --> 00:01:05,290
Also, whatever word occurs multiple times in a document? Is there a better representation you can think of?

7
00:01:05,290 --> 00:01:14,579
A more useful approach is to turn each document into a vector of numbers, representing how many times each word occurs in a document.

8
00:01:14,579 --> 00:01:20,689
A set of documents is known as a corpus, and this gives the context for the vectors to be calculated.

9
00:01:20,689 --> 00:01:35,689
First, collect all the unique words present in your corpus to form your vocabulary. Arrange these words in some order, and let them form the vector element positions or columns of a table, and assume each document is a row.

10
00:01:35,689 --> 00:01:43,019
Then count the number of occurrences of each word in each document and enter the value in the respective column.

11
00:01:43,019 --> 00:01:53,699
At this stage, it is easier to think of this as a Document-Term Matrix, illustrating the relationship between documents in rows, and words or terms in columns.

12
00:01:53,700 --> 00:02:01,129
Each element can be interpreted as a term frequency. How frequently does that term occur in this document?

13
00:02:01,129 --> 00:02:13,110
Now, consider what you can do with this representation. One possibility is to compare two documents based on how many words they have in common or how similar their term frequencies are.

14
00:02:13,110 --> 00:02:24,128
A more mathematical way of expressing that is to compute the dot product between the two row vectors, which is the sum of the products of corresponding elements.

15
00:02:24,128 --> 00:02:32,860
Greater the dot product, more similar the two vectors are. The dot product has one flaw, it only captures the portions of overlap.

16
00:02:32,860 --> 00:02:42,744
It is not affected by other values that are not uncommon. So, pairs that are very different can end up with the same product as ones that are identical.

17
00:02:42,745 --> 00:02:51,840
A better measure is cosine similarity, where we divide the dot product of two vectors by the product of their magnitudes or Euclidean norms.

18
00:02:51,840 --> 00:03:00,814
If you think of these vectors as arrows in some n-dimensional space, then this is equal to the cosine of the angle theta between them.

19
00:03:00,814 --> 00:03:10,310
Identical vectors have cosine equals one. Orthogonal vectors have cosine equal zero. And for vectors that are exactly opposite, it is minus one.


@@@
1
00:00:00,000 --> 00:00:11,640
One limitation of the bag-of-words approach is that it treats every word as being equally important, whereas intuitively, we know that some words occur frequently within a corpus.

2
00:00:11,640 --> 00:00:30,714
For example, when looking at financial documents, cost or price may be a pretty common term. We can compensate for this by counting the number of documents in which each word occurs, this can be called document frequency, and then dividing the term frequencies by the document frequency of that term.

3
00:00:30,714 --> 00:00:40,024
This gives us a metric that is proportional to the frequency of occurrence of a term in a document, but inversely proportional to the number of documents it appears in.

4
00:00:40,024 --> 00:00:51,185
It highlights the words that are more unique to a document, and thus better for characterizing it. You may have heard of, or used, the TF-IDF transform before.

5
00:00:51,185 --> 00:01:00,094
It's simply the product of two words, very similar to what we've seen so far, a term frequency and an inverse document frequency.

6
00:01:00,094 --> 00:01:22,385
The most commonly used form of TF-IDF defines term frequency as the raw count of a term, t, in a document, d, divided by the total number of terms in d, and inverse document frequency as the logarithm of the total number of documents in the collection, d, divided by the number of documents where t is present.

7
00:01:22,385 --> 00:01:30,909
Several variations exist that try to normalize, or smooth the resulting values, or prevent edge cases such as divide-by-zero errors.

8
00:01:30,909 --> 00:01:38,000
Overall, TF-IDF is an innovative approach to assigning weights to words that signify their relevance in documents.


@@@
1
00:00:00,000 --> 00:00:07,669
So far, we've looked at representations that tried to characterize an entire document or collection of words as one unit.

2
00:00:07,669 --> 00:00:18,059
As a result, the kinds of inferences we can make are also typically at a document level, mixture of topics in the document, documents similarity, documents sentiment, et cetera.

3
00:00:18,059 --> 00:00:34,850
For a deeper analysis of text, we need to come up with a numerical representation for each word. If you've dealt with categorical variables for data analysis or tried to perform multi-class classification, you may have come across this term, One-Hot Encoding.

4
00:00:34,850 --> 00:00:46,679
That is one way of representing words, treat each word like a class, assign it a vector that has one in a single pre-determined position for that word and zero everywhere else.

5
00:00:46,679 --> 00:00:55,000
Looks familiar? Yeah, it's just like the bag of words idea, only that we keep a single word in each bag and build a vector for it.


@@@
1
00:00:00,000 --> 00:00:11,879
One-hot encoding usually works in some situations but breaks down when we have a large vocabulary to deal with, because the size of our ward representation grows with the number of words.

2
00:00:11,880 --> 00:00:19,089
What we need as a way to control the size of our word representation by limiting it to a fixed-size vector.

3
00:00:19,089 --> 00:00:26,620
In other words, we want to find an embedding for each word in some vector space and we wanted to exhibit some desired properties.

4
00:00:26,620 --> 00:00:33,719
For example, if two words are similar in meaning, they should be closer to each other compared to words that are not.

5
00:00:33,719 --> 00:00:42,905
And if two pairs of words have a similar difference in their meanings, they should be approximately equally separated in the embedded space.

6
00:00:42,905 --> 00:00:55,890
We could use such a representation for a variety of purposes like finding synonyms and analogies, identifying concepts around which words are clustered, classifying words as positive, negative, neutral, et cetera.


@@@
1
00:00:00,000 --> 00:00:11,129
Word2Vec is perhaps one of the most popular examples of word embeddings used in practice. As the name Word2Vec indicates, it transforms words to vectors.

2
00:00:11,130 --> 00:00:30,804
But what the name doesn't give away is how that transformation is performed. The core idea behind Word2Vec is this, a model that is able to predict a given word, given neighboring words, or vice versa, predict neighboring words for a given word is likely to capture the contextual meanings of words very well.

3
00:00:30,804 --> 00:00:43,195
And these are, in fact, two flavors of Word2Vec models, one where you are given neighboring words called continuous bag of words, and the other where you are given the middle word called Skip-gram.

4
00:00:43,195 --> 00:00:57,880
In the Skip-gram model, you pick any word from a sentence, convert it into a one-hot encoded vector and feed it into a neural network or some other probabilistic model that is designed to predict a few surrounding words, its context.

5
00:00:57,880 --> 00:01:07,384
Using a suitable loss function, optimize the weights or parameters of the model and repeat this till it learns to predict context words as best as it can.

6
00:01:07,385 --> 00:01:17,505
Now, take an intermediate representation like a hidden layer in a neural network. The outputs of that layer for a given word become the corresponding word vector.

7
00:01:17,504 --> 00:01:29,004
The Continuous Bag of Words variation also uses a similar strategy. This yields a very robust representation of words because the meaning of each word is distributed throughout the vector.

8
00:01:29,004 --> 00:01:42,599
The size of the word vector is up to you, how you want to tune performance versus complexity. It remains constant no matter how many words you train on, unlike Bag of Words, for instance, where the size grows with the number of unique words.

9
00:01:42,599 --> 00:01:53,019
And once you pre-train a large set of word vectors, you can use them efficiently without having to transform again and again, just store them in a lookup table.

10
00:01:53,019 --> 00:02:01,645
Finally, it is ready to be used in deep learning architectures. For example, it can be used as the input vector for recurrent neural nets.

11
00:02:01,644 --> 00:02:19,000
It is also possible to use RNNs to learn even better word embeddings. Some other optimizations are possible that further reduce the model and training complexity such as representing the output words using Hierarchical Softmax, computing loss using Sparse Cross Entropy, et cetera.


@@@
1
00:00:00,000 --> 00:00:08,849
Word2vec is just one type of forward embedding. Recently, several other related approaches have been proposed that are really promising.

2
00:00:08,849 --> 00:00:24,269
GloVe or global vectors for word representation is one such approach that tries to directly optimize the vector representation of each word just using co- occurrence statistics, unlike word2vec which sets up an ancillary prediction task.

3
00:00:24,269 --> 00:00:36,090
First, the probably that word j appears in the context of word i is computed, pj given i for all word pairs ij in a given corpus.

4
00:00:36,090 --> 00:00:46,890
What do we mean by j appears in context of i? Simply that word j is present in the vicinity of word i, either right next to it, or a few words away.

5
00:00:46,890 --> 00:00:53,939
We count all such occurrences of i and j in our text collection, and then normalize account to get a probability.

6
00:00:53,939 --> 00:01:07,284
Then, a random vector is initialized for each word, actually two vectors. One for the word when it is acting as a context, and one when it is acting as the target. So far, so good.

7
00:01:07,284 --> 00:01:18,045
Now, for any pair of words, ij, we want the dot product of their word vectors, w_i times w_j, to be equal to their co-occurrence probability.

8
00:01:18,045 --> 00:01:24,845
Using this as our goal and a suitable last function, we can iteratively optimize these word vectors.

9
00:01:24,844 --> 00:01:31,489
The result should be a set of vectors that capture the similarities and differences between individual words.

10
00:01:31,489 --> 00:01:39,689
If you look at it from another point of view, we are essentially factorizing the co-occurrence probability matrix into two smaller matrices.

11
00:01:39,689 --> 00:01:54,139
This is the basic idea behind GloVe. All that sounds good, but why co-occurrence probabilities? Consider two context words, say ice and steam, and two target words, solid and water.

12
00:01:54,140 --> 00:02:03,840
You would come across solid more often in the context of ice than steam, right? But water could occur in either context with roughly equal probability.

13
00:02:03,840 --> 00:02:10,724
At least, that's what we would expect. Surprise. That's exactly what co-occurrence probabilities reflect.

14
00:02:10,724 --> 00:02:25,270
Given a large corpus, you'll find that the ratio of P solid given ice to P solid given steam is much greater than one, while the ratio of P water given ice and P water given steam is close to one.

15
00:02:25,270 --> 00:02:31,570
Thus, we see that co-occurrence probabilities already exhibit some of the properties we want to capture.

16
00:02:31,569 --> 00:02:38,919
In fact, one refinement over using raw probability values is to optimize for the ratio of probabilities.

17
00:02:38,919 --> 00:02:46,869
Now, there are a lot of subtleties here, not the least of which is the fact that the co-occurence probability matrix is huge.

18
00:02:46,870 --> 00:02:54,840
At the same time, co-occurrence probability values are typically very low, so it makes sense to work with the log of these values.

19
00:02:54,840 --> 00:03:01,000
I encourage you to read the original paper that introduced GloVe to get a better understanding of this technique.


@@@
1
00:00:00,000 --> 00:00:07,890
Where the embeddings are fast becoming the de facto choice for representing words, especially for use and deep neural networks.

2
00:00:07,889 --> 00:00:19,734
But why do these techniques work so well? Doesn't it seem almost magical that you can actually do arithmetic with words, like woman minus man plus king equals queen?

3
00:00:19,734 --> 00:00:28,574
The answer might lie in the distributional hypothesis, which states that words that occur in the same contexts tend to have similar meanings.

4
00:00:28,574 --> 00:00:38,609
For example, consider this sentence. Would you like to have a cup of blank? Okay. How about, I like my blank black.

5
00:00:38,609 --> 00:00:49,890
One more, I need my morning blank before I can do anything. What are you thinking? Tea? Coffee? What give you the hint?

6
00:00:49,890 --> 00:01:00,719
Cup? Black? Morning? But it could be either of the two, right? And that's the point. In these contexts, tea and coffee are actually similar.

7
00:01:00,719 --> 00:01:11,484
Therefore, when a large collection of sentences is used to learn in embedding, words with common context words tend to get pulled closer and closer together.

8
00:01:11,484 --> 00:01:19,689
Of course, there could also be contexts in which tea and coffee are dissimilar. For example, blank grounds are great for composting.

9
00:01:19,689 --> 00:01:28,045
Or, I prefer loose leaf blank. Here we are clearly talking about coffee grounds, and loose leaf tea.

10
00:01:28,045 --> 00:01:35,415
How do we capture these similarities and differences in the same embedding? By adding another dimension.

11
00:01:35,415 --> 00:01:45,295
Let's see how. Words can be close along one dimension. Here, tea and coffee are both beverages, but separated along some other dimension.

12
00:01:45,295 --> 00:01:54,495
Maybe this dimension captures all the variability among beverages. In a human language, there are many more dimensions along which word meanings can vary.

13
00:01:54,495 --> 00:02:00,679
And the more dimensions you can capture in your word vector, the more expressive that representation will be.

14
00:02:00,680 --> 00:02:10,055
But how many dimensions do you really need? Consider a typical neural network architecture designed for an NLP task, say word prediction.

15
00:02:10,055 --> 00:02:25,829
It's common to use a word embedding layer that produces a vector with a few hundred dimensions, but that's significantly small compared to using one heart encodings directly, which are as large as the size of the vocabulary, sometimes in tens of thousands of words.

16
00:02:25,830 --> 00:02:35,974
Also, if you learn the embedding as part of the model training process, you can obtain a representation that captures the dimensions that are most relevant for your task.

17
00:02:35,974 --> 00:02:46,798
This often adds complexity. So unless you're building a model for a very narrow application like one that deals with medical terminology, you can use a pre-trained embedding as a look-up.

18
00:02:46,798 --> 00:03:01,085
For example, work to veck or glove. Then you only need to train the layer specific to your task. Compare this with the network architecture for a computer vision task, say, image classification, the raw input here is also very high dimensional.

19
00:03:01,085 --> 00:03:15,145
For example, even 128 by 128 Image contains over 16 thousand pixels. We typically use convolutional layers to exploit the spatial relationships and image data and reduce this dimensionality.

20
00:03:15,145 --> 00:03:28,406
Early stages and visual processing are often transferable across tasks, so it is common to use some pre-trained layers from an existing network, like Alex nad or BTG 16 and only learn the later layers.

21
00:03:28,406 --> 00:03:34,905
Come to think of it, using an embedding look up for NLP is not on like using pre-treated layers for computer vision.


@@@
1
00:00:00,000 --> 00:00:10,050
Word embeddings need to have high dimensionality in order to capture sufficient variations in natural language, which makes them super hard to visualize.

2
00:00:10,050 --> 00:00:20,839
T-SNE, which stands for t-Distributed Stochastic Neighbor Embedding, is a dimensionality reduction technique that can map high dimensional vectors to a lower dimensional space.

3
00:00:20,839 --> 00:00:37,320
It's kind of like PCA, Principle Component Analysis, but with one amazing property. When performing the transformation, it tries to maintain relative distances between objects, so that similar ones stay closer together while dissimilar objects stay further apart.

4
00:00:37,320 --> 00:00:48,390
This makes t-SNE a great choice for visualizing word embeddings. It effectively preserves the linear substructures and relationships that have been learned by the embedding model.

5
00:00:48,390 --> 00:01:01,435
If we look at the larger vector space, we can discover meaningful groups of related words. Sometimes, that takes a while to realize why certain clusters are formed, but most of the groupings are very intuitive.

6
00:01:01,435 --> 00:01:22,609
T-SNE also works on other kinds of data, such as images. Here, we see pictures from the Caltech 101 dataset organized into clusters that roughly correspond to class labels, including airplanes with blue sky being the common theme, sailboats of different shapes and sizes, and human faces.

7
00:01:22,609 --> 00:01:30,310
This is a very useful tool for better understanding the representation that a network learns and for identifying any bugs or other issues.


@@@
1
00:00:00,000 --> 00:00:10,219
Congratulations on completing the lesson. But remember, this is only the beginning of a long and exciting journey into a world with limitless possibilities.

2
00:00:10,220 --> 00:00:21,484
It was a pleasure helping you take your first few steps and I'm looking forward to seeing what systems you will build, what new problems you will solve, and how you'll advance the field of natural language processing.


@@@
1
00:00:00,000 --> 00:00:08,160
So let's start with two questions, what is deep learning, and what is it used for? The answer to the second question is pretty much everywhere.

2
00:00:08,160 --> 00:00:24,635
Recent applications include things such as beating humans in games such as Go, or even jeopardy, detecting spam in emails, forecasting stock prices, recognizing images in a picture, and even diagnosing illnesses sometimes with more precision than doctors.

3
00:00:24,635 --> 00:00:31,980
And of course, one of the most celebrated applications of deep learning is in self-driving cars. And what is at the heart of deep learning?

4
00:00:31,980 --> 00:00:41,299
This wonderful object called neural networks. Neural networks vaguely mimic the process of how the brain operates, with neurons that fire bits of information.

5
00:00:41,299 --> 00:00:50,719
It sounds pretty scary, right? As a matter of fact, the first time I heard of a neural network, this is the image that came into my head, some scary robot with artificial brain.

6
00:00:50,719 --> 00:00:56,179
But then, I got to learn a bit more about neural networks and I realized that there are actually a lot scarier than that.

7
00:00:56,179 --> 00:01:09,029
This is how a neural network looks. As a matter of fact, this one here is a deep neural network. Has lots of nodes, lots of edges, lots of layers, information coming through the nodes and leaving, it's quite complicated.

8
00:01:09,030 --> 00:01:14,540
But after looking at neural networks for a while, I realized that they're actually a lot simpler than that.

9
00:01:14,540 --> 00:01:23,900
When I think of a neural network, this is actually the image that comes to my mind. There is a child playing in the sand, with some red and blue shells and we are the child.

10
00:01:23,900 --> 00:01:32,299
Can you draw a line that separates the red and the blue shells? And the child draws this line. That's it. That's what a neural network does.

11
00:01:32,299 --> 00:01:38,800
Given some data in the form of blue or red points, the neural network will look for the best line that separates them.

12
00:01:38,799 --> 00:01:44,079
And if the data is a bit more complicated like this one over here, then we'll need a more complicated algorithm.

13
00:01:44,079 --> 00:01:49,799
Here, a deep neural network will do the job and find a more complex boundary that separates the points.


@@@
1
00:00:00,000 --> 00:00:07,339
So, let's start with one classification example. Let's say we are the admissions office at a university and our job is to accept or reject students.

2
00:00:07,339 --> 00:00:14,698
So, in order to evaluate students, we have two pieces of information, the results of a test and their grades in school.

3
00:00:14,698 --> 00:00:21,800
So, let's take a look at some sample students. We'll start with Student 1 who got 9 out of 10 in the test and 8 out of 10 in the grades.

4
00:00:21,800 --> 00:00:32,000
That student did quite well and got accepted. Then we have Student 2 who got 3 out of 10 in the test and 4 out of 10 in the grades, and that student got rejected.

5
00:00:32,000 --> 00:00:40,795
And now, we have a new Student 3 who got 7 out of 10 in the test and 6 out of 10 in the grades, and we're wondering if the student gets accepted or not.

6
00:00:40,795 --> 00:00:53,020
So, our first way to find this out is to plot students in a graph with the horizontal axis corresponding to the score on the test and the vertical axis corresponding to the grades, and the students would fit here.

7
00:00:53,020 --> 00:01:04,560
The students who got three and four gets located in the point with coordinates (3,4), and the student who got nine and eight gets located in the point with coordinates (9,8).

8
00:01:04,560 --> 00:01:12,075
And now we'll do what we do in most of our algorithms, which is to look at the previous data. This is how the previous data looks.

9
00:01:12,075 --> 00:01:20,859
These are all the previous students who got accepted or rejected. The blue points correspond to students that got accepted, and the red points to students that got rejected.

10
00:01:20,858 --> 00:01:29,805
So we can see in this diagram that the students would did well in the test and grades are more likely to get accepted, and the students who did poorly in both are more likely to get rejected.

11
00:01:29,805 --> 00:01:38,000
So let's start with a quiz. The quiz says, does the Student 3 get accepted or rejected? What do you think? Enter your answer below.


@@@
1
00:00:00,000 --> 00:00:12,210
Correct. Well, it seems that this data can be nicely separated by a line which is this line over here, and it seems that most students over the line get accepted and most students under the line get rejected.

2
00:00:12,210 --> 00:00:22,019
So this line is going to be our model. The model makes a couple of mistakes since there are a few blue points that are under the line and a few red points over the line.

3
00:00:22,019 --> 00:00:33,600
But we're not going to care about those. I will say that it's safe to predict that if a point is over the line the student gets accepted and if it's under the line then the student gets rejected.

4
00:00:33,600 --> 00:00:39,495
So based on this model we'll look at the new student that we see that they are over here at the point 7:6 which is above the line.

5
00:00:39,494 --> 00:00:47,070
So we can assume with some confidence that the student gets accepted. So if you answered yes, that's the correct answer.

6
00:00:47,070 --> 00:00:55,199
And now a question arises. The question is, how do we find this line? So we can kind of eyeball it. But the computer can't.

7
00:00:55,200 --> 00:01:04,000
We'll dedicate the rest of the session to show you algorithms that will find this line, not only for this example, but for much more general and complicated cases.


@@@
1
00:00:00,000 --> 00:00:11,329
So, first let's add some math. We're going to label the horizontal axis corresponding to the test by the variable x1, and the vertical axis corresponding to the grades by the variable x2.

2
00:00:11,330 --> 00:00:17,504
So this boundary line that separates the blue and the red points is going to have a linear equation.

3
00:00:17,504 --> 00:00:34,685
The one drawn has equation 2x1+x2-18=0. What does this mean? This means that our method for accepting or rejecting students simply says the following: take this equation as our score, the score is 2xtest+grades-18.

4
00:00:34,685 --> 00:00:42,710
Now when the student comes in, we check their score. If their score is a positive number, then we accept the student and if the score is a negative number then we reject the student.

5
00:00:42,710 --> 00:00:49,484
This is called a prediction. We can say by convention that if the score is 0, we'll accept a student although this won't matter much at the end.

6
00:00:49,484 --> 00:01:00,420
And that's it. That linear equation is our model. In the more general case, our boundary will be an equation of the following wx1+w2x2+b=0.

7
00:01:00,420 --> 00:01:09,835
We'll abbreviate this equation in vector notation as wx+b=0, where w is the vector w1w2 and x is the vector x1x2.

8
00:01:09,834 --> 00:01:19,000
And we simply take the product of the two vectors. We'll refer to x as the input, to w as the weights and b as the bias.

9
00:01:19,000 --> 00:01:26,189
Now, for a student coordinates x1x2, we'll denote a label as Y and the label is what we're trying to predict.

10
00:01:26,189 --> 00:01:36,809
So if the student gets accepted, namely the point is blue, then the label is Y+1. And if the student gets rejected, namely the point is red and then the label is Y=0.

11
00:01:36,810 --> 00:01:51,919
Thus, each point is in the form x1x2Y or Y is 1 for the blue points and 0 for the red points. And finally, our prediction is going to be called Y-hat and it will be what the algorithm predicts that the label will be.

12
00:01:51,920 --> 00:01:59,010
In this case, Y-hat is one of the algorithm predicts that the student gets accepted, which means the point lies over the line.

13
00:01:59,010 --> 00:02:05,864
And, Y-hat is 0 if the algorithm predicts that this didn't get rejected, which means the point is under the line.

14
00:02:05,864 --> 00:02:17,469
In math terms, this means that the prediction Y-hat is 1 if wx+b is greater than or equal to zero and 0 if wx+b is less than 0.

15
00:02:17,469 --> 00:02:24,810
So, to summarize, the points above the line have Y hat=1 and the points below the line have Y-hat=0.

16
00:02:24,810 --> 00:02:43,009
And, the blue points have Y=1 and the red points have Y=0. And, the goal of the algorithm is to have Y-hat resembling Y as closely as possible, which is exactly equivalent to finding the boundary line that keeps most of the blue points above it and most of the red points below it.


@@@
1
00:00:00,000 --> 00:00:08,070
Now, you may be wondering what happens if we have more data columns so not just testing grades, but maybe something else like the ranking of the student in the class.

2
00:00:08,070 --> 00:00:16,955
How do we fit three columns of data? Well the only difference is that now, we won't be working in two dimensions, we'll be working in three.

3
00:00:16,954 --> 00:00:26,899
So now, we have three axis: x_1 for the test, x_2 for the grades and x_3 for the class ranking. And our data will look like this, like a bunch of blue and red points flying around in 3D.

4
00:00:26,899 --> 00:00:34,009
On our equation won't be a line in two dimension, but a plane in three dimensions with a similar equation as before.

5
00:00:34,009 --> 00:00:43,950
Now, the equation would be w_1_x_1 plus w_2_x_2 plus w_3_x_3 plus b equals zero, which will separate this space into two regions.

6
00:00:43,950 --> 00:00:52,489
This equation can still be abbreviated by Wx plus b equals zero, except our vectors will now have three entries instead of two.

7
00:00:52,490 --> 00:01:00,825
And our prediction will still be y head equals one if Wx plus b is greater than or equal to zero, and zero if Wx plus b is less than zero.

8
00:01:00,825 --> 00:01:09,429
And what if we have many columns like say n of them? Well, it's the same thing. Now, our data just leaps in n-dimensional space.

9
00:01:09,430 --> 00:01:32,084
Now, I have trouble picturing things in more than three dimensions. But if we can imagine that the points are just things with n coordinates called x_1, x_2, x_3 all the way up to x_n with our labels being y, then our boundaries just an n minus one dimensional hyperplane, which is a high dimensional equivalent of a line in 2D or a plane in 3D.

10
00:01:32,084 --> 00:01:49,040
And the equation of this n minus one dimensional hyperplane is going to be w_1_x_1 plus w_2_x_2 plus all the way to w_n_x_n plus b equals zero, which we can still abbreviate to Wx plus b equals zero, where our vectors now have n entries.

11
00:01:49,040 --> 00:02:01,000
And our prediction is still the same as before. It is y head equals one if Wx plus b is greater than or equal to zero and y head equals zero if Wx plus b is less than zero.


@@@
1
00:00:00,000 --> 00:00:09,629
So let's recap. We have our data which is all these students. The blue ones have been accepted and the red ones have been rejected.

2
00:00:09,630 --> 00:00:22,140
And we have our model which consists of the equation two times test plus grades minus 18, which gives rise to this boundary which the point where the score is zero and a prediction.

3
00:00:22,140 --> 00:00:28,839
The prediction says that the student gets accepted of the score is positive or zero, and rejected if the score is negative.

4
00:00:28,839 --> 00:00:36,310
So now we'll introduce the notion of a preceptron, which is the building block of neural networks, and it's just an encoding of our equation into a small graph.

5
00:00:36,310 --> 00:00:42,475
The way we've build it is the following. Here we have our data and our boundary line and we fit it inside a node.

6
00:00:42,475 --> 00:00:52,399
And now we add small nodes for the inputs which, in this case, they are the test and the grades. Here we can see an example where test equals seven and grades equals six.

7
00:00:52,399 --> 00:00:59,259
And what the perceptron does is it blocks the points seven, six and checks if the point is in the positive or negative area.

8
00:00:59,259 --> 00:01:05,784
If the point is in the positive area, then it returns a yes. And if it is in the negative area, it returns and no.

9
00:01:05,784 --> 00:01:20,709
So let's recall that our equation is score equals two times test plus one times grade minus 18, and that our prediction consists of accepting the student if the score is positive or zero, and rejecting them if the score is negative.

10
00:01:20,709 --> 00:01:30,409
These weights two, one, and minus 18, are what define the linear equation, and so we'll use them as labels in the graph.

11
00:01:30,409 --> 00:01:38,655
The two and the one will label the edges coming from X1 and X 2 respectively, and the bias unit minus 18 will label the node.

12
00:01:38,655 --> 00:01:48,165
Thus, when we see a node with these labels, we can think of the linear equation they generate. Another way to grab this node is to consider the bias as part of the input.

13
00:01:48,165 --> 00:01:54,609
Now since W1 gets multiplied by X1 and W2 by X2, It's natural to think that B gets multiplied by a one.

14
00:01:54,609 --> 00:02:05,760
So we'll have the B labeling and and edge coming from a one. Then what the node does is it multiplies the values coming from the incoming nodes by the values and the corresponding edges.

15
00:02:05,760 --> 00:02:18,924
Then it adds them and finally, it checks if the result is greater that are equal to zero. If it is, then the node returns a yes or a value of one, and if it isn't then the node returns a no or a value of zero.

16
00:02:18,925 --> 00:02:23,474
We'll be using both notations throughout this class although the second one will be used more often.

17
00:02:23,474 --> 00:02:42,179
In the general case, this is how the nodes look. We will have our node over here then end inputs coming in with values X1 up to Xn and one, and edges with weights W1 up to Wn, and B corresponding to the bias unit.

18
00:02:42,180 --> 00:02:53,545
And then the node calculates the linear equation Wx plus B, which is a summation from I equals one to n, of WIXI plus B.

19
00:02:53,545 --> 00:03:05,844
This node then checks if the value is zero or bigger, and if it is, then the node returns a value of one for yes and if not, then it returns a value of zero for no.

20
00:03:05,844 --> 00:03:16,919
Note that we're using an implicit function, here, which is called a step function. What the step function does is it returns a one if the input is positive or zero, and a zero if the input is negative.

21
00:03:16,919 --> 00:03:28,782
So in reality, these perceptrons can be seen as a combination of nodes, where the first node calculates a linear equation and the inputs on the weights, and the second node applies the step function to the result.

22
00:03:28,782 --> 00:03:37,305
These can be graphed as follows: the summation sign represents a linear function in the first node, and the drawing represents a step function in the second node.

23
00:03:37,305 --> 00:03:43,385
In the future, we will use different step functions. So this is why it's useful to specify it in the node.

24
00:03:43,384 --> 00:03:54,370
So as we've seen there are two ways to represent perceptions. The one on the left has a bias unit coming from an input node with a value of one, and the one in the right has the bias inside the node.


@@@
1
00:00:00,000 --> 00:00:08,759
So you may be wondering why are these objects called neural networks. Well, the reason why they're called neural networks is because perceptions kind of look like neurons in the brain.

2
00:00:08,759 --> 00:00:20,504
In the left we have a perception with four inputs. The number is one, zero, four, and minus two. And what the perception does, it calculates some equations on the input and decides to return a one or a zero.

3
00:00:20,504 --> 00:00:27,058
In a similar way neurons in the brain take inputs coming from the dendrites. These inputs are nervous impulses.

4
00:00:27,059 --> 00:00:35,054
So what the neuron does is it does something with the nervous impulses and then it decides if it outputs a nervous impulse or not through the axon.

5
00:00:35,054 --> 00:00:46,130
The way we'll create neural networks later in this lesson is by concatenating these perceptions so we'll be mimicking the way the brain connects neurons by taking the output from one and turning it into the input for another one.


@@@
1
00:00:00,000 --> 00:00:08,980
So we had a question we're trying to answer and the question is, how do we find this line that separates the blue points from the red points in the best possible way?

2
00:00:08,980 --> 00:00:14,663
Let's answer this question by first looking at a small example with three blue points and three red points.

3
00:00:14,663 --> 00:00:21,980
And we're going to describe an algorithm that will find the line that splits these points properly. So the computer doesn't know where to start.

4
00:00:21,980 --> 00:00:31,609
It might as well start at a random place by picking a random linear equation. This equation will define a line and a positive and negative area given in blue and red respectively.

5
00:00:31,609 --> 00:00:39,399
What we're going to do is to look at how badly this line is doing and then move it around to try to get better and better.

6
00:00:39,399 --> 00:00:49,344
Now the question is, how do we find how badly this line is doing? So let's ask all the points. Here we have four points that are correctly classified.

7
00:00:49,344 --> 00:01:04,409
They are these two blue points in the blue area and these two red points in the red area. And these points are correctly classified, so they say, "I'm good." And then we have these two points that are incorrectly classified.

8
00:01:04,409 --> 00:01:15,754
That's this red point in the blue area and this blue point in the red area. We want to get as much information from them so we want them to tell us something so that we can improve this line.

9
00:01:15,754 --> 00:01:22,560
So what is it that they can tell us? So here we have a misclassified point, this red point in the blue area.

10
00:01:22,560 --> 00:01:31,084
Now think about this. If you were this point, what would you tell the line to do? Would you like it to come closer to you or farther from you?

11
00:01:31,084 --> 00:01:37,000
That's our quiz. Will the misclassified point want the line to come closer to it or farther from it?


@@@
1
00:00:00,000 --> 00:00:06,509
Well, consider this. If you're in the wrong area, you would like the line to go over you, in order to be in the right area.

2
00:00:06,509 --> 00:00:12,000
Thus, the points just come closer! So the line can move towards it and eventually classify it correctly.


@@@
1
00:00:00,000 --> 00:00:09,019
Now, let me show you a trick that will make a line go closer to a point. Let's say we have our linear equation for example, 3x1 + 4x2 -10.

2
00:00:09,019 --> 00:00:15,054
And that linear equation gives us a line which is the points where the equation is zero and two regions.

3
00:00:15,054 --> 00:00:26,750
The positive region drawn in blue where 3x1 + 4x2 - 10 is positive, and the negative region drawn in red with 3x1 + 4x2 - 10 is negative.

4
00:00:26,750 --> 00:00:35,039
So here we have our lonely misclassified point, the 0.4, 5 which is a red point in the blue area, and the point has to come closer.

5
00:00:35,039 --> 00:00:48,195
So how do we get that point to come closer to the line? Well, the idea is we're going to take the four and five and use them to modify the equation of the line in order to get the line to move closer to the point.

6
00:00:48,195 --> 00:00:57,640
So here are parameters of the line 3, 4 and -10 and the coordinates of the point are 4 and 5, and let's also add a one here for the bias unit.

7
00:00:57,640 --> 00:01:06,003
So what we'll do is subtract these numbers from the parameters of the line to get 3 - 4, 4 - 5, and -10 -1.

8
00:01:06,004 --> 00:01:19,009
The new line will have parameters -1, -1, -11. And this line will move drastically towards the point, possibly even going over it and placing it in the correct area.

9
00:01:19,010 --> 00:01:26,659
Now, since we have a lot of other points, we don't want to make any drastic moves since we may accidentally misclassify all our other points.

10
00:01:26,659 --> 00:01:33,099
We want the line to make a small move towards that point and for this, we need to take small steps towards the point.

11
00:01:33,099 --> 00:01:51,109
So here's where we introduce the learning rate, the learning rate is a small number for example, 0.1 and what we'll do is instead of subtracting four, five and one from the coordinates of the line, we'll multiply these numbers by 0.1 and then subtract them from the equation of the line.

12
00:01:51,109 --> 00:02:04,714
This means we'll be subtracting 0.4, 0.5, and 0.1 from the equation of the line. Obtaining a new equation of 2.6x1 + 3.5x 2 - 10.1 = 0.

13
00:02:04,715 --> 00:02:16,393
This new line will actually move closer to the point. In the same way, if we have a blue point in the red area, for example, the point 1,1 is a positively labeled point in the negative area.

14
00:02:16,393 --> 00:02:27,110
This point is also misclassified and it says, come closer. So what do we do here is the same thing, except now instead of subtracting the coordinates to the parameters of the line, we add them.

15
00:02:27,110 --> 00:02:40,500
Again, we multiply by the learning rate in order to make small steps. So here we take the coordinates of the point 1,1 and put an extra one for the constant term and now, we multiply them by the learning rates 0.1.

16
00:02:40,500 --> 00:02:50,640
Now, we add them to the parameters of the line and we get a new line with equation 3.1x1 + 4.1x2 - 9.9.

17
00:02:50,639 --> 00:02:59,000
And magic, this line is closer to the point. So that's the trick we're going to use repeatedly for the Perceptron Algorithm.


@@@
1
00:00:00,000 --> 00:00:11,494
Now, we finally have all the tools for describing the perceptron algorithm. We start with the random equation, which will determine some line, and two regions, the positive and the negative region.

2
00:00:11,494 --> 00:00:17,265
Now, we'll move this line around to get a better and better fit. So, we ask all the points how they're doing.

3
00:00:17,265 --> 00:00:31,484
The four correctly classified points say, "I'm good." And the two incorrectly classified points say, "Come closer." So, let's listen to the point in the right, and apply the trick to make the line closer to this point.

4
00:00:31,484 --> 00:00:45,094
So, here it is. Now, this point is good. Now, let's listen to the point in the left. The points says, "Come closer." We apply the trick, and now the line goes closer to it, and it actually goes over it classifying correctly.

5
00:00:45,094 --> 00:00:52,670
Now, every point is correctly classified and happy. So, let's actually write the pseudocode for this perceptron algorithm.

6
00:00:52,670 --> 00:01:02,004
We start with random weights, w1 up to wn and b. This gives us the question wx plus b, the line, and the positive and negative areas.

7
00:01:02,005 --> 00:01:23,664
Now, for every misclassified point with coordinates x1 up to xn, we do the following. If the prediction was zero, which means the point is a positive point in the negative area, then we'll update the weights as follows: for i equals 1 to n, we change wi, to wi plus alpha times xi, where alpha is the learning rate.

8
00:01:23,665 --> 00:01:33,840
In this case, we're using 0.1. Sometimes, we use 0.01 etc. It depends. Then we also change the bi as unit to b plus alpha.

9
00:01:33,840 --> 00:01:46,950
That moves the line closer to the misclassified point. Now, if the prediction was one, which means a point is a negative point in the positive area, then we'll update the weights in a similar way, except we subtract instead of adding.

10
00:01:46,950 --> 00:01:57,995
This means for i equals 1, change wi, to wi minus alpha xi, and change the bi as unit b to b minus alpha.

11
00:01:57,995 --> 00:02:07,425
And now, the line moves closer to our misclassified point. And now, we just repeat this step until we get no errors, or until we have a number of error that is small.

12
00:02:07,424 --> 00:02:14,000
Or simply we can just say, do the step a thousand times and stop. We'll see what are our options later in the class.


@@@
1
00:00:00,000 --> 00:00:08,905
Okay, so let's look more carefully at this model for accepting and rejecting students. Let's say we have this student four, who got nine in the test, but only one on the grades.

2
00:00:08,906 --> 00:00:15,525
According to our model this student gets accepted since it's placed over here in the positive region of this line.

3
00:00:15,525 --> 00:00:22,190
But let's say we don't want that since we'll say, "If your grades were terrible, no matter what you got on the test, you won't get accepted".

4
00:00:22,190 --> 00:00:32,329
So our data should look more like this instead. This model is much more realistic but now we have a problem which is the data can no longer be separated by just a line.

5
00:00:32,329 --> 00:00:40,574
So what is the next thing after a line? Maybe a circle. A circle would work. Maybe two lines. That could work, too.

6
00:00:40,575 --> 00:00:49,795
Or maybe a curve like this. That would also work. So let's go with that. Let's go with the curve. Now, unfortunately, the perceptron algorithm won't work for us this time.

7
00:00:49,795 --> 00:01:00,000
We'll have to come up with something more complex and actually the solution will be, we need to redefine our perceptron algorithm for a line in a way that it'll generalize to other types of curves.


@@@
1
00:00:00,000 --> 00:00:09,419
So the way we'll solve our problems from now on is with the help of an error function. An error function is simply something that tells us how far we are from the solution.

2
00:00:09,419 --> 00:00:18,114
For example, if I'm here and my goal is to get to this plant, an error function will just tell me the distance from the plant.

3
00:00:18,114 --> 00:00:28,875
My approach would then be to look around myself, check in which direction I can take a step to get closer to the plant, take that step and then repeat.


@@@
1
00:00:00,000 --> 00:00:12,425
Here is obvious realization of the error function. We're standing on top a mountain, Mount Errorest and I want to descend but it's not that easy because it's cloudy and the mountain is very big, so we can't really see the big picture.

2
00:00:12,425 --> 00:00:18,234
What we'll do to go down is we'll look around us and we consider all the possible directions in which we can walk.

3
00:00:18,234 --> 00:00:26,100
Then we pick a direction that makes us descend the most. Let's say it's this one over here. So we take a step in that direction.

4
00:00:26,100 --> 00:00:39,210
Thus, we've decreased the height. Once we take the step and we start the process again and again always decreasing the height until we go all the way down the mountain, minimizing the height.

5
00:00:39,210 --> 00:00:45,533
In this case the key metric that we use to solve the problem is the height. We'll call the height the error.

6
00:00:45,533 --> 00:00:51,234
The error is what's telling us how badly we're doing at the moment and how far we are from an ideal solution.

7
00:00:51,234 --> 00:00:57,715
And if we constantly take steps to decrease the error then we'll eventually solve our problem, descending from Mt.

8
00:00:57,715 --> 00:01:06,968
Errorest. Some of you may be thinking, wait, that doesn't necessarily solve the problem. What if I get stuck in a valley, a local minimum, but that's not the bottom of the mountain.

9
00:01:06,968 --> 00:01:11,870
This happens a lot in machine learning and we'll see different ways to solve it later in this Nanodegree.

10
00:01:11,870 --> 00:01:18,129
It's also worth noting that many times a local minimum will give us a pretty good solution to a problem.

11
00:01:18,129 --> 00:01:25,424
This method, which we'll study in more detail later, is called gradient descent. So let's try that approach to solve a problem.

12
00:01:25,424 --> 00:01:30,634
What would be a good error function here? What would be a good way to tell the computer how badly it's doing?

13
00:01:30,634 --> 00:01:38,385
Well, here's our line with our positive and negative area. And the question is how do we tell the computer how far it is from a perfect solution?

14
00:01:38,385 --> 00:01:44,810
Well, maybe we can count the number mistakes. There are two mistakes here. So that's our height. That's our error.

15
00:01:44,810 --> 00:01:52,665
So just as we did to descend from the mountain, we look around all the directions in which we can move the line in order to decrease our error.

16
00:01:52,665 --> 00:02:00,810
So let's say we move in this direction. We'll decrease the number of errors to one and then if we're moving in that direction, we'll decrease the number of errors to zero.

17
00:02:00,810 --> 00:02:14,990
And then we're done, right? Well, almost. There's a small problem with that approach. In our algorithms we'll be taking very small steps and the reason for that is calculus, because our tiny steps will be calculated by derivatives.

18
00:02:14,990 --> 00:02:23,631
So what happens if we take very small steps here? We start with two errors and then move a tiny amount and we're still at two errors.

19
00:02:23,631 --> 00:02:30,784
Then move a tiny amount again and we're still two errors. Another tiny amount and we're still at two and again and again.

20
00:02:30,783 --> 00:02:37,900
So not much we can do here. This is equivalent to using gradient descent to try to descend from an Aztec pyramid with flat steps.

21
00:02:37,900 --> 00:02:46,909
If we're standing here in the second floor, for the two errors and we look around ourselves, we'll always see two errors and we'll get confused and not know what to do.

22
00:02:46,908 --> 00:02:56,224
On the other hand in Mt. Errorest we can detect very small variations in height and we can figure out in what direction it can decrease the most.

23
00:02:56,223 --> 00:03:04,818
In math terms this means that in order for us to do gradient descent our error function can not be discrete, it should be continuous.

24
00:03:04,818 --> 00:03:15,699
Mt. Errorest is continuous since small variations in our position will translate to small variations in the height but the Aztec pyramid does not since the high jumps from two to one and then from one to zero.

25
00:03:15,699 --> 00:03:26,484
As a matter of fact, our error function needs to be differentiable, but we'll see that later. So, what we need to do here is to construct an error function that is continuous and we'll do this as follows.

26
00:03:26,485 --> 00:03:40,710
So here are six points with four of them correctly classified, that's two blue and two red, and two of them incorrectly classified, that is this red point at the very left and this blue point at the very right.

27
00:03:40,710 --> 00:03:50,150
The error function is going to assign a large penalty to the two incorrectly classified points and small penalties to the four correctly classified points.

28
00:03:50,150 --> 00:04:02,938
Here we are representing the size of the point as the penalty. The penalty is roughly the distance from the boundary when the point is misclassified and almost zero when the point is correctly classified.

29
00:04:02,938 --> 00:04:10,729
We'll learn the formula for the error later in the class. So, now we obtain the total error by adding all the errors from the corresponding points.

30
00:04:10,729 --> 00:04:19,509
Here we have a large number so it is two misclassified points add a large amount to the error. And the idea now is to move the line around in order to decrease these error.

31
00:04:19,509 --> 00:04:28,129
But now we can do it because we can make very tiny changes to the parameters of the line which will amount to very tiny changes in the error function.

32
00:04:28,129 --> 00:04:43,360
So, if you move the line, say, in this direction, we can see that some errors decrease, some slightly increase, but in general when we consider the sum, the sum gets smaller and we can see that because we've now correctly classified the two points that were misclassified before.

33
00:04:43,360 --> 00:04:51,240
So once we are able to build an error function with this property, we can now use gradient descent to solve our problem.

34
00:04:51,240 --> 00:04:57,810
So here's the full picture. Here we are at the summit of Mt. Errorest. We're quite high up because our error is large.

35
00:04:57,810 --> 00:05:14,629
As you can see the error is the height which is the sum of the blue and red areas. We explore around to see what direction brings us down the most, or equivalently, what direction can we move the line to reduce the error the most, and we take a step in that direction.

36
00:05:14,629 --> 00:05:22,684
So in the mountain we go down one step and in the graph we've reduced the error a bit by correctly classifying one of the points. And now we do it again.

37
00:05:22,684 --> 00:05:32,365
We calculate the error, we look around ourselves to see in what direction we descend the most, we take a step in that direction and that brings us down the mountain.

38
00:05:32,365 --> 00:05:45,430
So on the left we have reduced the height and successfully descended from the mountain and on the right we have reduced the error to its minimum possible value and successfully classified our points.


@@@
1
00:00:00,000 --> 00:00:10,814
In the last section we pointed out the difference between a discrete and a continuous error function and discovered that in order for us to use gradient descent we need a continuous error function.

2
00:00:10,814 --> 00:00:18,190
In order to do this we also need to move from discrete predictions to continuous predictions. Let me show you what I mean by that.


@@@
1
00:00:00,000 --> 00:00:06,220
The prediction is basically the answer we get from the algorithm. A discreet answer will be of the form yes, no.

2
00:00:06,219 --> 00:00:11,615
Whereas a continued answer will be a number, normally between zero and one which we'll consider a probability.

3
00:00:11,615 --> 00:00:25,679
In the running example, here we have our students where blue is accepted and red is rejected. And the discrete algorithm will tell us if a student is accepted or rejected by typing a zero for rejected students and a one for accepted students.

4
00:00:25,679 --> 00:00:32,505
On the other hand, the farther our point is from the black line, the more drastic these probabilities are.

5
00:00:32,505 --> 00:00:40,050
Points that are well into the blue area get very high probabilities, such as this point with an 85% probability of being blue.

6
00:00:40,049 --> 00:00:48,159
And points that are well into the red region are given very low probabilities, such as this point on the bottom that is given a 20% probability of being blue.

7
00:00:48,159 --> 00:00:57,335
The points over the line are all given a 50% probability of being blue. As you can see the probability is a function of the distance from the line.

8
00:00:57,335 --> 00:01:07,655
The way we move from discrete predictions to continuous, is to simply change your activation function from the step function in the left, to the sigmoid function on the right.

9
00:01:07,655 --> 00:01:14,629
The sigmoid function is simply a function which for large positive numbers will give us values very close to one.

10
00:01:14,629 --> 00:01:25,599
For large negative numbers will give us values very close to zero. And for numbers that are close to zero, it'll give you values that are close to point five.

11
00:01:25,599 --> 00:01:38,245
The formula is sigmoid effects equals σ(x) = 1/(1 + exp(-x)) So, before our model consisted of a line with a positive region and a negative region.

12
00:01:38,245 --> 00:01:47,969
Now it consists of an entire probability space or for each point in the plane we are given the probability that the label of the point is one for the blue points, and zero for the red points.

13
00:01:47,969 --> 00:02:02,975
For example, for this point the probability of being blue is 50% and of being red is 50%. For this point, the probabilities are 40% for being blue, and 60% for being red.

14
00:02:02,974 --> 00:02:14,655
For this one over here it's 30% for blue, and 70% for red. And for this point all over here is 80% for being blue and 25 percent for being red.

15
00:02:14,655 --> 00:02:22,800
The way we obtain this probability space is very simple. We just combine the linear function WX + b with the sigmoid function.

16
00:02:22,800 --> 00:02:32,630
So in the left we have the lines that represent the points for which WX + b is zero, one, two, minus one, minus two, etc.

17
00:02:32,629 --> 00:02:40,515
And once we apply the sigmoid function to each of these values in the plane, we then obtain numbers from zero to one for each point.

18
00:02:40,514 --> 00:02:50,454
These numbers are just the probabilities of the point being blue. The probability of the point being blue is a prediction of the model Y hat to sigmoid of W x plus b.

19
00:02:50,455 --> 00:02:58,419
Here we can see the lines for which the prediction is point five, point six, point seven, point four, point three, et cetera.

20
00:02:58,419 --> 00:03:11,115
As you can see, as we get more into the blue area, σ(Wx + b) gets closer and closer to one. And as we move into the red area, σ(Wx + b) gets closer and closer to zero.

21
00:03:11,115 --> 00:03:19,319
When we're over the main line, W x plus b is zero, which means sigmoid of W s plus b is exactly zero point five.

22
00:03:19,319 --> 00:03:29,944
So here on the left we have our old perceptron with the activation function as a step function. And on the right we have our new perceptron, where the activation function is the sigmoid function.

23
00:03:29,944 --> 00:03:40,935
What our new perceptron does, it takes the inputs, multiplies them by the weights in the edges and adds the results, then applies the sigmoid function.

24
00:03:40,935 --> 00:03:49,984
So instead of returning one and zero like before it returns values between zero and one such as 0.99 or 0.67 etc.

25
00:03:49,985 --> 00:03:58,000
Before it used to say the student got accepted or not, and now it says the probability of the student got accepted is this much.


@@@
1
00:00:00,000 --> 00:00:07,099
So far we have models that give us an answer of yes/no or the probability of a label being positive or negative.

2
00:00:07,099 --> 00:00:14,160
What if we have more classes? What if we want our model to tell us if something is red, blue, yellow or dog, cat, bird?


@@@
1
00:00:00,000 --> 00:00:06,365
Let's switch to a different example for a moment. Let's say we have a model that will predict if you receive a gift or not.

2
00:00:06,365 --> 00:00:18,759
So, the model use predictions in the following way. It says, the probability that you get a gift is 0.8, which automatically implies that the probability that you don't receive a gift is 0.2.

3
00:00:18,760 --> 00:00:26,064
And what does the model do? What the model does is take some inputs. For example, is it your birthday or have it been good all year?

4
00:00:26,065 --> 00:00:37,350
And based on those inputs, it calculates a linear model which would be the score. Then, the probability that you get the gift or not is simply the sigmoid function applied to that score.

5
00:00:37,350 --> 00:00:49,890
Now, what if you had more options than just getting a gift or not a gift? Let's say we have a model that just tell us what animal we just saw, and the options are a duck, a beaver and a walrus.

6
00:00:49,890 --> 00:01:02,689
We want a model that tells an answer along the lines of, the probability of a duck is 0.67, the probability of a beaver is 0.24, and the probability of a walrus is 0.09.

7
00:01:02,689 --> 00:01:09,377
Notice that the probabilities need to add to one. Let's say we have a linear model based on some inputs.

8
00:01:09,378 --> 00:01:17,599
The inputs could be, does it have a beak or not? Number of teeth. Number of feathers. Hair, no hair. Does it live in the water? Does it fly?

9
00:01:17,599 --> 00:01:30,975
Etc. We calculate linear function based on those inputs, and let's say we get some scores. So, the duck gets a score of two, and the beaver gets a score of one, and the walrus gets a score of zero.

10
00:01:30,974 --> 00:01:41,539
And now the question is, how do we turn these scores into probabilities? The first thing we need to satisfy with probabilities is as we said, they need to add to one.

11
00:01:41,540 --> 00:02:02,150
So the two, the one, and the zero do not add to one. The second thing we need to satisfy is, since the duck had a higher score than the beaver and the beaver had a higher score than the walrus, then we want the probability of the duck to be higher than the probability of the beaver, and the probability of the beaver to be higher than the probability of the walrus.

12
00:02:02,150 --> 00:02:20,644
Here's a simple way of doing it. Let's take each score and divide it by the sum of all the scores. The two becomes two divided by two plus one plus zero, the one becomes one divided by two plus one plus zero, and the zero becomes zero divided by two plus one plus zero.

13
00:02:20,645 --> 00:02:29,419
This kind of works because the probabilities we obtain are two thirds for the duck, one third for the beaver, and zero for the walrus.

14
00:02:29,419 --> 00:02:35,610
That works but there's a little problem. Let's think about it. What could this problem be? The problem is the following.

15
00:02:35,610 --> 00:02:43,250
What happens if our scores are negative? This is completely plausible since the scores are linear function which could give negative values.

16
00:02:43,250 --> 00:02:55,930
What if we had, say, scores of 1, 0 and (-1)? Then, one of the probabilities would turn into one divided by one plus zero plus minus one which is zero, and we know very well that we cannot divide by zero.

17
00:02:55,930 --> 00:03:06,694
This unfortunately won't work, but the idea is good. How can we turn this idea into one that works all the time even for negative numbers?

18
00:03:06,694 --> 00:03:14,299
Well, it's almost like we need to turn these scores into positive scores. How do we do this? Is there a function that can help us?

19
00:03:14,300 --> 00:03:26,219
This is the quiz. Let's look at some options. There's sine, cosine, logarithm, and exponential. Quiz. Which one of these functions will turn every number into a positive number?


@@@
1
00:00:00,000 --> 00:00:06,115
So, if you said exponential, you are correct. Because this is a function that returns a positive number for every input.

2
00:00:06,115 --> 00:00:15,089
E to the X is always a positive number. So, what we're going to do is exactly what we did before, except, applying it to the X to the scores.

3
00:00:15,089 --> 00:00:25,774
So, instead of 2,1, 0, we have E to the 2, E to the 1 and E to the 0. So, that 2 becomes E to the 2 divided by E to the two plus E to the 1 plus E to the 0.

4
00:00:25,774 --> 00:00:36,910
And, similarly for 1 and 0. So, the probabilities we obtain now are as 0.67, 0.24 and 0.09. This clearly add to 1.

5
00:00:36,909 --> 00:00:43,009
And, also notice that since the exponential function is increasing, then the duck has a higher probability than the beaver.

6
00:00:43,009 --> 00:00:51,289
And this one has a higher probability than the walrus. This function is called the Softmax function and it's defined formally like this.

7
00:00:51,289 --> 00:00:59,005
Let's say we have N classes and a linear model that gives us the following scores. Z1, Z2, up to ZN.

8
00:00:59,005 --> 00:01:15,210
Each score for each of the classes. What we do to turn them into probabilities is to say the probability that the object is in class I is going to be E to the power of the ZI divided by the sum of E to the power of Z1 plus all the way to E to the power ZN.

9
00:01:15,209 --> 00:01:24,234
That's how we turn scores into probabilities. So, here's a question for you. When we had two classes, we applied the sigmoid function to the scores.

10
00:01:24,234 --> 00:01:33,424
Now, that we have more classes we apply the softmax function to the scores. The question is, is the softmax function for N equals to the same as the sigmoid function?

11
00:01:33,424 --> 00:01:39,000
I'll let you think about it. The answer is actually, yes, but it's not super trivial why. And, it's a nice thing to remember.


@@@
1
00:00:00,000 --> 00:00:12,600
So, as we've seen so far, all our algorithms are numerical. This means we need to input numbers, such as a score in a test or the grades, but the input data will not always look like numbers.

2
00:00:12,599 --> 00:00:18,640
Sometimes it looks like this. Let's say the module receives as an input the fact that you got a gift or didn't get a gift.

3
00:00:18,640 --> 00:00:24,994
How do we turn that into numbers? Well, that's easy. If you've got a gift, we'll just say that the input variable is 1.

4
00:00:24,995 --> 00:00:34,515
And, if you didn't get a gift, we'll just say that the input variable is 0. But, what if we have more classes as before or, let's say, our classes are Duck, Beaver and Walrus?

5
00:00:34,515 --> 00:00:45,287
What variable do we input in the algorithm? Maybe, we can input a 0 or 1 and a 2, but that would not work because it would assume dependencies between the classes that we can't have. So, this is what we do.

6
00:00:45,287 --> 00:00:50,739
What we do is, we come up with one variable for each of the classes. So, our table becomes like this.

7
00:00:50,740 --> 00:00:58,109
That's one variable for Duck, one for Beaver and one for Walrus. And, each one has its corresponding column.

8
00:00:58,109 --> 00:01:06,780
Now, if the input is a duck then the variable for duck is 1 and the variables for beaver and walrus are 0.

9
00:01:06,780 --> 00:01:14,770
Similarly for the beaver and the walrus. We may have more columns of data but at least there are no unnecessary dependencies.


@@@
1
00:00:00,000 --> 00:00:05,525
So we're still in our quest for an algorithm that will help us pick the best model that separates our data.

2
00:00:05,525 --> 00:00:12,894
Well, since we're dealing with probabilities then let's use them in our favor. Let's say I'm a student and I have two models.

3
00:00:12,894 --> 00:00:20,859
One that tells me that my probability of getting accepted is 80% and one that tells me the probability is 55%.

4
00:00:20,859 --> 00:00:28,210
Which model looks more accurate? Well, if I got accepted then I'd say the better model is probably the one that says 80%.

5
00:00:28,210 --> 00:00:33,983
What if I didn't get accepted? Then the more accurate model is more likely the one that says 55 percent.

6
00:00:33,984 --> 00:00:47,085
But I'm just one person. What if it was me and a friend? Well, the best model would more likely be the one that gives the higher probabilities to the events that happened to us, whether it's acceptance or rejection.

7
00:00:47,085 --> 00:00:58,353
This sounds pretty intuitive. The method is called maximum likelihood. What we do is we pick the model that gives the existing labels the highest probability.


@@@
1
00:00:00,000 --> 00:00:10,740
So let me be more specific. Let's look at the following four points: two blue and two red and two models that classify them, the one on the left and the one on the right.

2
00:00:10,740 --> 00:00:26,625
Quick. Which model looks better? You are correct. The model on the right is much better since it classifies the four points correctly whereas the model in the left gets two points correctly and two points incorrectly.

3
00:00:26,625 --> 00:00:37,289
But let's see why the model in the right is better from the probability perspective. And by that, we'll show you that the arrangement in the right is much more likely to happen than the one in the left.

4
00:00:37,289 --> 00:00:48,240
So let's recall that our prediction is ŷ = σ(Wx+b) and that that is precisely the probability of a point being labeled positive which means blue.

5
00:00:48,240 --> 00:00:56,835
So for the points in the figure, let's say the model tells you that the probability of being blue are 0.9, 0.6, 0.3, and 0.2.

6
00:00:56,835 --> 00:01:03,649
Notice that the points in the blue region are much more likely to be blue and the points in the red region are much less likely to be blue.

7
00:01:03,649 --> 00:01:16,319
Now obviously, the probability of being red is one minus the probability of being blue. So in this case, the probability of some of the points being red are 0.1, 0.4, 0.7 and 0.8.

8
00:01:16,319 --> 00:01:23,844
Now what we want to do is we want to calculate the probability of the four points are of the colors that they actually are.

9
00:01:23,843 --> 00:01:39,290
This means the probability that the two red points are red and that the two blue points are blue. Now if we assume that the colors of the points are independent events then the probability for the whole arrangement is the product of the probabilities of the four points.

10
00:01:39,290 --> 00:02:00,388
This is equal to 0.1 × 0.6 × 0.7 × 0.2 = 0.0084. This is very small. It's less than 1%. What we mean by this is that if the model is given by these probability spaces, then the probability that the points are of these colors is 0.0084.

11
00:02:00,388 --> 00:02:10,485
Now let's do this for both models. As we saw the model on the left tells us that the probabilities of these points being of those colors is 0.0084.

12
00:02:10,485 --> 00:02:23,259
If we do the same thing for the model on the right. Let's say we get that the probabilities of the two points in the right being blue are 0.7 and 0.9 and of the two points in the left being red are 0.8 and 0.6.

13
00:02:23,258 --> 00:02:39,478
When we multiply these we get 0.3024 which is around 30%. This is much higher than 0.0084. Thus, we confirm that the model on the right is better because it makes the arrangement of the points much more likely to have those colors.

14
00:02:39,479 --> 00:02:50,780
So now, what we do is the following? We start from the bad modeling, calculate the probability that the points are those colors, multiply them and we obtain the total probability is 0.0084.

15
00:02:50,780 --> 00:03:03,104
Now if we just had a way to maximize this probability we can increase it all the way to 0.3024. Thus, our new goal becomes precisely that, to maximize this probability.


@@@
1
00:00:00,000 --> 00:00:08,969
Well we're getting somewhere now. We've concluded that the probability is important. And that the better model will give us a better probability.

2
00:00:08,970 --> 00:00:20,829
Now the question is, how we maximize the probability. Also, if remember correctly we're talking about an error function and how minimizing this error function will take us to the best possible solution.

3
00:00:20,829 --> 00:00:34,000
Could these two things be connected? Could we obtain an error function from the probability? Could it be that maximizing the probability is equivalent to minimizing the error function? Maybe.


@@@
1
00:00:00,000 --> 00:00:11,730
So a quick recap. We have two models, the bad one on the left and the good one on the right. And the way to tell they're bad or good is to calculate the probability of each point being the color it is according to the model.

2
00:00:11,730 --> 00:00:20,960
Multiply these probabilities in order to obtain the probability of the whole arrangement and then check that the model on the right gives us a much higher probability than the model on the left.

3
00:00:20,960 --> 00:00:29,690
Now all we need to do is to maximize this probability. But probability is a product of numbers and products are hard.

4
00:00:29,690 --> 00:00:35,642
Maybe this product of four numbers doesn't look so scary. But what if we have thousands of datapoints?

5
00:00:35,642 --> 00:00:50,545
That would correspond to a product of thousands of numbers, all of them between zero and one. This product would be very tiny, something like 0.0000 something and we definitely want to stay away from those numbers.

6
00:00:50,545 --> 00:00:57,630
Also, if I have a product of thousands of numbers and I change one of them, the product will change drastically.

7
00:00:57,630 --> 00:01:07,265
In summary, we really want to stay away from products. And what's better than products? Well, let's ask our friend here.

8
00:01:07,265 --> 00:01:19,939
Products are bad, but sums are good. Let's do sums. So let's try to turn these products into sums. We need to find a function that will help us turn products into sums.

9
00:01:19,938 --> 00:01:26,745
What would this function be? It sounds like it's time for a quiz. Quiz. Which function will help us out here?


@@@
1
00:00:00,000 --> 00:00:11,929
Correct. The answer is logarithm, because logarithm has this very nice identity that says that the logarithm of the product A times B is the sum of the logarithms of A and B.

2
00:00:11,929 --> 00:00:21,854
So this is what we do. We take our products and we take the logarithms, so now we get a sum of the logarithms of the factors.

3
00:00:21,855 --> 00:00:40,040
So the ln(0.6*0.2*0.1*0.7) is equal to ln(0.6) + ln(0.2) + ln(0.1) + ln(0.7) etc. Now from now until the end of class, we'll be taking the natural logarithm which is base e instead of 10.

4
00:00:40,039 --> 00:00:44,945
Nothing different happens with base 10. Everything works the same as everything gets scaled by the same factor.

5
00:00:44,945 --> 00:00:58,164
So it's just more for convention. We can calculate those values and get minus 0.51, minus 1.61, minus 0.23 etc. Notice that they are all negative numbers and that actually makes sense.

6
00:00:58,164 --> 00:01:05,594
This is because the logarithm of a number between 0 and 1 is always a negative number since the logarithm of one is zero.

7
00:01:05,594 --> 00:01:11,260
So it actually makes sense to think of the negative of the logarithm of the probabilities and we'll get positive numbers.

8
00:01:11,260 --> 00:01:23,180
So that's what we'll do. We'll take the negative of the logarithm of the probabilities. That sums up negatives of logarithms of the probabilities, we'll call the cross entropy which is a very important concept in the class.

9
00:01:23,180 --> 00:01:30,255
If we calculate the cross entropies, we see that the bad model on left has a cross entropy 4.8 which is high.

10
00:01:30,254 --> 00:01:37,454
Whereas the good model on the right has a cross entropy of 1.2 which is low. This actually happens all the time.

11
00:01:37,454 --> 00:01:52,599
A good model will give us a low cross entropy and a bad model will give us a high cross entropy. The reason for this is simply that a good model gives us a high probability and the negative of the logarithm of a large number is a small number and vice versa.

12
00:01:52,599 --> 00:02:01,470
This method is actually much more powerful than we think. If we calculate the probabilities and pair the points with the corresponding logarithms, we actually get an error for each point.

13
00:02:01,469 --> 00:02:17,859
So again, here we have probabilities for both models and the products of them. Now, we take the negative of the logarithms which gives us sum of logarithms and if we pair each logarithm with the point where it came from, we actually get a value for each point.

14
00:02:17,860 --> 00:02:36,544
And if we calculate the values, we get this. Check it out. If we look carefully at the values we can see that the points that are mis-classified has like values like 2.3 for this point or 1.6 one for this point, whereas the points that are correctly classified have small values.

15
00:02:36,544 --> 00:02:46,915
And the reason for this is again is that a correctly classified point will have a probability that as close to 1, which when we take the negative of the logarithm, we'll get a small value.

16
00:02:46,914 --> 00:02:57,594
Thus we can think of the negatives of these logarithms as errors at each point. Points that are correctly classified will have small errors and points that are mis-classified will have large errors.

17
00:02:57,594 --> 00:03:12,580
And now we've concluded that our cross entropy will tell us if a model is good or bad. So now our goal has changed from maximizing a probability to minimizing a cross entropy in order to get from the model in left to the model in the right.


@@@
1
00:00:00,000 --> 00:00:13,350
So this cross entropy, it looks like kind of a big deal. Cross entropy really says the following. If I have a bunch of events and a bunch of probabilities, how likely is it that those events happen based on the probabilities?

2
00:00:13,349 --> 00:00:21,299
If it's very likely, then we have a small cross entropy. If it's unlikely, then we have a large cross entropy. Let's elaborate.


@@@
1
00:00:02,440 --> 00:00:08,935
Let's look a bit closer into Cross-Entropy by switching to a different example. Let's say we have three doors.

2
00:00:08,935 --> 00:00:18,720
And no this is not the Monty Hall problem. We have the green door, the red door, and the blue door, and behind each door we could have a gift or not have a gift.

3
00:00:18,720 --> 00:00:26,900
And the probabilities of there being a gift behind each door is 0.8 for the first one, 0.7 for the second one, 0.1 for the third one.

4
00:00:26,900 --> 00:00:36,780
So for example behind the green door there is an 80 percent probability of there being a gift, and a 20 percent probability of there not being a gift.

5
00:00:36,780 --> 00:00:46,630
So we can put the information in this table where the probabilities of there being a gift are given in the top row, and the probabilities of there not being a gift are given in the bottom row.

6
00:00:46,630 --> 00:00:53,375
So let's say we want to make a bet on the outcomes. So we want to try to figure out what is the most likely scenario here.

7
00:00:53,375 --> 00:01:03,440
And for that we'll assume they're independent events. In this case, the most likely scenario is just obtained by picking the largest probability in each column.

8
00:01:03,440 --> 00:01:09,230
So for the first door is more likely to have a gift than not have a gift. So we'll say there's a gift behind the first door.

9
00:01:09,230 --> 00:01:14,995
For the second door, it's also more likely that there's a gift. So we'll say there's a gift behind the second door.

10
00:01:14,995 --> 00:01:21,015
And for the third door it's much more likely that there's no gift, so we'll say there's no gift behind the third door.

11
00:01:21,015 --> 00:01:36,665
And as the events are independent, the probability for this whole arrangement is the product of the three probabilities which is 0.8, times 0.7, times 0.9, which ends up being 0.504, which is roughly 50 percent.

12
00:01:36,665 --> 00:01:48,815
So let's look at all the possible scenarios in the table. Here's a table with all the possible scenarios for each door and there are eight scenarios since each door gives us two possibilities each, and there are three doors.

13
00:01:48,815 --> 00:01:57,245
So we do as before to obtain the probability of each arrangement by multiplying the three independent probabilities to get these numbers.

14
00:01:57,245 --> 00:02:05,955
You can check that these numbers add to one. And from last video we learned that the negative of the logarithm of the probabilities across entropy.

15
00:02:05,955 --> 00:02:16,345
So let's go ahead and calculate the cross-entropy. And notice that the events with high probability have low cross-entropy and the events with low probability have high cross-entropy.

16
00:02:16,345 --> 00:02:34,441
For example, the second row which has probability of 0.504 gives a small cross-entropy of 0.69, and the second to last row which is very very unlikely has a probability of 0.006 gives a cross entropy a 5.12.

17
00:02:34,441 --> 00:02:46,445
So let's actually calculate a formula for the cross-entropy. Here we have our three doors, and our sample scenario said that there is a gift behind the first and second doors, and no gift behind the third door.

18
00:02:46,445 --> 00:02:57,915
Recall that the probabilities of these events happening are 0.8 for a gift behind the first door, 0.7 for a gift behind the second door, and 0.9 for no gift behind the third door.

19
00:02:57,915 --> 00:03:14,070
So when we calculate the cross-entropy, we get the negative of the logarithm of the product, which is a sum of the negatives of the logarithms of the factors, which is negative logarithm of 0.8 minus logarithm of 0.7 minus logarithm 0.9.

20
00:03:14,070 --> 00:03:27,940
And in order to drive the formula we'll have some variables. So let's call P1 the probability that there's a gift behind the first door, P2 the probability there's a gift behind the second door, and P3 the probability there's a gift behind the third door.

21
00:03:27,940 --> 00:03:41,460
So this 0.8 here is P1, this 0.7 here is P2, and this 0.9 here is one minus P3. So it's a probability of there not being a gift is one minus the probability of there being a gift.

22
00:03:41,460 --> 00:03:49,750
Let's have another variable called Yi, which will be one of there's a present behind the ith door, and zero there's no present.

23
00:03:49,750 --> 00:04:00,210
So Yi is technically a number of presents behind the ith door. In this case Y1 equals one, Y2 equals one, and Y3 equals zero.

24
00:04:00,210 --> 00:04:08,305
So we can put all this together and derive a formula for the cross-entropy and it's this sum. Now let's look at the formula inside the summation.

25
00:04:08,305 --> 00:04:17,180
Noted that if there is a present behind the ith door, then Yi equals one. So the first term is logarithm of the Pi.

26
00:04:17,180 --> 00:04:28,355
And the second term is zero. Likewise, if there is no present behind the ith door, then Yi is zero. So this first term is zero.

27
00:04:28,355 --> 00:04:39,935
And this term is precisely logarithm of one minus Pi. Therefore, this formula really encompasses the sums of the negative of logarithms which is precisely the cross-entropy.

28
00:04:39,935 --> 00:04:53,485
So the cross-entropy really tells us when two vectors are similar or different. For example, if you calculate the cross entropy of the pair one one zero, and 0.8, 0.7, 0.1, we get 0.69.

29
00:04:53,485 --> 00:05:11,715
And that is low because one one zero is a similar vector to 0.8, 0.7, 0.1. Which means that the arrangement of gifts given by the first set of numbers is likely to happen based on the probabilities given by the second set of numbers.

30
00:05:11,715 --> 00:05:23,210
But on the other hand if we calculate the cross-entropy of the pairs zero zero one, and 0.8, 0.7, 0.1, that is 5.12 which is very high.

31
00:05:23,210 --> 00:05:32,030
This is because the arrangement of gifts being given by the first set of numbers is very unlikely to happen from the probabilities given by the second set of numbers.


@@@
1
00:00:00,000 --> 00:00:09,285
Now that was when we had two classes namely receiving a gift or not receiving a gift. What happens if we have more classes? Let's take a look.

2
00:00:09,285 --> 00:00:14,940
So we have a similar problem. We still have three doors. And this problem is still not the Monty Hall problem.

3
00:00:14,940 --> 00:00:23,575
Behind each door there can be an animal, and the animal can be of three types. It can be a duck, it can be a beaver, or it can be a walrus.

4
00:00:23,575 --> 00:00:39,080
So let's look at this table of probabilities. According to the first column on the table, behind the first door, the probability of finding a duck is 0.7, the probability of finding a beaver is 0.2, and the probability of finding a walrus is 0.1.

5
00:00:39,080 --> 00:00:45,745
Notice that the numbers in each column need to add to one because there is some animal behind door one.

6
00:00:45,745 --> 00:00:53,825
The numbers in the rows do not need to add to one as you can see. It could easly be that we have a duck behind every door and that's okay.

7
00:00:53,825 --> 00:01:04,805
So let's look at a sample scenario. Let's say we have our three doors, and behind the first door, there's a duck, behind the second door there's a walrus, and behind the third door there's also a walrus.

8
00:01:04,805 --> 00:01:18,925
Recall that the probabilities are again by the table. So a duck behind the first door is 0.7 likely, a walrus behind the second door is 0.3 likely, and a walrus behind the third door is 0.4 likely.

9
00:01:18,925 --> 00:01:27,900
So the probability of obtaining this three animals is the product of the probabilities of the three events since they are independent events, which in this case it's 0.084.

10
00:01:27,900 --> 00:01:37,065
And as we learn, that cross entropy here is given by the sums of the negatives of the logarithms of the probabilities.

11
00:01:37,065 --> 00:01:46,740
So the first one is negative logarithm of 0.7. The second one is negative logarithm of 0.3. And the third one is negative logarithm of 0.4.

12
00:01:46,740 --> 00:01:55,490
The Cross entropy's and the sum of these three which is actually 2.48. But we want a formula, so let's put some variables here.

13
00:01:55,490 --> 00:02:04,535
So P11 is the probability of finding a duck behind door one. P12 is the probability of finding a duck behind door two etc.

14
00:02:04,535 --> 00:02:19,285
And let's have the indicator variables Y1j D1 if there's a duck behind door J. Y2j B1 if there's a beaver behind door J, and Y3j B1 if there's a walrus behind door J.

15
00:02:19,285 --> 00:02:35,630
And these variables are zero otherwise. And so, the formula for the cross entropy is simply the negative of the summation from i_ equals_ one to n, up to summation from y_ equals_ j to m of Yij_ times_ the logarithm of Pij.

16
00:02:35,630 --> 00:02:48,555
In this case, m is a number of classes. This formula works because Yij being zero one, makes sure that we're only adding the logarithms of the probabilities of the events that actually have occurred.

17
00:02:48,555 --> 00:02:55,080
And voila, this is the formula for the cross entropy in more classes. Now I'm going to leave this equestion.

18
00:02:55,080 --> 00:03:04,240
Given that we have a formula for cross entropy for two classes and one for m classes. These formulas look different but are they the same for m_ equals_ two?

19
00:03:04,240 --> 00:03:11,000
Obviously the answer is yes, but it's a cool exercise to actually write them down and convince yourself that they are actually the same.


@@@
1
00:00:00,000 --> 00:00:08,934
So this is a good time for a quick recap of the last couple of lessons. Here we have two models. The bad model on the left and the good model on the right.

2
00:00:08,935 --> 00:00:19,259
And for each one of those we calculate the cross entropy which is the sum of the negatives of the logarithms off the probabilities of the points being their colors.

3
00:00:19,260 --> 00:00:29,269
And we conclude that the one on the right is better because a cross entropy is much smaller. So let's actually calculate the formula for the error function.

4
00:00:29,269 --> 00:00:42,480
Let's split into two cases. The first case being when y=1. So when the point is blue to begin with, the model tells us that the probability of being blue is the prediction y_hat.

5
00:00:42,479 --> 00:00:55,000
So for these two points the probabilities are 0.6 and 0.2. As we can see the point in the blue area has more probability of being blue than the point in the red area.

6
00:00:55,000 --> 00:01:04,010
And our error is simply the negative logarithm of this probability. So it's precisely minus logarithm of y_hat.

7
00:01:04,010 --> 00:01:17,585
In the figure it's minus logarithm of 0.6. and minus logarithm of 0.2. Now if y=0, so when the point is red, then we need to calculate the probability of the point being red.

8
00:01:17,584 --> 00:01:27,750
The probability of the point being red is one minus the probability of the point being blue which is precisely 1 minus the prediction y_hat.

9
00:01:27,750 --> 00:01:35,870
So the error is precisely the negative logarithm of this probability which is negative logarithm of 1 - y_hat.

10
00:01:35,870 --> 00:01:46,605
In this case we get negative logarithm 0.1 and negative logarithm 0.7. So we conclude that the error is a negative logarithm of y_hat if the point is blue.

11
00:01:46,605 --> 00:01:53,625
And negative logarithm of one - y_hat the point is red. We can summarize these two formulas into this one.

12
00:01:53,625 --> 00:02:16,495
Error = - (1-y)(ln( 1- y_hat)) - y ln(y_hat). Why does this formula work? Well because if the point is blue, then y=1 which means 1-y=0 which makes the first term 0 and the second term is simply logarithm of y_hat.

13
00:02:16,495 --> 00:02:27,680
Similarly, if the point is red then y=0. So the second term of the formula is 0 and the first one is logarithm of 1- y_hat.

14
00:02:27,680 --> 00:02:35,510
Now the formula for the error function is simply the sum over all the error functions of points which is precisely the summation here.

15
00:02:35,509 --> 00:02:45,330
That's going to be this 4.8 we have over here. Now by convention we'll actually consider the average, not the sum which is where we are dividing by n over here.

16
00:02:45,330 --> 00:03:05,094
This will turn the 4.8 into a 1.2. From now on we'll use this formula as our error function. And now since y_hat is given by the sigmoid of the linear function wx + b, then the total formula for the error is actually in terms of w and b which are the weights of the model.

17
00:03:05,094 --> 00:03:14,449
And it's simply the summation we see here. In this case y_i is just the label of the point x_superscript_i.

18
00:03:14,449 --> 00:03:23,210
So now that we've calculated it our goal is to minimize it. And that's what we'll do next. And just a small aside, what we did is for binary classification problems.

19
00:03:23,210 --> 00:03:28,490
If we have a multiclass classification problem then the error is now given by the multiclass entropy.

20
00:03:28,491 --> 00:03:39,139
This formula is given here where for every data point we take the product of the label times the logarithm of the prediction and then we average all these values.

21
00:03:39,139 --> 00:03:45,000
And again it's a nice exercise to convince yourself that the two are the same when there are just two classes.


@@@
1
00:00:00,000 --> 00:00:09,330
Okay. So now our goal is to minimize the error function and we'll do it as follows. We started some random weights, which will give us the predictions σ(Wx+b).

2
00:00:09,330 --> 00:00:17,324
As we saw, that also gives us a error function given by this formula. Remember that the summands are also error functions for each point.

3
00:00:17,324 --> 00:00:24,009
So each point will give us a larger function if it's mis-classified and a smaller one if it's correctly classified.

4
00:00:24,010 --> 00:00:36,439
And the way we're going to minimize this function, is to use gradient decent. So here's Mt. Errorest and this is us, and we're going to try to jiggle the line around to see how we can decrease the error function.

5
00:00:36,439 --> 00:00:56,000
Now, the error function is the height which is E(W,b), where W and b are the weights. Now what we'll do, is we'll use gradient decent in order to get to the bottom of the mountain at a much smaller height, which gives us a smaller error function E of W', b'.

6
00:00:56,000 --> 00:01:08,000
This will give rise to new weights, W' and b' which will give us a much better prediction. Namely,  σ(W'x+b').


@@@
1
00:00:00,000 --> 00:00:07,799
So let's study gradient descent in more mathematical detail. Our function is a function of the weights and it can be graph like this.

2
00:00:07,799 --> 00:00:13,639
It's got a mathematical structure so it's not Mt. Everest anymore, it's more of a mount Math-Er-Horn.

3
00:00:13,640 --> 00:00:35,924
So we're standing somewhere in Mount Math-Er-Horn and we need to go down. So now the inputs of the functions are W1 and W2 and the error function is given by E. Then the gradient of E is given by the vector sum of the partial derivatives of E with respect to W1 and W2.

4
00:00:35,924 --> 00:00:42,879
This gradient actually tells us the direction we want to move if we want to increase the error function the most.

5
00:00:42,880 --> 00:00:48,884
Thus, if we take the negative of the gradient, this will tell us how to decrease the error function the most.

6
00:00:48,884 --> 00:00:58,634
And this is precisely what we'll do. At the point we're standing, we'll take the negative of the gradient of the error function at that point.

7
00:00:58,634 --> 00:01:12,902
Then we take a step in that direction. Once we take a step, we'll be in a lower position. So we do it again, and again, and again, until we are able to get to the bottom of the mountain.

8
00:01:12,902 --> 00:01:19,724
So this is how we calculate the gradient. We start with our initial prediction Y had equals sigmoid of W Expo's B.

9
00:01:19,724 --> 00:01:25,250
And let's say this prediction is bad because the error is large since we're high up in the mountain.

10
00:01:25,250 --> 00:01:37,025
The prediction looks like this, Y had equal sigmoid of W 1 x 1 plus all the way to WnXn plus b. Now the error function is given by the formula we saw before.

11
00:01:37,025 --> 00:01:50,170
But what matters here is the gradient of the error function. The gradient of the error function is precisely the vector formed by the partial derivative of the error function with respect to the weights and the bias.

12
00:01:50,170 --> 00:02:00,230
Now, we take a step in the direction of the negative of the gradient. As before, we don't want to make any dramatic changes, so we'll introduce a smaller learning rate alpha.

13
00:02:00,230 --> 00:02:12,204
For example, 0.1. And we'll multiply the gradient by that number. Now taking the step is exactly the same thing as updating the weights and the bias as follows.

14
00:02:12,205 --> 00:02:23,880
The weight Wi will now become Wi prime. Given by Wi minus alpha times the partial derivative of the error, with respect to Wi.

15
00:02:23,879 --> 00:02:33,545
And the bias will now become b prime given by b minus alpha times partial derivative of the error with respect to b.

16
00:02:33,544 --> 00:02:48,055
Now this will take us to a prediction with a lower error function. So, we can conclude that the prediction we have now with weights W prime b prime, is better than the one we had before with weights W and b.


@@@
1
00:00:00,000 --> 00:00:06,830
And now we finally have the tools to write the pseudocode for the grading descent algorithm, and it goes like this.

2
00:00:06,830 --> 00:00:19,270
Step one, start with random weights w_one up to w_n and b which will give us a line, and not just a line, but the whole probability function given by sigmoid of w x plus b.

3
00:00:19,270 --> 00:00:29,230
Now for every point we'll calculate the error, and as we can see the error is high for misclassified points and small for correctly classified points.

4
00:00:29,230 --> 00:00:42,950
Now for every point with coordinates x_one up to x_n, we update w_i by adding the learning rate alpha times the partial derivative of the error function with respect to w_i.

5
00:00:42,950 --> 00:00:48,440
We also update b by adding alpha times the partial derivative of the error function with respect to be.

6
00:00:48,440 --> 00:01:05,215
This gives us new weights, w_i_prime and then new bias b_prime. Now we've already calculated these partial derivatives and we know that they are y_hat minus y times x_i for the derivative with respect to w_i and y_hat minus y for the derivative with respect to b.

7
00:01:05,215 --> 00:01:15,765
So that's how we'll update the weights. Now repeat this process until the error is small, or we can repeat it a fixed number of times.

8
00:01:15,765 --> 00:01:21,935
The number of times is called the epochs and we'll learn them later. Now this looks familiar, have we seen something like that before?

9
00:01:21,935 --> 00:01:31,640
Well, we look at the points and what each point is doing is it's adding a multiple of itself into the weights of the line in order to get the line to move closer towards it if it's misclassified.

10
00:01:31,640 --> 00:01:39,000
That's pretty much what the Perceptron algorithm is doing. So in the next video, we'll look at the similarities because it's a bit suspicious how similar they are.


@@@
1
00:00:00,000 --> 00:00:13,915
So let's compare the Perceptron algorithm and the Gradient Descent algorithm. In the Gradient Descent algorithm, we take the weights and change them from Wi to Wi_ plus_ alpha_ times_ Y hat_ minus_ Y_ times_ Xi.

2
00:00:13,915 --> 00:00:29,785
In the Perceptron algorithm, not every point changes weights, only the misclassified ones. Here, if X is misclassified, we'll change the weights by adding Xi to Wi if the point label is positive, and subtracting if negative.

3
00:00:29,785 --> 00:00:37,350
Now the question is, are these two things the same? Well, let's remember that in that Perceptron algorithm, the labels are one and zero.

4
00:00:37,350 --> 00:00:48,440
And the predictions Y-hat are also one and zero. So, if the point is correct, classified, then Y_ minus_ Y-hat is zero because Y is equal to Y-hat.

5
00:00:48,440 --> 00:00:55,950
Now, if the point is labeled blue, then Y_ equals_ one. And if it's misclassified, then the prediction must be Y-hat_ equals_ zero.

6
00:00:55,950 --> 00:01:04,105
So Y-hat_ minus_ Y is minus one. Similarly, with the points labeled red, then Y_ equals_ zero and Y-hat_ equals_ one.

7
00:01:04,105 --> 00:01:13,620
So, Y-hat_ minus_ Y_ equals_ one. This may not be super clear right away. But if you stare at the screen for long enough, you'll realize that the right and the left are exactly the same thing.

8
00:01:13,620 --> 00:01:23,305
The only difference is that in the left, Y-hat can take any number between zero and one, whereas in the right, Y-hat can take only the values zero or one.

9
00:01:23,305 --> 00:01:40,770
It's pretty fascinating, isn't it? But let's study Gradient Descent even more carefully. Both in the Perceptron algorithm and the Gradient Descent algorithm, a point that is misclassified tells a line to come closer because eventually, it wants the line to surpass it so it can be in the correct side.

10
00:01:40,770 --> 00:01:47,315
Now, what happens if the point is correctly classified? Well, the Perceptron algorithm says do absolutely nothing.

11
00:01:47,315 --> 00:01:58,875
In the Gradient Descent algorithm, you are changing the weights. But what is it doing? Well, if we look carefully, what the point is telling the line, is to go farther away.

12
00:01:58,875 --> 00:02:13,060
And this makes sense, right? Because if you're correctly classified, say, if you're a blue point in the blue region, you'd like to be even more into the blue region, so your prediction is even closer to one, and your error is even smaller.

13
00:02:13,060 --> 00:02:19,590
Similarly, for a red point in the red region. So it makes sense that the point tells the line to go farther away.

14
00:02:19,590 --> 00:02:30,315
And that's precisely what the Gradient Descent algorithm does. The misclassified points asks the line to come closer and the correctly classified points asks the line to go farther away.

15
00:02:30,315 --> 00:02:37,000
The line listens to all the points and takes steps in such a way that it eventually arrives to a pretty good solution.


@@@
1
00:00:00,000 --> 00:00:19,495
So, this is just a small recap video that will get us ready for what's coming. Recall that if we have our data in the form of these points over here and the linear model like this one, for example, with equation 2x1 + 7x2 - 4 = 0, this will give rise to a probability function that looks like this.

2
00:00:19,495 --> 00:00:29,570
Where the points on the blue or positive region have more chance of being blue and the points in the red or negative region have more chance of being red.

3
00:00:29,570 --> 00:00:36,314
And this will give rise to this perception where we label the edges by the weights and the node by the bias.

4
00:00:36,314 --> 00:00:44,689
So, what the perception does, it takes to point (x1, x2), plots it in the graph and then it returns a probability that the point is blue.

5
00:00:44,689 --> 00:00:54,000
In this case, it returns a 0.9 and this mimics the neurons in the brain because they receive nervous impulses, do something inside and return a nervous impulse.


@@@
1
00:00:00,000 --> 00:00:05,543
Now we've been dealing a lot with data sets that can be separated by a line, like this one over here.

2
00:00:05,543 --> 00:00:12,150
But as you can imagine the real world is much more complex than that. This is where neural networks can show their full potential.

3
00:00:12,150 --> 00:00:20,109
In the next few videos we'll see how to deal with more complicated data sets that require highly non-linear boundaries such as this one over here.


@@@
1
00:00:00,000 --> 00:00:14,185
So, let's go back to this example of where we saw some data that is not linearly separable. So a line can not divide these red and blue points and we looked at some solutions, and if you remember, the one we considered more seriously was this curve over here.

2
00:00:14,185 --> 00:00:20,519
So what I'll teach you now is to find this curve and it's very similar than before. We'll still use grading dissent.

3
00:00:20,518 --> 00:00:36,240
In a nutshell, what we're going to do is for these data which is not separable with a line, we're going to create a probability function where the points in the blue region are more likely to be blue and the points in the red region are more likely to be red.

4
00:00:36,240 --> 00:00:44,329
And this curve here that separates them is a set of points which are equally likely to be blue or red.

5
00:00:44,329 --> 00:00:52,000
Everything will be the same as before except this equation won't be linear and that's where neural networks come into play.


@@@
1
00:00:00,000 --> 00:00:06,058
Now I'm going to show you how to create these nonlinear models. What we're going to do is a very simple trick.

2
00:00:06,059 --> 00:00:13,769
We're going to combine two linear models into a nonlinear model as follows. Visually it looks like this.

3
00:00:13,769 --> 00:00:20,084
The two models over imposed creating the model on the right. It's almost like we're doing arithmetic on models.

4
00:00:20,085 --> 00:00:26,824
It's like saying "This line plus this line equals that curve." Let me show you how to do this mathematically.

5
00:00:26,824 --> 00:00:36,478
So a linear model as we know is a whole probability space. This means that for every point it gives us the probability of the point being blue.

6
00:00:36,478 --> 00:00:43,890
So, for example, this point over here is in the blue region so its probability of being blue is 0.7.

7
00:00:43,890 --> 00:00:52,170
The same point given by the second probability space is also in the blue region so it's probability of being blue is 0.8.

8
00:00:52,170 --> 00:01:00,225
Now the question is, how do we combine these two? Well, the simplest way to combine two numbers is to add them, right?

9
00:01:00,225 --> 00:01:09,890
So 0.8 plus 0.7 is 1.5. But now, this doesn't look like a probability anymore since it's bigger than one.

10
00:01:09,890 --> 00:01:20,980
And probabilities need to be between 0 and 1. So what can we do? How do we turn this number that is larger than 1 into something between 0 and 1?

11
00:01:20,980 --> 00:01:27,744
Well, we've been in this situation before and we have a pretty good tool that turns every number into something between 0 and 1.

12
00:01:27,745 --> 00:01:44,568
That's just a sigmoid function. So that's what we're going to do. We applied the sigmoid function to 1.5 to get the value 0.82 and that's the probability of this point being blue in the resulting probability space.

13
00:01:44,569 --> 00:01:51,243
So now we've managed to create a probability function for every single point in the plane and that's how we combined two models.

14
00:01:51,243 --> 00:01:59,334
We calculate the probability for one of them, the probability for the other, then add them and then we apply the sigmoid function.

15
00:01:59,334 --> 00:02:07,849
Now, what if we wanted to weight this sum? What, if say, we wanted the model in the top to have more of a saying the resulting probability than the second?

16
00:02:07,849 --> 00:02:15,698
So something like this where the resulting model looks a lot more like the one in the top then like the one in the bottom. Well, we can add weights.

17
00:02:15,699 --> 00:02:24,240
For example, we can say "I want seven times the first model plus the second one." Actually, I can add the weights since I want.

18
00:02:24,241 --> 00:02:43,293
For example, I can say "Seven times the first one plus five times the second one." And when I do get the combine the model is I take the first probability, multiply it by seven, then take the second one and multiply it by five and I can even add a bias if I want.

19
00:02:43,294 --> 00:02:54,914
Say, the bias is minus 6, then we add it to the whole equation. So we'll have seven times this plus five times this minus six, which gives us 2.9.

20
00:02:54,913 --> 00:03:02,680
We then apply the sigmoid function and that gives us 0.95. So it's almost like we had before, isn't it?

21
00:03:02,680 --> 00:03:17,650
Before we had a line that is a linear combination of the input values times the weight plus a bias. Now we have that this model is a linear combination of the two previous model times the weights plus some bias.

22
00:03:17,650 --> 00:03:30,573
So it's almost the same thing. It's almost like this curved model in the right. It's a linear combination of the two linear models before or we can even think of it as the line between the two models.

23
00:03:30,574 --> 00:03:43,228
This is no coincidence. This is at the heart of how neural networks get built. Of course, we can imagine that we can keep doing this always obtaining more new complex models out of linear combinations of the existing ones.


@@@
1
00:00:00,000 --> 00:00:07,734
So in the previous session we learn that we can add to linear models to obtain a third model. As a matter of fact, we did even more.

2
00:00:07,735 --> 00:00:17,785
We can take a linear combination of two models. So, the first model times a constant plus the second model times a constant plus a bias and that gives us a non-linear model.

3
00:00:17,785 --> 00:00:26,784
That looks a lot like perceptrons where we can take a value times a constant plus another value times a constant plus a bias and get a new value.

4
00:00:26,785 --> 00:00:33,210
And that's no coincidence. That's actually the building block of Neural Networks. So, let's look at an example.

5
00:00:33,210 --> 00:00:42,689
Let's say, we have this linear model where the linear equation is 5x1 minus 2x2 plus 8. That's represented by this perceptron.

6
00:00:42,689 --> 00:00:52,045
And we have another linear model with equations 7x1 minus 3x2 minus 1 which is represented by this perceptron over here.

7
00:00:52,045 --> 00:01:06,420
Let's draw them nicely in here and let's use another perceptron to combine these two models using the Linear Equation, seven times the first model plus five times the second model minus six.

8
00:01:06,420 --> 00:01:16,480
And now the magic happens when we join these together and we get a Neural Network. We clean it up a bit and we obtain this. All the weights are there.

9
00:01:16,480 --> 00:01:31,629
The weights on the left, tell us what equations the linear models have. And the weights on the right, tell us what the linear combination is of the two models to obtain the curve non-linear model in the right.

10
00:01:31,629 --> 00:01:40,204
So, whenever you see a Neural Network like the one on the left, think of what could be the nonlinear boundary defined by the Neural Network.

11
00:01:40,204 --> 00:01:50,394
Now, note that this was drawn using the notation that puts a bias inside the node. This can also be drawn using the notation that keeps the bias as a separate node.

12
00:01:50,394 --> 00:02:04,160
Here, what we do is, in every layer we have a bias unit coming from a node with a one on it. So for example, the minus eight on the top node becomes an edge labelled minus eight coming from the bias node.


@@@
1
00:00:00,000 --> 00:00:11,931
Neural networks have a certain special architecture with layers. The first layer is called the input layer, which contains the inputs, in this case, x1 and x2.

2
00:00:11,932 --> 00:00:18,855
The next layer is called the hidden layer, which is a set of linear models created with this first input layer.

3
00:00:18,855 --> 00:00:26,614
And then the final layer is called the output layer, where the linear models get combined to obtain a nonlinear model.

4
00:00:26,614 --> 00:00:36,600
You can have different architectures. For example, here's one with a larger hidden layer. Now we're combining three linear models to obtain the triangular boundary in the output layer.

5
00:00:36,600 --> 00:00:43,460
Now what happens if the input layer has more nodes? For example, this neural network has three nodes in its input layer.

6
00:00:43,460 --> 00:00:59,820
Well, that just means we're not living in two-dimensional space anymore. We're living in three-dimensional space, and now our hidden layer, the one with the linear models, just gives us a bunch of planes in three space, and the output layer bounds a nonlinear region in three space.

7
00:00:59,820 --> 00:01:06,780
In general, if we have n nodes in our input layer, then we're thinking of data living in n-dimensional space.

8
00:01:06,780 --> 00:01:14,209
Now what if our output layer has more nodes? Then we just have more outputs. In that case, we just have a multiclass classification model.

9
00:01:14,209 --> 00:01:27,930
So if our model is telling us if an image is a cat or dog or a bird, then we simply have each node in the output layer output a score for each one of the classes: one for the cat, one for the dog, and one for the bird.

10
00:01:27,930 --> 00:01:36,090
And finally, and here's where things get pretty cool, what if we have more layers? Then we have what's called a deep neural network.

11
00:01:36,090 --> 00:01:45,364
Now what happens here is our linear models combine to create nonlinear models and then these combine to create even more nonlinear models.

12
00:01:45,364 --> 00:01:54,434
In general, we can do this many times and obtain highly complex models with lots of hidden layers. This is where the magic of neural networks happens.

13
00:01:54,435 --> 00:02:01,049
Many of the models in real life, for self-driving cars or for game-playing agents, have many, many hidden layers.

14
00:02:01,049 --> 00:02:08,370
That neural network will just split the n-dimensional space with a highly nonlinear boundary, such as maybe the one on the right.


@@@
1
00:00:00,000 --> 00:00:10,278
We briefly mentioned multi-class classification in the last video but let me be more specific. It seems that neural networks work really well when the problem consist on classifying two classes.

2
00:00:10,278 --> 00:00:18,625
For example, if the model predicts a probability of receiving a gift or not then the answer just comes as the output of the neural network.

3
00:00:18,625 --> 00:00:26,849
But what happens if we have more classes? Say, we want the model to tell us if an image is a duck, a beaver, or a walrus.

4
00:00:26,850 --> 00:00:37,408
Well, one thing we can do is create a neural network to predict if the image is a duck, then another neural network to predict if the image is a beaver, and a third neural network to predict if the image is a walrus.

5
00:00:37,408 --> 00:00:45,073
Then we can just use SoftMax or pick the answer that gives us the highest probability. But this seems like overkill, right?

6
00:00:45,073 --> 00:00:52,545
The first layers of the neural network should be enough to tell us things about the image and maybe just the last layer should tell us which animal it is.

7
00:00:52,545 --> 00:01:07,730
As a matter of fact, as you'll see in the CNN section, this is exactly the case. So what we need here is to add more nodes in the output layer and each one of the nodes will give us the probability that the image is each of the animals.

8
00:01:07,730 --> 00:01:15,989
Now, we take the scores and apply the SoftMax function that was previously defined to obtain well-defined probabilities.


@@@
1
00:00:00,000 --> 00:00:10,495
So now that we have defined what neural networks are, we need to learn how to train them. Training them really means what parameters should they have on the edges in order to model our data well.

2
00:00:10,495 --> 00:00:16,800
So in order to learn how to train them, we need to look carefully at how they process the input to obtain an output.

3
00:00:16,800 --> 00:00:27,195
So let's look at our simplest neural network, a perceptron. This perceptron receives a data point of the form x1, x2 where the label is Y=1.

4
00:00:27,195 --> 00:00:41,595
This means that the point is blue. Now the perceptron is defined by a linear equation say w1, x1 plus w2, x2 plus B, where w1 and w2 are the weights in the edges and B is the bias in the note.

5
00:00:41,595 --> 00:00:49,820
Here, w1 is bigger than w2, so we'll denote that by drawing the edge labelled w1 much thicker than the edge labelled w2.

6
00:00:49,820 --> 00:00:57,240
Now, what the perceptron does is it plots the point x1, x2 and it outputs the probability that the point is blue.

7
00:00:57,240 --> 00:01:03,795
Here is the point is in the red area and then the output is a small number, since the point is not very likely to be blue.

8
00:01:03,795 --> 00:01:11,070
This process is known as feedforward. We can see that this is a bad model because the point is actually blue.

9
00:01:11,070 --> 00:01:18,570
Given that the third coordinate, the Y is one. Now if we have a more complicated neural network, then the process is the same.

10
00:01:18,570 --> 00:01:35,025
Here, we have thick edges corresponding to large weights and thin edges corresponding to small weights and the neural network plots the point in the top graph and also in the bottom graph and the outputs coming out will be a small number from the top model.

11
00:01:35,025 --> 00:01:47,280
The point lies in the red area which means it has a small probability of being blue and a large number from the second model, since the point lies in the blue area which means it has a large probability of being blue.

12
00:01:47,280 --> 00:01:57,485
Now, as the two models get combined into this nonlinear model and the output layer just plots the point and it tells the probability that the point is blue.

13
00:01:57,485 --> 00:02:03,750
As you can see, this is a bad model because it puts the point in the red area and the point is blue.

14
00:02:03,750 --> 00:02:13,070
Again, this process called feedforward and we'll look at it more carefully. Here, we have our neural network and the other notations so the bias is in the outside.

15
00:02:13,070 --> 00:02:23,310
Now we have a matrix of weights. The matrix w superscript one denoting the first layer and the entries are the weights w1, 1 up to w3, 2.

16
00:02:23,310 --> 00:02:36,115
Notice that the biases have now been written as w3, 1 and w3, 2 this is just for convenience. Now in the next layer, we also have a matrix this one is w superscript two for the second layer.

17
00:02:36,115 --> 00:02:43,700
This layer contains the weights that tell us how to combine the linear models in the first layer to obtain the nonlinear model in the second layer.

18
00:02:43,700 --> 00:02:51,000
Now what happens is some math. We have the input in the form x1, x2, 1 where the one comes from the bias unit.

19
00:02:51,000 --> 00:03:01,250
Now we multiply it by the matrix w1 to get these outputs. Then, we apply the sigmoid function to turn the outputs into values between zero and one.

20
00:03:01,250 --> 00:03:08,280
Then the vector format these values gets a one attatched for the bias unit and multiplied by the second matrix.

21
00:03:08,280 --> 00:03:16,290
This returns an output that now gets thrown into a sigmoid function to obtain the final output which is y-hat.

22
00:03:16,290 --> 00:03:23,275
Y-hat is the prediction or the probability that the point is labeled blue. So this is what neural networks do.

23
00:03:23,275 --> 00:03:32,825
They take the input vector and then apply a sequence of linear models and sigmoid functions. These maps when combined become a highly non-linear map.

24
00:03:32,825 --> 00:03:42,995
And the final formula is simply y-hat equals sigmoid of w2 combined with sigmoid of w1 applied to x.

25
00:03:42,995 --> 00:04:00,405
Just for redundance, we do this again on a multi-layer perceptron or neural network. To calculate our prediction y-hat, we start with the unit vector x, then we apply the first matrix and a sigmoid function to get the values in the second layer.

26
00:04:00,405 --> 00:04:13,315
Then, we apply the second matrix and another sigmoid function to get the values on the third layer and so on and so forth until we get our final prediction, y-hat.

27
00:04:13,315 --> 00:04:20,000
And this is the feedforward process that the neural networks use to obtain the prediction from the input vector.


@@@
1
00:00:00,000 --> 00:00:05,950
So, our goal is to train our neural network. In order to do this, we have to define the error function.

2
00:00:05,950 --> 00:00:18,900
So, let's look again at what the error function was for perceptrons. So, here's our perceptron. In the left, we have our input vector with entries x_1 up to x_n, and one for the bias unit.

3
00:00:18,900 --> 00:00:30,275
And the edges with weights W_1 up to W_n, and b for the bias unit. Finally, we can see that this perceptor uses a sigmoid function.

4
00:00:30,275 --> 00:00:44,175
And the prediction is defined as y-hat equals sigmoid of Wx plus b. And as we saw, this function gives us a measure of the error of how badly each point is being classified.

5
00:00:44,175 --> 00:00:53,415
Roughly, this is a very small number if the point is correctly classified, and a measure of how far the point is from the line and the point is incorrectly classified.

6
00:00:53,415 --> 00:01:03,740
So, what are we going to do to define the error function in a multilayer perceptron? Well, as we saw, our prediction is simply a combination of matrix multiplications and sigmoid functions.

7
00:01:03,740 --> 00:01:12,000
But the error function can be the exact same thing, right? It can be the exact same formula, except now, y-hat is just a bit more complicated.

8
00:01:12,000 --> 00:01:20,000
And still, this function will tell us how badly a point gets misclassified. Except now, it's looking at a more complicated boundary.


@@@
1
00:00:00,000 --> 00:00:06,449
So now we're finally ready to get our hands into training a neural network. So let's quickly recall feedforward.

2
00:00:06,450 --> 00:00:19,804
We have our perceptron with a point coming in labeled positive. And our equation w1x1 + w2x2 + b, where w1 and w2 are the weights and b is the bias.

3
00:00:19,804 --> 00:00:25,405
Now, what the perceptron does is, it plots a point and returns a probability that the point is blue.

4
00:00:25,405 --> 00:00:36,164
Which in this case is small since the point is in the red area. Thus, this is a bad perceptron since it predicts that the point is red when the point is really blue.

5
00:00:36,164 --> 00:00:42,284
And now let's recall what we did in the gradient descent algorithm. We did this thing called Backpropagation.

6
00:00:42,284 --> 00:00:59,894
We went in the opposite direction. We asked the point, "What do you want the model to do for you?" And the point says, "Well, I am misclassified so I want this boundary to come closer to me." And we saw that the line got closer to it by updating the weights.

7
00:00:59,895 --> 00:01:07,239
Namely, in this case, let's say that it tells the weight w1 to go lower and the weight w2 to go higher.

8
00:01:07,239 --> 00:01:19,490
And this is just an illustration, it's not meant to be exact. So we obtain new weights, w1' and w2' which define a new line which is now closer to the point.

9
00:01:19,489 --> 00:01:35,857
So what we're doing is like descending from Mt. Errorest, right? The height is going to be the error function E(W) and we calculate the gradient of the error function which is exactly like asking the point what does is it want the model to do.

10
00:01:35,856 --> 00:01:43,969
And as we take the step down the direction of the negative of the gradient, we decrease the error to come down the mountain.

11
00:01:43,969 --> 00:01:53,480
This gives us a new error, E(W') and a new model W' with a smaller error, which means we get a new line closer to the point.

12
00:01:53,480 --> 00:02:02,760
We continue doing this process in order to minimize the error. So that was for a single perceptron. Now, what do we do for multi-layer perceptrons?

13
00:02:02,760 --> 00:02:19,554
Well, we still do the same process of reducing the error by descending from the mountain, except now, since the error function is more complicated then it's not Mt. Errorest, now it's Mt. Kilimanjerror. But same thing, we calculate the error function and its gradient.

14
00:02:19,555 --> 00:02:32,719
We then walk in the direction of the negative of the gradient in order to find a new model W' with a smaller error E(W') which will give us a better prediction.

15
00:02:32,719 --> 00:02:40,149
And we continue doing this process in order to minimize the error. So let's look again at what feedforward does in a multi-layer perceptron.

16
00:02:40,149 --> 00:02:50,570
The point comes in with coordinates (x1, x2) and label y = 1. It gets plotted in the linear models corresponding to the hidden layer.

17
00:02:50,569 --> 00:02:58,280
And then, as this layer gets combined the point gets plotted in the resulting non-linear model in the output layer.

18
00:02:58,280 --> 00:03:05,060
And the probability that the point is blue is obtained by the position of this point in the final model.

19
00:03:05,060 --> 00:03:11,094
Now, pay close attention because this is the key for training neural networks, it's Backpropagation.

20
00:03:11,094 --> 00:03:19,365
We'll do as before, we'll check the error. So this model is not good because it predicts that the point will be red when in reality the point is blue.

21
00:03:19,365 --> 00:03:35,195
So we'll ask the point, "What do you want this model to do in order for you to be better classified?" And the point says, "I kind of want this blue region to come closer to me." Now, what does it mean for the region to come closer to it?

22
00:03:35,194 --> 00:03:42,735
Well, let's look at the two linear models in the hidden layer. Which one of these two models is doing better?

23
00:03:42,735 --> 00:03:50,230
Well, it seems like the top one is badly misclassifying the point whereas the bottom one is classifying it correctly.

24
00:03:50,229 --> 00:04:02,519
So we kind of want to listen to the bottom one more and to the top one less. So what we want to do is to reduce the weight coming from the top model and increase the weight coming from the bottom model.

25
00:04:02,520 --> 00:04:12,014
So now our final model will look a lot more like the bottom model than like the top model. But we can do even more.

26
00:04:12,014 --> 00:04:28,635
We can actually go to the linear models and ask the point, "What can these models do to classify you better?" And the point will say, "Well, the top model is misclassifying me, so I kind of want this line to move closer to me.

27
00:04:28,634 --> 00:04:41,670
And the second model is correctly classifying me, so I want this line to move farther away from me." And so this change in the model will actually update the weights.

28
00:04:41,670 --> 00:04:57,589
Let's say, it'll increase these two and decrease these two. So now after we update all the weights we have better predictions at all the models in the hidden layer and also a better prediction at the model in the output layer.

29
00:04:57,589 --> 00:05:06,649
Notice that in this video we intentionally left the bias unit away for clarity. In reality, when you update the weights we're also updating the bias unit.

30
00:05:06,649 --> 00:05:12,070
If you're the kind of person who likes formality, don't worry, we'll calculate these gradients in detail soon.


@@@
1
00:00:00,000 --> 00:00:06,279
Okay. So, now we'll do the same thing as we did before, painting our weights in the neural network to better classify our points.

2
00:00:06,280 --> 00:00:18,879
But we're going to do it formally, so fasten your seat belts because math is coming. On your left, you have a single perceptron with the input vector, the weights and the bias and the sigmoid function inside the node.

3
00:00:18,879 --> 00:00:27,050
And on the right, we have a formula for the prediction, which is the sigmoid function of the linear function of the input.

4
00:00:27,050 --> 00:00:38,799
And below, we have a formula for the error, which is the average of all points of the blue term for the blue points and the red term for the red points.

5
00:00:38,798 --> 00:00:55,630
And in order to descend from Mount Errorest, we calculate the gradient. And the gradient is simply the vector formed by all the partial derivatives of the error function with respect to the weights w1 up to wn and and the bias b.

6
00:00:55,630 --> 00:01:05,405
They correspond to these edges over here, and what do we do in a multilayer perceptron? Well, this time it's a little more complicated but it's pretty much the same thing.

7
00:01:05,405 --> 00:01:13,503
We have our prediction, which is simply a composition of functions namely matrix multiplications and sigmoids.

8
00:01:13,504 --> 00:01:23,799
And the error function is pretty much the same, except the ŷ is a bit more complicated. And the gradient is pretty much the same thing, it's just much, much longer.

9
00:01:23,799 --> 00:01:29,984
It's a huge vector where each entry is a partial derivative of the error with respect to each of the weights.

10
00:01:29,983 --> 00:01:45,765
And these just correspond to all the edges. If we want to write this more formally, we recall that the prediction is a composition of sigmoids and matrix multiplications, where these are the matrices and the gradient is just going to be formed by all these partial derivatives.

11
00:01:45,765 --> 00:02:06,125
Here, it looks like a matrix but in reality, it's just a long vector. And the gradient descent is going to do the following; we take each weight, w_i_j super k and we update it by adding a small number, the learning rate times the partial derivative of E with respect to that same weight.

12
00:02:06,125 --> 00:02:20,270
This is the gradient descent step, so it will give us new updated weight w_i_j super k prime. That step is going to give us a whole new model with new weights that will classify the point much better.


@@@
1
00:00:00,000 --> 00:00:08,080
So before we start calculating derivatives, let's do a refresher on the chain rule which is the main technique we'll use to calculate them.

2
00:00:08,080 --> 00:00:41,704
The chain rule says, if you have a variable x on a function f that you apply to x to get f of x, which we're gonna call A, and then another function g, which you apply to f of x to get g of f of x, which we're gonna call B, the chain rule says, if you want to find the partial derivative of B with respect to x, that's just a partial derivative of B with respect to A times the partial derivative of A with respect to x.

3
00:00:41,704 --> 00:01:14,130
So it literally says, when composing functions, that derivatives just multiply, and that's gonna be super useful for us because feed forwarding is literally composing a bunch of functions, and back propagation is literally taking the derivative at each piece, and since taking the derivative of a composition is the same as multiplying the partial derivatives, then all we're gonna do is multiply a bunch of partial derivatives to get what we want. Pretty simple, right?


@@@
1
00:00:00,000 --> 00:00:12,775
So, let us go back to our neural network with our weights and our input. And recall that the weights with superscript 1 belong to the first layer, and the weights with superscript 2 belong to the second layer.

2
00:00:12,775 --> 00:00:22,830
Also, recall that the bias is not called b anymore. Now, it is called W31, W32 etc. for convenience, so that we can have everything in matrix notation.

3
00:00:22,830 --> 00:00:37,860
And now what happens with the input? So, let us do the feedforward process. In the first layer, we take the input and multiply it by the weights and that gives us h1, which is a linear function of the input and the weights.

4
00:00:37,860 --> 00:01:07,540
Same thing with h2, given by this formula over here. Now, in the second layer, we would take this h1 and h2 and the new bias, apply the sigmoid function, and then apply a linear function to them by multiplying them by the weights and adding them to get a value of h. And finally, in the third layer, we just take a sigmoid function of h to get our prediction or probability between 0 and 1, which is ŷ.

5
00:01:07,540 --> 00:01:33,540
And we can read this in more condensed notation by saying that the matrix corresponding to the first layer is W superscript 1, the matrix corresponding to the second layer is W superscript 2, and then the prediction we had is just going to be the sigmoid of W superscript 2 combined with the sigmoid of W superscript 1 applied to the input x and that is feedforward.

6
00:01:33,540 --> 00:01:52,611
Now, we are going to develop backpropagation, which is precisely the reverse of feedforward. So, we are going to calculate the derivative of this error function with respect to each of the weights in the labels by using the chain rule.

7
00:01:52,611 --> 00:02:02,760
So, let us recall that our error function is this formula over here, which is a function of the prediction ŷ.

8
00:02:02,760 --> 00:02:13,540
But, since the prediction is a function of all the weights wij, then the error function can be seen as the function on all the wij.

9
00:02:13,540 --> 00:02:23,500
Therefore, the gradient is simply the vector formed by all the partial derivatives of the error function E with respect to each of the weights.

10
00:02:23,500 --> 00:02:31,210
So, let us calculate one of these derivatives. Let us calculate derivative of E with respect to W11 superscript 1.

11
00:02:31,210 --> 00:02:41,650
So, since the prediction is simply a composition of functions and by the chain rule, we know that the derivative with respect to this is the product of all the partial derivatives.

12
00:02:41,650 --> 00:02:57,650
In this case, the derivative E with respect to W11 is the derivative of either respect to ŷ times the derivative ŷ with respect to h times the derivative h with respect to h1 times the derivative h1 with respect to W11.

13
00:02:57,650 --> 00:03:10,235
This may seem complicated, but the fact that we can calculate a derivative of such a complicated composition function by just multiplying 4 partial derivatives is remarkable.

14
00:03:10,235 --> 00:03:16,430
Now, we have already calculated the first one, the derivative of E with respect to ŷ. And if you remember, we got ŷ minus y.

15
00:03:16,430 --> 00:03:25,193
So, let us calculate the other ones. Let us zoom in a bit and look at just one piece of our multi-layer perceptron.

16
00:03:25,193 --> 00:03:44,670
The inputs are some values h1 and h2, which are values coming in from before. And once we apply the sigmoid and a linear function on h1 and h2 and 1 corresponding to the biased unit, we get a result h. So, now what is the derivative of h with respect to h1?

17
00:03:44,670 --> 00:03:55,940
Well, h is a sum of three things and only one of them contains h1. So, the second and the third summon just give us a derivative of 0.

18
00:03:55,940 --> 00:04:08,715
The first summon gives us W11 superscript 2 because that is a constant, and that times the derivative of the sigmoid function with respect to h1.

19
00:04:08,715 --> 00:04:27,600
This is something that we calculated below in the instructor comments, which is that the sigmoid function has a beautiful derivative, namely the derivative of sigmoid of h is precisely sigmoid of h times 1 minus sigmoid of h. Again, you can see this development underneath in the instructor comments.

20
00:04:27,600 --> 00:04:35,635
You also have the chance to code this in the quiz because at the end of the day, we just code these formulas and then use them forever, and that is it.


@@@
1
00:00:00,000 --> 00:00:11,400
Hello everyone and welcome to this lesson on deep learning with PyTorch. So, in this lesson I'm going to be showing you how we can build neural networks with pyTorch and train them.

2
00:00:11,400 --> 00:00:18,915
By working through all these notebooks I built, you'll be writing the actual code yourself for building these networks.

3
00:00:18,915 --> 00:00:31,605
By the end of the lesson, you will have built your own state of the art image classifier. But first we're going to start with basics, so how do you build just a simple neural network in pyTorch?

4
00:00:31,605 --> 00:00:43,460
So, as a reminder of how neural networks work, in general we have some input values so here x1, x2, and we multiply them by some weights w and bias.

5
00:00:43,460 --> 00:00:52,220
So, this b is this bias we just multiply it by one then you sum all these up and you get some value h. Then we have what's called an activation function.

6
00:00:52,220 --> 00:00:59,830
So, here f of h and passing these input values h through this activation function gets you output y.

7
00:00:59,830 --> 00:01:10,435
This is the basis of neural networks. You have these inputs, you multiply it by some waves, take the sum, pass it through some activation function and you get an output.

8
00:01:10,435 --> 00:01:18,250
You can stack these up so that the output of these units, of these neurons go to another layer like another set of weights.

9
00:01:18,250 --> 00:01:34,070
So, mathematically this what it looks like, y, our output is equal to this linear combination of the weights and the input values w's and x's plus your bias value b passes through your activation function f and you get y.

10
00:01:34,070 --> 00:01:42,345
You could also write it with this sum. So, sum of wi times xi and plus b, your bias term. That gives you y.

11
00:01:42,345 --> 00:01:52,685
So, what's nice about this is that you can actually think of the x's, your input features, your values, as a vector, and your weights as another vector.

12
00:01:52,685 --> 00:02:09,860
So, your multiplication and sum is the same as a dot or inner product of two vectors. So, if you consider your input as a vector and your weights as a vector, if you take the dot product of these two, then you get your value h and then you pass h through your activation function and that gets you your output y.

13
00:02:09,860 --> 00:02:20,330
So, now if we start thinking of our weights and our input values as vectors, so vectors are an instance of a tensor.

14
00:02:20,330 --> 00:02:32,285
So, a tensor is just a generalization of vectors and matrices. So, when you have these like regular structured arrangements of values and so a tensor with only one dimension is a vector.

15
00:02:32,285 --> 00:02:51,620
So, we just have this single one-dimensional array of values. So, in this case characters T-E-N-S-O-R. A matrix like this is a two-dimensional tensor and so we have values going in two directions from left to right and from top to bottom and so that we have individual rows and columns.

16
00:02:51,620 --> 00:02:58,490
So, you can do operations across the columns like along a row or you can do it across the rows like going down a column.

17
00:02:58,490 --> 00:03:05,990
You also have three-dimensional tensors so you can think of an image like an RGB color image as a three-dimensional tensor.

18
00:03:05,990 --> 00:03:17,070
So, for every pixel, there's some value for all the red and the green and the blue channels and so for every individual pixel, in a two-dimensional image, you have three values.

19
00:03:17,070 --> 00:03:27,710
So, that is a three-dimensional tensor. Like I said before, tensors are a generalization of this so you can actually have four-dimensional, five-dimensional, six-dimensional, and so on like tensors.

20
00:03:27,710 --> 00:03:32,800
It's just the ones that we normally work with are one and two-dimensional, three-dimensional tensors.

21
00:03:32,800 --> 00:03:40,715
So, these tensors are the base data structure that you use an pyTorch and other neural network frameworks.

22
00:03:40,715 --> 00:03:55,865
So, TensorFlow is named after tensors. So, these are the base data structures that you'll be using so you pretty much need to understand them really well to be able to use pretty much any framework that you'll be using for deep learning.

23
00:03:55,865 --> 00:04:03,605
So, let's get started. I'm going to show you how to actually create some tensors and use them to build a simple neural network.

24
00:04:03,605 --> 00:04:12,355
So, first we're going to import pyTorch and so just import torch here. Here I am creating activation function, so this is the Sigmoid activation function.

25
00:04:12,355 --> 00:04:21,065
It's the nice s shape that kind of squeezes the input values between zero and one. It's really useful for providing a probability.

26
00:04:21,065 --> 00:04:33,060
So, probabilities are these values that can only be between zero and one. So, you're Sigmoid activation if you want the output of your neural network to be a probability, then the sigmoid activation is what you want to use.

27
00:04:33,060 --> 00:04:44,815
So, here I'm going to create some fake data, I'm generating some data, I'm generating some weights and biases and with these you're actually going to do the computations to get the output of a simple neural network.

28
00:04:44,815 --> 00:04:53,255
So, here I'm just creating a manual seeds. So, I'm setting the seed for the random number generation that I'll be using and here I'm creating features.

29
00:04:53,255 --> 00:05:00,150
So, features are like the input features of the input data for your network. Here we see torch.randn.

30
00:05:00,150 --> 00:05:08,180
So, randn is going to create a tensor of normal variables. So, random normal variables as samples from a normal distribution.

31
00:05:08,180 --> 00:05:20,675
You give it a tuple of the size that you want. So, in this case I want the features to be a matrix, a 2-dimensional tensor of one row and five columns.

32
00:05:20,675 --> 00:05:33,270
So, you can think of this as a row vector that has five elements. For the weights, we're going to create another matrix of random normal variables and this time I'm using randn_like.

33
00:05:33,270 --> 00:05:41,330
So, what this does is it takes another tensor and it looks at the shape of this tensor and then it creates it, it creates another tensor with the same shape.

34
00:05:41,330 --> 00:05:50,115
So, that's what this this like means. So, I'm going to create a tensor of random normal variables with the same shape as features. So, it gives me my weights.

35
00:05:50,115 --> 00:05:58,700
Then I'm going to create a bias term. So, this is again just a random normal variable. Now I'm just creating one value.

36
00:05:58,700 --> 00:06:11,030
So, this is one row and one column. Here I'm going to leave this exercise up to you. So, what you're going to be doing is taking the features, weights, and the bias tensors and you're going to calculate the output of this simple neural network.

37
00:06:11,030 --> 00:06:26,000
So, remember with features and weights you want to take the inner product or you want to multiply the features by the weights and sum them up and then add the bias and then pass it through the activation function and from that you should get the output of your network.

38
00:06:26,000 --> 00:06:34,130
So, if you want to see how I did this, checkout my solution notebook or watch the next video which I'll show you my solution for this exercise.


@@@
1
00:00:00,000 --> 00:00:08,190
So now, this is my solution for this exercise on calculating the output of this small simple neural network.

2
00:00:08,190 --> 00:00:14,070
So, remember that what we want to do is multiply our features by our weights, so features times weights.

3
00:00:14,070 --> 00:00:33,060
So these tensors, they work basically the same as NumPy arrays, if you've used NumPy before. So, when you multiply features times weights, it'll just take the first element from each one, multiply them together, take the second element and multiply them together and so on and give you back a new tensor, where there's element by element multiplication.

4
00:00:33,060 --> 00:00:41,100
So, from that we can do torch.sum to sum it all up into one value, add our bias term and then pass it through the activation function and then we get Y.

5
00:00:41,100 --> 00:00:53,645
So, we can also do this where we do features times weights again, and this creates another tensor, but tensors have a method.sum, where you just take a tensor do.sum and then it sums up all the values in that tensor.

6
00:00:53,645 --> 00:01:01,430
So, we can either do it this way or we do torch.sum, or we can just take this method, this sum method of a tensor and some upper values that way.

7
00:01:01,430 --> 00:01:11,080
Again, pass it through our our activation function. So, here what we're doing, we're doing this element wise multiplication and taking the sum in two separate operations.

8
00:01:11,080 --> 00:01:17,420
We're doing this multiplication and then we're doing the sum. But you can actually do this in the same operation using matrix multiplication.

9
00:01:17,420 --> 00:01:32,985
So, in general, you're going to be wanting to use matrix multiplications most of the time, since they're the more efficient and these linear algebra operations have been accelerated using modern libraries, such as CUDA that run on GPUs.

10
00:01:32,985 --> 00:01:40,310
To do matrix multiplication in PyTorch with our two tensors features and weights, we can use one of two methods.

11
00:01:40,310 --> 00:01:52,760
So, either torch.mm or torch.matmul. So, torch.mm, so matrix multiplication is more simple and more strict about the tensors that you pass in.

12
00:01:52,760 --> 00:02:05,665
So, torch.matmul, it actually supports broadcasting. So, if you put in tensors that have weird sizes, weird shapes, then you could get an output that you're not expecting.

13
00:02:05,665 --> 00:02:19,085
So, what I tend to use torch.mm more often, so that it does what I expect basically, and then if I get something wrong it's going throw an error instead of just doing it and continuing the calculations.

14
00:02:19,085 --> 00:02:28,975
So, however, if we actually try to use torch.mm with features and weights, we'll get an error. So, here we see RuntimeError, size mismatch.

15
00:02:28,975 --> 00:02:39,815
So, what this means is that we passed in our two tensors to torch.mm, but there's a mismatch in the sizes and it can't actually do the matrix multiplication and it lists out the sizes here.

16
00:02:39,815 --> 00:03:00,520
So, the first tensor, M1 is one by five and the second tensor is one by five also. So, if you remember from your linear algebra classes or if you studied it recently, when you're doing matrix multiplication, the first matrix has to have a number of columns that's equal to the number of rows in the second matrix.

17
00:03:00,520 --> 00:03:07,765
So, really what we need is we need our weights tensor, our weights matrix to be five by one instead of one by five.

18
00:03:07,765 --> 00:03:35,780
To checkout the shape of tensors, as you're building your networks, you want to use tensor.shape. So, this is something you're going to be using all the time in PyTorch, but also in TensorFlow and in other deep learning frameworks So, most of the errors you're going to see when you're building networks and just a lot of the difficulty when it comes to designing the architecture of neural networks is getting the shapes of your tensors to work right together.

19
00:03:35,780 --> 00:03:42,635
So, what that means is that a large part of debugging, you're actually going to be trying to look at the shape of your tensors as they're going through your network.

20
00:03:42,635 --> 00:03:51,860
So, remember this, tensor.shape. So, for reshaping tensors, there are three, in general, three different options to choose from.

21
00:03:51,860 --> 00:04:04,805
So, we have these methods; reshape, resize, and view. The way these all work, in general, is that you take your tensor weights.reshape and then pass in the new shape that you want.

22
00:04:04,805 --> 00:04:11,630
So, in this case, you want to change our weights to be a five by one matrix, so we'd say.reshape and then five comma one.

23
00:04:11,630 --> 00:04:18,750
So, reshape here, what it will do is it's going to return a new tensor with the same data as weights.

24
00:04:18,750 --> 00:04:31,775
So, the same data that's sitting in memory at those addresses in memory. So, it's going to basically just create a new tensor that has the shape that you requested, but the actual data in memory isn't being changed.

25
00:04:31,775 --> 00:04:44,215
But that's only sometimes. Sometimes it does return a clone and what that means is that it actually copies the data to another part of memory and then returns you a tensor on top of that part of the memory.

26
00:04:44,215 --> 00:04:54,080
As you can imagine when it actually does that, when it's copying the data that's less efficient than if you had just changed the shape of your tensor without cloning the data.

27
00:04:54,080 --> 00:05:02,884
To do something like that, we can use resize, where there's underscore at the end. The underscore means that this method is an in-place operation.

28
00:05:02,884 --> 00:05:12,575
So, when it's in-place, that basically means that you're just not touching the data at all and all you're doing is changing the tensor that's sitting on top of that addressed data in memory.

29
00:05:12,575 --> 00:05:30,005
The problem with the resize method is that if you request a shape that has more or less elements than the original tensor, then you can actually cut off, you can actually lose some of the data that you had or you can create this spurious data from uninitialized memory.

30
00:05:30,005 --> 00:05:41,555
So instead, what you typically want is that you want a method that's going to return an error if you changed the shape from the original number of elements to a different number of elements.

31
00:05:41,555 --> 00:05:52,985
So, we can actually do that with.view. So.view is the one that I use the most, and basically what it does it just returns a new tensor with the same data in memory as weights.

32
00:05:52,985 --> 00:06:00,155
This is just all the time, 100 percent of the time, all it's going to do is return a new tensor without messing with any of the data in memory.

33
00:06:00,155 --> 00:06:09,885
If you tried to get a new size, a new shape for your tensor with a different number of elements, it'll return an error.

34
00:06:09,885 --> 00:06:17,905
So, you are basically using.view, you're ensuring that you will always get the same number of elements when you change the shape of your weights.

35
00:06:17,905 --> 00:06:30,420
So, this is why I typically use when I'm reshaping tensors. So, with all that out of the way, if you want to reshape weights to have five rows and one column, then you can use something like weights.view (5, 1), right.

36
00:06:30,420 --> 00:06:43,330
So, now, that you have seen how you can change the shape of a tensor and also do matrix multiplication, so this time I want you to calculate the output of this little neural network using matrix multiplication.


@@@
1
00:00:00,000 --> 00:00:08,700
Welcome to my solution for this exercise. So, for here, I had you calculate the output of our network using matrix multiplication.

2
00:00:08,700 --> 00:00:16,605
So remember, we wanted to use matrix multiplication because it's more efficient than doing these two separate operations of the multiplication and the sum.

3
00:00:16,605 --> 00:00:31,305
But to do the matrix multiplication, we actually needed to change the size of our weights tensor. So, to do that, just do weights.view 5, 1, and so this will change the shape of our weights tensor to be five rows and one column.

4
00:00:31,305 --> 00:00:37,145
If you remember, our features has the shape of one row and five columns, so we can do this matrix multiplication.

5
00:00:37,145 --> 00:00:49,130
So, there's just one operation that does the multiplication and the sum and just one go, and then we again add our bias term, pass it through the activation, and we get our output.

6
00:00:49,130 --> 00:01:02,255
So, as I mentioned before, you could actually stack up these simple neural networks into a multi-layer neural network, and this basically gives your network greater power to capture patterns and correlations in your data.

7
00:01:02,255 --> 00:01:13,745
Now, instead of a simple vector for our weights, we actually need to use a matrix. So, in this case, we have our input vector and our input data x_1, x_2, x_3.

8
00:01:13,745 --> 00:01:29,135
You think of this as a vector of just x, which our features. Then we have weights that connect our input to one hidden unit in this middle layers, usually called the hidden layer, hidden units, and we have two units in this hidden layer.

9
00:01:29,135 --> 00:01:41,485
So then, if we have our features, our inputs as a row vector, if we multiply it by this first column, then we're going to get the output, we're going to get this value of h_1.

10
00:01:41,485 --> 00:01:48,625
Then if we take our features and multiply it by the second column, then we're going to get the value for h_2.

11
00:01:48,625 --> 00:02:04,605
So again, mathematically looking at this with matrices and vectors and linear algebra, we see that to get the values for this hidden layer that we do a matrix multiplication between our feature vector, this x_1 to x_n, and our weight matrix.

12
00:02:04,605 --> 00:02:14,955
Then as before with these values, we're going to pass them through some activation function or maybe not an activation function, maybe we just want the row output of our network.

13
00:02:14,955 --> 00:02:26,500
So here, I'm generating some random data, some features, and some random weight matrices and bias terms that you'll be using to calculate the output of a multi-layer network.

14
00:02:26,500 --> 00:02:32,990
So, what I've built is basically we have three input features, two hidden units and one output unit.

15
00:02:32,990 --> 00:02:48,195
So, you can see that I've listed it here. So, our features we're going to create three features and this features vector here, and then we have an input equals three, so the shape is this, and two hidden units, one output unit.

16
00:02:48,195 --> 00:02:58,235
So, these weight matrices are created using these values. All right. I'll leave it up to you to calculate the output for this multi-layer network.

17
00:02:58,235 --> 00:03:06,670
Again, feel free to use the activation function defined earlier for the output of your network and the hidden layer. Cheers.


@@@
1
00:00:00,000 --> 00:00:11,905
All right. So, here's my solution for this exercise. So, here, I had you calculate the output of this multi-layer network using the weights and features that we've defined up here.

2
00:00:11,905 --> 00:00:24,945
So, it was really similar to what we did before with our single layer simple neural network. So, it's basically just taking the features and our weight matrix, our first weight matrix, and calculating a matrix multiplication.

3
00:00:24,945 --> 00:00:37,880
So, here's the torch.mm plus B1, and then that gives us values for our hidden layer H. Now, we can use the values H as the input for the next layer of our network.

4
00:00:37,880 --> 00:00:48,880
So, we just do, again, a matrix multiplication of these hidden values H, with our second weight matrix W2, and adding on our bias terms, and then we get the output.

5
00:00:48,880 --> 00:00:58,480
So, my favorite features of PyTorches is being able to convert between Numpy arrays and Torch tensors, in a very nice and easy manner.

6
00:00:58,480 --> 00:01:17,015
So, this is really useful because a lot of the times, you'll be preparing your data and to do some preprocessing using Numpy, and then you want to move it into your network, and so, you have to bridge these Numpy arrays, what you're using for your data, and then the Torch tensors that you're using for your network.

7
00:01:17,015 --> 00:01:35,280
So, actually, to do this, we can actually get a tensor from a Numpy array using torch.fromnumpy. So, here I've just created a random array, a four-by-three array, and then we can create a Torch tensor from this array just by doing.from Numpy, and passing an array.

8
00:01:35,280 --> 00:01:45,595
So, this creates a nice tensor for us. So, this is a tensor in PyTorch, we can use with all of our Torch methods and eventually, use it in a neural network.

9
00:01:45,595 --> 00:01:54,620
Then, we can go backwards, so we can take a tensor such as B here. This is our Torch tensor and we can go back to a Numpy array doing b.numpy.

10
00:01:54,620 --> 00:02:04,595
So, this gives us back our Numpy array. So, one thing to remember when you're doing this, is that the memory is actually shared between the Numpy array and this Torch tensor.

11
00:02:04,595 --> 00:02:13,825
So, what this means, is that if you do any operations in place on either the Numpy array or the tensor, then you're going to change the values for the other one.

12
00:02:13,825 --> 00:02:29,930
So, for example, if we do this in-place operation of multiplying by two, which means that we're actually changing the values in memory, and not creating a new tensor, then we will actually change the values in the Numpy array.

13
00:02:29,930 --> 00:02:42,560
So, you see here, we have our Numpy array. So initially, it's like this, convert it to a Torch tensor, and here, I'm doing this in-place multiplication, and we've changed our values for this tensor.

14
00:02:42,560 --> 00:02:56,840
Then, if you look back at the Numpy array, the values have changed. So, that's just something to keep in mind as you're doing this, so you're not caught off guard when you're seeing your arrays, your Numpy arrays, being changed because of operations you're doing on the tensor.


@@@
1
00:00:00,000 --> 00:00:11,280
Hello everyone and welcome back. So, in this notebook and series of videos, I'm going to be showing you a more powerful way to build neural networks and PyTorch.

2
00:00:11,280 --> 00:00:18,465
So, in the last notebook, you saw how you can calculate the output for network using tensors and matrix multiplication.

3
00:00:18,465 --> 00:00:31,050
But PyTorch has this nice module, nn, that has a lot of my classes and methods and functions that allow us to build large neural networks in a very efficient way.

4
00:00:31,050 --> 00:00:39,920
So, to show you how this works, we're going to be using a dataset called MNIST. So, MNIST it's a whole bunch of grayscale handwritten digits.

5
00:00:39,920 --> 00:00:51,970
So ,0, 1, 2, 3, 4 and so on through nine. Each of these images is 28 by 28 pixels and the goal is to actually identify what the number is in these images.

6
00:00:51,970 --> 00:00:59,620
So, that dataset consists of each of these images and it's labeled with the digit that is in that image.

7
00:00:59,620 --> 00:01:15,680
So, ones are labeled one, twos are labeled two and so on. So, what we can do is we can actually show our network and image and the correct label and then it learns how to actually determine what the number and the image is.

8
00:01:15,680 --> 00:01:29,990
This dataset is available through the torchvision package. So, this is a package that sits alongside PyTorch, that provides a lot of nice utilities like datasets and models for doing computer vision problems.

9
00:01:29,990 --> 00:01:38,930
We can run this cell to download and load the MNIST dataset. What it does is it gives us back an object which I'm calling trainloader.

10
00:01:38,930 --> 00:01:57,635
So, with this trainloader we can turn into an iterator with iter and then this will allow us to start getting good at it or we can actually just use this in a loop, in a for loop and so we can get our images and labels out of this generator with four image, comma label and trainloader.

11
00:01:57,635 --> 00:02:12,205
One thing to notice is that when I created the trainloader, I set the batch size to 64. So, what that means and every time we get a set of images and labels out, we're actually getting 64 images out from our data loader.

12
00:02:12,205 --> 00:02:20,620
So, then if you look at the shape and the size of these images, we'll see that they are 64 by one by 28 by 28.

13
00:02:20,620 --> 00:02:31,090
So, 64 images and then one color channels so it's grayscale, and then it's 28 by 28 pixels is the shape of these images and so we can see that here.

14
00:02:31,090 --> 00:02:42,980
Then our labels have a shape of 64 so it's just a vector that's 64 elements which with a label for each of our images and we can see what one of these images looks like this is a nice number four.

15
00:02:42,980 --> 00:02:48,215
So, we're going to do here is build a multi-layer neural network using the methods that we saw before.

16
00:02:48,215 --> 00:02:59,575
By that I mean you're going to initialize some weight matrices and some bias vectors and use those to calculate the output of this multi-layer network.

17
00:02:59,575 --> 00:03:08,280
Specifically, we want to build this network with 784 input units, 256 hidden units, and 10 output units.

18
00:03:08,280 --> 00:03:21,825
So, 10 output units, one for each of our classes. So, the 784 input units, this comes from the fact that with this type of network is called a fully connected network or a dense network.

19
00:03:21,825 --> 00:03:41,935
We want to think of our inputs as just one vector. So, our images are actually this 28 by 28 image, but we want to put a vector into our network and so what we need to do is actually convert this 28 by 28 image into a vector and so, 784 is 28 times 28.

20
00:03:41,935 --> 00:03:50,610
When we actually take this 28 by 28 image and flatten it into a vector then it's going to be 784 elements long.

21
00:03:50,610 --> 00:04:03,090
So, now what we need to do is take each of our batches which is 64 by one by 28 by 28 and then convert it into a shape that is to another tensor which shapes 64 by 784.

22
00:04:03,090 --> 00:04:23,270
This is going to be the tensor that's the input to our network. So, go and give this a shot. So again, build the networks 784 input units, 256 hidden units and 10 output units and you're going to be generating your own random initial weight and bias matrices. Cheers.


@@@
1
00:00:00,000 --> 00:00:08,505
Here is my solution for this multi-layer neural network for classifying handwritten digits from the MNIST dataset.

2
00:00:08,505 --> 00:00:15,465
So, here I've defined our activation function like before, so, again this is the sigmoid function and here I'm flattening the images.

3
00:00:15,465 --> 00:00:23,580
So, remember how to reshape your tensors. So, here I'm using.view. So, I'm just grabbing the batch size.

4
00:00:23,580 --> 00:00:32,925
So, images.shape. The first element zero here, gives you the number of batches in your images tensor.

5
00:00:32,925 --> 00:00:38,385
So, I want to keep the number of batches the same, but I want to flatten the rest of the dimensions.

6
00:00:38,385 --> 00:00:48,410
So, to do this, you actually can just put in negative one. So, I could type in 784 here but a kind of a shortcut way to do this is to put in negative one.

7
00:00:48,410 --> 00:01:02,270
So, basically what this does is it takes 64 as your batch size here and then when you put a negative one it sees this and then it just chooses the appropriate size to get the total number of elements.

8
00:01:02,270 --> 00:01:13,675
So, it'll work out on its own that it needs to make the second dimension, 784 so that the number of elements after reshaping matches the number elements before reshaping.

9
00:01:13,675 --> 00:01:21,245
So, this is just a kind of quick way to flatten a tensor without having to know what the second dimension used to be.

10
00:01:21,245 --> 00:01:33,020
Then here I'm just creating our weight and bias parameters. So, we know that we want an input of 784 units and we want 256 hidden units.

11
00:01:33,020 --> 00:01:41,360
So, our first weight matrix is going to be 784 by 256. Then, we need a bias term for each of our hidden units.

12
00:01:41,360 --> 00:01:53,570
So we have 256 bias terms here in b1. Then, for our second weight's going from the hidden layer to the output layer we want 256 inputs to 10 outputs.

13
00:01:53,570 --> 00:02:10,940
Then again 10 elements in our bias. Before we can do a matrix multiplication of our inputs with the first set of weights, our first weight parameters, add in the bias terms and passes through our activation functions so that gives us the output of our hidden layer.

14
00:02:10,940 --> 00:02:19,970
Then we can use that as the input to our output layer, and again, a matrix multiplication with a second set of weights and the second set of bias terms.

15
00:02:19,970 --> 00:02:29,450
This gives us the output of our network. All right. So, if we look at the output of this network, we see that we get those 64.

16
00:02:29,450 --> 00:02:46,415
So, first let me print the shape just to make sure we did that right. So, 64 rows for one of each of our sort of input examples and then 10 values, so, basically it's a value that's trying to say this image belongs to this class like this digit.

17
00:02:46,415 --> 00:02:53,640
So, we can inspect our output tensor and see what's going on here. So, we see these values are just sort of all over the place.

18
00:02:53,640 --> 00:03:04,160
So, you got like six and negative 11 and so on. But we really want is we want our network to kind of tell us the probability of our different classes given some image.

19
00:03:04,160 --> 00:03:16,565
So, kind of we want to pass in an image to our network and then the output should be a probability distribution that tells us which are the most likely classes or digits that belong to this image.

20
00:03:16,565 --> 00:03:24,995
So, if it's the image of a six, then we want a probability distribution where most of the probability is in the sixth class.

21
00:03:24,995 --> 00:03:31,520
So, it's telling us that it's a number six. So, we want it to look something like this. This is like a class probability.

22
00:03:31,520 --> 00:03:37,815
So, it's telling us the probabilities of the different classes given this image that we're passing in.

23
00:03:37,815 --> 00:03:43,985
So, you can see that the probability for each of these different classes is roughly the same, and so it's a uniform distribution.

24
00:03:43,985 --> 00:03:53,495
This represents an untrained network, so it's a uniform probability distribution. It's because it hasn't seen any data yet, so it hasn't learned anything about these images.

25
00:03:53,495 --> 00:04:03,370
So, whenever you give an image to it, it doesn't know what it is so it's just going to give an equal probability to each class, regardless of the image that you pass in.

26
00:04:03,370 --> 00:04:14,960
So, what we want is we want the output of our network to be a probability distribution that gives us the probability that the image belongs to any one of our classes.

27
00:04:14,960 --> 00:04:22,670
So for this, we use the softmax function. So what this looks like is the exponential. So,you pass in your 10 values.

28
00:04:22,670 --> 00:04:29,555
So, for each of those values, we calculate the exponential of that value divided by the sum of exponentials of all the values.

29
00:04:29,555 --> 00:04:42,870
So, what this does is it actually kind of squishes each of the input values x between zero and one, and then also normalizes all the values so that the sum of each of the probabilities is one.

30
00:04:42,870 --> 00:04:54,310
So, the entire thing sums up to one. So, this gives you a proper probability distribution. What I want you to do here is actually implement a function called softmax that performs this calculation.

31
00:04:54,310 --> 00:05:11,660
So, what you're going to be doing is taking the output from this simple neural network and has shaped 64 by 10 and pass it through a dysfunction softmax and make sure it calculates the probability distribution for each of the different examples that we passed in.


@@@
1
00:00:00,000 --> 00:00:12,180
Welcome back. Here is my solution for the softmax function. Here in the numerator, we know we want to take the exponential, so it's pretty straight forward with torch.exp.

2
00:00:12,180 --> 00:00:25,740
So we're going to use the exponential of x, which is our input tensor. In the denominator, we know we want to do something like, again take exponentials so torch.exp, and then take the sum across all those values.

3
00:00:25,740 --> 00:00:33,915
So, one thing we need to remember is that we want the sum across one single row. So, each of the columns in one single row for each example.

4
00:00:33,915 --> 00:00:40,170
So, for one example, we want to sum up those values. So, for here in torch.sum, we're going to use dimension equals one.

5
00:00:40,170 --> 00:00:53,530
So, this is basically going to take the sum across the columns. What this does, torch.sum here, is going to actually going to give us a tensor, that is just a vector of 64 elements.

6
00:00:53,530 --> 00:01:06,575
So, the problem with this is that, if this is 64 by 10, and this is just a 64-long vector, it's going to try to divide every element in this tensor by all 64 of these values.

7
00:01:06,575 --> 00:01:13,355
So, it's going give us a 64 by 64 tensor, and that's not what we want. We want our output to be 64 by 10.

8
00:01:13,355 --> 00:01:22,345
So, what you actually need to do is reshape this tensor here to have 64 rows, but only one value for each of those rows.

9
00:01:22,345 --> 00:01:31,505
So, what that's going do, it's going look at for each row in this tensor, is going to look at the equivalent row in this tensor.

10
00:01:31,505 --> 00:01:40,550
So, since each row in this tensor only has one value, it's going to divide this exponential by the one value in this denominator tensor.

11
00:01:40,550 --> 00:01:55,510
This can be really tricky, but it's also super important to understand how broadcasting works in PyTorch, and how to actually fit all these tensors together with the correct shape and the correct operations to get everything out right.

12
00:01:55,510 --> 00:02:11,340
So, if we do this, it look what we have, we pass our output through the softmax function, and then we get our probabilities, and we can look the shape and it is 64 by 10, and if you take the sum across each of the rows, then it adds up to one, like it should with a proper probability distribution.

13
00:02:11,340 --> 00:02:22,940
So, now, we're going to look at how you use this nn module to build neural networks. So, you'll find that it's actually in a lot of ways simpler and more powerful.

14
00:02:22,940 --> 00:02:38,430
You'll be able to build larger and larger neural networks using the same framework. The way this works in general, is that we're going to create a new class, and you can call it networking, you can call it whatever you want, you can call it classifier, you can call it MNIST.

15
00:02:38,430 --> 00:02:51,360
It doesn't really matter so much what you call it, but you need to subclass it from nn.module. Then, in the init method, it's __init method.

16
00:02:51,360 --> 00:03:03,290
You need to call it super and run the init method of nn.module. So, you need to do this because then, PyTorch will know to register all the different layers and operations that you're going to be putting into this network.

17
00:03:03,290 --> 00:03:08,680
If you don't do this part then, it won't be able to track the things that you're adding to your network, and it just won't work.

18
00:03:08,680 --> 00:03:19,250
So, here, we can create our hidden layers using nn.Linear. So, what this does, is it creates a operation for the linear transformation.

19
00:03:19,250 --> 00:03:26,205
So, when we take our inputs x and then multiply it by weights and add your bias terms, that's a linear transformation.

20
00:03:26,205 --> 00:03:43,540
So, what this does is calling NN.Linear, it creates an object that itself has created parameters for the weights and parameters for the bias and then, when you pass a tensor through this hidden layer, this object, it's going to automatically calculate the linear transformation for you.

21
00:03:43,540 --> 00:03:48,770
So, all you really need to do is tell it what's the size of the inputs, and then what are the size of the output.

22
00:03:48,770 --> 00:03:58,105
So, 784 by 256, we're going to use 256 outputs for this. So, it's kind of rebuilding the network that we saw before.

23
00:03:58,105 --> 00:04:17,530
Similarly, we want another linear transformation between our hidden units and our output. So, again, we have 256 hidden units, and we have 10 outputs, 10 output units, so we're going to create a output layer called self.output, and create this linear transformation operation.

24
00:04:17,530 --> 00:04:25,135
We also want to create a sigmoid operation for the activation and then, softmax for the output, so we get this probability distribution.

25
00:04:25,135 --> 00:04:34,890
Now, we're going to create a forward method and so, forward is basically going to be, as we pass a tensor in to the network.

26
00:04:34,890 --> 00:04:43,560
It's gonna go through all these operations, and eventually give us our output. So, here, x, the argument is going to be the input tensor and then, we're going to pass it through our hidden layer.

27
00:04:43,560 --> 00:05:02,060
So, this is again, like this linear transformation that we defined up here, and it's going to go through a sigmoid activation, and then through our output layer or output linear transformation, we have here, and then through the sigmoid function, and then finally return the output of our softmax.

28
00:05:02,060 --> 00:05:14,845
so we can create this. Then, if we kind of look at it, so it'll print it out, and it'll tell us the operations, and not necessarily the order, but at least it tells us the operations that we have defined for this network.

29
00:05:14,845 --> 00:05:24,950
You can also use some functional definitions for things like sigmoid and softmax, and it kind of makes the class the way you write the code a little bit cleaner.

30
00:05:24,950 --> 00:05:35,545
We can get that from torch.nn.functional. Most of the time, you'll see is like import torch.nn.functional as capital F. So, there's kind of that convention in PyTorch code.

31
00:05:35,545 --> 00:05:43,335
So, again, we define our linear transformations, self.hidden, self.output but now in our forward method.

32
00:05:43,335 --> 00:05:53,310
So, we can call self.hidden to get like our values for hidden layer, but then, we pass it through the sigmoid function, f.sigmoid, and the same thing with the output layers.

33
00:05:53,310 --> 00:05:59,510
So, we have our output linear transformations of the output, and we pass it through this softmax operation.

34
00:05:59,510 --> 00:06:09,665
So, the reason we can do this because, when we create these linear transformations, it's creating the weights and bias matrices on its own.

35
00:06:09,665 --> 00:06:25,700
But for sigmoid and softmax, it's just an element wise operation, so it doesn't have to create any extra parameters or extra matrices to do these operations, and so we can have these be purely functional without having to create any sort of object or classes.

36
00:06:25,700 --> 00:06:39,490
However, they are equivalent. So this way to build the network is equivalent to this way up here, but it's a little bit more succinct when you're doing it with these kind of functional pattern.

37
00:06:39,490 --> 00:06:47,660
So far, we've only been using the sigmoid function as an activation function, but there are, of course, a lot of different ones you want to use.

38
00:06:47,660 --> 00:07:07,010
Really the only requirement is that, these activation functions should typically be non-linear. So, if you want your network to be able to learn non-linear correlations and patterns, and we want the output to be non-linear, then you need to use non-linear activation functions in your hidden layers.

39
00:07:07,010 --> 00:07:20,670
So, a sigmoid is one example. The hyperbolic tangent is another. One that is pretty much used all the time, like almost exclusively as activation function and hidden layers, is the ReLU, so the rectified linear unit.

40
00:07:20,670 --> 00:07:35,465
This is basically the simplest non-linear function that you can use, and it turns out that networks tend to train a lot faster when using ReLU as compared to sigmoid and hyperbolic tangent, so ReLU was what we typically use.

41
00:07:35,465 --> 00:07:49,010
Okay. So, here, you're going to build your own neural network, that's larger. So, this time, it's going to have two hidden layers, and you'll be using the ReLU activation function for this on your hidden layers.

42
00:07:49,010 --> 00:08:09,000
So using this object-oriented class method within a.module, go ahead and build a network that looks like this, with 784 input units, a 128 units in the first hidden layer, 64 units and the second hidden layer, and then 10 output units. All right. Cheers.


@@@
1
00:00:00,000 --> 00:00:09,195
Hello, everyone, and welcome back. So, in this video and in this notebook, I'll be showing you how to actually train neural networks in PyTorch.

2
00:00:09,195 --> 00:00:20,610
So, previously, we saw how to define neural networks in PyTorch using the nn module, but now we're going to see how we actually take one of these networks that we defined and train it.

3
00:00:20,610 --> 00:00:26,415
So, what I mean by training is that we're going to use our neural networks as a universal function approximator.

4
00:00:26,415 --> 00:00:38,450
What that means is that, for basically any function, we have some desired input for example, an image of the number four, and then we have some desired output of this function.

5
00:00:38,450 --> 00:00:52,240
In this case a probability distribution that is telling us the probabilities of the various digits. So, in this case, if we passed it in image four, we want to get out a probability distribution where there's a lot of probability in the digit four.

6
00:00:52,240 --> 00:01:17,970
So, the cool thing about neural networks is that if you use non-linear activations and then you have the correct dataset of these images that are labeled with the correct ones, then basically you pass in an image and the correct output, the correct label or class, and eventually your neural network will build to approximate this function that is converting these images into this probability distribution, and that's our goal here.

7
00:01:17,970 --> 00:01:31,315
So, basically we want to see how in PyTorch, we can build a neural network and then we're going to give it the inputs and outputs, and then adjust the weights of that network so that it approximates this function.

8
00:01:31,315 --> 00:01:40,130
So, the first thing that we need for that is what is called a loss function. So, it's sometimes also called the cost, and what this is it's a measure of our prediction error.

9
00:01:40,130 --> 00:01:54,195
So, we pass in the image of a four and then our network predicts something else that's an error. So, we want to measure how far away our networks prediction is from the correct label, and we do that using loss function.

10
00:01:54,195 --> 00:02:04,430
So, in this case, it's the mean squared error. So, a lot of times you'll use this in regression problems, but use other loss functions and classification problems like this one here.

11
00:02:04,430 --> 00:02:14,080
So, the loss depends on the output of our network or the predictions our network is making. The output of a network depends on the weight.

12
00:02:14,080 --> 00:02:26,730
So, like the network parameters. So, we can actually adjust our weights such that this loss is minimized, and once the loss is minimized, then we know that our network is making as good predictions as it can.

13
00:02:26,730 --> 00:02:36,350
So, this is the whole goal to adjust our network parameters to minimize our loss, and we do this by using a process called gradient descent.

14
00:02:36,350 --> 00:02:45,380
So, the gradient is the slope of the loss function with respect to our perimeters. The gradient always points in the direction of fastest change.

15
00:02:45,380 --> 00:02:56,360
So, for example if you have a mountain, the gradient is going to always point up the mountain. So, you can imagine our loss function being like this mountain where we have a high loss up here and we have a low loss down here.

16
00:02:56,360 --> 00:03:02,825
So, we know that we want to get to the minimum of our loss when we minimize our loss, and so, we want to go downwards.

17
00:03:02,825 --> 00:03:17,085
So, basically, the gradient points upwards and so, we just go the opposite direction. So, we go in the direction of the negative gradient, and then if we keep following this down, then eventually we get to the bottom of this mountain, the lowest loss.

18
00:03:17,085 --> 00:03:26,015
With multilayered neural networks, we use an algorithm called backpropagation to do this. Backpropagation is really just an application of the chain rule from calculus.

19
00:03:26,015 --> 00:03:35,540
So, if you think about it when we pass in some data, some input into our network, it goes through this forward pass through the network to calculate our loss.

20
00:03:35,540 --> 00:03:52,430
So, we pass in some data, some feature input x and then it goes through this linear transformation which depends on our weights and biases, and then through some activation function like a sigmoid, through another linear transformation with some more weights and biases, and then that goes in, from that we can calculate our loss.

21
00:03:52,430 --> 00:04:02,645
So, if we make a small change in our weights here, W1, it's going to propagate through the network and end up like results in a small change in our loss.

22
00:04:02,645 --> 00:04:12,130
So, you can think of this as a chain of changes. So, if we change here, this is going to change. Even that's going to propagate through here, it's going to propagate through here, it's going to propagate through here.

23
00:04:12,130 --> 00:04:35,930
So, with backpropagation, we actually use these same changes, but we go in the opposite direction. So, for each of these operations like the loss and the linear transformation into the sigmoid activation function, there's always going to be some derivative, some gradient between the outputs and inputs, and so, what we do is we take each of the gradients for these operations and we pass them backwards through the network.

24
00:04:35,930 --> 00:04:45,650
Each step we multiply the incoming gradient with the gradient of the operation itself. So, for example just starting at the end with the loss.

25
00:04:45,650 --> 00:05:03,335
So, we pass this gradient or the loss dldL2. So, this is the gradient of the loss with respect to the second linear transformation, and then we pass that backwards again and if we multiply it by the loss of this L2.

26
00:05:03,335 --> 00:05:10,975
So, this is the linear transformation with respect to the outputs of our activation function, that gives us the gradient for this operation.

27
00:05:10,975 --> 00:05:21,865
If you multiply this gradient by the gradient coming from the loss, then we get the total gradient for both of these parts, and this gradient can be passed back to this softmax function.

28
00:05:21,865 --> 00:05:31,880
So, as the general process for backpropagation, we take our gradients, we pass it backwards to the previous operation, multiply it by the gradient there, and then pass that total gradient backwards.

29
00:05:31,880 --> 00:05:38,165
So, we just keep doing that through each of the operations in our network, and eventually we'll get back to our weights.

30
00:05:38,165 --> 00:05:53,895
What this does is it allows us to calculate the gradient of the loss with respect to these weights. Like I was saying before, the gradient points in the direction of fastest change in our loss, so, to maximize it.

31
00:05:53,895 --> 00:06:06,965
So, if we want to minimize our loss, we can subtract the gradient off from our weights, and so, what this will do is it'll give us a new set of weights that will in general result in a smaller loss.

32
00:06:06,965 --> 00:06:19,175
So, the way that backpropagation algorithm works is that it will make a forward pass through a network, calculate the loss, and then once we have the loss, we can go backwards through our network and calculate the gradient, and get the gradient for a weights.

33
00:06:19,175 --> 00:06:29,520
Then we'll update the weights. Do another forward pass, calculate the loss, do another backward pass, update the weights, and so on and so on and so on, until we get sufficiently minimized loss.

34
00:06:29,520 --> 00:06:38,770
So, once we have the gradient and like I was saying before, we can subtract it off from our weights, but we also use this term Alpha which is called the learning rate.

35
00:06:38,770 --> 00:06:45,864
This is basically just a way to scale our gradients so that we're not taking too large steps in this iterative process.

36
00:06:45,864 --> 00:06:55,250
So, what can happen if you're update steps are too large, you can bounce around in this trough around the minimum and never actually settle in the minimum of the loss.

37
00:06:55,250 --> 00:07:06,905
So, let's see how we can actually calculate losses in PyTorch. Again using the nn module, PyTorch provides us a lot of different losses including the cross-entropy loss.

38
00:07:06,905 --> 00:07:17,480
So, this loss is what we'll typically use when we're doing classification problems. In PyTorch, the convention is to assign our loss to its variable criterion.

39
00:07:17,480 --> 00:07:25,060
So, if we wanted to use cross-entropy, we just say criterion equals nn.crossEntropyLoss and create that class.

40
00:07:25,060 --> 00:07:39,060
So, one thing to note is that, if you look at the documentation for cross-entropy loss, you'll see that it actually wants the scores like the logits of our network as the input to the cross-entropy loss.

41
00:07:39,060 --> 00:07:46,405
So, you'll be using this with an output such as softmax, which gives us this nice probability distribution.

42
00:07:46,405 --> 00:07:55,705
But for computational reasons, then it's generally better to use the logits which are the input to the softmax function as the input to this loss.

43
00:07:55,705 --> 00:08:15,110
So, the input is expected to be the scores for each class and not the probabilities themselves. So, first I'm going to import the necessary modules here and also download our data and create it in, like you've seen before, as a trainloader, and so, we can get our data out of here.

44
00:08:15,110 --> 00:08:21,860
So, here I'm defining a model. So, I'm using nn.Sequential, and if you haven't seen this, checkout the end of the previous notebook.

45
00:08:21,860 --> 00:08:41,250
So, the end of part two, will show you how to use nn.Sequential. It's just a somewhat more concise way to define simple feed-forward networks, and so, you'll notice here that I'm actually only returning the logits, the scores of our output function and not the softmax output itself.

46
00:08:41,250 --> 00:09:00,710
Then here we can define our loss. So, criterions equal to nn.crossEntropyLoss. We get our data with images and labels, flatten it, pass it through our model to get the logits, and then we can get the actual loss by bypassing in our logits and the true labels, and so, again we get the labels from our trainloader.

47
00:09:00,710 --> 00:09:10,975
So, if we do this, we see we have calculated the loss. So, my experience, it's more convenient to build your model using a log-softmax output instead of just normal softmax.

48
00:09:10,975 --> 00:09:17,250
So, with a log-softmax output to get the actual probabilities, you just pass it through torch.exp. So, the exponential.

49
00:09:17,250 --> 00:09:33,980
With a log-softmax output, you'll want to use the negative log-likelihood loss or nn.NLLLoss. So, what I want you to do here is build a model that returns the log-softmax as the output, and calculate the loss using the negative log-likelihood loss.

50
00:09:33,980 --> 00:09:45,200
When you're using log-softmax, make sure you pay attention to the dim keyword argument. You want to make sure you set it right so that the output is what you want.

51
00:09:45,200 --> 00:09:53,730
So, go and try this and feel free to check out my solution. It's in the notebook and also in the next video, if you're having problems. Cheers.


@@@
1
00:00:00,000 --> 00:00:09,915
Hi and welcome back. Here's my solution for this model that uses a LogSoftmax output. It is a pretty similar to what I built before with an index sequential.

2
00:00:09,915 --> 00:00:20,055
So, we just use a linear transformation, ReLU, linear transformation, ReLU, another linear transformation for output and then we can pass this to our LogSoftmax module.

3
00:00:20,055 --> 00:00:31,215
So, what I'm doing here is I'm making sure I set the dimension to one for LogSoftmax and this makes it so that it calculates the function across the columns instead of the rows.

4
00:00:31,215 --> 00:00:40,750
So, if you remember, the rows correspond to our examples. So, we have a batch of examples that we're passing to our network and each row is one of those examples.

5
00:00:40,750 --> 00:00:52,240
So, we want to make sure that we're covering the softmax function across each of our examples and not across each individual feature in our batches.

6
00:00:52,240 --> 00:01:04,310
Here, I'm just defining our loss or criterion as the negative log likelihood loss and again get our images and labels from our train loader, flatten them, pass it through our model to get the logits.

7
00:01:04,310 --> 00:01:14,345
So, this is actually not the largest anymore, this is like a log probability, so we call it like logps, and then you do that.

8
00:01:14,345 --> 00:01:23,195
There you go. You see we get our nice loss. Now, we know how to calculate a loss, but how do we actually use it to perform backpropagation?

9
00:01:23,195 --> 00:01:30,460
So, PyTorch towards actually has this really great module called Autograd that automatically calculates the gradients of our tensors.

10
00:01:30,460 --> 00:01:47,015
So, the way it works is that, PyTorch will keep track of all the operations you do on a tensor and then when you can tell it to do a backwards pass, to go backwards through each of those operations and calculate the gradients with respect to the input parameters.

11
00:01:47,015 --> 00:02:03,845
In general, you need to tell PyTorch that you want to use autograd on a specific tensor. So, in this case, you would create some tensor like x equals torch.zeros, just to make it a scalar, say one and then give it requires grad equals true.

12
00:02:03,845 --> 00:02:13,540
So, this tells PyTorch to track the operations on this tensor x, so that if you want to get the gradient then it will calculate it for you.

13
00:02:13,540 --> 00:02:21,790
So, in general, if you're creating a tensor and you don't want to calculate the gradient for it, you want to make sure this is set to false.

14
00:02:21,790 --> 00:02:31,255
You can also use this context torch.no grad to make sure all the gradients are shut off for all of the operations that you're doing while you're in this context.

15
00:02:31,255 --> 00:02:39,060
Then, you can also turn on or off gradients globally with torch.set grad enabled and give it true or false, depending on what you want to do.

16
00:02:39,060 --> 00:02:49,290
So, the way this works in PyTorch is that you basically create your tensor and again, you set requires grad equals true and then you just perform some operations on it.

17
00:02:49,290 --> 00:03:04,595
Then, once you are done with those operations, you type in.backwards. So, if you use x, this tensor x, then calculate some other tensor z then if you do z.backward, it'll go backwards through your operations and calculate the total gradient for x.

18
00:03:04,595 --> 00:03:12,320
So, for example, if I just create this random tensor, random two-by-two tensor, and then I can square it like this.

19
00:03:12,320 --> 00:03:25,125
What it does, you can actually see if you look at y, so y is our secondary or squared tensor. If you look at y.grad function, then it actually shows us that this grad function is a power.

20
00:03:25,125 --> 00:03:36,335
So, PyTorch just track this and it knows that the last operation done was a power operation. So, now, we can take the mean of y and get another tensor z.

21
00:03:36,335 --> 00:03:45,850
So, now this is just a scalar tensor, we've reduced y, y is a two-by-two matrix, two by two array and then we take in the mean of it to get z.

22
00:03:45,850 --> 00:03:59,410
Ingredients for tensor show up in this attribute grad, so we can actually look at what's the gradient of our tensor x right now, and we've only done this forward pass, we haven't actually calculated the gradient yet and so it's just none.

23
00:03:59,410 --> 00:04:05,550
So, now if we do z.backward, it's going to go backwards through this tiny little set of operations that we've done.

24
00:04:05,550 --> 00:04:10,795
So, we did a power and then a mean and let's go backwards through this and calculate the gradient for x.

25
00:04:10,795 --> 00:04:21,680
So, if you actually work out the math, you find out that the gradient of z with respect to x should be x over two and if we look at the gradient, then we can also look at x divided by two then they are the same.

26
00:04:21,680 --> 00:04:31,430
So, our gradient is equal to what it should be mathematically, and this is the general process for working with gradients, and autograd, and PyTorch.

27
00:04:31,430 --> 00:04:42,350
Why this is useful, is because we can use this to get our gradients when we calculate the loss. So, if remember, our loss depends on our weight and bias parameters.

28
00:04:42,350 --> 00:04:52,960
We need the gradients of our weights to do gradient descent. So, what we can do is we can set up our weights as tensors that require gradients and then do a forward pass to calculate our loss.

29
00:04:52,960 --> 00:04:59,990
With the loss, you do a backwards pass which calculates the gradients for your weights, and then with those gradients, you can do your gradient descent step.

30
00:04:59,990 --> 00:05:20,930
Now, I'll show you how that looks in code. So, here, I'm defining our model like I did before with LogSoftmax output, then using the negative log-likelihood loss, get our images and labels from our train loader, flatten it, and then we can get our log probabilities from our model and then pass that into our criterion, which gives us the actual loss.

31
00:05:20,930 --> 00:05:29,600
So, now, if we look at our models weights, so model zero gives us the parameters for this first linear transformation.

32
00:05:29,600 --> 00:05:38,260
So, we can look at the weight and then we can look at the gradient, then we'll do our backwards pass starting from the loss and then we can look at the weight gradients again.

33
00:05:38,260 --> 00:05:47,630
So, we see before the backward pass, we don't have any because we haven't actually calculated it yet but then after the backwards pass, we have calculated our gradients.

34
00:05:47,630 --> 00:05:57,830
So, we can use these gradients in gradient descent to train our network. All right. So, now you know how to calculate losses and you know how to use those losses to calculate gradients.

35
00:05:57,830 --> 00:06:09,230
So, there's one piece left before we can start training. So, you need to see how to use those gradients to actually update our weights, and for that we use optimizers and these come from PyTorch's Optim package.

36
00:06:09,230 --> 00:06:23,940
So, for example, we can use stochastic gradient descent with optim.SGD. The way this is defined is we import this module optim from PyTorch and then we'd say optim.SGD, we give it our model parameters.

37
00:06:23,940 --> 00:06:32,245
So, these are the parameters that we want this optimizer to actually update and then we give it a learning rate, and this creates our optimizer for us.

38
00:06:32,245 --> 00:06:48,540
So, the training pass consists of four different steps. So, first, we're going to make a forward pass through the network then we're going to use that network output to calculate the loss, then we'll perform a backwards pass through the network with loss.backwards and this will calculate the gradients.

39
00:06:48,540 --> 00:07:00,410
Then, we'll make a step with our optimizer that updates the weights. I'll show you how this works with one training step and then you're going to write it up for real and a loop that is going to train a network.

40
00:07:00,410 --> 00:07:08,350
So, first, we're going to start by getting our images and labels like we normally do from our train loader and then we're going to flatten them.

41
00:07:08,350 --> 00:07:14,970
Then next, what we want to do is actually clear the gradients. So, PyTorch by default accumulates gradients.

42
00:07:14,970 --> 00:07:27,100
That means that if you actually do multiple passes and multiple backwards like multiple forward passes, multiple backwards passes, and you keep calculating your gradient, it's going to keep summing up those gradients.

43
00:07:27,100 --> 00:07:38,725
So, if you don't clear gradients, then you're going to be getting gradients from the previous training step in your current training step and it's going to end up where your network is just not training properly.

44
00:07:38,725 --> 00:07:54,480
So, for this in general, you're going to be calling zero grad before every training passes. So, you just say optimizer.zero grad and this will just clean out all the gradients and all the parameters in your optimizer, and it'll allow you to train appropriately.

45
00:07:54,480 --> 00:08:07,725
So, this is one of the things in PyTorch that is easy to forget, but it's really important. So, try your hardest to remember to do this part, and then we do our forward pass, backward pass, then update the weights.

46
00:08:07,725 --> 00:08:20,360
So, we get our output, so we do a forward pass through our model with our images, then we calculate the loss using the output of the model and our labels, then we do a backwards pass and then finally we take an optimizer step.

47
00:08:20,360 --> 00:08:32,330
So, if we look at our initial weights, so it looks like this and then we can calculate our gradient, and so the gradient looks like and then if we take an optimizer step and update our weights, then our weights have changed.

48
00:08:32,330 --> 00:08:42,185
So, in general, what has worked is you're going to be looping through your training set and then for each batch out of your training set, you'll do the same training pass.

49
00:08:42,185 --> 00:08:56,285
So, you'll get your data, then clear the gradients, pass those images or your input through your network to get your output, from that, in the labels, calculate your loss, and then do a backwards pass on the loss and then update your weights.

50
00:08:56,285 --> 00:09:20,120
So, now, it's your turn to implement the training loop for this model. So, the idea here is that we're going to be looping through our data-set, so grabbing images and labels from train loader and then on each of those batches, you'll be doing the training pass, and so you'll do this pass where you calculate the output of the network, calculate the loss, do backwards pass on loss, and then update your weights.

51
00:09:20,120 --> 00:09:26,980
Each pass through the entire training set is called an epoch and so here I just have it set for five epochs.

52
00:09:26,980 --> 00:09:36,620
So, you can change this number if you want to go more or less. Once you calculate the loss, we can accumulate it to keep track, so we're going to be looking at a loss.

53
00:09:36,620 --> 00:09:47,555
So, this is running loss and so we'll be printing out the training loss as it's going along. So, if it's working, you should see the loss start falling, start dropping as you're going through the data.


@@@
1
00:00:00,000 --> 00:00:13,095
Hi again. So, here's my solution for the train pass that I had you implement. So, here we're just defining our model like normal and then our negative log-likelihood loss using stochastic gradient descent and pass in our parameters.

2
00:00:13,095 --> 00:00:23,190
Then, here is our training pass. So, for each image in labels in trainloader, we're going to flatten it and then zero out the gradients using optimizer. zero_ grad.

3
00:00:23,190 --> 00:00:32,960
Pass our images forward through the model and the output and then from that, we can calculate our loss and then do a backward pass and then finally with the gradients, we can do this optimizer step.

4
00:00:32,960 --> 00:00:40,305
So, if I run this and we wait a little bit for to train, we can actually see the loss dropping over time, right?

5
00:00:40,305 --> 00:00:51,950
So, after five epochs, we see that the first one, it starts out fairly high at 1.9 but after five epochs, continuous drop as we're training and we see it much lower after five epochs.

6
00:00:51,950 --> 00:00:58,445
So, if we kept training then our network would learn the data better and better and the training loss would be even smaller.

7
00:00:58,445 --> 00:01:05,655
So, now with our training network, we can actually see what our network thinks it's seen in these images.

8
00:01:05,655 --> 00:01:14,610
So, for here, we can pass in an image. In this case, it's the image of a number two and then this is what our network is predicting now.

9
00:01:14,610 --> 00:01:23,480
So, you can see pretty easily that it's putting most of the probability, most of its prediction into the class for the digit two.

10
00:01:23,480 --> 00:01:34,970
So we try it again and put in passes in number eight and again, it's predicting eight. So, we've managed to actually train our network to make accurate predictions for our digits.

11
00:01:34,970 --> 00:01:46,070
So next step, you'll write the code for training a neutral network on a more complex dataset and you'll be doing the whole thing, defining the model, running the training loop, all that. Cheers.


@@@
1
00:00:00,000 --> 00:00:08,724
Welcome back. So, in this notebook, you'll be building your own neural network to classify clothing images.

2
00:00:08,724 --> 00:00:18,914
So, like I talked about in the last video, MNIST is actually a fairly trivial dataset these days. It's really easy to get really high accuracy with a neural network.

3
00:00:18,914 --> 00:00:28,199
So instead, you're going to be using Fashion-MNIST, and this is basically just a drop-in replacement for MNIST so we have 28 by 28 grayscale images, but this time it's clothing.

4
00:00:28,199 --> 00:00:41,219
So, you have a lot more variation in the classes, and it just ends up being a much more difficult problem to classify like there's a t-shirt, there's pants, there's a sweater, there's shoes instead of handwritten digits.

5
00:00:41,219 --> 00:00:50,330
So it's a better representation of datasets that you'd use in the real world. So, I've left this up to you to actually build a network and train it.

6
00:00:50,329 --> 00:00:59,884
So here you can define your network architecture, then here you will create your network to define the criterion and optimizer and then write the code for the training pass.

7
00:00:59,884 --> 00:01:19,519
Once you have your network built and trained, you can test out your network. So here, you'd want to do a forward pass, get your logits, calculate the class probabilities, maybe output of your network, and then pass in one of these images from the test set and check out if your network can actually predict it correctly.

8
00:01:19,519 --> 00:01:25,200
If you want to see my solution, it's in the next notebook, part five, and you'll also see it in the next video. Cheers.


@@@
1
00:00:00,000 --> 00:00:09,015
Again. So, in the last video, I hand you try out building your own neural network to classify this fashion in this dataset.

2
00:00:09,015 --> 00:00:18,270
Here is my solution like how I decided to build this. So first, building our network. So, here, I'm going to import our normal modules from PyTorch.

3
00:00:18,270 --> 00:00:23,565
So, nn and optim, so, nn is going to allow us to build our network, and optim is going to give us our optimizers.

4
00:00:23,565 --> 00:00:33,810
I must going to import this functional modules, so, we can use functions like ReLU and log softmax. I decided to define my network architectures using the class.

5
00:00:33,810 --> 00:00:41,270
So, in nn.modules subclassing from this, and it's called a classifier. Then I created four different linear transformations.

6
00:00:41,270 --> 00:00:49,190
So, in this case, it's three hidden layers and then one output layer. Our first hidden layer has 256 units.

7
00:00:49,190 --> 00:00:57,330
The second hidden layer has a 128, one after that has 64. Then our output has 10 units. So, in the forward pass, I did something a little different.

8
00:00:57,330 --> 00:01:08,335
So, I made sure here that the input tensor is actually flattened. So now, you don't have to flatten your input tensors in the training loop, it'll just do it in the forward pass itself.

9
00:01:08,335 --> 00:01:16,185
So, to do this is do x.view, which is going to change our shape. So, x.shape zero is going to give us our batch size.

10
00:01:16,185 --> 00:01:24,320
Then the negative one here is going to basically fill out the the second dimension with as many elements as it needs to keep the same total number of elements.

11
00:01:24,320 --> 00:01:30,560
So, what this does is it basically gives us another tensor, that is the flattened version of our input tensor.

12
00:01:30,560 --> 00:01:43,205
It doing a pass these through our linear transformations, and then ReLU activation functions. Then finally, we use a log softmax with a dimension set to one, as our output, and return that from our forward function.

13
00:01:43,205 --> 00:01:53,435
With the model defined, I can do model equals classifiers. So, this actually creates our model. Then we define our criterion with the negative log likelihood loss.

14
00:01:53,435 --> 00:02:03,150
So, I'm using log softmax as the output my model. So, I want to use the NLLLoss as the criterion. Then, here I'm using the Adam optimizer.

15
00:02:03,150 --> 00:02:14,570
So, this is basically the same as stochastic gradient descent, but it has some nice properties where it uses momentum which speeds up the actual fitting process.

16
00:02:14,570 --> 00:02:24,465
It also adjust the learning rate for each of the individual parameters in your model. Here, I wrote my training loop.

17
00:02:24,465 --> 00:02:36,900
So, again I'm using five epochs. So, for e in range epoch, so, this is going to basically loop through our dataset five times, I'm tracking the loss with running loss, and just kind of instantiated it here.

18
00:02:36,900 --> 00:02:45,635
Then, getting our images. So, from images labels in train loader, so I get our log probabilities by passing in the images to a model.

19
00:02:45,635 --> 00:02:53,810
So, one thing to note, you can kind of do a little shortcut. If you just pass these in to model as if it was a function, then it will run the forward method.

20
00:02:53,810 --> 00:03:03,560
So, this is just a kind of a shorter way to run the forward pass through your model. Then with the log probabilities and the labels, I can calculate the loss.

21
00:03:03,560 --> 00:03:11,890
Then here, I am zeroing the gradients. Now I'm doing the lost up backwards to calculating our gradients, and then with the gradients, I can do our optimizer step.

22
00:03:11,890 --> 00:03:20,555
If we tried it, we can see at least for these first five epochs that are loss actually drops. Now the network is trained, we can actually test it out.

23
00:03:20,555 --> 00:03:33,980
So, we pass in data to our model, calculate the probabilities. So, here, doing the forward pass through the model to get our actual log probabilities and with the log probabilities, you can take the exponential to get the actual probabilities.

24
00:03:33,980 --> 00:03:44,195
Then with that, we can pass it into this nice little view classify function that I wrote, and it shows us, if we pass an image of a shirt, it tells us that it's a shirt.


@@@
1
00:00:00,000 --> 00:00:09,555
Hey there. So now, we're going to start talking about inference and validation. So, when you have your trained network, you typically want to use it for making predictions.

2
00:00:09,555 --> 00:00:21,170
This is called inference, it's a term borrowed from statistics. However, neural networks have a tendency to perform too well on your training data and they aren't able to generalize the data that your network hasn't seen before.

3
00:00:21,170 --> 00:00:37,025
This is called overfitting. This happens because as you're training more and more and more on your training set, your network starts to pick up correlations and patterns that are in your training set but they aren't in the more general dataset of all possible handwritten digits.

4
00:00:37,025 --> 00:00:45,260
So, to test for overfitting, we measure the performance of the network on data that isn't in the training set.

5
00:00:45,260 --> 00:00:57,545
This data is usually called the validation set or the test set. So, while we measure the performance on the validation set, we also tried to reduce overfitting through regularization such as dropout.

6
00:00:57,545 --> 00:01:06,985
So, in this notebook, I'll show you how we can both look at our validation set and also use dropout to reduce overfitting.

7
00:01:06,985 --> 00:01:14,855
So, to get the training set for your data like from PyTorch, then we say train equals true and for fashionMNIST.

8
00:01:14,855 --> 00:01:21,175
To get our test set, we're actually going to set train equals false here. Here, I'm just defining the model like we did before.

9
00:01:21,175 --> 00:01:28,190
So, the goal of validation is to measure our model's performance on data that is not part of our training set.

10
00:01:28,190 --> 00:01:34,105
But what we mean by performance is up to you, up to the developer, the person who's writing the code.

11
00:01:34,105 --> 00:01:45,340
A lot of times, it'll just be the accuracy. So, like how many correct classifications did our model make compared to all of the predictions?

12
00:01:45,340 --> 00:01:55,475
And other options for metrics are precision and recall, and the top five error rate. So here, I'll show you how to actually measure the accuracy on the validation set.

13
00:01:55,475 --> 00:02:01,850
So first, I'm going to do a forward pass that is one batch from the test set. So, see in our test set we get our probabilities.

14
00:02:01,850 --> 00:02:14,445
So, just 64 examples in a batch. Then 10 columns like one for each of the classes. So, the accuracy, we want to see if our model made the correct prediction of the class given the image.

15
00:02:14,445 --> 00:02:24,780
The prediction we can consider it to be whichever class has the highest probability. So, for this, we can use this top-k method on our tensors.

16
00:02:24,780 --> 00:02:33,105
This returns the k highest values. So, if we pass in one, then this is going to give us the one highest value.

17
00:02:33,105 --> 00:02:50,500
This one highest value is the most likely class that our network is predicting. So, for the first ten examples, and this batch of test data that I grabbed, we see that the class four and class five are what are being predicted for these.

18
00:02:50,500 --> 00:02:58,690
So, remember that this network actually hasn't been trained yet, and so it's just making these guesses randomly because it doesn't really know anything about the data yet.

19
00:02:58,690 --> 00:03:07,760
So, top-k actually returns a tuple with two tensors. So, the first tensor is the actual probability values, and the second tensor are the class indices themselves.

20
00:03:07,760 --> 00:03:16,220
So typically, we just want this top class here. So, I'm calling top-k here and I'm separating out the probabilities in the classes.

21
00:03:16,220 --> 00:03:25,085
So, we'll just use this top class going forward. So, now that we have the predicted classes from our network, we can compare that with the true labels.

22
00:03:25,085 --> 00:03:34,700
So, we say, we can say like top class equals equals labels. The only trick here is that we need to make sure our top class tensor and the labels tensor has the same shape.

23
00:03:34,700 --> 00:03:52,240
So, this equality actually operates appropriately like we expect. So, labels from the test loader is actually a 1D tensor with 64 elements, but top class itself is a 2D tensor, 64 by one.

24
00:03:52,240 --> 00:04:01,250
So here, I'm just like changing the shape of labels to match the shape of top class. This gives us this equals tensor.

25
00:04:01,250 --> 00:04:07,745
We can actually see it looks like. So, it gives us a bunch of zeros and ones. So, zeros are where they don't match, and then ones are where they do match.

26
00:04:07,745 --> 00:04:14,850
Now, we have this tensor that's all just a bunch of zeros and ones. So, if we want to know the accuracy, right?

27
00:04:14,850 --> 00:04:20,960
We can just sum up all the correct things, all the correct predictions, and then divide by the total number of predictions.

28
00:04:20,960 --> 00:04:36,100
If you're tensor is all zeros and ones, that's actually equivalent to taking the mean. So for that, we can do torch.mean, but the problem is that equals is actually a byte tensor, and torch.mean won't work on byte tensors.

29
00:04:36,100 --> 00:04:45,200
So, we actually need to convert equals until a float tensor. If we do that, then we can actually see our accuracy for this one particular batch is 15.6 percent.

30
00:04:45,200 --> 00:04:51,590
So, this is roughly what we expect. So, our network hasn't been trained yet. It's making pretty much random guesses.

31
00:04:51,590 --> 00:05:01,910
That means that we should see our accuracy be about one in ten for any particular image because it's just uniformly guessing one of the classes, okay?

32
00:05:01,910 --> 00:05:12,260
So here, I'm going to have you actually implement this validation loop, where you'll pass in data from the test set through the network and calculate the loss and the accuracy.

33
00:05:12,260 --> 00:05:18,860
So, one thing to note, I think I mentioned this before. For the validation paths, we're not actually going to be doing any training.

34
00:05:18,860 --> 00:05:24,695
So, we don't need the gradients. So, you can actually speed up your code a little bit if you turn off the gradients.

35
00:05:24,695 --> 00:05:34,435
So, using this context, so with torch.no_grad, then you can put your validation pass in here. So, for images and labels in your test loader and then do the validation pass here.

36
00:05:34,435 --> 00:05:45,590
So, I've basically built a classifier for you, set all this up. Here's the training pass, and then it's up to you to implement the validation pass, and then print out the accuracy.


@@@
1
00:00:00,000 --> 00:00:10,125
Welcome back. So, here's my solution for the validation pass. So, here, our model has been defined, our loss, and our optimizer, and all this stuff.

2
00:00:10,125 --> 00:00:19,800
I've set to 30 epochs so we can see like how this trains or how the training loss drops, and how the validation loss changes over time.

3
00:00:19,800 --> 00:00:28,155
The way this works is that after each pass, after each epoch, after each pass through the training set, then we're going to do a validation pass.

4
00:00:28,155 --> 00:00:38,060
That's what this else here means. So, basically, four of this stuff and then else basically says after this four loop completes, then run this code.

5
00:00:38,060 --> 00:00:53,725
That's what this else here means. As seen before, we want to turn off our gradients so with torch.no_grad, and then we're going to get our images and labels from a test set, pass it into the images into our model to get our log probabilities, calculate our loss.

6
00:00:53,725 --> 00:01:05,120
So, here, I am going to just be updating our test_loss. So, test_loss is just an integer that's going to count up our loss on our test set as we're training, as we're doing more of these validation passes.

7
00:01:05,120 --> 00:01:17,030
So, this way, we can actually track the test loss over all the epochs that we're training. So, from the law of probabilities, I can get our actual probability distributions using torch.exponential.

8
00:01:17,030 --> 00:01:34,285
Again, topk1 gives us our predicted classes and we can measure, we can calculate the equalities. So, here, we can get our probabilities from our log probabilities using torch.exp, taking the exponential of a log gives you back into the probabilities.

9
00:01:34,285 --> 00:01:43,110
From that, we do ps.topk, so one, and this gives us the top_class or predicted class from the network.

10
00:01:43,110 --> 00:01:52,535
Then, using checking for equality, we can see where our predicted classes match with the true classes from labels.

11
00:01:52,535 --> 00:01:59,125
Again, measure our calculator accuracy. So, using torch.mean and changing equals into a FloatTensor.

12
00:01:59,125 --> 00:02:08,965
So, I'm going to run this and then let it run for a while, and then we can see what the actual training and validation losses look like as we were training this network.

13
00:02:08,965 --> 00:02:16,985
Now, the network is trained, we can see how the validation loss and the training loss actually changed over time like as we continue training on more and more data.

14
00:02:16,985 --> 00:02:32,530
So, we see is the training loss drops but the validation loss actually starts going up over time. There's actually a clear sign of overfitting so our network is getting better and better and better on the training data, but it's actually starting to get worse on the validation data.

15
00:02:32,530 --> 00:02:38,795
This is because as it's learning the training data, it's failing to generalize the data outside of that.

16
00:02:38,795 --> 00:02:51,560
Okay. So, this is what the phenomenon of overfitting looks like. The way that we combine it, the way that we try to avoid this and prevent it is by using regularization and specifically, dropout.

17
00:02:51,560 --> 00:03:03,965
So, deep behind dropout is that we randomly drop input units between our layers. What this does is it forces the network to share information between the weights, and so this increases this ability to generalize to new data.

18
00:03:03,965 --> 00:03:21,685
PyTorch adding dropout is pretty straightforward. We just use this nn.Dropout module. So, we can basically create our classifier like we had before using the linear transformations to do our hidden layers, and then we just add self.dropout, nn.Dropout, and then you give it some drop probability.

19
00:03:21,685 --> 00:03:28,655
In this case, this is 20 percent, so this is the probability that you'll drop a unit. In the forward method, it's pretty similar.

20
00:03:28,655 --> 00:03:44,265
So, we just pass in x, which is our input tensor, we're going to make sure it's flattened, and then we pass this tensor through each of our fully connected layers into an activation, relu activation and then through dropout.

21
00:03:44,265 --> 00:03:50,760
Our last layer is the output layer so we're not going to use dropout here. There's one more thing to note about this.

22
00:03:50,760 --> 00:03:59,390
So, when we're actually doing inference, if we're trying to make predictions with our network, we want to have all of our units available, right?

23
00:03:59,390 --> 00:04:05,970
So, in this case, we want to turn off dropout when we're doing validation, testing, when we're trying to make predictions.

24
00:04:05,970 --> 00:04:19,715
So, to do that, we do model.eval. So, model.eval will turn off dropout and this will allow us to get the most power, the highest performance out of our network when we're doing inference.

25
00:04:19,715 --> 00:04:25,780
Then, to go back in the train mode, use model.train. So, then, the validation pass looks like this now.

26
00:04:25,780 --> 00:04:36,720
So, first, we're going to turn off our gradients. So, with torch.no_grad, and then we set our model to evaluation mode, and then we do our validation pass through the test data.

27
00:04:36,720 --> 00:04:42,545
Then, after all this, we want to make sure the model is set back to train mode so we do model.train.

28
00:04:42,545 --> 00:04:52,050
Okay. So, now, I'm going to leave it up to you to create your new model. Try adding dropout to it and then try training your model with dropout.


@@@
1
00:00:00,000 --> 00:00:14,985
Hi. Here's my solution for your building and training this network using dropout now. Just like I showed you before, we can define our dropout module as self.dropout and then nn.dropout to give it some drop probability.

2
00:00:14,985 --> 00:00:20,895
So, in this case, 20 percent, and then just adding it to our forward method now on each of our hidden layers.

3
00:00:20,895 --> 00:00:32,725
Now, our validation code looks basically the same as before except now we're using model.eval. So, again, this turns our model into evaluation or inference mode which turns off dropout.

4
00:00:32,725 --> 00:00:48,860
Then, like the same way before, we just go through our data and the test say, calculate the losses and accuracy and after all that, we do model.train to set the model back into train mode, turn dropout back on, and then continue on in train smart.

5
00:00:48,860 --> 00:01:03,445
So, now, we're using dropout and if you look at again the training loss and the validation loss over these epochs that we're training, you actually see that the validation loss sticks a lot closer to the train loss as we train.

6
00:01:03,445 --> 00:01:17,610
So, here, with dropout, we've managed to at least reduce overfitting. So, the validation losses isn't as low as we got without dropout being is still, you can see that it's still dropping.

7
00:01:17,610 --> 00:01:25,600
So, if we kept training for longer, we would most likely manage to get our validation loss lower than without dropout.


@@@
1
00:00:00,000 --> 00:00:07,395
Welcome back. So, in this video, I'm going to be showing you how to save and load models that you've trained with PyTorch.

2
00:00:07,394 --> 00:00:22,560
Again, this is important because most of the time you'll want to train your network on some data and then just save it to disk, and then you can later load it up and train more or use it for inference, like making predictions.

3
00:00:22,559 --> 00:00:35,924
So I've already gone ahead and ran most of this codes, trained my network on fashion hymnist. Again, we see we get about an accuracy of 84-85 percent.

4
00:00:35,924 --> 00:00:47,385
Now that the network is trained, we actually don't want to take these weights, never really learned this problem, and we want to save them so that we can in the future load them back up and use it again.

5
00:00:47,384 --> 00:00:56,295
These weights, these parameters, are actually stored in model.state_dict. So we print this out, so we can see our network here.

6
00:00:56,295 --> 00:01:07,170
So we have two hidden layers. So, this one goes to 500, 500 to 100, and then this goes to our output layer of 10 output units, and then we have drop out.

7
00:01:07,170 --> 00:01:15,740
So this is what our network looks like. If we look at the state_dict keys, this is showing us we have hidden layers at zero weight. So that's here.

8
00:01:15,739 --> 00:01:23,790
Hidden layers gives us a bias, the second hidden layer gives us the weight and also the weights and bias for our output layer.

9
00:01:23,790 --> 00:01:32,789
So that the tensors that for our weights and biases are actually stored and model.state_dict. So this is what typically you want to save.

10
00:01:32,790 --> 00:01:42,629
So to save the state_dict, it's pretty simple. So use the torch.save, and then pass in your model.state_dict and then call it checkpoint.pth.

11
00:01:42,629 --> 00:01:52,245
There you go. It's saved. So once it's saved, we can then load in the state_dict. So state_dict is equal to torch.load.

12
00:01:52,245 --> 00:02:03,750
Let's give it the same thing, checkpoint.pth, and then one p to state_dict, then we can print this out just see.keys.

13
00:02:03,750 --> 00:02:09,479
So it gives us the same layers that we saw up here, so in layers.0.weight. So, we all see the same thing.

14
00:02:09,479 --> 00:02:19,139
But the state_dict, it's self loaded. How do we actually load this into a model? So we say model.load_state_dict and we pass in our state_dict.

15
00:02:19,139 --> 00:02:28,470
So notice that we need to have our model existing already. We need to have it already created, and it's going to be created with randomly initialize the weights and biases.

16
00:02:28,469 --> 00:02:38,685
The parameters are going to be randomly initialized. Basically we just pass in our state dict and then those random parameters are replaced by the ones that we trained previously.

17
00:02:38,685 --> 00:02:48,549
This seems pretty straightforward, you just save your state_dict and you can load it backup into a model, but it's not actually all that simple, and I'll show you why.

18
00:02:48,550 --> 00:03:04,370
So, if I create a new network, 784 at normal, 10 output units, but this time I'm going to use three hidden layers with 400, 200 and 100 units each.

19
00:03:04,370 --> 00:03:16,820
Okay. So now, I have this pre-trained State_dict, right? So it's like, okay well I created this new model, I'm going to try to load this state_dict, and it gives me an error.

20
00:03:16,819 --> 00:03:23,924
So basically it says, inconsistent tensor size, expected tensor, this size but the sources is dense.

21
00:03:23,925 --> 00:03:33,219
Right? So basically what that means is that when I trained this state_dict with this checkpoint, I had a different network architecture.

22
00:03:33,219 --> 00:03:39,699
So that in the original network I had 500 units in the first layer, and now I have 400 units in the first layer.

23
00:03:39,699 --> 00:03:50,424
So, the thing is that when you build your network and you train it, when you load the state_dict back up, it has to go to a model with exactly the same architecture.

24
00:03:50,424 --> 00:04:01,519
So if you have code that can generate networks with different architectures when you load your your state_dict back up, you also have to include information about the architecture.

25
00:04:01,520 --> 00:04:13,010
So, for instance, here with this network since I have an arbitrary number of hidden layers, I need to include this information about the hidden layers in my checkpoint that I saved.

26
00:04:13,009 --> 00:04:20,680
To rebuild this model exactly as it was trained, I'm going to store the state_dict and all the information about the model architecture.

27
00:04:20,680 --> 00:04:40,960
Here I can say let's say, create a dictionary, given it my input size it's 784, my output size 10. So if I have a model, then I can do up features for each model.

28
00:04:41,089 --> 00:04:50,790
So remember here that model.hidden_layers is a list of these linear operations. So basically hidden_layers is a list of the layers in the network.

29
00:04:50,790 --> 00:04:59,475
So, to get the actual values I had set for the hidden_layers depending on the number of units, I just call out features on each of those layers.

30
00:04:59,475 --> 00:05:07,469
So this is basically just going through each of the layers, in this list of hidden layers and then getting the number of features or units in that layer.

31
00:05:07,470 --> 00:05:14,459
Then I can save the state_dict. Then once I have that dictionary, then I can simply save the dictionary.

32
00:05:14,459 --> 00:05:27,225
Checkpoint, checkpoint.pth again. There you go. Now our checkpoint has all the necessary information we need to rebuild our model.

33
00:05:27,225 --> 00:05:38,280
So what I'd like to do is create a function just called like load_checkpoint and give it a file path, and then we can load our checkpoint.

34
00:05:38,279 --> 00:05:52,580
So our checkpoint is torch.load(file path) then we can create our model. So our model is network, then we get our parameters for this model from our checkpoint.

35
00:05:52,579 --> 00:06:15,085
So we can put in the input size, we can put in the output size, we pass in the hidden layers. So what this does is passing in all the necessary arguments parameters to create our model.

36
00:06:15,084 --> 00:06:29,589
So, once we have our model, then we can load the state_dict from uh, I want to return the model. Then we can call the checkpoint, passed in checkpoint.pth.

37
00:06:32,750 --> 00:06:44,859
So then we load the model and we successfully load the state_dict and everything is present. So in general that's how you're going to save and load your networks.

38
00:06:44,860 --> 00:06:54,100
So, it's important to remember that every parameter that you use for building your network, you have to also include that in your checkpoints.


@@@
1
00:00:00,000 --> 00:00:08,670
In this video, I'll be showing you how to load image data. This is really useful for what you'll be doing in real projects.

2
00:00:08,669 --> 00:00:25,475
So previously, we used MNIST. Fashion-MNIST were just toy datasets for testing your networks, but you'll be using full-size images like you'd get from smartphone cameras and your actual projects that you'll be doing with deep learning networks.

3
00:00:25,475 --> 00:00:34,350
So with this, we'll be using a dataset of cat and dog photos, super cute. That come from Kaggle. So, if you want to learn more about it, you can just click on this link.

4
00:00:34,350 --> 00:00:44,609
So, you can see our images are now much larger, much higher resolution and they're coming in different shapes and sizes than what we saw with MNIST and fashion-MNIST.

5
00:00:44,609 --> 00:00:54,165
So, the first step to using these is to actually load them in with PyTorch. Then once you have them in, you can train a network using these things.

6
00:00:54,164 --> 00:01:01,019
So, the easiest way to load in our image data is with datasets.ImageFolder. This is from torchvision, that datasets module.

7
00:01:01,020 --> 00:01:11,420
So basically, you just pass in a path to your dataset, so into the folder where your data is sitting into image folder and give us some transforms, which we talked about before.

8
00:01:11,420 --> 00:01:22,609
I'll go into some more detail about transforms next. So, the image folder, it expects your files and directories to look like this, where you have some root directory that's where all your data.

9
00:01:22,609 --> 00:01:29,280
Then each of the different classes has their own folder. So in this case, we have two classes. We have dog and cat.

10
00:01:29,280 --> 00:01:35,659
So, we have these two folders, dog and cat. Get more classes like for MNIST, now you have ten classes.

11
00:01:35,659 --> 00:01:48,795
There will be one folder for each of the different digits, right? Those are our classes or labels. Then within each of the specific class folders, you have your images that belong to those classes.

12
00:01:48,795 --> 00:01:53,939
So, in your dog folder are going to be all of your dog pictures and the cat folder are going to be all of your cat pictures.

13
00:01:53,939 --> 00:02:01,665
So, if you're working in a workspace, then the data should already be there, but if you're working on your local computer, you can get the data by clicking here.

14
00:02:01,665 --> 00:02:09,180
I've also already split this into a training set and test set for you. When you load in the image folder, you need to define some transforms.

15
00:02:09,180 --> 00:02:22,060
So, what I mean by this is you'll want to resize it, you can crop it, you can do a lot of things like typically you'll want to convert it to a PyTorch tensor and it is loaded in as a pillow image.

16
00:02:22,060 --> 00:02:30,489
So, you need to change the image into a tensor. Then you combine these transforms into a pipeline of transforms, using transforms.compose.

17
00:02:30,490 --> 00:02:43,099
So, if you want to resize your image to be 255 by 255, then you say transforms.resize 255 and then you take just the center portion, you just crop that out with a size of 224 by 224.

18
00:02:43,099 --> 00:02:54,224
Then you can convert it to a tensor. So, these are the transforms that you'll use and you pass this into ImageFolder to define the transforms that you're performing on your images.

19
00:02:54,224 --> 00:03:01,830
Once you have your dataset from your image folder, defining your transforms and then you pass that to dataloader.

20
00:03:01,830 --> 00:03:13,125
From here, you can define your batch size, so it's the number of images you get per batch like per loop through this dataloader and then you can also do things like set shuffle to true.

21
00:03:13,125 --> 00:03:31,064
So basically, what shuffle does is it randomly shuffles your data every time you start a new epoch. This is useful because when you're training your network, we prefer it the second time it goes through to see your images in a different order, the third time it goes through you see your images in a different order.

22
00:03:31,064 --> 00:03:39,944
Rather than just learning in the same order every time because then this could introduce weird artifacts in how your network is learning from your data.

23
00:03:39,944 --> 00:03:47,580
So, the thing to remember is that this dataloader that you get from this class dataloader, the actual dataloader object itself, is a generator.

24
00:03:47,580 --> 00:03:57,314
So, this means to get data out of it you actually have to loop through it like in a for loop or you need to call iter on it, to turn into an iterator.

25
00:03:57,314 --> 00:04:09,160
Then call next to get the data out of it. Really what's happening here in this for loop, this for images comma labels in dataloader is actually turning this into an iterator.

26
00:04:09,159 --> 00:04:14,795
Every time you go through a loop, it calls next. So basically, this for loop is an automatic way of doing this.

27
00:04:14,794 --> 00:04:23,589
Okay. So, I'm going to leave up to you is to define some transforms, create your image folder and then pass that image folder to create a dataloader.

28
00:04:23,589 --> 00:04:32,649
Then if you do everything right, you should see an image that looks like this. So, that's the basic way of loading in your data.

29
00:04:32,649 --> 00:04:41,870
You can also do what's called data augmentation. So, what this is is you want to introduce randomness into your data itself.

30
00:04:41,870 --> 00:04:55,449
What this can do is you can imagine if you have images, you can translate where a cat shows up and you can rotate the cat, you can scale the cat, you can crop different parts of things, you can mirror it horizontally and vertically.

31
00:04:55,449 --> 00:05:05,500
What this does is it helps your network generalized because it's seen these images in different scales, at different orientations and so on.

32
00:05:05,500 --> 00:05:14,819
This really helps your network train and will eventually lead to better accuracy on your validation tests.

33
00:05:14,819 --> 00:05:30,149
Here, I'll let you define some transforms for training data. So here, you want to do the data augmentation thing, where you're randomly cropping and resizing and rotating your images and also define transforms for the test dataset.

34
00:05:30,149 --> 00:05:36,660
So, one thing to remember is that for testing when you're doing your validation, you don't want to do any of this data augmentation.

35
00:05:36,660 --> 00:05:49,469
So basically, you just want to just do a resize and center crop of your images. This is because you want your validation to be similar to the eventual like in state of your model.

36
00:05:49,470 --> 00:06:00,370
Once you train your data, you're going to be sending in pictures of cats and dogs. So, you want your validation set to look pretty much exactly like what your eventual input images will look like.

37
00:06:00,370 --> 00:06:06,530
If you do all that correctly, you should see training examples are like this. So, you can see how these are rotated.

38
00:06:06,529 --> 00:06:13,280
Then you're testing examples should look like this, where they are scaled proportionally and they're not rotated.

39
00:06:13,279 --> 00:06:22,200
Once you've loaded this data, you should try to build a network based on what you've already learned that can then classify cats and dogs from this dataset.

40
00:06:22,199 --> 00:06:30,450
I should warn you this is actually a pretty tough challenge and it probably won't work. So, don't try too hard at it.

41
00:06:30,449 --> 00:06:37,829
Before you used MNIST and fashion-IMNIST. Those are very simple images, right? So, there are 20 by 28.

42
00:06:37,829 --> 00:06:47,204
They only have grayscale colors. But now these cat and dog images, they're much larger. Their colors, so you have those three channels.

43
00:06:47,204 --> 00:06:56,009
Just in general, it's going to be very difficult to build a classifier that can do this just using this fully connected network.

44
00:06:56,009 --> 00:07:04,130
The next part, I'll show you how to use a pre-trained network to build a model that can actually classify these cat and dog images. Cheers.


@@@
1
00:00:00,000 --> 00:00:07,305
Hello, and welcome back. So, here are my solutions for the exercises I had you do on loading image data.

2
00:00:07,305 --> 00:00:17,790
Here, I had you define some transforms and then load the actual dataset with image folder and then turn that into a data loader using this torch utils data loader class.

3
00:00:17,790 --> 00:00:36,630
So, here, I chose a couple transforms. So, first, I'm resizing the images to be 255 by 255 squares. So, basically, even if your image is actually a rectangle, then this will resize it to be square with 255 pixels on each size.

4
00:00:36,630 --> 00:00:44,240
The first transform I used was resize. So, this resizes your images to be squares with 255 pixels on each side.

5
00:00:44,240 --> 00:00:53,535
So, even if your original image is a rectangle, this will change it into a square. Then, I did a center crop with 224 pixels.

6
00:00:53,535 --> 00:01:04,285
So, this crops a square out of the center of the image with 224 pixels on each side. Then, I convert it into a tensor which we can then use in our networks.

7
00:01:04,285 --> 00:01:13,720
With the transform defined, we can pass that into this image folder and along with the path to our dataset and that creates a dataset object.

8
00:01:13,720 --> 00:01:21,640
Then, with the dataset object, we can pass that to data loader. So, this will give us back a generator were we actually can get our images and labels.

9
00:01:21,640 --> 00:01:36,010
So, here, I just chose a batch size of 32 and this shuffle set to true. So, basically, every time you loop through the generator again like multiple times, every time you do that, it'll randomly shuffle the images and labels.

10
00:01:36,010 --> 00:01:45,020
So, that loaded, here's what it looks like. We have a nice little dogs now here. So, here, I had you define transforms for our training data and our testing data.

11
00:01:45,020 --> 00:01:58,810
So, like I was saying before, with training data, you typically want to do data augmentation. So, that means rotating it, resizing it, flipping it, et cetera, to create this simulated dataset of more images than we actually have.

12
00:01:58,810 --> 00:02:06,865
Firstly, it just gives you more data to actually train with. But secondly, it helps the network generalize to images that aren't in the training set.

13
00:02:06,865 --> 00:02:15,140
So, my transformations here, I first chose to do a random rotation with 30 degrees. So, this is going to rotate in either direction up to 30 degrees.

14
00:02:15,140 --> 00:02:26,000
Then, I did a random resize crop. So, this is going to randomly resize the image and then take a crop from the center of 224 pixels square.

15
00:02:26,000 --> 00:02:33,150
Then, after that crop, then it do a random horizontal flip. So, it's going to mirror it horizontally and change it to a tensor.

16
00:02:33,150 --> 00:02:42,155
Then, with the test transforms kind of the same as before resize it to 255 pixels and then do a center crop 224, and it finally change it to a tensor.

17
00:02:42,155 --> 00:02:51,560
Then, here with the train data and test data, we can pass our data directories and our transforms through this image folder.

18
00:02:51,560 --> 00:03:07,600
I should actually load the data, and then give our loaded data to our data loaders to actually get our load our datasets so that we can see data from the train loader, it looks like this, and we can see data from our test loader, so like that.


@@@
1
00:00:00,000 --> 00:00:11,079
Hello everyone, welcome back. So in this network, we will be using a pre-trained network to solve this challenging problem of creating a classifier for your cat and dog images.

2
00:00:11,080 --> 00:00:19,605
These pre-trained networks were trained on ImageNet which is a massive dataset of over one million labeled images from 1,000 different categories.

3
00:00:19,605 --> 00:00:36,790
These are available from torchvision and this module, torchvision.models. And so, we see we have six different architectures that we can use, and here's a nice breakdown of the performance of each of these different models.

4
00:00:36,789 --> 00:00:51,429
So, AlexNet gives us the top one error and the top five error. So basically, as you see, some of these networks and these numbers here, 19, 11, 34, and so on, they usually indicate the number of layers in this model.

5
00:00:51,429 --> 00:01:00,005
So, the larger this number, the larger the model is. And accordingly, the larger the model is, you get better accuracy, you get lower errors.

6
00:01:00,005 --> 00:01:08,310
At the same time, again, the larger the model is, the longer it's going to take to compute your predictions and to train and all that.

7
00:01:08,310 --> 00:01:20,015
So when you're using these, you need to think about the tradeoff between accuracy and speed. So, all these networks use an architecture called convolutional layers.

8
00:01:20,015 --> 00:01:28,760
What these do, they exploit patterns and regularities in images. I'm not going to get into the details but if you want to learn more about them, you can watch this video.

9
00:01:28,760 --> 00:01:40,659
So we're saying, these deep learning networks are typically very deep. So that means, they have dozens or even hundreds of different layers, and they were trained on this massive ImageNet dataset.

10
00:01:40,659 --> 00:01:46,715
It turns out that they were astonishingly well as future detectors for images that they weren't trained on.

11
00:01:46,715 --> 00:01:52,524
So using a pre-trained network like this on a training set that it hasn't seen before is called transfer learning.

12
00:01:52,525 --> 00:02:04,774
So basically, what's learned from the ImageNet dataset is being transferred to your dataset. So here, we're going to use transfer learning to train our own network to classify our cat and dog photos.

13
00:02:04,775 --> 00:02:17,170
What you'll see is you'll get really good performance with very little work on our side. So again, you can download these models from torchvision.models, this model here, so we can include this in our imports, right here.

14
00:02:17,169 --> 00:02:26,719
Most of these pre-trained models require a 224 by 224 image as the input. You'll also need to match a normalization used when these models were trained on ImageNet.

15
00:02:26,719 --> 00:02:36,500
So when they train these models, each color channel and images were normalized separately. And you can see the means here and the standard deviations here.

16
00:02:36,500 --> 00:02:42,115
So, I'm going to leave it up to you to define the transformations for the training data and the testing data now.

17
00:02:42,115 --> 00:02:48,560
And if you're done, we can get to a new one. Now, let's see how we can actually load in one of these models.

18
00:02:48,560 --> 00:03:03,375
So here, I'm going to use the Densenet-121 model. So you see, it has very high accuracy on the ImageNet dataset and it's one 121 tells us that it has 121 layers.

19
00:03:03,375 --> 00:03:14,280
To load this in our code and use it, so we just say model models.densenet121 and then we say pretrained equals true.

20
00:03:14,280 --> 00:03:21,594
So this is going to download the pre-trained network, the weights, the parameters themselves, and then load it into our model.

21
00:03:21,594 --> 00:03:31,515
So now, we can do that and then we can look at what the architecture of this model. And this is what our DenseNet architecture looks like.

22
00:03:31,514 --> 00:03:44,025
So, you'll notice that we have this features part here and then a bunch of these layer. So this is like a convolutional layer which again I'm not going to talk about here but you don't really need to understand it to be able to actually use this thing.

23
00:03:44,025 --> 00:03:53,025
There's two main parts that we're interested in. So firstly, again, this features part, but then if we scroll all the way to the bottom, we also see this classifier part.

24
00:03:53,025 --> 00:04:05,324
So we see here is that we have the classifier. This has been defined as a linear combination layer, it's a fully connected dense layer, and it has 1,024 input features and then 1,000 output features.

25
00:04:05,324 --> 00:04:15,629
So again, the ImageNet dataset has 1,000 different classes. And so, the the number of outputs of this network should be 1,000 for each of those classes.

26
00:04:15,629 --> 00:04:26,740
So, the thing to know is that this whole thing was trained on ImageNet. Now, the features will work for other datasets but the classifier itself has been trained for ImageNet.

27
00:04:26,740 --> 00:04:32,199
So this is the part that we need to retrain, the classifier. We want to keep the feature part static.

28
00:04:32,199 --> 00:04:41,035
We don't want to update that, but we just need to update the classifier part. So then, the first thing we need to do is freeze our feature parameters.

29
00:04:41,035 --> 00:04:54,000
To do that, we go through our parameters in our model. And then, we just say, requires_grad equals false.

30
00:04:54,000 --> 00:05:03,250
So what this will do is that when we run our tensors through the model, it's not going to calculate the gradients.

31
00:05:03,250 --> 00:05:19,689
It's not going to keep track of all these operations. So firstly, this is going to ensure that our our feature parameters don't get updated but it also will speed up training because we're not keeping track of these operations for the features.

32
00:05:19,689 --> 00:05:26,410
Now, we need to replace the classifier with our own classifier. So here, I'm going to use a couple of new things.

33
00:05:26,410 --> 00:05:39,608
I'm going to use the sequential module available from PyTorch. And so, what this does, you basically just give it a list of different operations you want to do and then it will automatically pass a tensor through them sequentially.

34
00:05:39,608 --> 00:05:59,425
So, you can pass in an ordered dict to name each of these layers. So I'll show you how this works. So we want a fully connected layer, so I'll just name it FC1, and then that is a fully connected layer coming from 1,024 inputs and I'm going to say 500 for this hidden layer.

35
00:05:59,425 --> 00:06:10,404
And then we want to pass this through ReLu activation and then this should go through another fully connected layer and this will be our output layer.

36
00:06:10,404 --> 00:06:20,740
So, 500 to two, so we have cat and dog, so we want two outputs here. And finally, our output is going to be the LogSoftmax like before.

37
00:06:20,740 --> 00:06:37,189
Okay, and that is how we define the classifier. So now, we can take this classifier, just a classifier built from fully connected layers, and we can attach it to our model.classifier.

38
00:06:37,189 --> 00:06:45,935
So now, the new classifier that we built that is untrained is attached to our model and this model also has the features parts.

39
00:06:45,935 --> 00:06:52,914
The features parts are going to remain frozen. We're not going to update those weights but we need to train our new classifier.

40
00:06:52,915 --> 00:06:59,929
Now, if we want to train our network that we're using, this Densenet-121 is really deep and it has 121 layers.

41
00:06:59,930 --> 00:07:07,514
So, if we can try to train this on the CPU like normal, it's going to take pretty much forever. So instead, what we can do is use the GPU.

42
00:07:07,514 --> 00:07:18,104
GPUs are built specifically for doing a bunch of linear algebra computations in parallel and our neural networks are basically just a bunch of linear algebra computations.

43
00:07:18,105 --> 00:07:25,170
So if we run these on the GPU, they're done in parallel and we get something like 100 times increase speeds.

44
00:07:25,170 --> 00:07:44,035
In PyTorch, it's pretty straightforward to use the GPU. If you have your model, so model, the idea is that your model has all these parameters in there tensors that are sitting in your memory on your computer, but we can move them over to our GPU by saying model.cuda.

45
00:07:44,035 --> 00:07:53,905
So what this does is it moves the parameters for your model to the GPU and then all of the computations and the processing and are going to be done on the GPU.

46
00:07:53,904 --> 00:08:07,690
Similarly, if you have a tensor like your images, select images, if you want to run your images through your model, you have to make sure that the tensors that you're putting through your model or on the GPU if your model's on the GPU.

47
00:08:07,689 --> 00:08:15,699
So you just have to make those match up. So to do that, to move a tensor from computer to the GPU, you just, again, say images.cuda.

48
00:08:15,699 --> 00:08:43,804
So that will move a tensor, that's images, to the GPU. Then oftentimes, you'll want to move your model and your tensors back from the GPU to your local memory and CPU, and so, to do that, you just say like model.cpu or images.cpu, so this'll bring your tensors back from the GPU to your local computer to run on your CPU.

49
00:08:43,804 --> 00:08:53,860
Now, I'm going to give you a demonstration of how this all works and the amazing increased speed we get by using the GPU.

50
00:08:53,860 --> 00:09:06,085
So here, I'm just going to do for cuda and false, true. So this way, I'm going to be able to basically like loop through and try it once where we're not using the GPU, and once where we are using the GPU.

51
00:09:06,085 --> 00:09:13,594
So let's define my criterion which is going to be natural log_loss like we'd normally do, define our optimizer.

52
00:09:13,595 --> 00:09:23,284
So again, here, remember that we only want to update the parameters for the classifier. So we're just going to pass in model.classifier.parameters.

53
00:09:23,284 --> 00:09:32,344
This will work and that it's going to update the premise for our classifier but it's going to lead the parameters for the feature detector part of the model static.

54
00:09:32,345 --> 00:09:42,394
So I typically do is, say like, if cuda, then we want to move our model to the GPU. Otherwise, let's leave it on the CPU.

55
00:09:42,394 --> 00:10:06,585
And then I'm going to write a little training loop. We'll get our inputs and our labels, changes into variables like normal, then again, if we have cuda enabled, so if we have GPUs, then we can do inputs, labels, and we'll just move these over to the GPU.

56
00:10:06,585 --> 00:10:19,254
We're using the GPU now and we're also using this pre-trained network, but in general, you're going to do the training loop exactly the same way you have been doing it with these feed forward networks that you've been building.

57
00:10:19,254 --> 00:10:28,315
So first, I'm actually going to define a start time just so I can time things, then you just do your training pass like normal.

58
00:10:28,315 --> 00:10:35,189
So, you just do a forward pass through your model and you can calculate the loss, do your backward pass.

59
00:10:35,190 --> 00:10:47,950
Finally, update your weights with your optimizer. So I'm going to do here, I'm going to break this training loop after the first three iterations.

60
00:10:47,950 --> 00:11:08,260
So I want to time the difference between using a GPU and not using the GPU. What happens is the very first batch to go through the training loop tends to take longer than the other batches, so I'm just going to take the first three or four and then average over those just so we get a better sense of how long it actually takes to process one batch.

61
00:11:09,649 --> 00:11:23,710
So, that will just print out our training times. So we can see that if we're not using the GPU, then each batch takes five and a half seconds to actually go through this training step.

62
00:11:23,710 --> 00:11:33,095
Whereas, with the GPU, it only takes 0.012 seconds. So, I mean, this is a speedup of over 100 times.

63
00:11:33,095 --> 00:11:49,949
So here, I basically set cuda manually but you can also check if a GPU is available so you say torch.cuda is available, and this will give you back true or false depending if you have a GPU available that can use cuda.

64
00:11:49,950 --> 00:12:02,799
Okay, so from here, I'm going to let you finish training this model. So you can either continue with a DenseNet model that is already loaded or you can try ResNet which is also a good model to try out.


@@@
1
00:00:00,000 --> 00:00:05,910
Hi everyone, here is my solution for the transfer learning exercise that I had to do. So, this one's going to be a little different.

2
00:00:05,910 --> 00:00:14,565
I'm going to be typing it out as I do it so you can understand my that process is kind of the combination of everything you've learned in this lesson.

3
00:00:14,565 --> 00:00:23,895
So, the first thing I'm going do is, if I have a GPU available, I'm going to write this code in agnostic way so that I can use the GPU.

4
00:00:23,895 --> 00:00:40,290
So, what I'm going to say, device = torch.device and then this is going to be cuda. So, it's going to run on our GPU if torch.cuda is available, else CPU.

5
00:00:40,290 --> 00:00:50,330
So, what this will do is, if our GPU is available, then this will be true and then we'll return cuda here and then otherwise, it'll be CPU.

6
00:00:50,330 --> 00:00:57,700
So, now we can just pass device to all the tensors and models and then it will just automatically go to the GPU if we have it available.

7
00:00:57,700 --> 00:01:07,290
So, next, I'm going to get our pre-trained model. So, here I'm actually going to use ResNet. So, to do this, model dot models.

8
00:01:07,290 --> 00:01:12,850
So, we already imported models from torch vision then we can kind of like took out all the ones they have.

9
00:01:12,850 --> 00:01:22,905
So, there's ResNet there. So, I'm just going to use a fairly small one, ResNet 50 and then we want pre-trained true and that should get us our model.

10
00:01:22,905 --> 00:01:31,490
So, now if we look, so we can just print it out like this and it will tell us all the different operations and layers and everything that's going on.

11
00:01:31,490 --> 00:01:40,840
So, if we scroll down, we can see that at the end here has fc. So, this is the last layer, this fully connected layer that's acting like a classifier.

12
00:01:40,840 --> 00:01:47,660
So, we can see that it has, it expects 2,048 inputs to this layer and then the out features are 1,000.

13
00:01:47,660 --> 00:01:56,610
So, remember that this was trained on ImageNet and so ImageNet is typically trained with 1,000 different classes of images.

14
00:01:56,610 --> 00:02:11,615
But here we're only using cat and dog, so we just need to output features and in our classifier. So, we can load the model like that and now I'm going to make sure that our models' perimeters are frozen so that when we're training they don't get updated.

15
00:02:11,615 --> 00:02:19,325
So, I'll just run this make sure it works. So, now, we can load the model and we can turn off gradients.

16
00:02:19,325 --> 00:02:29,975
Turn off gradients for our model. So, then, the next step is we want to define our new classifier which we will be training.

17
00:02:29,975 --> 00:02:39,220
So, here, we can make it pretty simple. So, models= nn.sequential. You can define this in a lot of different ways, so I'm just using an industrial sequential here.

18
00:02:39,220 --> 00:02:53,080
So, our first layer, so linear, so remember we needed 248 inputs and then let's say, let's drop it down to 512 at a ReLu layer, a dropout.

19
00:02:53,080 --> 00:03:05,850
Now our output layer, 512 to two and then we're going to do log softmax. I should change this to be a classifier.

20
00:03:05,850 --> 00:03:13,580
Okay. So, that's to finding our classifier and now we can attach it to our model, so to say, model.fc= classifier.

21
00:03:13,580 --> 00:03:32,740
Now, if we look at our model again, so we can scroll down to the bottom here. So, we see now this fully-connected module layer here is a sequential classifier linear operation ReLu, dropout, another linear transformation and then log softmax.

22
00:03:32,740 --> 00:03:40,700
So, the next thing we do is define my loss, my criterion. So, this is going to be the negative log like we had loss.

23
00:03:40,700 --> 00:03:57,160
Then, define our optimizer, optim.Adam. So, we want to use the parameters from our classifier which is fc here and then set our learning rate.

24
00:03:57,160 --> 00:04:07,825
The final thing to do is to move our model to whichever device we have available. So, now we have the model all set up and it's time to train it.

25
00:04:07,825 --> 00:04:13,430
Here's is the first thing I'm going to do is define some variables that we're going to be using during the training.

26
00:04:13,430 --> 00:04:20,555
So, for example, I'm going to set our epochs, so I'm going to set to one. I'll be tracking the number of train steps we do, so set that to zero.

27
00:04:20,555 --> 00:04:33,820
I'll be tracking our loss, so also set this to zero, and finally, we want to kind of set a loop for how many steps we're going to go before we print out the validation loss.

28
00:04:33,820 --> 00:04:52,005
So, now we want to loop through our epochs. So, for epoch and range epochs. Now, we're going to loop through our data for images, labels in trainloader, cumulate steps.

29
00:04:52,005 --> 00:05:04,160
So, basically, every time we go through one of these batches, we're going to increment steps here. So, now that we have our images and our labels, we're going to want to move them over to the GPU, if that's available.

30
00:05:04,160 --> 00:05:17,820
So, we're just going to do is images.to (device), labels.to(device). Now we're just going to write out our training loop.

31
00:05:17,820 --> 00:05:25,170
So, the first thing we need to do is zero our gradients. So, it's very important, don't forget to do this.

32
00:05:25,170 --> 00:05:37,520
Then, get our log probabilities from our model, model passed in the images with the log probabilities, we can get our loss from the criterion in the labels.

33
00:05:37,520 --> 00:05:47,630
Then do a backwards pass and then finally with our optimizer we take a step. Here, we can increment are running loss like so.

34
00:05:47,630 --> 00:05:52,440
So, this way we can keep track of our training loss as we are going through more and more data. All right.

35
00:05:52,440 --> 00:05:58,730
So, that is the training loop. So, now every once in a while so which is set by this like print every variable.

36
00:05:58,730 --> 00:06:07,400
We actually want to drop out of the train loop and test our network's accuracy and loss on our test dataset.

37
00:06:07,400 --> 00:06:15,860
So, for step modulo print_every, this is equal to zero, then we're going to go into our validation loop.

38
00:06:15,860 --> 00:06:24,965
So, what we need to do first is set model.eval. So, this'll turn our model into evaluation inference mode which turns off dropout.

39
00:06:24,965 --> 00:06:34,260
So, we can actually accurately use our network for making predictions instead a test loss and accuracy.

40
00:06:34,260 --> 00:06:47,030
So, now we'll get our images and labels from our test data. Now we'll do our validation loop. So, with our model so we'll pass in the images.

41
00:06:47,030 --> 00:07:01,280
So, these are the images from our test set. So, we're going to get our logps from our test set so again, get the loss with our criterion and keep track of our loss to test loss plus+= loss.item.

42
00:07:01,280 --> 00:07:08,200
So, this will allow us to keep track of our test loss as we're going through these validation rules.

43
00:07:08,200 --> 00:07:27,380
So, next we want to calculate our accuracy. So, probabilities= torch.exponential(logps). So, remember that our model is returning log softmax, so it's the log probabilities of our classes and to get the actual probabilities, we're going to use torch.exponential.

44
00:07:27,380 --> 00:07:39,950
So we get our top probabilities and top classes from ps.topk(1). So, that's going to give us our first largest value in our probabilities.

45
00:07:39,950 --> 00:07:47,215
Here, we need to make sure we set dimension to one to make sure it's actually like looking for the top probabilities along the columns.

46
00:07:47,215 --> 00:07:58,280
Go to the top classes, now we can check for equality with our labels and then with the equality tensor, we can update our accuracy.

47
00:07:58,280 --> 00:08:13,320
So, here remember we can calculate our accuracy from equality. Once we change it to a FloatTensor then we can do torch.mean and get our accuracy and so again just kind of incremented accumulated into this accuracy variable. All right.

48
00:08:13,320 --> 00:08:29,140
Now, we are in this loop here so this four step every print_every. So basically, now we have a running loss of our training loss and we have a test loss that we passed our test data through our model and measured the loss in accuracy.

49
00:08:29,140 --> 00:08:33,580
So now we can print all this out and I'm just going to copy and paste this because it's a lot to type.

50
00:08:33,580 --> 00:08:40,475
So, basically here, we're just printing out our epochs. So, we can keep track and know where we are and keep track of that.

51
00:08:40,475 --> 00:08:49,610
So, running_loss divided by print_every so basically we're taking the average of our training loss. So every time we print it out, we're just going to take the average.

52
00:08:49,610 --> 00:09:00,540
Then, test_loss over length the testloader. So basically length test loader tells us how many batches are actually in our test dataset that we're getting from testloader.

53
00:09:00,540 --> 00:09:09,815
So, since we're we're summing up all the losses for each of our batches, if we take the total loss and divide by the number of batches and that gives us our average loss, we do the same thing with accuracy.

54
00:09:09,815 --> 00:09:18,540
So, we're summing up the accuracy for each batch here and then we just divide by the total number of batches and that gives us our average accuracy for the test set.

55
00:09:18,540 --> 00:09:29,095
Then at the end, we can set our running loss back to zero and then we also want to put our model back into training mode.

56
00:09:29,095 --> 00:09:38,410
Great. So, that should be the training code and we'll see if it works. Now, this should be an if instead of a for.

57
00:09:38,410 --> 00:09:51,095
So, here I forgot this happens a lot. I forgot to transfer my tensors over to the GPU. So, hopefully this will work. All right.

58
00:09:51,095 --> 00:09:57,820
So, even like pretty quickly, we see that we can actually get our test accuracy on this above 95 percent.

59
00:09:57,820 --> 00:10:06,950
So, this is, remember that we're printing this out every five steps and so this is a total of 15 batches, training batches that were updated in the model.

60
00:10:06,950 --> 00:10:15,530
So, we're able to easily fine tune these classifiers on top and get greater than a 95 percent accuracy on our dataset.


@@@
1
00:00:00,000 --> 00:00:05,429
Okay so, let's say we have a regular neural network which recognizes images and we fitted this image.

2
00:00:05,429 --> 00:00:13,070
And the neural neural network guesses that the image is most likely a dog with a small chance of being a wolf and an even smaller chance of being a goldfish.

3
00:00:13,070 --> 00:00:24,588
But, what if this image is actually a wolf? How would the neural network know? So, let's say we're watching a TV show about nature and the previous image before the wolf was a bear and the previous one was a fox.

4
00:00:24,588 --> 00:00:30,274
So, in this case, we want to use this information to hint to us that the last image is a wolf and not a dog.

5
00:00:30,274 --> 00:00:39,765
So, what we do is analyze each image with the same copy of a neural network. But, we use the output of the neural network as a part of the input of the next one.

6
00:00:39,765 --> 00:00:52,600
And, that will actually improve our results. Mathematically, this is simple. We just combine the vectors in a linear function, which will then be squished with an activation function, which could be sigmoid or hyperbolic tan.

7
00:00:52,600 --> 00:01:04,025
This way, we can use previous information and the final neural network will know that the show is about wild animals in the forest and actually use this information to correctly predict that the image is of a wolf and not a dog.

8
00:01:04,025 --> 00:01:12,640
And, this is basically how recurrent neural networks work. However, this has some drawbacks. Let's say the bear appeared a while ago and the two recent images are a tree and a squirrel.

9
00:01:12,640 --> 00:01:22,349
Based on those two, we don't really know if the new image is a dog or a wolf. Since trees and squirrels are just as associated to domestic animals as they are with forest animals.

10
00:01:22,349 --> 00:01:40,238
So, the information about being in the forest comes all the way back from the bear. But, as we've already experienced, information coming in gets repeatedly squished by sigmoid functions and even worse than that, training a network using back propagation all the way back, will lead to problems such as the vanishing gradient problem etc.

11
00:01:40,239 --> 00:01:49,213
So, by this point pretty much all the bear information has been lost. That's a problem with recurring neural networks; that the memory that is stored is normally short term memory.

12
00:01:49,213 --> 00:01:57,559
RNNs, have a hard time storing long term memory and this is where LSTMs or long short term memory networks will come to the rescue.

13
00:01:57,560 --> 00:02:06,299
So, as a small summary, an RNN works as follows; memory comes in and merges with a current event and the output comes out as a prediction of what the input is.

14
00:02:06,299 --> 00:02:18,254
And also, as part of the input for the next iteration of the neural network. And in a similar way, an LSTM works as follows; it keeps track not just of memory but of long term memory, which comes in and comes out.

15
00:02:18,253 --> 00:02:25,695
And also, short term memory, which also comes in and comes out. And in every stage, the long and short term memory in the event get merged.

16
00:02:25,694 --> 00:02:33,538
And from there, we get a new long term memory, short term memory and a prediction. In here, we protect old information more.

17
00:02:33,538 --> 00:02:43,000
If we deem it necessary, the network can remember things from long time ago. So, in the next few videos, I will show you the architecture of LSTMs and how they work.


@@@
1
00:00:00,000 --> 00:00:10,759
So let's recap. We have the following problem: we are watching a TV show and we have a long term memory which is that the show is about nature and science and lots of forest animal have appeared.

2
00:00:10,759 --> 00:00:21,704
We also have a short term memory which is what we have recently seen which is squirrels and trees. And we have a current event which is what we just saw, the image of a dog which could also be a wolf.

3
00:00:21,704 --> 00:00:34,060
And we want these three things to combine to form a prediction of what our image is. In this case, the long term memory which says that the show is about forest animals will give us a hint that the picture is of a wolf and not a dog.

4
00:00:34,060 --> 00:00:40,743
We also want the three pieces of information, long term memory, short term memory, and the event, to help us update the long term memory.

5
00:00:40,743 --> 00:00:49,590
So let's say we keep the fact that the show is about nature and we forget that it's about science. And we also remember that the show is about forest animals and trees since we recently saw a tree.

6
00:00:49,590 --> 00:00:57,664
So we add a bit and remove a bit to the long term memory. And finally we also want to use these three pieces of information to help us update the short term memory.

7
00:00:57,664 --> 00:01:06,055
So let's say in our short term memory you want to forget that the show has trees and remember that it has wolves since the trees happened a few images ago and we just saw a wolf.

8
00:01:06,055 --> 00:01:10,769
So basically we have an architecture like this and we use even more animals to represent our stages of memory.

9
00:01:10,769 --> 00:01:22,129
The long term memory is represented by an elephant since elephants have long term memory. The short term memory will be represented by a forgetful fish and the event will still be represented by the Wolf we just saw.

10
00:01:22,129 --> 00:01:30,760
So LSTM works as follows: the three pieces of information go inside the node and then some math happens and then the new pieces of information get updated and come out.

11
00:01:30,760 --> 00:01:38,799
There is a long term memory, a short term memory and the prediction of the event. More specifically the architecture of the LSTM contains a few gates.

12
00:01:38,799 --> 00:01:45,954
It contains a forget gate, a learn gate, a remember gate, and a use gate. And here's basically how they work.

13
00:01:45,953 --> 00:01:51,650
So the long term memory goes to the forget gate where it forgets everything that it doesn't consider useful.

14
00:01:51,650 --> 00:02:00,930
The short term memory and the event are joined together in the learn gate, containing the information that we've recently learned and it removes any unnecessary information.

15
00:02:00,930 --> 00:02:07,864
Now the long term memory that we haven't forgotten yet plus the new information that we've learned get joined together in the remember gate.

16
00:02:07,864 --> 00:02:14,860
This gate puts these two together and since it's called remember gate, what it does is it outputs an updated long term memory.

17
00:02:14,860 --> 00:02:31,093
So this is what we'll remember for the future. And finally, the use gate is the one that decides what information we use from what we previously know plus what we just learned to make a prediction so it also takes those inputs the long term memory, and the new information joins them and decides what to output.

18
00:02:31,093 --> 00:02:44,370
The output becomes both the prediction and the new short term memory. And so the big unfolded picture that we have is as follows: we have the long term memory and the short term memory coming in which we call LTM and STM.

19
00:02:44,370 --> 00:02:53,004
And then an event and an output are coming in and out of the LSTM. And then this passes to the next node, and so on and so forth.

20
00:02:53,003 --> 00:03:02,000
So in general at time t we label everything with an underscore t as we can see information passes from time t -1 to time t.


@@@
1
00:00:00,000 --> 00:00:20,184
So in order to study the architecture of an LSTM, let's quickly recall the architecture of an RNN. Basically what we do is we take our event E_t and our memory M_t-1, coming from the previous point in time, and we apply a simple tanh or sigmoid activation function to obtain the output and then your memory M_t.

2
00:00:20,184 --> 00:00:31,149
So to be more specific, we join these two vectors and multiply them by a matrix W and add a bias b, and then squish this with the tanh function, and that gives us the output M_t.

3
00:00:31,149 --> 00:00:43,990
This output is a prediction and also the memory that we carry to the next node. The LSTM architecture is very similar, except with a lot more nodes inside and with two inputs and outputs since it keeps track of the long- and short-term memories.

4
00:00:43,990 --> 00:00:51,274
And as I said, the short-term memory is, again, the output or prediction. Don't get scared. These are actually not as complicated as they look.


@@@
1
00:00:00,000 --> 00:00:06,708
So, let's keep this our base case. We have a long term memory which is at the show we're watching it's about nature and science.

2
00:00:06,708 --> 00:00:17,769
We also have a short term memory which is what we've recently seen, a squirrel and a tree. And finally, we have our current event which is a picture we just saw that looks like a dog but it could also be a wolf.

3
00:00:17,768 --> 00:00:26,429
So let's study the learn gate. What the learn gate does is the following. It takes a short term memory and the event and it joins it. Actually, it does a bit more.

4
00:00:26,428 --> 00:00:33,783
It takes the short term memory and the event and it combines them and then it ignores a bit of it keeping the important part of it.

5
00:00:33,783 --> 00:00:41,685
So here it forgets the fact that there's a tree and it remembers how we recently saw a squirrel and a dog/wolf.

6
00:00:41,685 --> 00:01:01,890
And how does this work mathematically? Well, it works like this. We have the short term memory STMt minus one and the event Et and it combines them by putting them through a linear function which consists of joining the vectors multiplying by a matrix adding a bias and finally squishing the result with a tanh activation function.

7
00:01:01,890 --> 00:01:11,593
Then the new information Nt has this form over here. Now, how do we ignore part of it? Well, by multiplying by an ignore factor, I-T.

8
00:01:11,593 --> 00:01:19,795
The ignore factor, I-T, is actually a vector but it multiplies element wise. And how do we calculate I-T?

9
00:01:19,795 --> 00:01:29,969
Well, we use our previous information of the short term memory and the event. So again, we create a small neural network whose inputs are the short term memory and the event.

10
00:01:29,968 --> 00:01:39,045
We'll pass them through a small linear function with a new matrix and a new bias and squish them with the sigmoid function to keep it between zero and one.


@@@
1
00:00:00,000 --> 00:00:08,035
Now, we go to the Forget Gate, this one works as follows: It takes a long term memory and it decides what parts to keep and to forget.

2
00:00:08,035 --> 00:00:15,724
In this case, the show is about nature and science and the forget gate decides to forget that the show is about science and keep the fact that it's about nature.

3
00:00:15,724 --> 00:00:29,885
How does the Forget Gate work mathematically? Very simple. The long-term memory (LTM) from time T minus 1 comes in, and it gets multiplied by a Forget Factor ft. And how does the forget factor ft get calculated?

4
00:00:29,885 --> 00:00:47,570
Well, simple. We'll use a short term memory STM and the event information to calculate ft. So, just as before, we run a small one layer neural network with a linear function combined with the sigmoid function to calculate this Forget Factor and that's how the Forget Gate works.


@@@
1
00:00:00,000 --> 00:00:10,570
And now we're going to learn the Remember Gate. This one is the simplest. It take the long-term memory coming out of the Forget Gate and the short-term memory coming out of the Learn Gate and simply combines them together.

2
00:00:10,570 --> 00:00:21,460
And how does this work mathematically? Again, very simple. We just take the outputs coming from the Forget Gate and from the Learn Gate and we just add them.


@@@
1
00:00:00,000 --> 00:00:13,389
And finally, we come to the use gate or output gate. This is the one that uses the long term memory that just came out of the forget gate and the short term memory that just came out of the learned gate, to come up with a new short term memory and an output.

2
00:00:13,390 --> 00:00:28,114
These are the same thing. In this case, we'll take what's useful from the long term memory which is this bear over here, and what's useful from the short term memory which is these dark wolf, and the squirrel, and that's what's going to be our new short term memory.

3
00:00:28,114 --> 00:00:34,990
So our output basically says, your image is most likely a wolf but it also carry some of the other animals that I've seen recently.

4
00:00:34,990 --> 00:00:50,955
And mathematically what this does is the following: it applies a small neural network on the output of the forget gate using the tanh activation function, and it applies to another small neural network on the short term memory and the events using the sigmoid activation function.

5
00:00:50,954 --> 00:00:59,134
And as a final step, it multiplies these two in order to get the new output. The output also worth of the new short term memory.


@@@
1
00:00:00,000 --> 00:00:09,993
So here we go. As we've seen before, here is the architecture for an LSTM with the four gates. There is the forget gate, which takes the long-term memory and forgets part of it.

2
00:00:09,993 --> 00:00:16,035
The learn gate puts the short-term memory together with the event as the information we've recently learned.

3
00:00:16,035 --> 00:00:24,760
The remember gate joins the long-term memory that we haven't yet forgotten plus the new information we've learned in order to update our long-term memory and output it.

4
00:00:24,760 --> 00:00:34,875
And finally, the use gate also takes the information we just learned together with long-term memory we haven't yet forgotten, and it uses it to make a prediction and update the short-term memory.

5
00:00:34,875 --> 00:00:42,350
So this is how it looks all put together. It's not so complicated after all, isn't it? Now you may be thinking, wait a minute, this looks too arbitrary.

6
00:00:42,350 --> 00:00:50,369
Why use tanh sometimes and sigmoid other times? Why multiply sometimes and add other times, and other times apply a more complicated linear function?

7
00:00:50,369 --> 00:00:55,908
You can probably think of different architectures that make more sense or that are simpler, and you are absolutely right.

8
00:00:55,908 --> 00:01:03,170
This is an arbitrary construction. And as many things in machine learning, the reason why it is like this is because it works.

9
00:01:03,170 --> 00:01:09,818
And in the following section, we'll see some other architectures which can be simpler or more complex and that also do the job.

10
00:01:09,819 --> 00:01:18,000
But you're welcome to look for others and experiment. This is an area very much under development so if you come up with a different architecture and it works, that is wonderful.


@@@
1
00:00:00,000 --> 00:00:09,099
In this video, I will show you a pair of similar architectures that also work well, but there are many variations to LSTMs and we encourage you to study them further.

2
00:00:09,099 --> 00:00:16,140
Here's a simple architecture which also works well. It's called the gated recurring unit or GRU for short.

3
00:00:16,140 --> 00:00:21,565
It combines the forget and the learn gate into an update gate and then runs this through a combine gate.

4
00:00:21,565 --> 00:00:28,285
It only returns one working memory instead of a pair of long- and short-term memories, but it actually seems to work in practice very well too.

5
00:00:28,285 --> 00:00:35,005
I won't go much into details, but in the instructor comments I'll recommend some very good reference to learn more about gated recurrent units.

6
00:00:35,005 --> 00:00:44,875
Here's another observation. Let's remember the forget gate. The forget factor f_t was calculating using as input a combination of the short-term memory and the event.

7
00:00:44,875 --> 00:00:54,515
But what about the long term memory? It seems like we left it away from the decision. Why does a long-term memory not have a say into which things get remembered or not? Well let's fix that.

8
00:00:54,515 --> 00:01:06,420
Let's also connect the long-term memory into the neural network that calculates the forget factor. Mathematically, this just means the input matrix is larger since we're also concatenating it with the long-term memory matrix.

9
00:01:06,420 --> 00:01:12,775
This is called a peephole connection since now the long-term memory has more access into the decisions made inside the LSTM.

10
00:01:12,775 --> 00:01:20,000
We can do this for every one of the forget-type nodes, and this is what we get: an LSTM with peephole connections.


@@@
1
00:00:00,000 --> 00:00:07,715
To introduce you to RNNs in PyTorch, I've created a notebook that will show you how to do simple time series prediction with an RNN.

2
00:00:07,715 --> 00:00:17,485
Specifically, we'll look at some data and see if we can create an RNN to accurately predict the next data point given a current data point, and this is really easiest to see in an example.

3
00:00:17,485 --> 00:00:24,960
So, let's get started. I'm importing our usual resources, and then I'm actually going to create some simple input and target training data.

4
00:00:24,960 --> 00:00:32,730
A classic example is to use a sine wave as input because it has enough variance and shape to be an interesting task, but it's also very predictable.

5
00:00:32,730 --> 00:00:39,475
So, I want to create a sample input and target sequence of data points of length 20, which I specify here as sequence length.

6
00:00:39,475 --> 00:00:46,550
Recall that RNNs are meant to work with sequential data, and so the sequence length is just the length of a sequence that it will look at as input.

7
00:00:46,550 --> 00:00:53,735
Often, the sequence length will indicate the number of words in a sentence or just some length of numerical data as is the case here.

8
00:00:53,735 --> 00:01:00,035
So, in these two lines, I'm just going to generate the start of a sine wave in a range from zero to Pi time steps.

9
00:01:00,035 --> 00:01:11,135
At first, I'm going to create a number of points that sequence length 20 plus 1, then I'm going to reshape my sine wave data to give it one extra dimension, the input size, which is just going to be one.

10
00:01:11,135 --> 00:01:21,890
Then, to create an input and target sequence of the length I want, I'm going to say an input X is equal to all but the last point in data, and the target Y is equal to all but the first point.

11
00:01:21,890 --> 00:01:29,865
So, X and Y should contain 20 data points and have an input size of one. Finally, I'm going to display this data using the same x-axis.

12
00:01:29,865 --> 00:01:44,705
You can see the input X is in red and the target Y is shifted over by one in blue. So, if we look at this point as an example at the same time step, Y is basically X shifted one time step in the future, and that's exactly what we want.

13
00:01:44,705 --> 00:01:55,470
So, now we have our training data and the next step is defining an RNN to learn from this data. We can define an RNN as usual, which is to say as a class using PyTorche's NN library.

14
00:01:55,470 --> 00:02:05,170
The syntax will look similar to how we've defined CNNs in the past. Let's actually click on the RNN documentation to read about the parameters that our recurrent layer takes in as input.

15
00:02:05,170 --> 00:02:12,550
So, here's the documentation for an RNN layer. We can see that this layer is responsible for calculating a hidden state based on its inputs.

16
00:02:12,550 --> 00:02:20,040
Now, to define a layer like this, we have these parameters: an input size, a hidden size, a number of layers and a few other arguments.

17
00:02:20,040 --> 00:02:29,755
The input size is just the number of input features, and in our specific case we're going to have inputs that are 20 values in sequence and one in input size features.

18
00:02:29,755 --> 00:02:40,440
This is like when we thought about the depth of an input image when we made CNN's. Next, we have a hidden size that defines how many features the output of an RNN will have and its hidden state.

19
00:02:40,440 --> 00:02:46,130
We also have a number of layers, which if it's greater than one, just means we're going to stack two RNNs on top of each other.

20
00:02:46,130 --> 00:02:57,580
Lastly, I want you to pay attention to this batch first parameter. If it is true, that means the input and output tensors that we provide are going to have the batch size as the first dimension, which in most cases that we go through will be true.

21
00:02:57,580 --> 00:03:07,965
So, this is how you define an RNN layer, and later in the forward function we'll see that it takes in an input and an initial hidden state, and it produces an output and a new hidden state.

22
00:03:07,965 --> 00:03:20,410
Back to our notebook. Here, I'm defining an RNN layer, self- doubt RNN. This RNN is taking in an input size and a hidden dimension that defines how many features the output of this RNN will have.

23
00:03:20,410 --> 00:03:28,115
Then it takes in a number of layers which allows you to create a stacked RNN if you want and this is typically a value kept between one and three layers.

24
00:03:28,115 --> 00:03:34,645
Finally, I'm setting batch first to true because I'm shaping the input such that the batch size will be the first dimension.

25
00:03:34,645 --> 00:03:39,675
Okay. Then to complete this model I have to add one more layer which is a final fully-connected layer.

26
00:03:39,675 --> 00:03:46,345
This layer is responsible for producing the number of outputs, output size that I want given the output of the RNN.

27
00:03:46,345 --> 00:03:56,115
So, all of these parameters are just going to be passed into our RNN when we create it. You'll also note that I'm storing the value of our hidden dimension so I can use it later in our forward function.

28
00:03:56,115 --> 00:04:01,700
In the forward function, I'm going to specify how a batch of input sequences will pass through this model.

29
00:04:01,700 --> 00:04:10,205
Note that this forward takes in an input X and the hidden state. The first thing I'm doing is grabbing the batch size of our input calling X dot size of 0.

30
00:04:10,205 --> 00:04:17,940
Then I'm passing my initial input and hidden state into the RNN layer. This produces the RNN output and a new hidden state.

31
00:04:17,940 --> 00:04:28,185
Then I'm going to call view on the RNN output to shape it into the size I want. In this case that's going to be batch size, times sequence length rows and the hidden dimension number of columns.

32
00:04:28,185 --> 00:04:41,260
This is a flattening step where I'm preparing the output to be fed into a fully-connected layer. So, I'll pass this shaped output to the final fully-connected layer, and return my final output here and my hidden state generated from the RNN.

33
00:04:41,260 --> 00:04:48,290
Now, as a last step here, I'm going to actually create some text data and to test RNN and see if it's working as I expect.

34
00:04:48,290 --> 00:04:53,540
The most common error I get when programming RNNs is that I've messed up the data dimension somewhere.

35
00:04:53,540 --> 00:05:06,765
So, I'm just going to check that there as I expect. So, here I'm just creating a test RNN with an input and output size of one, a hidden dimension of 10, and the number of layers equal to two, and you can change the hidden dimension and the number of layers.

36
00:05:06,765 --> 00:05:14,240
I basically just want to see that this is making the shape of outputs I expect. So, here I'm creating some test data that are sequence length along.

37
00:05:14,240 --> 00:05:21,620
I'm converting that data into a tensor datatype, and I'm squeezing the first dimension to give it a batch size of one as a first dimension.

38
00:05:21,620 --> 00:05:30,665
Then I'm going to print out this input size and I'll pass it into our test RNN as input. Recall that this takes an initial hidden state, and an initial one here is just going to be none.

39
00:05:30,665 --> 00:05:35,545
Then this should return an output and a hidden state, and I'm going to print out those sizes as well.

40
00:05:35,545 --> 00:05:46,370
Okay. So, our input size is a 3D tensor which is exactly what I expect. If first dimension is one our batch size, then 20 our sequence length, and finally our input number of features.

41
00:05:46,370 --> 00:05:58,280
Which is just one as we specified here. The output size is a 2D tensor. This is because in the forward function of our model definition actually smooshed the batch size and sequence length into one parameter.

42
00:05:58,280 --> 00:06:05,020
So, batch size times sequence length is 20 and then we have an output size of one. Finally we have our hidden state.

43
00:06:05,020 --> 00:06:15,005
Now, the first dimension here is our number of layers that I specified in the model definition two. Next we have the value one which is just the batch size of our input here.

44
00:06:15,005 --> 00:06:23,390
Finally, the last dimension here is 10 which is just our hidden dimension. So, all of these look pretty good and as I expect and I can proceed.


@@@
1
00:00:00,000 --> 00:00:06,380
Last time, we defined a model, and next, I want to actually instantiate it and train it using our training data.

2
00:00:06,380 --> 00:00:20,250
First, I'll specify my model hyperparameters. The input and output will just be one, it's just one sequence at a time that we're processing and outputting, then I'll specify a hidden dimension which is just the number of features expect to generate with the RNN layer.

3
00:00:20,250 --> 00:00:27,345
I'll set this to 32, but for a small data set like this, I may even be able to go smaller. I'll set n_layers to one for now.

4
00:00:27,345 --> 00:00:33,890
So, I'm not stacking any RNN layers. I'll create this RNN and printed out. I should see the variables that I expect.

5
00:00:33,890 --> 00:00:40,600
My RNN layer with an input size and hidden dimension, and a linear layer with an input number of features and output number.

6
00:00:40,600 --> 00:00:49,830
Before training, I'm defining my loss and optimization functions. Now in this case, we're training our model to generate data points that are going to be basically coordinate values.

7
00:00:49,830 --> 00:00:57,670
So to compare a predicted and ground truth point like this, we'll use a regression loss because this is just a quantity rather than something like a class probability.

8
00:00:57,670 --> 00:01:03,650
So for the loss function, I'm going to use mean squared error loss which will just measure the distance between two points.

9
00:01:03,650 --> 00:01:09,655
I'll use an Adam optimizer which is standard for recurrent models, passing in my parameters and our learning rate.

10
00:01:09,655 --> 00:01:19,315
Next, I have a function train that's going to take in and RNN a number of steps to train for, and the parameter that will determine when it will print out law statistics.

11
00:01:19,315 --> 00:01:27,470
Now, at the very start of this function, I'm initializing my hidden state. At first, this is going to be nothing and it will default to a hidden state of all zeros.

12
00:01:27,470 --> 00:01:35,880
Then let's take a look at our batch loop. Now, this is a little unconventional, but I'm just generating data on the fly here according to how many steps we will train for.

13
00:01:35,880 --> 00:01:42,990
So, in these lines, I'm just generating a sequence of 20 sine wave values at a time. As we saw when I generated data at the start.

14
00:01:42,990 --> 00:01:55,275
Here, I'm getting my input x and a target y that's just shifted by one time step in the future. Here, I'm converting this data into tensors and squeezing the first dimension of our x_tensor to give it a batch size of one.

15
00:01:55,275 --> 00:02:03,020
Then I can pass my input tensor into my RNN model. So this is taking in my x input tensor and my initial hidden state at first.

16
00:02:03,020 --> 00:02:14,860
It produces a predicted output and a new hidden state. Next is an important part. I want to feed this new hidden state into the RNN as input at the next time step when we loop around once more.

17
00:02:14,860 --> 00:02:26,810
So I'm just copying the values from this produced hidden state into a new variable. This essentially detaches the hidden state from its history and I will not have to backpropagate through a series of accumulated hidden states.

18
00:02:26,810 --> 00:02:33,790
So this is what's going to be passed as input to the RNN at the next time step or next point in our sequence.

19
00:02:33,790 --> 00:02:42,720
So then, I have the usual training commands, I zero out any accumulated gradients. Calculate the loss, and perform a backpropagation in optimization step.

20
00:02:42,720 --> 00:02:47,770
Down here, I have some code to print out the loss and show what our input and predicted outputs are.

21
00:02:47,770 --> 00:02:53,030
Finally, this function returns a trained RNN which will be useful if you want to save a model for example.

22
00:02:53,030 --> 00:03:02,480
So, let's run this code. I'll choose to train our RNN, and that we defined above for 75 steps. I'll print out the result every 15 steps.

23
00:03:02,480 --> 00:03:09,080
We can see the mean squared error loss here and the difference between our red input in our blue output values.

24
00:03:09,080 --> 00:03:14,560
Recall that we want the blue output values to be one times step in the future when compared to the red ones.

25
00:03:14,560 --> 00:03:20,230
So it starts out pretty incorrect. Then we can see the loss decreases quite a lot after the first 15 steps.

26
00:03:20,230 --> 00:03:30,840
Our blue line is getting closer to our red one. As we train the blue predicted line gets closer to what we know our target is, at the end of 75 steps, our loss is pretty low.

27
00:03:30,840 --> 00:03:43,895
Our blue line looks very similar to what we know or output should be. If we look at the same time step for a red input dot, and a blue input dot, we we shouldn't see that the blue input is one time-step shifted in the future. It's pretty close.

28
00:03:43,895 --> 00:03:50,680
You could imagine getting even better performance after training for more steps or if you wanted to add more layers to your RNN.

29
00:03:50,680 --> 00:04:00,010
So, in this video, I wanted to demonstrate the basic structure of a simple RNN and show you how to keep track of the hidden state and represent memory over time as you train.

30
00:04:00,010 --> 00:04:07,010
You could imagine doing something very similar with data about world temperature or stock prices which are a little bit more complicated than this.

31
00:04:07,010 --> 00:04:16,520
But it will be really interesting to see if you could predict the future given that kind of data. Okay, so this is just an example, you can check out this code in our program GitHub, which is linked to below.

32
00:04:16,520 --> 00:04:25,570
I encourage you to play around with these model parameters until you have a good handle on the dimensions of an RNN input and output and how hyperparameters might change, how this model trains.


@@@
1
00:00:00,000 --> 00:00:11,794
Coming up in this lesson you'll implement a character-wise RNN. That is, the network will learn about some text one character at a time and then generate new text one character at a time.

2
00:00:11,794 --> 00:00:21,875
Let's say, we want to generate new Shakespeare plays. As an example, to be or not to be. We'd pass the sequence into our RNN one character at a time.

3
00:00:21,875 --> 00:00:28,230
Once trained the network will generate new text by predicting the next character based on the characters it's already seen.

4
00:00:28,230 --> 00:00:39,630
So then to train this network we wanted to predict the next character in the input sequence. In this way the network will learn to produce a sequence of characters that look like the original text.

5
00:00:39,630 --> 00:00:47,960
Let's consider what the architecture of this network will look like. First, let's unroll the RNN so we can see how this all works as a sequence.

6
00:00:47,960 --> 00:00:55,603
Here, we have our input layer where we'll pass in the characters as one hot encoded vectors. These vectors go to the hidden layer.

7
00:00:55,603 --> 00:01:03,625
The hidden layer is built with LSTM cells where the hidden state and cell state pass from one cell to the next in the sequence.

8
00:01:03,625 --> 00:01:12,920
In practice, we'll actually use multiple layers of LSTM cells. You just stack them up like this. The output of these cells go to the output layer.

9
00:01:12,920 --> 00:01:22,409
The output layer is used to predict to the next character. We want the probabilities for each character the same way you did image classification with the cabinet.

10
00:01:22,409 --> 00:01:34,299
That means that we want a Softmax activation on the output. Our target here will be the input sequence but shifted over one so that each character is predicting the next character in the sequence.

11
00:01:34,299 --> 00:01:46,015
Again, we'll use cross entropy loss for training with gradient descent. When this network is trained up we can pass in one character and get out a probability distribution for the likely next character.

12
00:01:46,015 --> 00:01:54,915
Then we can sample from that distribution to get the next character. Then we can take that character, pass it in and get another one.

13
00:01:54,915 --> 00:02:04,250
We keep doing this and eventually we'll build up some completely new text. We'll be training this network on the text from Anna Karenina, one of my favorite books.


@@@
1
00:00:00,000 --> 00:00:09,175
One of the most difficult parts of building networks for me is getting the batches right. It's more of a programming challenge than anything deep learning specific.

2
00:00:09,175 --> 00:00:20,565
So here I'm going to walk you through how batching works for RNN. With RNNs we're training on sequences of data like text, stock values, audio etc.

3
00:00:20,565 --> 00:00:28,890
By taking a sequence and splitting it into multiple shorter sequences, we can take advantage of matrix operations to make training more efficient.

4
00:00:28,890 --> 00:00:38,579
In the fact, the RNN is training on multiple sequences in parallel. Let's look at a simple example, a sequence of numbers from 1 to 12.

5
00:00:38,579 --> 00:00:46,590
We can pass these into an RNN as one sequence. What's better. We could split it in half and pass in two sequences.

6
00:00:46,590 --> 00:00:52,914
The batch size corresponds to the number of sequences we're using. So here we'd say the batch size is 2.

7
00:00:52,914 --> 00:01:01,070
Along with the batch size we also choose the length of the sequences we feed to the network. For example, let's consider using a sequence length of 3.

8
00:01:01,070 --> 00:01:11,474
Then the first batch of data we pass into the network are the first 3 values in each mini sequence. The next batch contains the next three values and so on until we run out of data.

9
00:01:11,474 --> 00:01:21,954
We can retain the hidden state from one batch and use it at the start of the next batch. This way the sequence information is transferred across batches for each mini sequence.


@@@
1
00:00:00,000 --> 00:00:09,345
This is a notebook where you'll be building a characterwise RNN. You're going to train this on the text of Anna Karenina, which is a really great but also quite sad a book.

2
00:00:09,345 --> 00:00:15,870
The general idea behind this, is that we're going to be passing one character at a time into a recurrent neural network.

3
00:00:15,870 --> 00:00:23,750
We're going to do this for a whole bunch of text, and at the end what's going to happen, is that our network is going to be able to generate new text, one character at a time.

4
00:00:23,750 --> 00:00:35,215
This is the general structure. We have our input characters and we want to one-hot encode them. This one-hot vector, will be fed into a hidden recurrent layer, and then the hidden layer has two outputs.

5
00:00:35,215 --> 00:00:43,850
First, it produces some RNN output, and it produces a hidden state, which will continue to change and be fed to this hidden layer at the next time step in the sequence.

6
00:00:43,850 --> 00:00:53,625
We saw something similar in the last code example. So, our recurrent layer keeps track of our hidden state, and its output goes to a final fully connected output layer.

7
00:00:53,625 --> 00:01:05,855
Our linear output layer, will produce a series of character class scores. So, this output will be as long as our input vector, and we can apply a Softmax function to get a probability distribution for the most likely next character.

8
00:01:05,855 --> 00:01:14,920
So, this network is based off of Andrej Karpathy's post on RNNs, which you can find here. It's a really good post and so you can check out these links to read more about RNNs.

9
00:01:14,920 --> 00:01:23,615
[inaudible] notebook is broken into a small series of exercises that you can implement yourself. For each exercise, I'm also going to provide a solution to consult.

10
00:01:23,615 --> 00:01:30,180
I recommend that you open the exercise notebook in one window and watch videos in another. That way you can work alongside me.

11
00:01:30,180 --> 00:01:40,285
Okay, so first things first, I'm loading in and taking a look at our text data. Here, I'm loading in the Anna Karenina text file and I'm printing out the first 100 characters.

12
00:01:40,285 --> 00:01:48,465
The characters are everything from letters, to spaces, to newline characters, and we can see the classic first-line, "Happy families are all alike.

13
00:01:48,465 --> 00:01:55,220
Every unhappy family is unhappy in its own way." Then, I'll actually want to turn our text into numerical tokens.

14
00:01:55,220 --> 00:02:02,060
This is because our network can only learn from numerical data, and so we want to map every character in the text to a unique index.

15
00:02:02,060 --> 00:02:13,385
So, first off, with the text, we can just create a unique vocabulary as a set. Sets, are a built in python data structure, and what this will do, is look at every character in the past in the text.

16
00:02:13,385 --> 00:02:20,325
Separate it out as a string and get rid of any duplicates. So, chars, is going to be a set of all our unique characters.

17
00:02:20,325 --> 00:02:31,430
This is also sometimes referred to as a vocabulary. Then, I'm creating a dictionary from a vocabulary of all our characters, that maps the actual character to a unique integer.

18
00:02:31,430 --> 00:02:38,210
So, it's just giving a numerical value to each of our unique characters, and putting it in a dictionary int2char.

19
00:02:38,210 --> 00:02:43,429
Then I'm doing this the other way, where we have a dictionary that goes from integers to characters.

20
00:02:43,429 --> 00:02:53,070
Recall that any dictionary is made of a set of key and value pairs. In the int2char case, the keys are going to be integers and the values are going to be string characters.

21
00:02:53,070 --> 00:02:59,430
In the char2int case, our keys are going to be the characters and our values are going to be their unique integers.

22
00:02:59,430 --> 00:03:08,855
So, these basically give us a way to encode text as numbers. Here, I am doing just that. I'm encoding each character in the text as an integer.

23
00:03:08,855 --> 00:03:16,670
This creates an encoded text, and just like I printed the first 100 characters before, I can print the first 100 encoded values.

24
00:03:16,670 --> 00:03:21,850
If you look at the length of our unique characters, you'll see that we have 83 unique characters in the text.

25
00:03:21,850 --> 00:03:30,800
So, our encoded values will fall in this range. You can also see some repeating values here like 82, 82, 82 and 19,19.

26
00:03:30,800 --> 00:03:47,950
If we scroll back up to our actual text, we can surmise that the repeated 82s are probably this new line character, and 19 is maybe a p. Okay, so are encodings are working, and now what we want to do, is turn these encodings into one-hot vectors, that our RNN can take in as input, just like in our initial diagram.

27
00:03:47,950 --> 00:03:54,710
Here, I've actually written a function that takes in an encoded array, and turns it into a one-hot vector of some specified length.

28
00:03:54,710 --> 00:04:03,975
I can show you what this does with an example below. I've made a short test sequence three, five, one and a vector length that I specify, eight.

29
00:04:03,975 --> 00:04:09,765
So, I'm passing this test sequence and the number of labels that I expect into our one-hot function.

30
00:04:09,765 --> 00:04:20,760
I can see that the result is an array of three one-hot vectors. All of these vectors are of length eight and the index three, five, and one are on for their respective encodings.

31
00:04:20,760 --> 00:04:36,975
Now, for our vocabulary of 83 characters, these are just going to be much longer vectors. Cool. So, we have our preprocessing functions and data in place, and now your first task will be to take our encoded characters, and actually turn them into mini batches that we can feed into our network.

32
00:04:36,975 --> 00:04:43,430
So, as Matt mentioned before, the idea is that we actually want to run multiple sequences through our network at a time.

33
00:04:43,430 --> 00:04:54,405
Where one mini batch of data contains multiple sequences. So, here's an example starting sequence. If we say we want a batch size of two, we're going to split this data into two batches.

34
00:04:54,405 --> 00:05:03,900
Then, we'll have these sequence length windows that specify how big we want our sequences to be. In this case, we have a sequence length of three, and so our window will be three in width.

35
00:05:03,900 --> 00:05:09,370
For a batch size of two and sequence length of three these values will make up our first mini-batch.

36
00:05:09,370 --> 00:05:17,370
We'll just slide this window over by three to get the next mini-batch. So, each mini-batch is going to have the dimensions batch size by sequence length.

37
00:05:17,370 --> 00:05:26,345
In this case, we have a two by three window on are encoded array that we pass into our network. If you scroll down, I have more specific instructions.

38
00:05:26,345 --> 00:05:35,320
The first thing you're going to be doing is taking in an encoded array, and you'll want to discard any values that don't fit into completely full mini-batches.

39
00:05:35,320 --> 00:05:47,410
Then, you want to reshape this array into batch size number of rows. Finally, once you have that batch data, you're going to want to create a window that iterates over the batches a sequence length at a time, to get your mini batches.

40
00:05:47,410 --> 00:05:54,385
So, here's the skeleton code. Your array is going to be some encoded data, then you have a batch size and sequence length.

41
00:05:54,385 --> 00:06:01,805
Basically, you want to create an input x that should be a sequence length or number of timesteps wide and a batch size tall.

42
00:06:01,805 --> 00:06:12,550
This will make up our input data and you'll also want to provide targets. The targets y, for this network are going to be just like the input characters x, only shifted over by one.

43
00:06:12,550 --> 00:06:17,965
That's because we want our network to predict the most likely next character more some input sequence.

44
00:06:17,965 --> 00:06:30,635
So, you'll have your input sequence x and our targets y shifted over by one. Then finally, when we [inaudible] batches, we're going to create a generator that iterates through our array and returns x and y with this yield command.

45
00:06:30,635 --> 00:06:37,050
Okay, I'll leave implementing this batching function up to you. You can find more information about how you could do this in the notebook.

46
00:06:37,050 --> 00:06:43,570
There's some code for testing out your implementation below. In fact, this is what your batches should look like when you run this code.

47
00:06:43,570 --> 00:06:49,470
If you need any help or you just want to see my solution, go ahead and check out the solution video next.


@@@
1
00:00:00,000 --> 00:00:11,170
So, this is my complete get_batches code that generates mini-batches of data. So, the first thing I wanted to do here is get the total number of complete batches that we can make in batches.

2
00:00:11,170 --> 00:00:21,200
To do that, I first calculated how many characters were in a complete mini-batch. So, in one mini-batch, there's going to be batch size times sequence length number of characters.

3
00:00:21,200 --> 00:00:28,470
Then, the number of complete batches that we can make is just the length of the array divided by the total number of characters in a mini-batch.

4
00:00:28,470 --> 00:00:34,365
This double slash is an integer division which we'll just round down any decimal leftover from this division.

5
00:00:34,365 --> 00:00:45,275
With that, we have the number of completely full batches that we can make. Then, we get our array and we take all the characters in the array up to n_batches times this total character size for a mini-batch.

6
00:00:45,275 --> 00:00:51,715
So here, we're making sure that we're keeping only enough characters to make full batches, and we may lose some characters here.

7
00:00:51,715 --> 00:00:57,345
But in general, you're going to have enough data that getting rid of a last unfold batch is not really going to matter.

8
00:00:57,345 --> 00:01:06,920
Next, with reshaping, we can take our array and we can make the number of rows equal to our batch size, and that's just how many sequences we want to include in a mini-batch.

9
00:01:06,920 --> 00:01:21,145
So, we just say we want the number of rows to be batch size and then we put this negative one. Negative one here is kind of a dimension placeholder, and it'll just automatically fill up the second dimension to whatever size it needs to be to accommodate all the data.

10
00:01:21,145 --> 00:01:38,790
Then finally, I'm iterating over my batch data using a window of length sequence length. So here, I'm taking in a reshaped complete array and then looking at all our rows, all our batches, and the columns are in a range from n to n plus sequence length, which makes our sequence length window.

11
00:01:38,790 --> 00:01:52,730
This completes x our input mini-batch. Then, what I did here for the target y is I just initialized an array of all zeros that's the same shape as x, and I just kind of fill it up with values from our x array shifted by one.

12
00:01:52,730 --> 00:02:04,310
From the start to the end, I just shifted over x by one. Then in the case of reaching the very end of our array, I'm going to make the last element of y equal to the first element in our array.

13
00:02:04,310 --> 00:02:11,095
I'm not super sure why most people do it this way, wrapping our array around so that the last element of y is the first element of x.

14
00:02:11,095 --> 00:02:16,580
But I've seen this many times, and so I did it in the cyclical way and it seems like the network trains perfectly fine doing this.

15
00:02:16,580 --> 00:02:27,595
So, it does not seem to be a problem. The main thing is we want x and y to be the same size. So, if you did this right and you want to test your implementation, you should have gotten batches that looks something like this.

16
00:02:27,595 --> 00:02:33,495
Right here, we have a batch size of eight, so you have eight rows here, and then we're just printing out the first 10 items in a sequence.

17
00:02:33,495 --> 00:02:43,915
So, you should see 10 items here. The important thing to note here is that you want to make sure that the elements in x, like the actual encoded values, are shifted over by one in y.

18
00:02:43,915 --> 00:02:59,420
So, we have 51 as the first item here and as the zeroth item here, then 23 and 23. Likewise, 55 is here and 55 is here in y. I basically want to make sure that everything is shifted over correctly, and this looks good.

19
00:02:59,420 --> 00:03:04,530
So now that we have our batch data, the next step we'll talk about is actually building the network.


@@@
1
00:00:00,000 --> 00:00:08,115
All right. So, we have our mini batches of data and now it's time to define our model. This is a little diagram of what the model will look like.

2
00:00:08,115 --> 00:00:22,920
We'll have our character's put into our input layer and then a stack of LSTM cells. These LSTM cells make up our hidden recurrent layer and when they look at a mini batch of data as input they'll look at one character at a time and produce an output and a hidden state.

3
00:00:22,920 --> 00:00:37,495
So, we will pass an input character into our first LSTM cell which produces a hidden state. Then at the next time step, we'll look at the next character in our sequence and pass that into this LSTM cell which will see the previous hidden state as input.

4
00:00:37,495 --> 00:00:56,330
You have so far seen this behavior in a one layer RNN but in this case we plan on using a two-layer model that has stacked LSTM layers and that means that the output of this LSTM layer is going to go to the next one as input and each of these cells is sharing its hidden state with the next cell in the unrolled series.

5
00:00:56,330 --> 00:01:03,635
Finally, the output of the last LSTM layer will include some character class scores that will be the length of our vocabulary.

6
00:01:03,635 --> 00:01:11,660
We'll put this through a Softmax activation function which we'll use to get the probability distribution for predicting the most likely next character.

7
00:01:11,660 --> 00:01:23,370
So, to start you off on this task, you've been given some skeleton code for creating a model. First, we're going to check to see if a GPU is available for training then you'll see this class character RNN.

8
00:01:23,370 --> 00:01:35,070
You can see that this character RNN class has our usual init and forward functions and later you've been given some code to initialize the hidden state of an LSTM layer and I'll go over this in a moment.

9
00:01:35,070 --> 00:01:41,770
You can definitely take a look at this given code and how we're creating our initial character dictionaries but you won't need to change it.

10
00:01:41,770 --> 00:01:49,555
We also have several parameters that are going to be passed in when a character RNN is instantiated and I've saved some of these as class variables.

11
00:01:49,555 --> 00:01:55,990
So, using these input parameters and variables, it will be up to you to create our model layers and complete the forward function.

12
00:01:55,990 --> 00:02:08,930
You'll first create an LSTM layer which you can read about in the documentation here. We can see that an LSTM layer is created using our usual parameters; an input size, hidden size, number of layers, and a batch first parameter.

13
00:02:08,930 --> 00:02:17,855
We'll also add a dropout value. This introduces a dropout layer in between the outputs of LSTM layers if you've decided to stack multiple layers.

14
00:02:17,855 --> 00:02:27,320
So, after you define an LSTM layer, I'll ask you to define two more layers; one dropout layer and a final fully-connected layer for getting our desired output size.

15
00:02:27,320 --> 00:02:33,770
Once you've defined these layers, you'll move on to define the forward function. This takes in an input x and hidden state.

16
00:02:33,770 --> 00:02:44,350
You'll pass this input through the layers of the model and return a final output and hidden state. You'll have to make sure to shape the LSTM output so that it can be fed into the last fully connected layer.

17
00:02:44,350 --> 00:02:49,720
Okay. Then at the bottom here, you'll see this function for initializing the hidden state of an LSTM.

18
00:02:49,720 --> 00:03:04,730
An LSTM has a hidden and a cell state that are saved as a tuple hidden. The shape of the hidden and cell state is defined first by the number of layers in our model, the batch size of our input, and then the hidden dimension that we specified in model creation.

19
00:03:04,730 --> 00:03:10,250
In this function, we're initializing the hidden weights all to zero and moving them to GPU if it's available.

20
00:03:10,250 --> 00:03:16,690
Okay, so all the code that you see you don't need to change, you just need to define the model layers and feedforward behavior.

21
00:03:16,690 --> 00:03:24,230
If you've implemented this correctly, you should be able to set your model hyperparameters and proceed with training and generating some sample text.


@@@
1
00:00:00,000 --> 00:00:11,275
We wanted to define a character RNN with a two layer LSTM. Here in my solution, I am running this code on GPU and here's my code for defining our character level RNN.

2
00:00:11,275 --> 00:00:23,365
First, I defined an LSTM layer, self.lstm. This takes in an input size, which is going to be the length of a one-hot encoded input character and that's just the length of all of my unique characters.

3
00:00:23,365 --> 00:00:29,030
Then, it takes a hidden dimension a number of layers and a dropout probability that we've specified.

4
00:00:29,030 --> 00:00:39,190
Remember that this will create a dropout layer in between multiple LSTM layers, and all of these are parameters that are going to be passed in as input to our RNN when it's constructed.

5
00:00:39,190 --> 00:00:47,215
Then, I've set batch first to true because when we created our batch data, the first dimension is the batch size, rather than the sequence length.

6
00:00:47,215 --> 00:00:56,510
Okay. Next, I've defined a dropout layer to go in-between my LSTM and a final linear layer. Then, I have FC, my final fully connected linear layer.

7
00:00:56,510 --> 00:01:05,555
This takes in our LSTM outputs, which are going to be dimension and hidden. It's going to output our character class scores for the most likely next character.

8
00:01:05,555 --> 00:01:13,605
So, these are the class scores for each possible next character. This output size is the same size as our input, the length of our character vocabulary.

9
00:01:13,605 --> 00:01:20,275
Then, I move to the forward function. I'm passing my input X and a hidden state to my LSTM layer here.

10
00:01:20,275 --> 00:01:29,120
This produces my LSTM output and a new hidden state. I'm going to pass the LSTM output through the dropout layer that I defined here to get a new output.

11
00:01:29,120 --> 00:01:38,610
Then, I'm making sure to reshape this output, so that the last dimension is our hidden dim. This negative one basically means I'm going to be stacking up the outputs of the LSTM.

12
00:01:38,610 --> 00:01:47,595
Finally, I'm passing this V-shaped output to the final fully connected layer. Then, I'm returning this final output and the hidden state that was generated by our LSTM.

13
00:01:47,595 --> 00:01:57,475
These two functions in addition to the init hidden function complete my model. Next, it's time to train and let's take a look at the training loop that was provided.

14
00:01:57,475 --> 00:02:06,330
This function takes in a model to train some data, and the number of epics to train for, and a batch size, and sequence length that define our mini batch size.

15
00:02:06,330 --> 00:02:13,140
It also takes in a few more training parameters. First in here, I've defined my optimizer and my loss function.

16
00:02:13,140 --> 00:02:19,070
The optimizer is a standard Adam optimizer with a learning rate set to the past and learning rate up here.

17
00:02:19,070 --> 00:02:24,775
The last function is cross entropy loss, which is useful for when we're outputting character class scores.

18
00:02:24,775 --> 00:02:31,105
Here, you'll see some details about creating some validation data and moving our model to GPU if it's available.

19
00:02:31,105 --> 00:02:38,255
Here, you can see the start of our epic loop. At the start of each epic, I'm initializing the hidden state of our LSTM.

20
00:02:38,255 --> 00:02:45,970
Recall that this takes in the batch size of our data to define the size of the hidden state and it returns a hidden in cell state that are all zeros.

21
00:02:45,970 --> 00:02:53,335
Then, inside this epic loop, I have my batch loop. This is getting our X and Y mini batches from our get batches generator.

22
00:02:53,335 --> 00:03:10,335
Remember that this function basically iterates through are encoded data, and returns batches of inputs X and targets Y. I'm then converting the input into a one-hot encoded representation, and I'm converting both X and Y are inputs and targets into Tensors that can be seen by our model.

23
00:03:10,335 --> 00:03:20,590
If GPU's available, I'm moving those inputs and targets to our GPU device. The next thing that you see is making sure that we detach any past in hidden state from its history.

24
00:03:20,590 --> 00:03:25,630
Recall that the hidden state of an LSTM layer is a Tuple, and so here, we are getting the data as a tuple.

25
00:03:25,630 --> 00:03:34,615
Then, we proceed with back propagation as usual. We zero out any accumulated gradients and pass in our input Tensors to our model.

26
00:03:34,615 --> 00:03:44,885
We also pass in the latest hidden state here. In this returns of final output and a new hidden state, then we calculate the loss by looking at the predicted output and the targets.

27
00:03:44,885 --> 00:03:55,150
Recall that in the forward function of our model, I smashed the batch size and sequence length of our LSTM outputs into one dimension, and so I'm doing the same thing for our targets here.

28
00:03:55,150 --> 00:04:00,690
Then, we're performing back propagation and moving one step in the right direction updating the weights of our network.

29
00:04:00,690 --> 00:04:07,105
Now before the optimization step, I've added one line of code that may look unfamiliar. I'm calling clip grad norm.

30
00:04:07,105 --> 00:04:13,220
Now, this kind of LSTM model has one main problem with gradients. They can explode and get really, really big.

31
00:04:13,220 --> 00:04:27,840
So, what we do is we can clip the gradients, we just set some clip threshold, and then if the gradient is larger than that threshold, we set it to that clip threshold, and encode we do this by just passing in the parameters and the value that we want to clip the gradients at.

32
00:04:27,840 --> 00:04:37,520
In this case, this value is passed in, in our train function as a value five. Okay. So, we take a backwards step, then we clip our gradients, and we perform an optimization step.

33
00:04:37,520 --> 00:04:43,860
At the end here, I'm doing something very similar for processing our validation data except not performing the back propagation step.

34
00:04:43,860 --> 00:04:51,740
Then, I'm printing out some statistics about our loss. Now with this train function defined, I can go about instantiating and training a model.

35
00:04:51,740 --> 00:05:02,040
In the exercise notebook, I've left these hyper parameters for you to define. I've set our hidden dimension to the value of 512 and a number of layers up two, which we talked about before.

36
00:05:02,040 --> 00:05:11,420
Then, I have instantiated our model, and printed it out, and we can see that we have 83 unique characters as input, 512 as a hidden dimension, and two layers in our LSTM.

37
00:05:11,420 --> 00:05:23,610
For a dropout layer, we have the default dropout value of 0.5 and for our last fully connected layer, we have our Input features, which is the same as this hidden dimension and our output features, the number of characters.

38
00:05:23,610 --> 00:05:29,565
Then, there are more hyper parameters that define our batch size sequence length and number of epics to train for.

39
00:05:29,565 --> 00:05:36,740
Here, I've set the sequence length to 100, which is a lot of characters, but it gives our model a great deal of context to learn from.

40
00:05:36,740 --> 00:05:41,840
I also want to note that the hidden dimension is basically the number of features that your model can detect.

41
00:05:41,840 --> 00:05:50,830
Larger values basically allow a network to learn more text features. There's some more information below in this notebook about defining hyper parameters.

42
00:05:50,830 --> 00:05:58,105
In general, I'll try to start out with a pretty big model like this, multiple LSTM layers and a large hidden dimension.

43
00:05:58,105 --> 00:06:04,475
Then, I'll basically take a look at the loss as this model trains and if it's decreasing, I'll keep going.

44
00:06:04,475 --> 00:06:14,930
But if it's not decreasing as I expect, then I'll probably change some hyper parameters. Our text data is pretty large and here, I've trained our entire model for 20 epics on GPU.

45
00:06:14,930 --> 00:06:23,430
I can see the training and validation loss over time decreasing. Around epic 15, I'm seeing the lost slow down a bit.

46
00:06:23,430 --> 00:06:28,470
But it actually looks like the validation and training loss are still decreasing even after epic 20.

47
00:06:28,470 --> 00:06:38,135
I could have stood to train for an even longer amount of time. I encourage you to read this information about setting the hyper parameters of a model and really getting the best model.

48
00:06:38,135 --> 00:06:48,600
Then, after you've trained a model like I've just done, you can save it by name and then there's one last step, which is using that model to make predictions and generate some new text, which I'll go over next.


@@@
1
00:00:00,000 --> 00:00:09,590
Now, the goal of this model is to train it so that it can take in one character and produce a next character and that's what this next step, Making Predictions is all about.

2
00:00:09,590 --> 00:00:15,955
We basically want to create functions that can take in a character and have our network predict the next character.

3
00:00:15,955 --> 00:00:21,675
Then, we want to take that character, pass it back in, and get more and more predicted next characters.

4
00:00:21,675 --> 00:00:27,770
We'll keep doing this until we generate a bunch of text. So, you've been given this predict function which will help with this.

5
00:00:27,770 --> 00:00:39,330
This function takes in a model and occurring character and its job is to basically give us back the encoded value of the predictive next character and the hidden state that's produced by our model.

6
00:00:39,330 --> 00:00:47,040
So, let's see what it's actually doing step-by-step. It's taking in our input character and converting it into it's encoded integer value.

7
00:00:47,040 --> 00:00:54,905
Then, as part of pre-processing, we're turning that into a one-hot encoded representation and then converting these inputs into a tensor.

8
00:00:54,905 --> 00:01:01,285
These inputs we can then pass to our model, and then you'll see a couple of steps that are really similar to what we saw in our training loop.

9
00:01:01,285 --> 00:01:13,820
We put our inputs on a GPU if it's available and we detach our hidden state from its history here. Then, we pass in the inputs and the hidden state to our model which returns an output and a new hidden state.

10
00:01:13,820 --> 00:01:21,790
Next, we're processing the output a little more. We're applying a softmax function to get p probabilities for the likely next character.

11
00:01:21,790 --> 00:01:28,930
So, p is a probability distribution over all the possible mixed characters given the input character x.

12
00:01:28,930 --> 00:01:41,880
Now, we can generate more sensible characters by only considering the k most probable characters. So, here we're giving you a couple of lines of code to use top k sampling, which finds us the k most likely next characters.

13
00:01:41,880 --> 00:01:48,280
Then, here we're adding an element of randomness, something that selects from among those top likely next characters.

14
00:01:48,280 --> 00:02:05,145
So, then we have a most likely next character and we're actually returning the encoded value of that character and the hidden state produced by our model, but we'll basically want to call the predict function several times, generating one character's output, then passing that in as input and predicting the next and next characters.

15
00:02:05,145 --> 00:02:12,385
That brings me to our next function sample. Sample will take in our trained model and the size of text that we want to generate.

16
00:02:12,385 --> 00:02:18,440
It will also take in prime, which is going to be a set of characters that we want to start our model off with.

17
00:02:18,440 --> 00:02:24,950
Lastly, we will take in a value for top k which will just return our k most probable characters in our predict function.

18
00:02:24,950 --> 00:02:37,520
So, in here, we're starting off by moving our model to GPU if it's available, and here we're also initializing the hidden state with a batch size of one because, for one character that we're inputting at a time, the batch size will be one.

19
00:02:37,520 --> 00:02:44,560
In this way, prediction is quite different than training a model. Then, you'll see that we're getting each character in our prime word.

20
00:02:44,560 --> 00:02:50,960
The prime word basically helps us answer the question, how do we start to generate text? We shouldn't just start out randomly.

21
00:02:50,960 --> 00:03:02,315
So, what is usually done is to provide a prime word or a set of characters. Here the default prime set is just the, T-H-E, but you can pass in any set of characters that you want as the prime.

22
00:03:02,315 --> 00:03:08,035
The sample function first processes these characters in sequence adding them to a list of characters.

23
00:03:08,035 --> 00:03:17,485
It then calls predict on these characters passing in our model, each character and hidden state and this returns the next character after our prime sequence and the hidden state.

24
00:03:17,485 --> 00:03:25,970
So, here we have all our prime characters in the default case. This is going to be T, H, and E and then we're going to append the next most likely character.

25
00:03:25,970 --> 00:03:32,660
So, we're basically building up a list of characters here, then we're going to generate more and more characters.

26
00:03:32,660 --> 00:03:40,450
In this loop, we're passing in our model and the last character in our character list. This returns the next character and the hidden state.

27
00:03:40,450 --> 00:03:52,005
This character is appended to our list and the cycle starts all over again. So, predict is generating a next likely character which is appended to our list and then that goes back as input into our predict function.

28
00:03:52,005 --> 00:04:00,265
The effect is that we're getting next and next and next characters and adding them to our characters list, that is until we reach our desired text length.

29
00:04:00,265 --> 00:04:06,770
Finally, we join all these characters together to return a sample text, and here I've generated a couple samples.

30
00:04:06,770 --> 00:04:15,145
You can see that I've passed in my model that was trained for 20 epochs, and I said, generate a text that's 1,000 characters long starting with the prime word Anna.

31
00:04:15,145 --> 00:04:24,195
I've also passed in a value for top k equal to five. You can see that this starts with the prime word and generates what might be thought of as a paragraph of text in a book.

32
00:04:24,195 --> 00:04:30,395
Even with just a few prime characters, our model is definitely making complete and real words that make sense.

33
00:04:30,395 --> 00:04:47,740
The structure and spelling looks pretty good even if the content itself is a little confusing, and here's another example where I've loaded in a model by name and I'm using this loaded model to generate a longer piece of text, starting with the prime words, "And Levin said." So, this is pretty cool.

34
00:04:47,740 --> 00:04:56,655
A well-trained model can actually generate some text that makes some sense. It learned just from looking at long sequences of characters what characters were likely to come next.

35
00:04:56,655 --> 00:05:04,115
Then in our sampling and prediction code, we used top-k sampling and some randomness in selecting the best likely next character.

36
00:05:04,115 --> 00:05:12,755
You can train a model like this on any other text data. For example, you could try it on generating Shakespeare sonnets or another text of your choice.


@@@
1
00:00:00,000 --> 00:00:11,550
So by now we've learned how to build a deep neural network and how to train it to fit our data. Sometimes however, we go out there and train on ourselves and find out that nothing works as planned.

2
00:00:11,550 --> 00:00:21,823
Why? Because there are many things that can fail. Our architecture can be poorly chosen, our data can be noisy, our model could maybe be taking years to run and we need it to run faster.


@@@
1
00:00:00,000 --> 00:00:10,105
So let's look at the following data form by blue and red points, and the following two classification models which separates the blue points from the red points.

2
00:00:10,105 --> 00:00:21,980
The question is which of these two models is better? Well, it seems like the one on the left is simpler since it's a line and the one on the right is more complicated since it's a complex curve.

3
00:00:21,980 --> 00:00:32,515
Now the one in the right makes no mistakes. It correctly separates all the points, on the other hand, the one in the left does make some mistakes.

4
00:00:32,515 --> 00:00:42,774
So we're inclined to think that the one in the right is better. In order to really find out which one is better, we introduce the concept of training and testing sets.

5
00:00:42,774 --> 00:00:52,414
We'll denote them as follows: the solid color points are the training set, and the points with the white inside are the testing set.

6
00:00:52,414 --> 00:01:03,810
And what we'll do is we'll train our models in the training set without looking at the testing set, and then we'll evaluate the results on that testing to see how we did.

7
00:01:03,810 --> 00:01:12,534
So according to this, we trained the linear model and the complex model on the training set to obtain these two boundaries.

8
00:01:12,534 --> 00:01:23,379
Now we reintroduce the testing set and we can see that the model in the left made one mistake while the model in the right made two mistakes.

9
00:01:23,379 --> 00:01:32,980
So in the end, the simple model was better. Does that match our intuition?. Well, it does, because in machine learning that's what we're going to do.

10
00:01:32,980 --> 00:01:43,000
Whenever we can choose between a simple model that does the job and a complicated model that may do the job a little bit better, we always try to go for the simpler model.


@@@
1
00:00:00,000 --> 00:00:08,330
So, let's talk about life. In life, there are two mistakes one can make. One is to try to kill Godzilla using a flyswatter.

2
00:00:08,330 --> 00:00:16,125
The other one is to try to kill a fly using a bazooka. What's the problem with trying to kill Godzilla with a flyswatter?

3
00:00:16,125 --> 00:00:22,379
That we're oversimplifying the problem. We're trying a solution that is too simple and won't do the job.

4
00:00:22,379 --> 00:00:29,442
In machine learning, this is called underfitting. And what's the problem with trying to kill a fly with a bazooka?

5
00:00:29,443 --> 00:00:37,840
It's overly complicated and it will lead to bad solutions and extra complexity when we can use a much simpler solution instead.

6
00:00:37,840 --> 00:00:45,049
In machine learning, this is called overfitting Let's look at how overfitting and underfitting can occur in a classification problem.

7
00:00:45,049 --> 00:00:52,324
Let's say we have the following data, and we need to classify it. So what is the rule that will do the job here?

8
00:00:52,325 --> 00:00:59,820
Seems like an easy problem, right? The ones in the right are dogs while the ones in the left are anything but dogs.

9
00:00:59,820 --> 00:01:08,280
Now what if we use the following rule? We say that the ones in the right are animals and the ones in the left are anything but animals.

10
00:01:08,280 --> 00:01:15,729
Well, that solution is not too good, right? What is the problem? It's too simple. It doesn't even get the whole data set right.

11
00:01:15,730 --> 00:01:26,200
See? It misclassified this cat over here since the cat is an animal. This is underfitting. It's like trying to kill Godzilla with a flyswatter.

12
00:01:26,200 --> 00:01:43,900
Sometimes, we'll refer to it as error due to bias. Now, what about the following rule? We'll say that the ones in the right are dogs that are yellow, orange, or grey, and the ones in the left are anything but dogs that are yellow, orange, or grey.

13
00:01:43,900 --> 00:01:55,390
Well, technically, this is correct as it classifies the data correctly. There is a feeling that we went too specific since just saying dogs and not dogs would have done the job.

14
00:01:55,390 --> 00:02:05,512
But this problem is more conceptual, right? How can we see the problem here? Well, one way to see this is by introducing a testing set.

15
00:02:05,512 --> 00:02:12,805
If our testing set is this dog over here, then we'd imagine that a good classifier would put it on the right with the other dogs.

16
00:02:12,805 --> 00:02:25,105
But this classifier will put it on the left since the dog is not yellow, orange, or grey. So, the problem here, as we said, is that the classifier is too specific.

17
00:02:25,104 --> 00:02:35,009
It will fit the data well but it will fail to generalize. This is overfitting. It's like trying to kill a fly with a bazooka.

18
00:02:35,009 --> 00:02:44,489
Sometimes, we'll refer to overfitting as error due to variance. The way I like to picture underfitting and overfitting is when studying for an exam.

19
00:02:44,490 --> 00:02:52,179
Underfitting, it's like not studying enough and failing. A good model is like studying well and doing well in the exam.

20
00:02:52,180 --> 00:03:05,125
Overfitting is like instead of studying, we memorize the entire textbook word by word. We may be able to regurgitate any questions in the textbook but we won't be able to generalize properly and answer the questions in the test.

21
00:03:05,125 --> 00:03:14,099
But now, let's see how this would look like in neural networks. So let's say this data where, again, the blue points are labeled positive and the red points are labeled negative.

22
00:03:14,099 --> 00:03:22,504
And here, we have the three little bears. In the middle, we have a good model which fits the data well.

23
00:03:22,504 --> 00:03:30,788
On the left, we have a model that underfits since it's too simple. It tries to fit the data with the line but the data is more complicated than that.

24
00:03:30,788 --> 00:03:36,870
And on the right, we have a model that overfits since it tries to fit the data with an overly complicated curve.

25
00:03:36,870 --> 00:03:46,344
Notice that the model in the right fits the data really well since it makes no mistakes, whereas the one in the middle makes this mistake over here.

26
00:03:46,344 --> 00:03:59,564
But we can see that the model in the middle will probably generalize better. The model in the middle looks at this point as noise while the one in the right gets confused by it and tries to feed it too well.

27
00:03:59,564 --> 00:04:05,685
Now the model in the middle will probably be a neural network with a slightly complex architecture like this one.

28
00:04:05,685 --> 00:04:15,366
The one in the left will probably be an overly simplistic architecture. Here, for example, the entire neural network is just one preceptors since the model is linear.

29
00:04:15,366 --> 00:04:21,790
The model in the right is probably a highly complex neural network with more layers and weights than we need.

30
00:04:21,790 --> 00:04:37,110
Now here's the bad news. It's really hard to find the right architecture for a neural network. We're always going to end either with an overly simplistic architecture like the one in the left or an overly complicated one like the one in the right.

31
00:04:37,110 --> 00:04:49,435
Now the question is, what do we do? Well, this is like trying to fit in a pair of pants. If we can't find our size, do we go for bigger pants or smaller pants?

32
00:04:49,435 --> 00:04:58,574
Well, it seems like it's less bad to go for a slightly bigger pants and then try to get a belt or something that will make them fit better, and that's what we're going to do.

33
00:04:58,574 --> 00:05:07,000
We'll err on the side of an overly complicated models and then we'll apply certain techniques to prevent overfitting on it.


@@@
1
00:00:00,000 --> 00:00:08,929
So, let's start from where we left off, which is, we have a complicated network architecture which would be more complicated than we need but we need to live with it.

2
00:00:08,929 --> 00:00:17,885
So, let's look at the process of training. We start with random weights in her first epoch and we get a model like this one, which makes lots of mistakes.

3
00:00:17,885 --> 00:00:30,589
Now as we train, let's say for 20 epochs we get a pretty good model. But then, let's say we keep going for a 100 epochs, we'll get something that fits the data much better, but we can see that this is starting to over-fit.

4
00:00:30,588 --> 00:00:42,399
If we go for even more, say 600 epochs, then the model heavily over-fits. We can see that the blue region is pretty much a bunch of circles around the blue points.

5
00:00:42,399 --> 00:00:49,170
This fits the training data really well, but it will generalize horribly. Imagine a new blue point in the blue area.

6
00:00:49,170 --> 00:01:00,445
This point will most likely be classified as red unless it's super close to a blue point. So, let's try to evaluate these models by adding a testing set such as these points.

7
00:01:00,445 --> 00:01:14,840
Let's make a plot of the error in the training set and the testing set with respect to each epoch. For the first epoch, since the model is completely random, then it badly misclassifies both the training and the testing sets.

8
00:01:14,840 --> 00:01:27,608
So, both the training error and the testing error are large. We can plot them over here. For the 20 epoch, we have a much better model which fit the training data pretty well, and it also does well in the testing set.

9
00:01:27,608 --> 00:01:36,358
So, both errors are relatively small and we'll plot them over here. For the 100 epoch, we see that we're starting to over-fit.

10
00:01:36,358 --> 00:01:48,393
The model fits the data very well but it starts making mistakes in the testing data. We realize that the training error keeps decreasing, but the testing error starts increasing, so, we plot them over here.

11
00:01:48,394 --> 00:02:00,858
Now, for the 600 epoch, we're badly over-fitting. We can see that the training error is very tiny because the data fits the training set really well but the model makes tons of mistakes in the testing data.

12
00:02:00,858 --> 00:02:09,938
So, the testing error is large. We plot them over here. Now, we draw the curves that connect the training and testing errors.

13
00:02:09,938 --> 00:02:23,569
So, in this plot, it is quite clear when we stop under-fitting and start over-fitting, the training curve is always decreasing since as we train the model, we keep fitting the training data better and better.

14
00:02:23,568 --> 00:02:37,158
The testing error is large when we're under-fitting because the model is not exact. Then it decreases as the model generalizes well until it gets to a minimum point - the Goldilocks spot.

15
00:02:37,158 --> 00:02:47,530
And finally, once we pass that spot, the model starts over-fitting again since it stops generalizing and just starts memorizing the training data.

16
00:02:47,530 --> 00:02:59,204
This plot is called the model complexity graph. In the Y-axis, we have a measure of the error and in the X-axis we have a measure of the complexity of the model.

17
00:02:59,205 --> 00:03:08,064
In this case, it's the number of epochs. And as you can see, in the left we have high testing and training error, so we're under-fitting.

18
00:03:08,064 --> 00:03:19,335
In the right, we have a high testing error and low training error, so we're over-fitting. And somewhere in the middle, we have our happy Goldilocks point.

19
00:03:19,335 --> 00:03:31,644
So, this determines the number of epochs we'll be using. So, in summary, what we do is, we degrade in descent until the testing error stops decreasing and starts to increase.

20
00:03:31,645 --> 00:03:40,090
At that moment, we stop. This algorithm is called Early Stopping and is widely used to train neural networks.


@@@
1
00:00:00,000 --> 00:00:12,359
Now let me show you a subtle way of overfitting a model. Let's look at the simplest data set in the world, two points, the point one one which is blue and the point minus one minus one which is red.

2
00:00:12,359 --> 00:00:21,679
Now we want to separate them with a line. I'll give you two equations and you tell me which one gives a smaller error and that's going to be the quiz.

3
00:00:21,678 --> 00:00:35,673
Equation one is x1 plus x2. That means w1 equals w2 is one and the bias b is zero. And then equation two is 10x1 plus 10x2.

4
00:00:35,673 --> 00:00:46,925
So that means w1 equals w2 equals 10 and the bias b equals zero. Now the question is, which prediction gives a smaller error?

5
00:00:46,924 --> 00:00:52,000
This is not an easy question but I want you to think about it and maybe make some calculations, if necessary.


@@@
1
00:00:00,000 --> 00:00:07,500
Well the first observation is that both equations give us the same line, the line with equation X1+X2=0.

2
00:00:07,500 --> 00:00:15,839
And the reason for this is that solution two is really just a scalar multiple of solution one. So let's see.

3
00:00:15,839 --> 00:00:27,600
Recall that the prediction is a sigmoid of the linear function. So in the first case, for the 0.11, it would be sigmoid of 1+1, which is sigmoid of 2, which is 0.88.

4
00:00:27,600 --> 00:00:40,304
This is not bad since the point is blue, so it has a label of one. For the point (-1, -1), the prediction is sigmoid of -1+-1, which is sigmoid of -2, which is 0.12.

5
00:00:40,304 --> 00:00:48,009
It's also not best since a point label has a label of zero since it's red. Now let's see what happens with the second model.

6
00:00:48,009 --> 00:01:06,015
The point (1, 1) has a prediction sigmoid of 10 times 1 plus 10 times 1 which is sigmoid of 20. This is a 0.9999999979, which is really close to 1, so it's a great prediction.

7
00:01:06,015 --> 00:01:23,409
And the point (-1, -1) has prediction sigmoid of 10 times negative one plus 10 times negative one, which is sigmoid of minus 20, and that is 0.0000000021.

8
00:01:23,409 --> 00:01:32,150
That's a really, really close to zero so it's a great prediction. So the answer to the quiz is the second model, the second model is super accurate.

9
00:01:32,150 --> 00:01:38,909
This means it's better, right? Well after the last section you may be a bit reluctant since this hint's a bit towards overfitting.

10
00:01:38,909 --> 00:01:49,206
And your hunch is correct. The problem is overfitting but in a subtle way. Here's what's happening and here's why the first model is better even if it gives a larger error.

11
00:01:49,206 --> 00:01:59,484
When we apply sigmoid to small values such as X1+X2, we get the function on the left which has a nice slope to the gradient descent.

12
00:01:59,484 --> 00:02:11,919
When we multiply the linear function by 10 and take sigmoid of 10X1+10X2, our predictions are much better since they're closer to zero and one.

13
00:02:11,919 --> 00:02:24,960
But the function becomes much steeper and it's much harder to do great descent here. Since the derivatives are mostly close to zero and then very large when we get to the middle of the curve.

14
00:02:24,960 --> 00:02:33,585
Therefore, in order to do gradient descent properly, we want a model like the one in the left more than a model like the one in the right.

15
00:02:33,585 --> 00:02:41,454
In a conceptual way, the model in the right is too certain and it gives little room for applying gradient descent.

16
00:02:41,455 --> 00:02:51,250
Also as we can imagine, the points that are classified incorrectly in the model in the right, will generate large errors and it will be hard to tune the model to correct them.

17
00:02:51,250 --> 00:03:07,634
These can be summarized in the quote by the famous philosopher and mathematician BertrAIND Russell. The whole problem with artificial intelligence, is that bad models are so certain of themselves, and good models are so full of doubts.

18
00:03:07,634 --> 00:03:16,775
Now the question is, how do we prevent this type of overfitting from happening? This seems to not be easy since the bad model gives smaller errors.

19
00:03:16,775 --> 00:03:24,250
Well, all we have to do is we have to tweak the error function a bit. Basically we want to punish high coefficients.

20
00:03:24,250 --> 00:03:32,164
So what we do is we take the old error function and add a term which is big when the weights are big.

21
00:03:32,164 --> 00:03:40,935
There are two ways to do this. One way is to add the sums of absolute values of the weights times a constant lambda.

22
00:03:40,935 --> 00:03:52,082
The other one is to add the sum of the squares of the weights times that same constant. As you can see, these two are large if the weights are large.

23
00:03:52,082 --> 00:03:59,085
The lambda parameter will tell us how much we want to penalize the coefficients. If lambda is large, we penalized them a lot.

24
00:03:59,085 --> 00:04:13,969
And if lambda is small then we don't penalize them much. And finally, if we decide to go for the absolute values, we're doing L1 regularization, and if we decide to go for the squares, then we're doing L2 regularization.

25
00:04:13,969 --> 00:04:20,724
Both are very popular, and depending on our goals or application, we'll be applying one or the other.

26
00:04:20,725 --> 00:04:32,634
Here are some general guidelines for deciding between L1 and L2 regularization. When we apply L1, we tend to end up with sparse vectors.

27
00:04:32,634 --> 00:04:41,944
That means, small weights will tend to go to zero. So if we want to reduce the number of weights and end up with a small set, we can use L1.

28
00:04:41,944 --> 00:04:55,129
This is also good for feature selections and sometimes we have a problem with hundreds of features, and L1 regularization will help us select which ones are important, and it will turn the rest into zeroes.

29
00:04:55,129 --> 00:05:02,894
L2 on the other hand, tends not to favor sparse vectors since it tries to maintain all the weights homogeneously small.

30
00:05:02,894 --> 00:05:09,854
This one normally gives better results for training models so it's the one we'll use the most. Now let's think a bit.

31
00:05:09,855 --> 00:05:18,495
Why would L1 regularization produce vectors with sparse weights, and L2 regularization will produce vectors with small homogeneous weights?

32
00:05:18,495 --> 00:05:30,267
Well, here's an idea of why. If we take the vector (1, 0), the sums of the absolute values of the weights are one, and the sums of the squares of the weights are also one.

33
00:05:30,267 --> 00:05:46,024
But if we take the vector (0.5, 0.5), the sums of the absolute values of the weights is still one, but the sums of the squares is 0.25+0.25, which is 0.5.

34
00:05:46,024 --> 00:05:57,020
Thus, L2 regularization will prefer the vector point (0.5, 0.5) over the vector (1, 0), since this one produces a smaller sum of squares.


@@@
1
00:00:00,000 --> 00:00:07,589
Here's another way to prevent overfitting. So, let's say this is you, and one day you decide to practice sports.

2
00:00:07,589 --> 00:00:20,644
So, on Monday you play tennis, on Tuesday you lift weights, on Wednesday you play American football, on Thursday you play baseball, on Friday you play basketball, and on Saturday you play ping pong.

3
00:00:20,643 --> 00:00:31,254
Now, after a week you've kind of noticed that you've done most of them with your dominant hand. So, you're developing a large muscle on that arm but not on the other arm.

4
00:00:31,254 --> 00:00:44,054
This is disappointing. So, what can you do? Well, let's spice it up on the next week. What we'll do is on Monday we'll tie our right hand behind our back and try to play tennis with the left hand.

5
00:00:44,054 --> 00:00:56,250
On Tuesday, we'll tie our left hand behind your back and try to lift weights with the right hand. Then on Wednesday again, we'll tie our right hand and play American football with the left one.

6
00:00:56,250 --> 00:01:05,500
On Thursday we'll take it easy and play baseball with both hands, that's fine. Then, on Friday we'll tie both hands behind our back and try to play basketball.

7
00:01:05,500 --> 00:01:15,635
That won't work out too well. But it's OK. It's the training process. And then on Saturday again, we tie our left hand behind our back and play ping pong with the right.

8
00:01:15,635 --> 00:01:25,029
After a week, we see that we've developed both of our biceps. Pretty good job. This is something that happens a lot when we train neural networks.

9
00:01:25,030 --> 00:01:36,504
Sometimes one part of the network has very large weights and it ends up dominating all the training, while another part of the network doesn't really play much of a role so it doesn't get trained.

10
00:01:36,504 --> 00:01:43,084
So, what we'll do to solve this is sometimes during training, we'll turn this part off and let the rest of the network train.

11
00:01:43,084 --> 00:01:52,129
More thoroughly, what we do is as we go through the epochs, we randomly turn off some of the nodes and say, you shall not pass through here.

12
00:01:52,129 --> 00:02:03,549
In that case, the other nodes have to pick up the slack and take more part in the training. So, for example, in the first epoch we're not allowed to use this node.

13
00:02:03,549 --> 00:02:13,503
So, we do our feat forward and our back propagation passes without using it. In the second epoch, we can't use these two nodes.

14
00:02:13,503 --> 00:02:20,965
Again, we do our feet forward and back prop. And in the third epoch we can't use these nodes over here.

15
00:02:20,965 --> 00:02:29,818
So, again, we do forward and back prop. And finally in last epoch, we can't use these two nodes over here.

16
00:02:29,818 --> 00:02:42,218
So, we continue like that. What we'll do to drop the nodes is we'll give the algorithm a parameter. This parameter is the probability that each node gets dropped at a particular epoch.

17
00:02:42,218 --> 00:02:51,264
For example, if we give it a 0.2 it means each epoch, each node gets turned off with a probability of 20 percent.

18
00:02:51,264 --> 00:02:56,810
Notice that some nodes may get turned off more than others and some others may never get turned off.

19
00:02:56,810 --> 00:03:03,209
And this is OK since we're doing it over and over and over. On average each node will get the same treatment.


@@@
1
00:00:00,000 --> 00:00:09,213
So let's recall a gradient descent does. What it does is it looks at the direction where you descend the most and then it takes a step in that direction.

2
00:00:09,214 --> 00:00:14,379
But in Mt. Everest, everything was nice and pretty since that was going to help us go down the mountain.

3
00:00:14,380 --> 00:00:27,960
But now, what if we try to do it here in this complicated mountain range. The Himalayans lowest point that we want to go is around here, but if we do gradient descent, we get all the way here.

4
00:00:27,960 --> 00:00:35,240
And once we're here, we look around ourselves and there's no direction where we can descend more since we're at a local minimum.


@@@
1
00:00:00,000 --> 00:00:10,574
One way to solve this is to use random restarts, and this is just very simple. We start from a few different random places and do gradient descend from all of them.

2
00:00:10,573 --> 00:00:17,000
This increases the probability that we'll get to the global minimum, or at least a pretty good local minimum.


@@@
1
00:00:00,000 --> 00:00:08,169
Here's another problem that can occur. Let's take a look at the sigmoid function. The curve gets pretty flat on the sides.

2
00:00:08,169 --> 00:00:17,129
So, if we calculate the derivative at a point way at the right or way at the left, this derivative is almost zero.

3
00:00:17,129 --> 00:00:26,405
This is not good cause a derivative is what tells us in what direction to move. This gets even worse in most linear perceptrons. Check this out.

4
00:00:26,405 --> 00:00:37,733
We call that the derivative of the error function with respect to a weight was the product of all the derivatives calculated at the nodes in the corresponding path to the output.

5
00:00:37,734 --> 00:00:47,733
All these derivatives are derivatives as a sigmoid function, so they're small and the product of a bunch of small numbers is tiny.

6
00:00:47,734 --> 00:01:01,250
This makes the training difficult because basically grading the [inaudible] gives us very, very tiny changes to make on the weights, which means, we make very tiny steps and we'll never be able to descend Mount Everest.


@@@
1
00:00:00,000 --> 00:00:15,795
The best way to fix this is to change the activation function. Here's another one, the Hyperbolic Tangent, is given by this formula underneath, e to the x minus e to the minus x divided by e to the x plus e to the minus x.

2
00:00:15,795 --> 00:00:23,089
This one is similar to sigmoid, but since our range is between minus one and one, the derivatives are larger.

3
00:00:23,089 --> 00:00:33,115
This small difference actually led to great advances in neural networks, believe it or not. Another very popular activation function is the Rectified Linear Unit or ReLU.

4
00:00:33,115 --> 00:00:44,395
This is a very simple function. It only says, if you're positive, I'll return the same value, and if your negative, I'll return zero.

5
00:00:44,395 --> 00:00:59,850
Another way of seeing it is as the maximum between x and zero. This function is used a lot instead of the sigmoid and it can improve the training significantly without sacrificing much accuracy, since the derivative is one if the number is positive.

6
00:00:59,850 --> 00:01:06,495
It's fascinating that this function which barely breaks linearity can lead to such complex non-linear solutions.

7
00:01:06,495 --> 00:01:21,340
So now, with better activation functions, when we multiply derivatives to obtain the derivative to any sort of weight, the products will be made of slightly larger numbers which will make the derivative less small, and will allow us to do gradient descent.

8
00:01:21,340 --> 00:01:30,730
We'll represent the ReLU unit by the drawing of it's function. Here's an example of a Multi-layer Perceptron with a bunch of ReLU activation units.

9
00:01:30,730 --> 00:01:38,385
Note that the last unit is a sigmoid, since our final output still needs to be a probability between zero and one.

10
00:01:38,385 --> 00:01:45,280
However, if we let the final unit be a ReLU, we can actually end up with regression models, the predictive value.


@@@
1
00:00:00,000 --> 00:00:08,990
First, let's look at what the gradient descent algorithm is doing. So, recall that we're up here in the top of Mount Everest and we need to go down.

2
00:00:08,990 --> 00:00:19,280
In order to go down, we take a bunch of steps following the negative of the gradient of the height, which is the error function.

3
00:00:19,280 --> 00:00:25,960
Each step is called an epoch. So, when we refer to the number of steps, we refer to the number of epochs.

4
00:00:25,960 --> 00:00:34,920
Now, let's see what happens in each epoch. In each epoch, we take our input, namely all of our data and run it through the entire neural network.

5
00:00:34,920 --> 00:00:41,859
Then we find our predictions, we calculate the error, namely, how far they are from where their actual labels.

6
00:00:41,859 --> 00:00:51,405
And finally, we back-propagate this error in order to update the weights in the neural network. This will give us a better boundary for predicting our data.

7
00:00:51,405 --> 00:01:03,993
Now this is done for all the data. If we have many, many data points, which is normally the case, then these are huge matrix computations, I'd use tons and tons of memory and all that just for a single step.

8
00:01:03,993 --> 00:01:09,059
If we had to do many steps, you can imagine how this would take a long time and lots of computing power.

9
00:01:09,060 --> 00:01:17,765
Is there anything we can do to expedite this? Well, here's a question: do we need to plug in all our data every time we take a step?

10
00:01:17,765 --> 00:01:24,775
If the data is well distributed, it's almost like a small subset of it would give us a pretty good idea of what the gradient would be.

11
00:01:24,775 --> 00:01:31,599
Maybe it's not the best estimate for the gradient but it's quick, and since we're iterating, it may be a good idea.

12
00:01:31,599 --> 00:01:49,030
This is where stochastic gradient descent comes into play. The idea behind stochastic gradient descent is simply that we take small subsets of data, run them through the neural network, calculate the gradient of the error function based on those points and then move one step in that direction.

13
00:01:49,030 --> 00:01:56,564
Now, we still want to use all our data, so, what we do is the following; we split the data into several batches.

14
00:01:56,563 --> 00:02:14,000
In this example, we have 24 points. We'll split them into four batches of six points. Now we take the points in the first batch and run them through the neural network, calculate the error and its gradient and back-propagate to update the weights.

15
00:02:14,000 --> 00:02:19,370
This will give us new weights, which will define a better boundary region as you can see on the left.

16
00:02:19,370 --> 00:02:29,259
Now, we take the points in the second batch and we do the same thing. This will again give us better weights and a better boundary region.

17
00:02:29,258 --> 00:02:37,244
Now, we do the same thing for the third batch. And finally, we do it for the fourth batch and we're done.

18
00:02:37,245 --> 00:02:46,710
Notice that with the data, we took four steps whereas, when we did normal gradient descent, we took only one step with all the data.

19
00:02:46,710 --> 00:02:58,566
Of course, the four steps we took were less accurate but in the practice, it's much better to take a bunch of slightly inaccurate steps than to take one good one.

20
00:02:58,566 --> 00:03:04,568
Later in this nanodegree, you'll have the chance to apply stochastic gradient descents and really see the benefits of it.


@@@
1
00:00:00,000 --> 00:00:06,343
The question of what learning rate to use is pretty much a research question itself but here's a general rule.

2
00:00:06,344 --> 00:00:18,120
If your learning rate is too big then you're taking huge steps which could be fast at the beginning but you may miss the minimum and keep going which will make your model pretty chaotic.

3
00:00:18,120 --> 00:00:25,079
If you have a small learning rate you will make steady steps and have a better chance of arriving to your local minimum.

4
00:00:25,079 --> 00:00:34,935
This may make your model very slow, but in general, a good rule of thumb is if your model's not working, decrease the learning rate.

5
00:00:34,935 --> 00:00:43,298
The best learning rates are those which decrease as the model is getting closer to a solution. We'll see that Keras has some options to let us do this.


@@@
1
00:00:00,000 --> 00:00:16,559
So, here's another way to solve a local minimum problem. The idea is to walk a bit fast with momentum and determination in a way that if you get stuck in a local minimum, you can, sort of, power through and get over the hump to look for a lower minimum.

2
00:00:16,559 --> 00:00:31,454
So let's look at what normal gradient descent does. It gets us all the way here. No problem. Now, we want to go over the hump but by now the gradient is zero or too small, so it won't give us a good step.

3
00:00:31,454 --> 00:00:43,435
What if we look at the previous ones? What about say the average of the last few steps. If we take the average, this will takes us in direction and push us a bit towards the hump.

4
00:00:43,435 --> 00:00:50,484
Now the average seems a bit drastic since the step we made 10 steps ago is much less relevant than the step we last made.

5
00:00:50,484 --> 00:01:02,420
So, we can say, for example, the average of the last three or four steps. Even better, we can weight each step so that the previous step matters a lot and the steps before that matter less and less.

6
00:01:02,420 --> 00:01:25,950
Here is where we introduce momentum. Momentum is a constant beta between 0 and 1 that attaches to the steps as follows: the previous step gets multiplied by 1, the one before, by beta, the one before, by beta squared, the one before, by beta cubed, etc.

7
00:01:25,950 --> 00:01:32,079
In this way, the steps that happened a long time ago will matter less than the ones that happened recently.

8
00:01:32,078 --> 00:01:41,723
We can see that that gets us over the hump. But now, once we get to the global minimum, it'll still be pushing us away a bit but not as much.


@@@
1
00:00:00,000 --> 00:00:09,505
So, in this nano degree, we covered a few error functions, but there are a bunch of other error functions around the world that made the shortlist, but we didn't have time to study them.

2
00:00:09,505 --> 00:00:19,708
So, here they are. These are the ones you met: there is Mount Everest and Mount Kilimanjerror. The ones you didn't meet: there is Mt.

3
00:00:19,708 --> 00:00:33,478
Reinerror, he's a big Seahawks fan. Straight from Italy, we got Mount Ves-oops-vius, and my favorite, from Iceland, we got the Eyjafvillajokull.

4
00:00:33,478 --> 00:00:52,270
This is the famous volcano that stopped all the European flights back in 2012. See what I did there is I took the original name which is Eyjafjallajokull and then I changed the word Jalla to Villa, which is Icelandic for error.


@@@
1
00:00:00,050 --> 00:00:06,150
In this lesson, I want to talk a bit more about using neural networks for natural language processing.

2
00:00:06,150 --> 00:00:16,005
We'll be discussing word embedding, which is the collective term for models that learned to map a set of words or phrases in a vocabulary to vectors of numerical values.

3
00:00:16,005 --> 00:00:25,330
These vectors are called embeddings, and we can use neural networks to learn to do word embedding. In general this technique is used to reduce the dimensionality of text data.

4
00:00:25,330 --> 00:00:33,470
But these embedding models can also learn some interesting traits about words in a vocabulary. In fact, we'll focus on the Word2Vec embedding model.

5
00:00:33,470 --> 00:00:42,560
Which learns to map words to embeddings that contain semantic meaning. For example embeddings can learn the relationship between verbs in the present and past tense.

6
00:00:42,560 --> 00:00:49,775
The relationship between the embeddings for walking and walked, should be the same as the relationship between the embeddings for swimming and swam.

7
00:00:49,775 --> 00:00:58,145
Similarly embeddings can learn the relationships between words and common genders. Such as between woman and Queen and between man and King.

8
00:00:58,145 --> 00:01:04,875
You can think of these embeddings as vectors that have learned to mathematically represent the relationship between words in a vocabulary.

9
00:01:04,875 --> 00:01:14,065
A word of caution here. The embeddings are learned from a body of text and so any word associations in that source text will be replicated in the embeddings.

10
00:01:14,065 --> 00:01:21,415
If your text contains false information or gender biased associations. These traits will be replicated in your embeddings.

11
00:01:21,415 --> 00:01:26,695
In fact debiasing word embeddings is an active area of research and you can read more about it below.

12
00:01:26,695 --> 00:01:34,975
In this lesson we'll first talk about how word embedding works in theory, then a walk through a series of notebooks in which you'll learn to implement the Word2Vec model.

13
00:01:34,975 --> 00:01:41,080
Before we start coding, let's learn more about how embeddings can reduce the dimensionality of text data.


@@@
1
00:00:00,000 --> 00:00:11,200
We've talked a bit about how neural networks are designed to learn from numerical data. In our case, word embedding is really all about improving the ability of networks to learn from texted data.

2
00:00:11,200 --> 00:00:19,785
The idea is this, embeddings can greatly improve the ability of networks to learn from text data, by representing that data as lower-dimensional vectors.

3
00:00:19,785 --> 00:00:28,960
Let's think about this in an example. Usually, when you're dealing with text and you split things up into words, you tend to have tens of thousands of different words in a large data set.

4
00:00:28,960 --> 00:00:34,835
When you're using these words as input to a network like an R&amp;N, we've seen that we can one-hot encode them.

5
00:00:34,835 --> 00:00:42,730
What that means is that you have these giant vectors that are like 50,000 units long, and only one of them is set to one, and all the others are set to zero.

6
00:00:42,730 --> 00:00:52,960
Then, you pass this long vector as input to some hidden layer in the network. The output of this hidden layer is calculated by multiplying that input vector by some matrix of learned weights.

7
00:00:52,960 --> 00:00:58,625
The result is a huge matrix of values. Most of which are zero, because of the initial one-hot vector.

8
00:00:58,625 --> 00:01:05,320
So, all these computing resources are used on values that do not hold any information, and this is really computationally inefficient.

9
00:01:05,320 --> 00:01:11,745
To solve this problem, we can use embeddings, which basically provide a shortcut for doing this matrix multiplication.

10
00:01:11,745 --> 00:01:20,455
To learn word embeddings, we use a fully-connected linear layer like you've seen before. We'll call this layer the embedding layer, and its weights are the embedding weights.

11
00:01:20,455 --> 00:01:26,260
These weights will be values that are learned during training this embedding model, and they make up a useful weight matrix.

12
00:01:26,260 --> 00:01:36,215
With this matrix, we can skip the big multiplication step from before, by instead grabbing the values for the output of our hidden layer directly from a row in our weight matrix.

13
00:01:36,215 --> 00:01:46,660
We can do this because the multiplication of a one-hot encoded vector with a weight matrix, returns only the row of the matrix that corresponds to the index of the one or the on input unit.

14
00:01:46,660 --> 00:01:52,505
So, instead of doing matrix multiplication, we can use the embedding weight matrix as a lookup table.

15
00:01:52,505 --> 00:02:03,410
Instead of representing words as one-hot vectors, we can encode each word as a unique integer. As an example, say we have the word heart encoded as the integer 958.

16
00:02:03,410 --> 00:02:10,235
Then, to get the hidden layer values for heart, we just take the 958th row of the embedding weight matrix.

17
00:02:10,235 --> 00:02:15,770
This process is called an embedding lookup, and the number of hidden units is the embedding dimension.

18
00:02:15,770 --> 00:02:21,400
So, the embedding lookup table is just a weight matrix, and the embedding layer is just a hidden layer.

19
00:02:21,400 --> 00:02:26,950
It's important to know that the lookup table holds weights that are learned during training just like any weight matrix.

20
00:02:26,950 --> 00:02:38,720
So, this is the basic idea behind how embedding works. In the next few sections, you'll see how word "the vec" uses the embedding layer to find vector representations of words that contain semantic meaning.


@@@
1
00:00:00,000 --> 00:00:08,955
So in this notebook, I'll be leading you through a Word2Vec implementation in PyTorch. Now, you've just learned about the idea behind embeddings in general.

2
00:00:08,955 --> 00:00:20,655
For any dataset with lots of classes or input dimensions like a large word vocabulary, we're basically skipping the one-hot encoding step, which would result in extremely long input vectors of mostly zeros.

3
00:00:20,655 --> 00:00:28,505
We're taking advantage of the fact that when one-hot vectors are multiplied by a weight matrix, we will just get one row of values back.

4
00:00:28,505 --> 00:00:37,600
For example, if we have a one-hot vector that has its fourth index on, and we multiply this by a weight matrix, we'll get the fourth row of weights back as a result.

5
00:00:37,600 --> 00:00:46,135
So, what we can do, is actually just have input numbers instead of one-hot vectors, and then we can use an embedding weight matrix to look up the correct output.

6
00:00:46,135 --> 00:00:59,395
In this case, we see the word heart is encoded as the integer 958, and we can look up the embedding vector for this word in the 958 row of an embedding weight matrix, this is also often called a lookup table.

7
00:00:59,395 --> 00:01:06,110
In text analysis, this is great. Because we already know that we can convert a vocabulary of words into integer tokens.

8
00:01:06,110 --> 00:01:18,075
So, each unique word will have a corresponding integer value. If we have a vocabulary of 10,000 words, we'll have a 10,000 row embedded weight matrix, where we can look up the correct output values.

9
00:01:18,075 --> 00:01:24,785
These output values which will just be rows in this weight matrix will be a vector representation of that input word.

10
00:01:24,785 --> 00:01:30,980
These representations are called embeddings, and they have as many values as is weight matrix has columns.

11
00:01:30,980 --> 00:01:51,110
This width is called the embedding dimension, it's usually some value in the hundreds. Now, Word2Vec is a special algorithm that basically says, "Any words that appear in the same context in a given text should have similar vector representations." So context in this case, basically means the words that come before and after a word of interest.

12
00:01:51,110 --> 00:01:59,190
Here are a couple of examples. In a body of text, you'll find a variety of sentences, and these ones all involve drinking some beverage.

13
00:01:59,190 --> 00:02:06,900
In some of these cases, even if I removed a word of interests, you may be able to guess what goes in there, just based on the context words surrounding it.

14
00:02:06,900 --> 00:02:17,855
So here it says, "I often drink coffee in the mornings. When I'm thirsty, I drink water, and I drink tea before I go to sleep." The mention of I drink before these words, makes these contexts similar.

15
00:02:17,855 --> 00:02:30,400
So, we'll expect these words coffee, water, and tea to have similar word embeddings. You can imagine that if we look at a large enough text, we may also see that coffee is more closely associated with morning time and so on.

16
00:02:30,400 --> 00:02:38,750
So, just by looking at a word of interest and some context words that surround it, Word2Vec can find similarities between words and relationships between them.

17
00:02:38,750 --> 00:02:47,770
In fact, for such similar words, Word2Vec should produce vectors that are very close in vector space, different words will be some distance away from each other.

18
00:02:47,770 --> 00:02:55,330
In this way, we're actually able to do vector arithmetic, and that's how Word2Vec confined mappings between words in the past and present tense for example.

19
00:02:55,330 --> 00:03:02,030
So, mapping the verb drink to drinking, and swam to swimming, is going to be the same transformation in vector space.

20
00:03:02,030 --> 00:03:14,330
In practice, Word2Vec is implemented in one of two ways. The first option is to basically give our model the context, so several words surrounding a word of interest, and have it tried to predict the missing word.

21
00:03:14,330 --> 00:03:22,385
So, context words in and a single word out, this is called the continuous bag of words or a CBOW model.

22
00:03:22,385 --> 00:03:28,699
The second option is the reverse, to input our word of interests and have our model tried to predict the context.

23
00:03:28,699 --> 00:03:37,775
So, one word n and a few you context words out. This is the skip-gram model, and we'll be implementing Word2Vec in this way because it's been shown to work a bit better.

24
00:03:37,775 --> 00:03:44,935
You'll notice that for either of these models, we'll also have to formalize the idea of context to be a window of a specified size.

25
00:03:44,935 --> 00:03:59,285
So, something like two words before and two words after a word of interest. Here, for an input word w at time t, we have context words from t minus two to t plus two, that is minus two words in the past and plus two in the future.

26
00:03:59,285 --> 00:04:07,570
Notice that the context does not include the original word of interest. So, now that you've been introduced to this notebook and the Word2Vec skip-gram model.


@@@
1
00:00:00,000 --> 00:00:07,455
Okay, let's get started with implementing the skip-gram word2vec model. The first thing you want to do is load in the necessary data.

2
00:00:07,455 --> 00:00:13,095
In this example, I'm using a large body of text that was scraped from Wikipedia articles by Matt Mahoney.

3
00:00:13,095 --> 00:00:20,700
If you're working locally, you'll actually need to click this link to download this data as a zip file, and we'll move it into our data directory, and unzip it.

4
00:00:20,700 --> 00:00:31,600
You should then be left with a file just called text8 in our data directory. So, I've already put that data in the data directory, and here, I'm loading that file in my name and printing out the first 100 characters.

5
00:00:31,600 --> 00:00:40,140
It looks like the first section of text is about anarchism and the working class. So I loaded that in correctly, and then I want to do some preprocessing.

6
00:00:40,140 --> 00:00:46,130
Essentially, I want to break this text up into a giant list of words so that I can build up a vocabulary.

7
00:00:46,130 --> 00:00:54,530
So here, I'm going to do that using a function in the provided utils.py file called preprocess. Let's actually take a look at this code.

8
00:00:54,530 --> 00:01:02,675
So here's our utils.py file and our preprocess function. This function takes in some text and you can see that it does a few things.

9
00:01:02,675 --> 00:01:11,415
First, in all of these lines, it converts any punctuation into tokens. So a period is changed to a bracketed period token and so on.

10
00:01:11,415 --> 00:01:22,745
Next, we see that it stores the number of times certain words appear in the text using a Counter. A Counter is a collection that will basically return a dictionary of words and their frequency of occurrence.

11
00:01:22,745 --> 00:01:29,800
Here, we're creating a list of trimmed words that basically cuts all words that show up five or fewer times in this dataset.

12
00:01:29,800 --> 00:01:35,610
This should greatly reduce issues due to noise in the data, and it should improve the quality of the vector representations.

13
00:01:35,610 --> 00:01:45,610
Then, finally, it returns those trimmed words. So back to our notebook, I'm going to say words equal utils.preprocess text, and I'll print out the first trim 30 words.

14
00:01:45,610 --> 00:01:51,495
This may take a few moments to run since our text data is quite big. Then you should see an output like this.

15
00:01:51,495 --> 00:01:59,190
Pretty much the same text that we saw above, only the words are split into a list. Here, I'm going to print out some statistics about this data.

16
00:01:59,190 --> 00:02:05,460
I'm printing out the length of the text so a word count of our data, and I'll print out the number of unique words.

17
00:02:05,460 --> 00:02:13,580
To get the number of unique words, I'm using the built-in Python data type set, which if you recall from the last lesson, will get rid of any duplicate words.

18
00:02:13,580 --> 00:02:26,350
So, we have a set of only unique words in this text. So, you can see that we have over 16 million words in this text, and over 60 thousand unique words, and these numbers will be useful to keep in mind as we continue processing.

19
00:02:26,350 --> 00:02:34,525
Next, I'm creating two dictionaries to convert words to integers and back again, integers to words. This is our usual tokenization step.

20
00:02:34,525 --> 00:02:41,950
This is again done with a function in the utils.py file, create lookup tables. So, let's take a look at what this function is doing.

21
00:02:41,950 --> 00:02:49,750
So, this function takes in a list of words in a text and it returns two dictionaries that map from our vocabulary to integer values and back.

22
00:02:49,750 --> 00:03:00,720
You may notice an interesting use of counter here. First, this is creating a sorted vocabulary. So this is a list of words from most to least frequent according to the word counts returned by counter.

23
00:03:00,720 --> 00:03:11,100
Then integers are assigned in descending frequency order. So the most frequent word like B is given the integer 0, and the next most frequent is 1 and so on.

24
00:03:11,100 --> 00:03:19,560
So in our notebook, this function returns are two dictionaries. Once we have those, the words are then converted to integers and stored in the list into words.

25
00:03:19,560 --> 00:03:32,755
I'll print out the first 30 tokenized words here just to check that they make sense. So, if we look at these values and back to our list of words above, we'll be able to see that 'the' and 'of' are some of the most common words in our dictionary.

26
00:03:32,755 --> 00:03:40,615
We can see that 'the' is tokenized as the integer 0, and it looks like 'of' is the next most frequent word tokenized as 1.

27
00:03:40,615 --> 00:03:48,115
We have over 60,000 words in our vocabulary, so all of these token value should be integer values in that range.

28
00:03:48,115 --> 00:03:54,370
Now, our goal is to implement word2vec, which relies on looking at the context around a word of interest.

29
00:03:54,370 --> 00:04:02,105
We want to define our context very carefully, basically looking at a window of the most relevant words around a word of interests.

30
00:04:02,105 --> 00:04:11,170
There are some words that are almost never going to be relevant because they're so common, words that show up anywhere and really often such as the, of, and for.

31
00:04:11,170 --> 00:04:22,850
These don't provide much context to other nearby words. So if we discard some of these common words, we can remove some noise from our data, and in return, get faster training and better vector representations.

32
00:04:22,850 --> 00:04:35,800
This process is called subsampling. This will be your first task. Subsampling works like this. For each word wi in our training set, you want to discard it with a probability given by this equation.

33
00:04:35,800 --> 00:04:45,575
The probability of discarding a word w is equal to 1 minus the square root of t over that words frequency, and t is the threshold value that we set.

34
00:04:45,575 --> 00:04:54,200
So say, we're thinking of discarding the word 'the', word index 0. Let's say, it occurs one million times in our 16-million long dataset.

35
00:04:54,200 --> 00:05:03,800
These are approximations but this is one million over 16 million here the frequency of occurrence. The numerator is a threshold I've set, which is 1 times 10 to the negative fifth.

36
00:05:03,800 --> 00:05:11,475
So, if I just run these values through our equation, I'm going to get a probability of getting rid of this word 98.7 percent of the time.

37
00:05:11,475 --> 00:05:19,740
Even after discarding the majority of these inner text will still leave over 12,000 of the original one million these inner text.

38
00:05:19,740 --> 00:05:32,330
The idea with subsampling is really to just get rid of a lot of these frequently occurring words so that they're not always affecting the context of other words while simultaneously keeping enough examples to learn a word embedding for that word.

39
00:05:32,330 --> 00:05:39,385
So, the subsampling equation says the probability that we discard a word is going to be higher if that word's frequency is higher.

40
00:05:39,385 --> 00:05:53,555
Here I provided some code, a threshold to start you out, and a dictionary of word counts. This is using the counter collection which takes in our list of encoded words, and returns how many times they appear in that list, and I can print out the first key value pair in this list.

41
00:05:53,555 --> 00:06:14,510
So here, I can see that the word token 5233 appears 303 times in our text. I want you to use this information to calculate the discard probability for each word in our vocabulary, then use that to create a new set of data train words, which will basically be our original list of int words only with some of our most frequent words discarded.

42
00:06:14,510 --> 00:06:24,330
This is more of a programming challenge rather than a deep learning task but preparing data is an important skill to have, so try to solve this task, and next, I'll show you my solution.


@@@
1
00:00:00,000 --> 00:00:09,750
Here is my solution for creating a new list of train words. First, I calculated the frequency of occurrence for each word in our vocabulary.

2
00:00:09,750 --> 00:00:16,590
So, I stored the total length of our text in a variable, total_count. Then, I created a dictionary of frequencies.

3
00:00:16,590 --> 00:00:30,500
For each word token and count in the word counter dictionary that was given, I added an item to this dictionary, where the word was the key and the value was the count of that word over the total number of words in our text, the frequency.

4
00:00:30,500 --> 00:00:37,615
Then, I calculated the discard probability as p_drop. This is another dictionary that maps words to the drop probability.

5
00:00:37,615 --> 00:00:45,120
Here, I'm just using the subsampling equation to get that, which is 1 minus the square root of our threshold over that word's frequency.

6
00:00:45,120 --> 00:00:53,420
Finally, I created a new list of train words. For each word in our list of int_words, I said I'll keep this word with some probability.

7
00:00:53,420 --> 00:01:02,215
So, I generated a random value between zero and one, and I checked if that value was less than 1 minus the drop probability for that word.

8
00:01:02,215 --> 00:01:17,330
This is saying, okay, I want to keep this word with a probability of 1 minus p_drop. So, if I have a drop probability of 0.98, then the keyboard probability is 1 minus this p_drop, which will be 0.02.

9
00:01:17,330 --> 00:01:24,990
If I generate a value less than 0.02, which is unlikely, only then will I keep this word in my list of train words.

10
00:01:24,990 --> 00:01:30,220
There are other ways to solve this problem, but I like to frame this as a which words do I keep task.

11
00:01:30,220 --> 00:01:37,665
Okay. Then I'm printing out the first 30 words of this train data. This should look similar to the first 30 tokens in our int_words list.

12
00:01:37,665 --> 00:01:50,800
Only you'll notice that most of the zeros and ones are gone. These were our most common words from before, and so this is looking as I expect, and I can move on to the next step, which will be defining a context window and batching our data.


@@@
1
00:00:00,000 --> 00:00:05,560
Now that our data is in good shape, we need to get it into the proper form to pass it into our network.

2
00:00:05,560 --> 00:00:17,765
With the skip-gram architecture for each word in the text, we want to define a surrounding context and grab all the words in a window around that word with size C. When I talk about a window, I mean a window in time.

3
00:00:17,765 --> 00:00:42,150
So, like two words in the past and two words in the future from our given input word. More generally than two words in the past and future, I'm going to say we want to define a window of size C. Here, I have some text from the Mikolov paper on Word2Vec, "Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.

4
00:00:42,150 --> 00:01:01,910
If we choose C equals five, for each training word, we'll randomly select a number R in range one to C, and then use R words from history and R words from the future of the current word as correct labels." So, this is saying that we don't want to choose too big of a window because too big of a window will give us irrelevant context.

5
00:01:01,910 --> 00:01:11,090
In other words, good context words are usually the ones closest to the current word rather than farther away, and we want to include some randomness in how we define our context.

6
00:01:11,090 --> 00:01:18,465
If we define a context window of size C equals five, then we'll create a range R that's going to be a random integer between one or five.

7
00:01:18,465 --> 00:01:27,920
So, say we get an R equal to two as an example, then we'll define the context around a given word to be the two words that appear right before and after our word of interest.

8
00:01:27,920 --> 00:01:40,120
I have an example here. Say we're interested in the word at the second index in this list, 741. If we randomly generate an R equal to two, we'll be interested in the two tokens before and after this word.

9
00:01:40,120 --> 00:01:52,610
I want you to write a function that will return context words in a list like this. This will be the function get_target, which takes in a list of word IDs, and index of interests, and a context window size.

10
00:01:52,610 --> 00:02:05,070
So, the effect of getting words within a random rage R instead of a consistent larger range C is that you're more likely to get words that are right next to your current word, and less likely to get words that are further away from your current word.

11
00:02:05,070 --> 00:02:13,015
So, what you're really doing is going to be training on context words that are closer to your word of interests and likely more relevant, more often.

12
00:02:13,015 --> 00:02:27,505
So here, I've left this function for you to fill out. Now, there are some special cases. If the index that's passed in is zero or your range cannot go back in the past as far as you want, then you can start your context at the start of the past and list of words.

13
00:02:27,505 --> 00:02:36,390
You can test out your implementation in this cell below. Next, we'll use this function to actually batch the data, and so it's important that this is implemented correctly.


@@@
1
00:00:00,000 --> 00:00:13,980
Here's how I'm defining the context targets around a given word index. First, according to the excerpt from the paper, I'm going to define a range R. R is going to be a random integer in the range one to c, the window size.

2
00:00:13,980 --> 00:00:19,790
randint takes in a range that is not inclusive of the last number. So, that's why I have a plus one here.

3
00:00:19,790 --> 00:00:26,060
Then I define the start and stop indices of my context window. The start will be a range of words in the past.

4
00:00:26,060 --> 00:00:33,870
That is the index of my word of interest minus my range R. This will only happen as long as that doesn't get us to a negative index.

5
00:00:33,870 --> 00:00:41,060
If this operation does give us a negative value, then I just set my start index to the startup my list of words index zero.

6
00:00:41,060 --> 00:00:52,375
Then my stop index is where my feature words end. So, my word of interests index, plus our range R. Finally, I do not want my return target context to include the word at the past in index.

7
00:00:52,375 --> 00:01:02,375
So, I'm defining my target words as the words behind my index of interest from start to idx, plus the words in front, idx plus one to stop plus one.

8
00:01:02,375 --> 00:01:14,390
Then I'm returning these words as a list. Then when I go to test this out on a test set of word tokens, and I can run this a couple of times, I see that I get a variable length of words around my past an index of five.

9
00:01:14,390 --> 00:01:21,860
I can see the target does not include my index of interest. These line up just because I've created some input data, that's the integer zero through nine.

10
00:01:21,860 --> 00:01:28,800
If you run the cell multiple times, you will see a different target based on a different randomly generated R. So, this looks good.

11
00:01:28,800 --> 00:01:35,630
Right below this function, I've defined a generator function. This function we'll use our get_target function that we've just defined.

12
00:01:35,630 --> 00:01:42,740
get_batches takes in a list of word tokens a batch_size and a window_size. It makes sure that we can make complete batches of data.

13
00:01:42,740 --> 00:01:51,435
In this four loop, I'm iterating over our words one batch length at a time. I get a batch of words then for each word in a batch I'm calling get_target.

14
00:01:51,435 --> 00:02:01,115
This should return a batch of target words in a window around the given batch word. I'm calling extend here so that each batch x and y will be one row of values.

15
00:02:01,115 --> 00:02:11,360
Here, I'm making x the same length as y. Finally, it returns this list of input words x and target context words why using yield, which makes this a generator function.

16
00:02:11,360 --> 00:02:16,725
Then in the blue cell, we can test this batching out to see what it looks like when applied to some fake data here.

17
00:02:16,725 --> 00:02:25,770
So, I'm getting an x and y batch of data by calling next on our generator function. Here, I've passed in some int_text, a batch_size of four, and a window_size of five.

18
00:02:25,770 --> 00:02:31,425
When I run this cell, this output might look a little weird because everything's been extended into one row.

19
00:02:31,425 --> 00:02:38,550
But I can see that I've made my desired four batches because I have four different input x values; zero, one, two, and three.

20
00:02:38,550 --> 00:02:44,700
If we take a look at the first input zero, we see is length three. So, the target must have also been length three.

21
00:02:44,700 --> 00:02:52,375
The corresponding context is one, two, three. All the targets in the future window that surround the input index zero, which is what I expect.

22
00:02:52,375 --> 00:02:59,830
For the other input, output batches, I can see that I'm generating targets that surround the input values one, two, and three.

23
00:02:59,830 --> 00:03:09,670
So, we have our batch inputs, and our target context. Now, we can get to defining and training a word to vec model on this batch data, which I'll go over next.


@@@
1
00:00:00,000 --> 00:00:05,665
Now that we've taken the time to preprocess and batch our data, it's time to actually start building the network.

2
00:00:05,665 --> 00:00:18,670
Here, we can see the general structure of the network that we're going to build. So, we have our inputs, which are going to be like batches of our train word tokens, and as we saw when we loaded in a batch, a lot of these values will actually be repeated in this input vector.

3
00:00:18,670 --> 00:00:24,710
So, we're going to be parsing in a long list of integers, which are going into this hidden layer, our embedding layer.

4
00:00:24,710 --> 00:00:30,640
The embedding layer, is responsible for looking at these input integers and basically creating a lookup table.

5
00:00:30,640 --> 00:00:39,035
So, for each possible integer value, there will be a row in our embedding weight matrix, and the width of the matrix will be the embedding dimension that we define.

6
00:00:39,035 --> 00:00:47,810
That dimension will be the size of the embedding layers outputs. Then these embeddings are fed into a final fully connected softmax output layer.

7
00:00:47,810 --> 00:00:54,800
Remember, that in the skip gram model, we're parsing in some input words and we're training this whole model to generate target context words.

8
00:00:54,800 --> 00:01:02,050
So, for one input value, the targets will be randomly selected context words from a window around the input word.

9
00:01:02,050 --> 00:01:12,265
Our output layer, is going to output the probability that a randomly selected context word is going to be the word the, or of, or nine, or any other word in our vocabulary.

10
00:01:12,265 --> 00:01:17,675
We're going to be trying to predict our target context words using the outputs of the softmax layer.

11
00:01:17,675 --> 00:01:28,640
Basically, looking at the words with the highest probability that they are context words. Then when we train everything, what's going to happen, is that our hidden layer is going to form these vector representations of the input words.

12
00:01:28,640 --> 00:01:36,790
So, each row in the embedding look-up table will be a vector representation for a word. Row zero, will be the embedding for the word the, for example.

13
00:01:36,790 --> 00:01:43,475
These vectors contain some semantic meaning, and that's what we're really interested in. We only really care about these embeddings.

14
00:01:43,475 --> 00:01:54,045
From these embeddings, we can do some interesting things. Performing vector math to see which of our words are most similar, or we can use these embeddings as input to another model that works with the same text input data.

15
00:01:54,045 --> 00:02:02,940
So, when we're done training, we can actually just get rid of this last softmax layer, because it's just there to help us train this model and create correct embeddings in the first place.

16
00:02:02,940 --> 00:02:09,065
Okay. So, right before we define the model, I have a function that will help us see what kind of word relationships this model is learning.

17
00:02:09,065 --> 00:02:17,675
When I introduced the idea of word2vec, I mentioned that representing words as vectors, gives us the ability to mathematically operate on these words in vector space.

18
00:02:17,675 --> 00:02:23,935
To see which words are similar, I'm going to calculate how similar vectors are using cosine similarity.

19
00:02:23,935 --> 00:02:40,500
Cosine similarity, looks at two vectors a and b and the angle between them, theta. It says, "Okay. The similarity between these two vectors is just the cosine of the angle between them." If you're familiar with vector math, that can also be calculated as the normalized dot product of a and b.

20
00:02:40,500 --> 00:02:48,820
You can really just think of it like this. When theta is zero, cosine of theta is equal to one. This is the maximum value that cosine can take.

21
00:02:48,820 --> 00:02:55,620
When theta is 90 degrees or rather these vectors are orthogonal to one another, then the cosine is going to be zero.

22
00:02:55,620 --> 00:03:02,440
So, the similarity really ends up being a value between zero and one, that indicates how similar two vectors are in vector space.

23
00:03:02,440 --> 00:03:10,080
So, let's look at this cosine similarity function. This function takes in an embedding layer, a validation size, and a validation window.

24
00:03:10,080 --> 00:03:19,355
In here, I'm getting the embeddings from the pasting layer. These are just the layer weights. Then, I'm doing some math and storing the magnitudes of these embedding vectors.

25
00:03:19,355 --> 00:03:29,510
That magnitude is just going to be the square root of the sum of the embedded vectors squared. Then, I'm randomly selecting some common and uncommon validation word examples.

26
00:03:29,510 --> 00:03:36,695
These are just integers in a range, in this case, from zero to 1,000 for common words, and for a higher range for uncommon words.

27
00:03:36,695 --> 00:03:46,780
Recall that lower indices indicate that a word appears more frequently. So, I'm generating half of our validation examples from a more common range, and half from a more uncommon range.

28
00:03:46,780 --> 00:03:55,105
These are collected in an np array and then converted into a long tensor type. Then I'm passing these validation examples into the embedding layer.

29
00:03:55,105 --> 00:04:07,365
In return, I get their vector representations back. So, these validation words are encoded as our vectors, a, and we're going to calculate the similarity between a, and each word vector, b, in the embedding table.

30
00:04:07,365 --> 00:04:19,095
We mentioned that the similarity is a dot product of a and b over the magnitude. This dot product is just a matrix multiplication between the validation vectors a and the transpose of the embedded vectors b.

31
00:04:19,095 --> 00:04:27,390
Here, I'm dividing by the magnitude, and this is not the exact equation here, but it will give us valid values for similarities, just scaled by a constant.

32
00:04:27,390 --> 00:04:38,440
This function returns the validation examples and similarities. This gives us all we need to later print out the validation words and the words in our embedding table that are semantically similar to those words.

33
00:04:38,440 --> 00:04:44,425
It's going to be a nice way to check that our embedding table is grouping together words with similar semantic meanings.

34
00:04:44,425 --> 00:04:50,030
So, this is a given function, you don't have to change anything about this. Now, on to defining the model.

35
00:04:50,030 --> 00:04:56,045
So, we know our model accepts some inputs, then it has an embedding layer, and a final softmax output layer.

36
00:04:56,045 --> 00:05:01,965
You'll have to define this using PyTorch's embedding layer, which you can read about here. Here's the documentation.

37
00:05:01,965 --> 00:05:12,360
So, the embedding layer is known as a sparse layer type. It takes in a number of input embeddings, which is going to be the number of rows in your embedding weight lookup matrix and an embedding dimension.

38
00:05:12,360 --> 00:05:21,085
This is the size of each embedding vector. The number of columns in your embedding look-up table. These two are the most important inputs when defining this layer.

39
00:05:21,085 --> 00:05:27,790
So, after the embedding layer, you'll define a linear layer to go from our embedding size to our predicted context words.

40
00:05:27,790 --> 00:05:33,415
You'll also have to apply a softmax function to the output, so that this model returns word probabilities.

41
00:05:33,415 --> 00:05:44,730
So, here's the skeleton code for this model, and when we instantiate this model, we're going to be parsing in input values for n_vocab, the size of our vocabulary, and n_embed, our embedding dimension.

42
00:05:44,730 --> 00:05:53,855
So, you should be able to complete the init and forward functions for this model. When you do that, you should be able to proceed with training using the provided training loops below.

43
00:05:53,855 --> 00:06:00,035
I'd really recommend training on GPU. Training this particular model takes quite a while even on GPU.

44
00:06:00,035 --> 00:06:09,930
So, I'd start training with maybe just one or two epics for now. All right. So, I'll leave this as an exercise, and next I'll go over one solution for defining a skip-gram model and training it.


@@@
1
00:00:00,000 --> 00:00:12,240
This is what we want our model to look like. It should take in some inputs and then put those through an embedding layer, which produces some embedded vectors that are sent to a final softmax output layer, and here's my model definition.

2
00:00:12,240 --> 00:00:17,605
You can see that it's a pretty simple model. First, I'm defining my embedding layer, the self.embed.

3
00:00:17,605 --> 00:00:29,025
This takes n the length of my word vocabulary. This means that it will create an embedding weight matrix that has a row for each of the words in our vocabulary, and this will output vectors of size n_embed, our embedding dimension.

4
00:00:29,025 --> 00:00:36,915
Then, I have a fully-connected layer that takes in that embedding dimension as input, and its output size is also the length of our vocabulary.

5
00:00:36,915 --> 00:00:46,830
That's because this output is a series of word class scores that tells us the likely context word for a given input word, and then I've defined a softmax activation layer here.

6
00:00:46,830 --> 00:00:54,895
You could have just done this in the forward function too. This is just one solution. Then at my forward function, I'm passing in my input X into the embedding layer.

7
00:00:54,895 --> 00:01:00,080
This returns are embeddings which moves to our fully-connected layer, which returns a series of class scores.

8
00:01:00,080 --> 00:01:06,170
Finally, a softmax activation function is applied and I'll be left with my log probabilities for context words.

9
00:01:06,170 --> 00:01:16,895
Below in this training section, I'm actually going to instantiate this model. So here, I've defined an embedding dimension and I've set this to 300, but you're welcome to experiment with larger or smaller values.

10
00:01:16,895 --> 00:01:23,285
The embedding dimension can be thought of as the number of word features that we can detect, like the length, the type of word, and so on.

11
00:01:23,285 --> 00:01:30,070
So, this takes in the entire length of our vocabulary and the embedding dimension, and I've moved this to a GPU for training.

12
00:01:30,070 --> 00:01:38,960
Here, you'll see that I'm using negative log-likelihood loss, and this is because a softmax in combination with negative log-likelihood basically equals cross entropy loss.

13
00:01:38,960 --> 00:01:49,080
So, this is a great loss for looking at probabilities of context words, and I'm using an Adam optimizer, which is just my go-to and I'm passing in my model parameters and a learning rate.

14
00:01:49,080 --> 00:02:01,055
Then I have my training loop and I've decided to train for five epochs. In this training, actually took a few hours even on GPU, so I'd recommend that you train for a shorter amount of time or wait until I show you how to train more efficiently.

15
00:02:01,055 --> 00:02:09,345
So, in my training loop, I'm getting batches of data by calling the generator function that we defined above and passing in my list of chain words and a batch size.

16
00:02:09,345 --> 00:02:24,520
I'm getting my inputs and my target context words, and I'm converting them into LongTensor types, and moving these two GPU if it's available, and then I'm performing backpropagation as usual, and passing my inputs into my skip-gram model to get the log probabilities for the context words.

17
00:02:24,520 --> 00:02:36,090
Then I'm applying my loss function to these contexts words and my targets, then performing backpropagation and updating the weights of my model, not forgetting to zero out any accumulated gradients before these two steps.

18
00:02:36,090 --> 00:02:46,965
Then I'm printing out some validation examples using my cosine similarity function. Here, I'm passing in my model and a GPU device, and I'm getting back some validation examples and their similarities.

19
00:02:46,965 --> 00:02:56,240
Here, I'm actually using topk sampling to get the top six most similar words to a given example. Here, I'm iterating through my validation examples.

20
00:02:56,240 --> 00:03:03,810
I'm printing out the first validation word and then the five closest words next to it after a line character, and here are some initial results.

21
00:03:03,810 --> 00:03:10,490
I printed a lot of data after training for five epochs. At first, these word associations look pretty random.

22
00:03:10,490 --> 00:03:18,480
We have and, returns, liverpudlians, and so on. But as I train, I should see that these validation words are getting more and more similar.

23
00:03:18,480 --> 00:03:24,130
If I scroll down all the way to the end of my training, I can see that similar words are nicely grouped together.

24
00:03:24,130 --> 00:03:35,930
You can see a bunch of number words are grouped together. Here, I have a bunch of animals and mammals grouped in one line, some lines that are related to states and politics, and even lines that are related to a place and a language.

25
00:03:35,930 --> 00:03:41,480
So, it looks like my word2vec model is learning, and I can visualize these embeddings in another way too.

26
00:03:41,480 --> 00:03:48,994
Another really powerful method for visualization is called t-SNE, which stands for t-distributed stochastic neighbor embeddings.

27
00:03:48,994 --> 00:03:57,615
It's a non-linear dimensionality reduction technique that aims to separate data in a way that cluster similar data close together and separates different data.

28
00:03:57,615 --> 00:04:09,980
In this case, it's an algorithm that I'm loading in from the sklearn library. I give it the number of embeddings that I want to visualize, and I get these embeddings from the weights of our embedded layer which I'm calling by name from our model.

29
00:04:09,980 --> 00:04:16,360
So, remember that our embedding layer was just named embed, and I can get the weights by same model.embed.weight.

30
00:04:16,360 --> 00:04:23,740
So here, I'm applying t-SNE to 600 of our embeddings, and this is what this t-SNE clustering ends up looking like.

31
00:04:23,740 --> 00:04:29,425
We can actually see that similar words are grouped together. Here we have east, west, north, and south.

32
00:04:29,425 --> 00:04:42,700
If we look to the right, we can see some musical terms: rock, music, album, band, and song. Lower down, we can see some religious terms, some colors over here, some academic terms: school, university, and college.

33
00:04:42,700 --> 00:04:48,950
On the left side here, I can see clusters of the months in the year and it looks like a few integer values here.

34
00:04:48,950 --> 00:05:00,720
So, this clustering indicates that my word2vec model has worked. It learns to generate embeddings that hold semantic meaning, and this also gives us a cool way to visualize the relationships between words in space.

35
00:05:00,720 --> 00:05:06,730
So, one problem with this model was that it took quite a while to train, and next I'm going to address that challenge.


@@@
1
00:00:00,000 --> 00:00:05,115
Now, the last model took quite a while to train, and there are some ways that we can speed up this process.

2
00:00:05,115 --> 00:00:16,690
In this video, I'll talk about one such method which is called negative sampling. So, this is a new notebook, but it contains basically the same info as our previous notebook including this architecture diagram.

3
00:00:16,690 --> 00:00:27,645
This is our current architecture where we have a softmax layer on the output, and since we're working with tens of thousands of words, the softmax layer is going to have tens of thousands of units.

4
00:00:27,645 --> 00:00:41,660
But with any one input, we're really going to have one true context target. What that means is, when we train, we're going to be making very small changes to the weights between these two layers even though we only have one true output that we care about.

5
00:00:41,660 --> 00:00:54,535
So, very few of the weights are actually going to be updated in a meaningful way. Instead what we can do is approximate the loss from the softmax layer, and we do this by only updating a small subset of all the weights at once.

6
00:00:54,535 --> 00:01:06,445
We'll update the weights for what we know to be the correct target output, but then we'll only update a small number of incorrect or noise targets usually around 100 or so as opposed to 60,000.

7
00:01:06,445 --> 00:01:13,865
This process is called negative sampling. To implement this, there are two main modifications we need to make to our model.

8
00:01:13,865 --> 00:01:20,915
First, since we're not taking the softmax output over all the words, we're really only concerned with one output one at a time.

9
00:01:20,915 --> 00:01:31,060
Similar to how we used an embedding layer to map an input word to a row of embedding weights, we can now use another embedding layer to map the output words to a row of hidden weights.

10
00:01:31,060 --> 00:01:46,340
So, we'll have two embedding layers, one for input words and one for output words. Second, we have to use a modified loss function that only cares about the true target and a small subset of noisy and correct target context words, and that's this big loss function here.

11
00:01:46,340 --> 00:01:52,210
It's a little heavy on notation, so I'll go over it one part at a time. Let's take a look at the first term.

12
00:01:52,210 --> 00:01:59,900
We can see that this is a negative log operation, and this little loop, this lowercase sigma is a sigmoid activation function.

13
00:01:59,900 --> 00:02:07,965
A sigmoid activation function scales any input from a range from zero to one. So, let's look at the input inside the parentheses.

14
00:02:07,965 --> 00:02:18,970
UW0 transpose is the embedding vector for our output target word. So, this is the embedding vector that we know as the correct contexts target for a given input word.

15
00:02:18,970 --> 00:02:26,275
This T here is the transpose symbol. Then we have VWI which is the embedding vector for our input word.

16
00:02:26,275 --> 00:02:38,085
In general, you will indicate an output embedding and V are input. If you remember from doing cosine similarity a transpose multiplication like this is equivalent to doing a.product operation.

17
00:02:38,085 --> 00:02:49,800
So, this whole first term is same that we take the log sigmoid of the.product of our correct output word vector with our input word vector, and this represents our correct target loss.

18
00:02:49,800 --> 00:02:56,460
Next, we want to sample our outputs and get some noisy target words, and that's what the second part of this equation is all about.

19
00:02:56,460 --> 00:03:03,025
So, let's look at this piece by piece. This capital sigma means we're going to take a sum over all of our words WI.

20
00:03:03,025 --> 00:03:13,990
This P and W indicates that these words are drawn from a noise distribution. The noise distribution is our vocabulary of words that are not in the context of our input word.

21
00:03:13,990 --> 00:03:19,925
In effect, we want to randomly sample words from our vocabulary to get these noisy irrelevant target words.

22
00:03:19,925 --> 00:03:27,160
So P and W is an arbitrary probability distribution which means we can get to decide how to weight the words that we're sampling.

23
00:03:27,160 --> 00:03:38,595
This could be a uniform distribution where we sample all words with equal probability or it could be according to the frequency that each word shows up in our text corpus, the unigram distribution UW.

24
00:03:38,595 --> 00:03:45,710
In fact the authors of the negative sampling paper found the best distribution to be a unigram distribution raised to the three-fourths.

25
00:03:45,710 --> 00:03:56,530
Then we get to this last part which looks very similar to our first term. This takes the log sigmoid of the negated.product between a noise vector UWI, and our input vector from before.

26
00:03:56,530 --> 00:04:03,390
To give you an intuition for what this whole loss is doing here, remember that this sigmoid function returns a probability between zero and one.

27
00:04:03,390 --> 00:04:10,280
So, the first term in this loss is going to push the probability that our network will predict the correct context word towards one.

28
00:04:10,280 --> 00:04:19,380
In the second term, since we're negating the sigmoid input, we're pushing the summed probabilities that our network will predict the incorrect noisy words towards zero.


@@@
1
00:00:00,000 --> 00:00:09,885
All right. So, we have two tasks to complete, to define a more efficient Word2vec skip-gram model. Here, I'm calling this model skip-gram neg to include negative sampling.

2
00:00:09,885 --> 00:00:17,110
This model takes in our usual vocab and embedding dimension. It also takes in a noise distribution, if it's provided.

3
00:00:17,110 --> 00:00:26,235
Okay. So, first, we want to define two embedding layers, one for input and one for output words. Here, I'm calling those in_embed and out_embed.

4
00:00:26,235 --> 00:00:35,225
I want you to define these layers, such that they can accept an input or output target as input and return an embedding that's a vector of dimension in_embed.

5
00:00:35,225 --> 00:00:41,770
I'll also suggest that you initialize the weights of these layers using a uniform distribution between negative one and one.

6
00:00:41,770 --> 00:00:49,860
Now, let's look at our loss function for a moment. When we think about defining a negative sampling loss, we know that this loss will take in a few things as input.

7
00:00:49,860 --> 00:01:01,355
It will for sure take in our input word embedding, vwi. It will also take in our correct output word embedding uw0 and several noisy incorrect embeddings uwi.

8
00:01:01,355 --> 00:01:08,405
So, in this model definition, I'm actually going to ask you to define three different forward functions for creating these embeddings.

9
00:01:08,405 --> 00:01:15,785
The first forward input should return our input embeddings, which are just going to be our input words passed through our input embedding layer.

10
00:01:15,785 --> 00:01:23,990
Similarly, forward output, which should return output vectors for passed and output words. Finally, a forward noise function, this one is special.

11
00:01:23,990 --> 00:01:29,535
It takes in a batch size and a number of noise samples to generate for performing negative sampling.

12
00:01:29,535 --> 00:01:38,125
This function first gets noisy samples from a passed in noise distribution. If no distribution is passed in, this will default to uniform distribution.

13
00:01:38,125 --> 00:01:45,440
Now, it gets a sample of noise words using torch.multinomial and gets batch size times n_samples of values.

14
00:01:45,440 --> 00:01:56,215
In this line, those words are being moved to a GPU, if available, and what you need to do to complete this function is pass these words through the output embedding layer to get their respective embeddings.

15
00:01:56,215 --> 00:02:04,540
So, you get our noise embeddings and then you should reshape these embeddings to be batch size by n_samples, by n_embed in dimension. All right.

16
00:02:04,540 --> 00:02:09,540
So, complete these forward functions, making sure to return correct embeddings for each forward function.

17
00:02:09,540 --> 00:02:20,000
If you've completed this implementation, you should be able to proceed with training this model. Next, I'll go over one solution for this model and I'll show you how I defined a custom negative sampling loss.


@@@
1
00:00:00,000 --> 00:00:07,040
So, I ran all the cells in my notebook and here's my solution and definition for the SkipGramNeg Module.

2
00:00:07,040 --> 00:00:17,030
First, I've defined my two embedding layers, in-embed and out-embed, and they'll both take in the size of our word vocabulary and produce embeddings of size and embed.

3
00:00:17,030 --> 00:00:27,755
So, mapping from our vocab to our embedding dimension. Here, I'm doing an additional step which is initializing the embedding look-up tables with uniform weights between negative one and one.

4
00:00:27,755 --> 00:00:32,725
I'm doing this for both of our layers and I believe this helps our model reached the best way faster.

5
00:00:32,725 --> 00:00:41,810
Then I've defined my three forward functions. Forward input passes our input words through our input embedding layer and returns input embedding vectors.

6
00:00:41,810 --> 00:00:47,165
I do the same thing in forward output only passing that through our output embedding layer to get output vectors.

7
00:00:47,165 --> 00:00:56,565
Notice that there are no linear layers or softmax activation functions here. The last forward function is forward noise, which will return a noisy target embeddings.

8
00:00:56,565 --> 00:01:03,125
So, this samples noisy words from our noise distribution and returns the number of samples batch size times N samples.

9
00:01:03,125 --> 00:01:16,940
Then we get the embeddings bypassing those noise words through our output embedding layer. In the same line, I'm reshaping these to be the size I want, which is batch size by N samples by our embedding dimension and I return those vectors.

10
00:01:16,940 --> 00:01:34,510
Okay, so this completes the SkipGramNeg Module. Next, I'm defining a custom negative sampling loss. This was carefully defined above in our equations and I haven't ever gotten into the details of defining a custom loss, but suffice to say that it is really similar to defining a model class.

11
00:01:34,510 --> 00:01:40,130
Only in this case, the init function is left empty and we're really left with defining the forward function.

12
00:01:40,130 --> 00:01:47,460
The forward function should take in some inputs and targets typically and you can define what it takes in as parameters here.

13
00:01:47,460 --> 00:02:01,430
This should return a single value that indicates the average loss over a batch of data. So, in this case, I know I what my loss to look at an input embedded vector, my correct output embedding, and my incorrect noisy vectors.

14
00:02:01,430 --> 00:02:20,260
So here, I am getting the batch size and embedding dimension from the shape of my input vector, then I'm shaping my input vector into a shape that is batch first, and I'm doing something similar to my output vector here, only I'm swapping these last two dimensions one an embed size effectively making this the output vector transpose.

15
00:02:20,260 --> 00:02:28,930
This way, I'll be able to calculate the.product between these two vectors by performing batch matrix multiplication on them, and that's just what I'm doing here.

16
00:02:28,930 --> 00:02:38,990
First, I'm calculating the loss term between my input vector and my correct target vector. I'm using batch matrix multiplication and then applying a sigmoid and a log function.

17
00:02:38,990 --> 00:02:49,375
Here, I'm squeezing the output so that no empty dimensions are left in the output. Next, I'm doing something similar only between my input vector and my negated noise vectors.

18
00:02:49,375 --> 00:03:00,015
So, this is the second term in our loss function. I'm using batch matrix multiplication, applying a sigmoid and a log function, and then I'm summing the losses over the sample of noise vectors.

19
00:03:00,015 --> 00:03:08,560
Okay finally, I'm adding these two losses up negating them since I kept them positive during my calculations and taking the mean of this total loss.

20
00:03:08,560 --> 00:03:16,230
This way, I'm returning the average negative sample loss over a batch of data. Then I can move on to creating this model and training it.

21
00:03:16,230 --> 00:03:29,980
This training loop will look pretty similar to before, but with some key differences. First, I'm creating a unigram noise distribution that relates noisy vectors to their frequency of occurrence, and this is a value I calculated earlier in this notebook.

22
00:03:29,980 --> 00:03:37,030
So, I'm defining our noise distribution as the unigram distribution raised to a power of three-fourths as was specified in the paper.

23
00:03:37,030 --> 00:03:47,510
Then, I'm defining my model passing in the length of our vocabulary and embedding dimension which I left as 300, and this noise distribution that I've just created, and I'm moving this altered GPU.

24
00:03:47,510 --> 00:03:54,990
Then I have another key difference, instead of using NLL loss, I'm using my custom negative sampling loss that I defined above.

25
00:03:54,990 --> 00:04:04,070
In my training loop, I'll have to pass in three parameters to this loss function. So, I'm training for five epochs again, getting batches of input and target words.

26
00:04:04,070 --> 00:04:11,105
Then using my three different forward functions I'm getting my input embedding, my desired output embedding, and my noise embeddings.

27
00:04:11,105 --> 00:04:18,185
So, forward input takes in my inputs, forward output takes in my targets and forward noise takes in two parameters.

28
00:04:18,185 --> 00:04:27,765
It takes in a batch size and a number of noise vectors to generate. Then to calculate my loss, I'm passing in my input, output and noise embeddings here.

29
00:04:27,765 --> 00:04:40,000
Then, I just have the same code as before, performing backpropagation and optimization steps as usual, and I have my validation similarities that I'm going to print out along with the epoch and loss, a little more information.

30
00:04:40,000 --> 00:04:48,275
So, note that I chose to define my three different forward functions just so I a get the vectors that I needed to calculate my negative sampling loss here.

31
00:04:48,275 --> 00:04:56,545
You can try training this yourself just to see how much faster this training is. Then imprinting data less frequently because it's generated quicker.

32
00:04:56,545 --> 00:05:05,395
So here, after the first epoch, we see our usual sort of noisy relationships. But by the end of training, we see words grouped together that makes sense.

33
00:05:05,395 --> 00:05:15,125
So, we have mathematics, algebra, calculus, we have ocean, islands, Pacific, Atlantic, and some smaller words that all seem to be grouped together as well.

34
00:05:15,125 --> 00:05:24,820
Once again, I visualize the word vectors using T-SNE. This time I'm visualizing fewer words and I'm getting the embeddings from our input embedding layer only.

35
00:05:24,820 --> 00:05:36,545
Then I'm passing these embeddings into our T-SNE model and this is the result I get. I can see some individual integers grouped over here, some educational terms and war and military terms over here.

36
00:05:36,545 --> 00:05:42,845
I see some governmental terms and other relationships and it's pretty interesting to poke around a visualization like this.

37
00:05:42,845 --> 00:05:48,505
The word2vec model always makes me think about how a learned vector space can be really interesting.

38
00:05:48,505 --> 00:05:56,885
Just think about how you might embed images and find relationships between colors and objects or how you might transform words using vector arithmetic.

39
00:05:56,885 --> 00:06:11,940
Building and training this model was also quite involved and if you feel comfortable with this model code and especially manipulating models to add your own forward functions and custom loss types, you've really learned a lot about the Pythonic nature of PyTorch and model customization.

40
00:06:11,940 --> 00:06:20,200
In addition to implementing a very effective word2vec model. So, great job on making it this far, and I hope you're excited to learn even more.


@@@
1
00:00:00,000 --> 00:00:14,730
Welcome to this lesson on Sentiment Analysis with an RNN. All right. So, in this notebook, I want to give you one more LSTM example, and in this case, we'll actually be training an RNN to solve our sentiment analysis task from a few lessons ago.

2
00:00:14,730 --> 00:00:21,960
Sentiment analysis is all about taking in some text, in this case movie reviews, and predicting the sentiment of that review.

3
00:00:21,960 --> 00:00:33,135
So, whether it's positive or negative. Recall that Andrew Trask showed you how to build a sentiment analysis model from scratch, and he also did some really cool visualizations with this data.

4
00:00:33,135 --> 00:00:41,675
What I want to do now is use this as an opportunity to show you how well an RNN versus just a feedforward network performs on this task.

5
00:00:41,675 --> 00:00:49,625
My thinking is that an RNN should work really well because we can include information about the sequence of words in a movie review.

6
00:00:49,625 --> 00:01:02,255
I've broken this notebook up into a series of exercises, and I'll leave them pretty open-ended. You'll be tasked with pre-processing some text data and building a model that includes both an embedding and LSTM layer.

7
00:01:02,255 --> 00:01:08,075
I'll also be showing you a few more things that are useful to know about batching data and making predictions.

8
00:01:08,075 --> 00:01:13,550
Next, you'll be able to access this notebook, and I'd suggest that you keep the exercise notebook open.

9
00:01:13,550 --> 00:01:22,270
You can have it open in one tab and watch my exercise and solution videos in another, moving back and forth between absorbing information and practicing what you've learned.


@@@
1
00:00:00,000 --> 00:00:06,220
So, let's get started with sentiment analysis. First, I'm going to load in data from our data directory.

2
00:00:06,220 --> 00:00:16,875
In here, there are two files reviews.txt and labels.txt. These are just the text files for our movie reviews data and their corresponding labels, positive or negative.

3
00:00:16,875 --> 00:00:25,800
So, I'm going to load these in and print out some of their contents. Here, you can see some example review text that's talking about a comedy called bromwell high.

4
00:00:25,800 --> 00:00:35,680
And here you see some of the text and the label's file, which just has lines positive and negative. Actually, this looks like just one review and I want to see if I can print out more than one.

5
00:00:35,680 --> 00:00:44,800
All right. So here, I've started printing out a second review here and you can see that these two are separated by new line characters, much like positive and negative are separated by new lines.

6
00:00:44,800 --> 00:00:53,180
Now, we already know that we need to pre-process this data and to organize all of the words in our vocabulary so that we have numerical data to feed to our model later.

7
00:00:53,180 --> 00:01:00,190
Since we're using an embedding layer, we'll need to encode each word as an integer and we'll also want to clean up our data a bit.

8
00:01:00,190 --> 00:01:06,505
The first pre-processing steps I want to take are turn our text to lowercase and getting rid of extraneous punctuation.

9
00:01:06,505 --> 00:01:13,175
Punctuation that, in this case, will not really have any bearing on whether our review is classified as positive or negative.

10
00:01:13,175 --> 00:01:20,640
Okay. So in this cell, I'm converting all my review text to lowercase and I'm getting rid of everything that is punctuation.

11
00:01:20,640 --> 00:01:27,220
And I'm using a built-in Python list here, which is from string import punctuation, and I'm going to print out what all is in there.

12
00:01:27,220 --> 00:01:36,920
So, punctuation is just a list of all of these punctuation characters. Then for our reviews, I'm looking at every character and if it's not and the punctuation list, I'm keeping it.

13
00:01:36,920 --> 00:01:43,360
This gives me a version of the review text that is all text no punctuation. So, I'm storing that in this variable all_text.

14
00:01:43,360 --> 00:01:55,000
Next, I know that my reviews are separated by a new line characters slash n. So, to separate out our reviews, I'm going to split the text into each review using slash n as the delimiter here.

15
00:01:55,000 --> 00:02:03,765
Then I can combine all the reviews back together as one big string. Finally, I get to my end goal, which is splitting that text into individual words.

16
00:02:03,765 --> 00:02:16,480
So, I'll run the cell and print out the first 30 words, and it looks just as I expect. Essentially, the original text that I printed out only all the punctuation is removed and we've separated everything into individual words.

17
00:02:16,480 --> 00:02:26,225
So, our data is in good shape, and by now you should know what's coming next. We have to take our word data and our label text data and convert this into numerical data.

18
00:02:26,225 --> 00:02:34,430
Your first couple of exercises will be to create a dictionary vocab_to_int that can convert any unique word into an integer token.

19
00:02:34,430 --> 00:02:43,145
Then using this dictionary, I want you to create a new list of tokenized words, all the words in our data but converted into their integer values.

20
00:02:43,145 --> 00:02:54,180
I'd also like it so that our dictionary maps more frequent words to lower integer tokens. One important thing to note here is that later, we're going to pad our input vectors with zeros.

21
00:02:54,180 --> 00:03:05,110
So, I actually do not want zero as a word token. I want the tokenized values to start at one. And so, the most common word in our vocabulary should be mapped to the integer value one.

22
00:03:05,110 --> 00:03:12,200
So, create that dictionary, use it to tokenize our words, and then store those tokens in a list, reviews_ints.

23
00:03:12,200 --> 00:03:21,980
Below this, I provided some code that lets you test your implementation. It'll print the length of your vocabulary and it will print the first review in your tokenized review list.

24
00:03:21,980 --> 00:03:38,015
Your next and similar task is going to be to encode our label text into numerical values. We saw that this text was just lines of positive or negative, and I want you to create an array encoded labels that converts the word positive to one and negative to zero.

25
00:03:38,015 --> 00:03:44,990
I'm not providing any testing code here, but I encourage you to get in the habit of testing out your own code piece by piece as you build.

26
00:03:44,990 --> 00:03:52,990
It's good practice and can be as simple as a few print statements to check that your data is converted as you expect or that it's the correct size and so on.

27
00:03:52,990 --> 00:04:00,295
These checks can really save some time later on because these code blocks really build on one another, and it's good to debug early and often.

28
00:04:00,295 --> 00:04:08,870
Okay. So, try encoding all of our words and labels on your own. And if you get stuck or want to check your solution, feel free to look at the solution video next.


@@@
1
00:00:00,000 --> 00:00:07,315
First, here's how I went about creating a vocab to int dictionary and encoding our word data, and there are a few ways to do this.

2
00:00:07,315 --> 00:00:14,830
I chose to use this important counter to create a dictionary that maps the most common words in our reviews text to the smallest integers.

3
00:00:14,830 --> 00:00:23,480
So the first thing I'm doing is to get a count of how many times each of our words actually appears in our data using counter and passing in our words.

4
00:00:23,480 --> 00:00:30,185
Then with these counts, I'm creating assorted vocabulary. This sorts each unique word by its frequency of occurrence.

5
00:00:30,185 --> 00:00:37,235
So this vocab should hold all of the unique words that make up our word data without any repeats, and it will be sorted by commonality.

6
00:00:37,235 --> 00:00:48,275
I also know that I want to start encoding my words with the integer value of one rather than zero. So the most common word like be or of should actually be encoded as one.

7
00:00:48,275 --> 00:00:55,510
I'm making sure that we start our indexing at one by using enumerate and passing in our vocab and our starting index, one.

8
00:00:55,510 --> 00:01:02,365
Enumerate is going to return a numerical value, ii, and a word in our vocabulary. It will do this in order.

9
00:01:02,365 --> 00:01:08,795
So our first index is going to be one, and the first word is going to be the most common word in our assorted vocabulary.

10
00:01:08,795 --> 00:01:16,190
So to create the dictionary vocab to int, I'm taking each unique word in our vocab and mapping it to an index starting at the value one.

11
00:01:16,190 --> 00:01:23,550
Great. Next, I'm using this dictionary to tokenize all of our word data. So here, I'm looking at each individual review.

12
00:01:23,550 --> 00:01:29,060
Each of these is one item and review split from before when I separated reviews by the newline character.

13
00:01:29,060 --> 00:01:38,215
Then, for each word in a review, I'm using my dictionary to convert that word into its integer value, and I'm appending the token as review to reviews_ints.

14
00:01:38,215 --> 00:01:47,540
So the end result will be a list of tokenized reviews. Here in the cells below, I'm printing out the length of my dictionary and my first sample encoded review.

15
00:01:47,540 --> 00:01:55,050
I can see that my dictionary is a bit over 74,000 words long, which means that we have this many unique words that make up our reviews data.

16
00:01:55,050 --> 00:02:03,020
Let's take a look at this tokenized review. I'm not seeing any zero values which is good, and these encoded values look as I might expect.

17
00:02:03,020 --> 00:02:09,050
So I've successfully encoded the review words, and I'll move on to the next task, which is encoding our labels.

18
00:02:09,050 --> 00:02:15,840
So in this case, I want to look at my label's text data and turn the word positive into one and negative into zero.

19
00:02:15,840 --> 00:02:23,590
Now we haven't much processed our labels data, and I know much like the reviews text that a new label is on every new line in this file.

20
00:02:23,590 --> 00:02:30,410
So I can get a list of labels, labels_split, by splitting our loaded in data using the newline character as a delimiter.

21
00:02:30,410 --> 00:02:40,555
Then I just have a statement that says, for every label in this label_split list, I'm going to add one to my array if it reads as positive, and a zero otherwise.

22
00:02:40,555 --> 00:02:47,300
I'm wrapping this in np.array, and that's all I need to do to create an array of encoded labels. All right.

23
00:02:47,300 --> 00:02:54,305
This is a good start. There are still a few data clean up and formatting steps that I'll want to take before we get to defining our model.


@@@
1
00:00:00,000 --> 00:00:09,000
After encoding all our word and label data as an additional preprocessing step, we want to make sure that our reviews are in good shape for standard processing.

2
00:00:09,000 --> 00:00:16,885
That is, our network will expect a standard input text size, and so we'll want to shape our reviews into a consistent specific length.

3
00:00:16,885 --> 00:00:28,440
There are two things we'll need to do to approach this task. First, I'm going to take a look at the review data and see do we have any especially sure or longer views that might mess with our training process.

4
00:00:28,440 --> 00:00:36,830
I'll especially look to see if we have any reviews of length zero which will not provide any text information and will just act as noisy data.

5
00:00:36,830 --> 00:00:48,710
If I find any of those zero length reviews, I'll want to remove them from our data entirely. Then second, I'll look at the remaining reviews, and for really long reviews, I'll actually truncate them at a specific length.

6
00:00:48,710 --> 00:00:54,695
I'll do something similar for shortest reviews and make sure that I'm creating a set of reviews that are all the same length.

7
00:00:54,695 --> 00:01:03,410
This will be our padding and truncation step, where we basically pad our data with columns of zeros or remove columns until we get our desired input shape.

8
00:01:03,410 --> 00:01:10,910
Okay. Before we pad our review text, we should check for reviews of length zero. The way I'm gonna do this is to use a counter.

9
00:01:10,910 --> 00:01:19,370
For each review length that's currently in our data, whether that's a length of zero or thousands of words, I'll look at how many reviews are of that length.

10
00:01:19,370 --> 00:01:25,935
So this returns a dictionary of review lengths and account for how many our reviews fall into those lengths.

11
00:01:25,935 --> 00:01:32,735
So, here I'm looking at how many of our reviews are zero length and I'll also print out the longest review length just to see.

12
00:01:32,735 --> 00:01:40,365
So, when I run this cell, I can see that I have one review that is zero length and that my longest review has over 2,000 words in it.

13
00:01:40,365 --> 00:01:53,650
This zero length review is just going to add noise into our dataset. So next, your task will be to create a new list of reviews_ints and an array of encoded labels, where any reviews of zero length will be removed from this data.

14
00:01:53,650 --> 00:02:04,130
So, remove any zero length reviews from reviews_ints and remove that corresponding label as well. In this particular case, after running this cell, I expect to see that one of our reviews was removed.

15
00:02:04,130 --> 00:02:09,530
Try to solve this task on your own, and next, I'll present my solution and introduce you to your next exercise.


@@@
1
00:00:00,000 --> 00:00:08,420
So last time, we noticed that we had one review with zero length in our dataset. This review will not contribute any meaningful training information.

2
00:00:08,420 --> 00:00:16,245
So here, I'm removing any reviews of zero length from our reviews ends list. I'll do the same thing with their corresponding label.

3
00:00:16,245 --> 00:00:33,535
The way I went about this task was I thought, "Okay, I'm going to want to find any reviews of zero length and remove that data from my existing reviews ends and encoded labels data." So, I first identified the indices in our data that I want to keep which I'll call my non-zero indices.

4
00:00:33,535 --> 00:00:44,355
I'm checking the length of each review in our reviews end data. If the length is not equal to zero, that means I want to keep it and I'm recording its index in our list of non-zero indices.

5
00:00:44,355 --> 00:00:54,525
Then I'm just getting those indices from my existing reviews ends list and encoded labels array. I'm just trying the new clean data in these variables of the same name.

6
00:00:54,525 --> 00:01:03,315
When I do my length check, I can see that this is effectively removed one review from our data. In this case, there was only one review of zero length, so this looks good.

7
00:01:03,315 --> 00:01:10,625
Now, the next thing I want to deal with is very long review text data and standardizing the length of our reviews in general.

8
00:01:10,625 --> 00:01:17,585
We saw that the maximum review length was about 2,500 words and that's going to be too many steps for our RNN.

9
00:01:17,585 --> 00:01:24,535
In cases like this, I want to truncate this data to a reasonable size and number of steps. This brings me to the next exercise.

10
00:01:24,535 --> 00:01:32,240
To deal with both short and very long reviews, we'll either pad or truncate all reviews to a specific sequence length.

11
00:01:32,240 --> 00:01:43,155
For reviews that are shorter than some sequence length, we'll pad it on the left with zeros. For reviews longer than the sequence length, we can truncate them to the first sequence length worth of words.

12
00:01:43,155 --> 00:01:50,345
So, here's a padding example. Say, we have a short sequence of words and we specify that we want a sequence length equal to 10.

13
00:01:50,345 --> 00:01:58,385
The resultant padded sequence should be this, padded on the left with seven zeros and the original three-word tokens are at the end.

14
00:01:58,385 --> 00:02:09,565
Now in the case of a long review, it would just be cut at the sequence length of 10. This is just a small example and for our movie review data, a good sequence length is going to be around 200.

15
00:02:09,565 --> 00:02:21,205
An exercise, I want you to complete this function pad features. This takes in our list of reviews ends and a sequence length and it should either pad or truncate every review in the past end list.

16
00:02:21,205 --> 00:02:35,720
It should return an array of transformed reviews which I'll call R features which are our tokenized reviews of the same sequence length and you'll often hear transform data like this referred to as the features or input features for a model.

17
00:02:35,720 --> 00:02:44,145
So, at the end, each of the rows in the feature's array will be transformed review of a standard sequence length that we can then feed into a model as input.

18
00:02:44,145 --> 00:02:51,410
So, try to solve this and then the next cell I've included some print statements and assertions that act as tests on the shape of your feature's array.


@@@
1
00:00:00,000 --> 00:00:10,280
So here's my solution for creating an array of features, reviews that have either been padded on the left with zeros until their sequence length on or truncated at that length.

2
00:00:10,280 --> 00:00:26,815
First, I'm actually creating an array of zeros, that's just the final shape that I know I want. That is, it should have as many rows as I have reviews in the input reviews_ints data into as many columns as the specified sequence length, and this will just hold all the zero integers for now.

3
00:00:26,815 --> 00:00:35,955
Then for each review in my list, I'll put it as a row in my features array. The first review is going to go on the first row, and the second in the second, and so on.

4
00:00:35,955 --> 00:00:45,295
I started out thinking of my short review case. I want to keep a left padding of zeros, up until I reach where that review can fill the remaining values.

5
00:00:45,295 --> 00:00:53,670
So, I'm looking at filling my features, starting at the index that's at the end of the features row, minus the length of the input review.

6
00:00:53,670 --> 00:01:01,580
So, if a reviewer show, this means our features are going to keep the zeros which are padding on the left, and the review tokens will be on the right side.

7
00:01:01,580 --> 00:01:07,130
It turns out that I only have to add one more piece to this line to make this work for a long reviews too.

8
00:01:07,130 --> 00:01:15,720
Hear for annual review including those longer than the given sequence length, I'm truncating them at that sequence length, and this should fill the corresponding features row.

9
00:01:15,720 --> 00:01:23,020
So, this loop will do this for every review in reviews_ints, and then returns these features. Below I'm running my test code.

10
00:01:23,020 --> 00:01:36,555
Here I'm creating features passing in my list of reviews_ints and a sequence length equal to 200. I don't trigger any of these error messages, so I know my dimensions are correct, and then printing out the first ten values of the first three rows here.

11
00:01:36,555 --> 00:01:45,620
And here's what these rows look like. A lot of these start with zeros, which is what I expect for left padding, and others have filled up these rows with various token values.

12
00:01:45,620 --> 00:01:52,465
So, this is great. And I'll also add that. In this step, we've actually introduced a new token into our review features.

13
00:01:52,465 --> 00:02:00,205
Remember that before, all words in our vocabulary hadn't associated integer value, and we started organizing with the value one.

14
00:02:00,205 --> 00:02:12,450
So, in our vocab_to_int dictionary, we had integers from one up to 74,000 or so. And here by adding zero as padding, I've effectively inserted the zero token into our vocabulary.

15
00:02:12,450 --> 00:02:26,660
Okay. Now for your next and the last data transformation exercise with our data in nice shape, next, I want you to split the features and encoded labels into three different datasets, training, validation, and test sets.

16
00:02:26,660 --> 00:02:32,560
You'll need to create datasets for grouping our features and labels like train_x and train_y, for example.

17
00:02:32,560 --> 00:02:41,775
And we'll use these different sets to train and test our model. So, I've defined a split fraction, split_frac, as the fraction of data to keep in the training set.

18
00:02:41,775 --> 00:02:51,415
This is set to 0.8 or 80 percent of data. The 20 percent of the data that's left should be split in half to create the validation and testing data respectively.

19
00:02:51,415 --> 00:03:02,600
So, I'll leave this as an exercise. And next, I'll go over how I split the data and I'll show you some PyTorch resources we can use to effectively batch and iterate through these different datasets.


@@@
1
00:00:00,000 --> 00:00:11,130
In this cell, I've split our features and encoded labels into training test and validation sets. I started by splitting our features and label data according to their split frac.

2
00:00:11,130 --> 00:00:22,235
So, I'm reserving 80 percent of my data for training and I'm basically getting the index at which I should split my features and label data based on this value 0.8 and actually I could have just put in this variable here.

3
00:00:22,235 --> 00:00:32,735
Then I'm splitting my features first, getting the features up until my 80 percent split index. This makes up my training features train_x then I'm getting the remaining data.

4
00:00:32,735 --> 00:00:49,930
So, after the split index and that makes up my remaining_x. Then, I'm doing the exact same thing but for my labels data, splitting it at the 80 percent index to get my training labels and my remaining data then I'm doing something similar all over again only with the remaining data.

5
00:00:49,930 --> 00:01:04,240
I'm getting an index to split this data in half, so at the 0.5 mark. Then each half of our remaining_x will make up our validation and test sets of features and each half of remaining_y will make up our validation and test set of labels.

6
00:01:04,240 --> 00:01:09,820
That's it. The last step I'm doing is checking my work and printing out the shapes of my features data.

7
00:01:09,820 --> 00:01:18,760
I can see that I have the largest number of reviews in my training set with a sequence length of 200 and my validation and test sets are of the same size.

8
00:01:18,760 --> 00:01:24,015
If you want, you can do the same thing for your labels data and you should see the same number of rows here.

9
00:01:24,015 --> 00:01:34,615
So, this is 80 percent of my data, 10 percent, and 10 percent. After creating training test and validation data, we want to batch this data so that we can train on batches at a time.

10
00:01:34,615 --> 00:01:45,985
Typically, you've seen this done with a generator function which we could definitely do here but I want to show you a really nice way to batch our datasets when we've split up our input features and labels like this.

11
00:01:45,985 --> 00:01:58,530
We can actually create data loaders for our data by following a couple of steps. First, we can use pytorch's tensor dataset to wrap tensor data into a known format and I can look at the documentation for this here.

12
00:01:58,530 --> 00:02:13,360
This dataset basically takes any amount of tensors with the same first dimension, so the same number of rows, and in our case this is our input features and the label tensors and it creates a dataset that can be processed and batched by pytorch's data loader class.

13
00:02:13,360 --> 00:02:19,595
So, once we create our data wrapping it in a tensor dataset, we can then pass that to a data loader as usual.

14
00:02:19,595 --> 00:02:27,305
Data loader just takes in some data and a batch size and it returns a data loader that batches our data as we typically might.

15
00:02:27,305 --> 00:02:32,450
This is a great alternative to creating a generator function for batching our data into full batches.

16
00:02:32,450 --> 00:02:38,725
The data loader class is going to take care of a lot of behind-the-scenes work for us and here's what this looks like in code.

17
00:02:38,725 --> 00:02:52,570
First, I'm creating my tensor datasets. To create my training data, I'm passing in the tensor version of my train_x and train_y that I created above and torch that from numpy just takes in numpy arrays and converts them into tensors.

18
00:02:52,570 --> 00:02:59,520
So, I'm doing that for my training validation and test data and if you named your data differently above, you'll have to change those names here.

19
00:02:59,520 --> 00:03:08,955
In fact, I could have actually done these steps the other way around. Creating a tensor dataset for all my data and then splitting the data into different sets. Both approaches work.

20
00:03:08,955 --> 00:03:17,810
Then for each tensor dataset that I just created, I'm passing it into pytorch's data loader or I can specify a batch size parameter equal to 50 in this case.

21
00:03:17,810 --> 00:03:27,710
So, without the messiness of loops and yield commands, this defines training validation and test data loaders that I can use in my train loop to batch data into the size I want.

22
00:03:27,710 --> 00:03:40,670
So, this gives me three different iterators and I just want to show you what a sample of data from this data loader looks like looking at our train loader and getting an iterator, then grabbing one batch of data using a call to next.

23
00:03:40,670 --> 00:04:00,220
So, this should return some sample input features and some sample labels. Then I'm printing out the size of my input which I can see is the batch size 50 and the sequence length 200 and the output label size which is just 50, one label for each review in the input batch and I see my tokens and the encoded labels as well.

24
00:04:00,220 --> 00:04:06,200
So, this is looking really great. Next, we can proceed with defining and training the model on this data.


@@@
1
00:00:00,000 --> 00:00:09,135
By now, you've had a lot of practice with data processing and with defining RNNs. So, I'm not going to give you too much guidance here, when it comes to defining this model.

2
00:00:09,135 --> 00:00:18,360
Here's what it should look like generally. The model should be able to take in our word tokens, and the first thing that these go through will be an embedding layer.

3
00:00:18,360 --> 00:00:28,255
We have about 74,000 different words, and so this layer is going to be responsible for converting our word tokens, our integers into embeddings of a specific size.

4
00:00:28,255 --> 00:00:35,930
Now, you could train a Word2Vec model separately, and actually just use the learned word embeddings as input to an LSTM.

5
00:00:35,930 --> 00:00:43,100
But it turns out that these embedding layers are still useful even if they haven't been trained to learn the semantic relationships between words.

6
00:00:43,100 --> 00:00:54,945
So in this case, what we're mainly using this embedding layer for is dimensionality reduction. It will learn to look at our large vocabulary and map each word into a vector of a specified embedding dimension.

7
00:00:54,945 --> 00:01:02,150
Then, after our embedding layer, we have an LSTM layer. This is defined by a hidden state size and number of layers as you know.

8
00:01:02,150 --> 00:01:14,110
At each step, these LSTM cells will produce an output and a new hidden state. The hidden state will be passed to the next cell as input, and this is how we represent a memory in this model.

9
00:01:14,110 --> 00:01:25,560
The output is going to be fed into a Sigmoid activated fully connected output layer. This layer will be responsible for mapping the LSTM layer outputs to a desired output size.

10
00:01:25,560 --> 00:01:36,800
In this case, this should be the number of our sentiment classes, positive or negative. Then, the Sigmoid activation function is responsible for turning all of those outputs into a value between zero and one.

11
00:01:36,800 --> 00:01:43,615
This is the range we expect for our encoded sentiment labels. Zero is a negative and one is a positive review.

12
00:01:43,615 --> 00:01:57,875
So, this model is going to look at a sequence of words that make up a review. Here, we're interested in only the last Sigmoid output because this will produce the one label we're looking for at the end of processing a sequence of words in a review.

13
00:01:57,875 --> 00:02:07,370
So here's a little more explanation and some links to documentation if you need it. Then below, in this first cell, I'm going to check if a GPU is available for training.

14
00:02:07,370 --> 00:02:23,045
Then here, I want you to complete this model. It should take in all these parameters: our vocab size, output size, embedding dimension, hidden dimension, number of layers, and an optional dropout probability, and create an entire sentiment RNN model.

15
00:02:23,045 --> 00:02:32,810
You'll be responsible for completing the init and forward functions for this model. Remember that the output should just be the last value from our Sigmoid output layer.

16
00:02:32,810 --> 00:02:43,235
I'll also ask you to complete the init hidden function for the LSTM layer. This should initialize the hidden and cell state to be all zeros and move them to a GPU, if available.

17
00:02:43,235 --> 00:02:51,635
I'd encourage you to look at the documentation when helpful or your other code examples. You should have all the information you need to complete this model on your own.

18
00:02:51,635 --> 00:02:59,110
If you're confident in your model definition, later in this code, you'll be able to define your model hyperparameters and train it.

19
00:02:59,110 --> 00:03:06,375
Next, I'll show you one solution for defining the sentiment RNN model. But I do think this is a fun task to try out on your own in earnest too.

20
00:03:06,375 --> 00:03:12,670
I think it's a great exercise in thinking about how data is shaped as it moves through a model. So, good luck.


@@@
1
00:00:03,290 --> 00:00:17,445
Hi there. I'm Juno and I'm really excited you want to learn Python. Before coming to Udacity, I worked as a data scientist and used Python for data analysis, machine learning, and deep learning projects.

2
00:00:17,445 --> 00:00:28,695
Python is one of the most widely used programming languages in industry. It's a powerful, general purpose language with applications ranging from web development to data science.

3
00:00:28,695 --> 00:00:35,689
In this course you'll learn how to apply Python and good programming practices to solve practical problems.


@@@
1
00:00:03,629 --> 00:00:18,519
As you learn Python throughout this course, there are a few things you should keep in mind. First, unlike languages like SQL, Python is case sensitive, meaning these two words mean two different things.

2
00:00:18,519 --> 00:00:33,039
Second, spacing is important in Python. Since it isn't very heavy on syntax like other languages, it relies a lot on spacing, which can be confusing at first, but really nice and clean when you get the hang of it.

3
00:00:33,039 --> 00:00:40,649
Third, there will be times you get errors in this course. Keep in mind, errors don't make you a bad programmer.

4
00:00:40,649 --> 00:00:51,780
It's just a computer telling you it doesn't understand. Use error messages to help you learn how to write code that can be interpreted the way you wish. Let's get started.


@@@
1
00:00:02,299 --> 00:00:13,000
Whether you're a beginner in programming or have experience in other languages, this course is well-structured to help you develop fluency in Python.

2
00:00:13,000 --> 00:00:29,085
Each lesson includes videos, text summaries, quizzes and coding activities for each concept. If you'd like to move through the course faster, you can skip directly to text summaries and quizzes to test your knowledge, and go back to videos if you get stuck.

3
00:00:29,085 --> 00:00:37,009
However, if you are new to programming, I strongly recommend watching the videos for a detailed walk-through of each topic.

4
00:00:37,009 --> 00:00:46,310
Here's an overview of what you learn. You'll start off learning about the basics of python in lesson two; data types and operators.

5
00:00:46,310 --> 00:00:54,429
These are the building blocks you use to write your programs. In lesson three, you'll dive into control flow tools.

6
00:00:54,429 --> 00:01:06,055
You learn about conditional statements, loops, built-in functions, and list comprehensions. With control flow tools, you'll be able to write more complex and creative code.

7
00:01:06,055 --> 00:01:16,479
In lesson four, you'll learn about function definitions, variable scope, documentation, lambda expressions, iterators, and generators.

8
00:01:16,480 --> 00:01:25,450
This is where things can become a little tricky. But these are also some of the most important tools you'll need to use in day-to-day practice.

9
00:01:25,450 --> 00:01:33,265
Finally, the last lesson will show you how to run your code locally and scripts that take in raw input and work with files.

10
00:01:33,265 --> 00:01:45,000
You'll also learn about error handling and importing different libraries. I know this looks like a lot of content, but there's plenty of review and practice throughout the course to reinforce what you're learning.


@@@
1
00:00:00,000 --> 00:00:11,445
Welcome to this lesson on data types and operators. This is a very important lesson because it's where you'll learn about the building blocks of Python datatypes and operators.

2
00:00:11,445 --> 00:00:21,285
Students tend to learn how to program at different rates. Take your time understanding each of the concepts that follow and getting practice with activities in this lesson.

3
00:00:21,285 --> 00:00:30,464
Having a firm grasp on these building blocks will provide a strong foundation for future lessons and essential skills to continue programming beyond this course.

4
00:00:30,464 --> 00:00:45,045
Throughout this course, we'll go through code examples using these boxes. This first box represents the Code Editor which is where we will input our Python code, and the second box represents the output window which is where our results will be displayed.

5
00:00:45,045 --> 00:00:51,974
You can follow along with the video examples in the classroom by typing in the code editor below each video.

6
00:00:51,975 --> 00:01:01,079
Click the Test Run button to run your code and view the results in the output window. You'll be seeing this print function pretty often.

7
00:01:01,079 --> 00:01:08,335
It helps us see what's going on in our code. For example, let's say you wanted to compute three plus five.

8
00:01:08,334 --> 00:01:17,734
If we run this line of code, Python would still add three and five but we wouldn't see the result because we didn't tell Python to do anything with it.

9
00:01:17,734 --> 00:01:30,679
Print is a useful built-in function in Python that we can use to display our results. All we have to do is type in Print followed by parentheses around whatever we want printed.

10
00:01:30,680 --> 00:01:36,849
You will be using Print often as you get to know the building blocks of Python, its data types and operators.


@@@
1
00:00:01,940 --> 00:00:13,274
In the last video, you saw this line of Python that computes the sum of 3 and 5. The plus sign and this line is an arithmetic operator.

2
00:00:13,275 --> 00:00:29,804
Python has several arithmetic operators most of which follow the usual rules of Mathematics. Let's look at the first four, in Python addition and subtraction are performed with the usual symbols, plus and minus.

3
00:00:29,804 --> 00:00:40,989
Multiplication uses an asterisk, and division uses a forward slash. Here you can see that multiplication happens before addition.

4
00:00:40,988 --> 00:00:49,155
This is because Python follows Mathematical Order of Operations which you can get a refresher on in the notes below.

5
00:00:49,155 --> 00:01:00,450
If you want addition to come first, you can enclose this part in parentheses. Moving on from those four, here's the operator for exponentiation.

6
00:01:00,450 --> 00:01:12,076
You can raise one number to the power of another with two asterisks. For example, this line prints three to the power of two which results in nine.

7
00:01:12,076 --> 00:01:23,649
There's another operator that is sometimes mistaken for the exponentiation operator. The caret. This actually performs a more obscure operation called bitwise XOR.

8
00:01:23,650 --> 00:01:38,500
This is an arithmetic operator that doesn't follow the usual rules of Mathematics. Bitwise operators are not something you need to know for this course but, if you're interested there's information about this in the notes below.

9
00:01:38,500 --> 00:01:50,219
All you need to remember is that if you perform exponentiation, you use two asterisks and not with a caret or you'll accidentally produce very confusing results.

10
00:01:50,219 --> 00:02:01,515
Another useful operator is this percent sign which performs the modulo operation. It returns the remainder after you've divided the first number by the second.

11
00:02:01,515 --> 00:02:12,090
In this example, nine divided by two is four with a remainder of one. So this line would print one since modulo gives us the remainder.

12
00:02:12,090 --> 00:02:26,835
You might also find use for integer division denoted by two forward slashes. It divides one integer by another but rather than giving the exact answer, it rounds down the answer down to an integer.

13
00:02:26,835 --> 00:02:36,329
Seven divided by two is three point five which rounds down to three. Notice it rounds down even if the answer is negative.

14
00:02:36,330 --> 00:02:49,449
In this case, negative three point five was rounded down to negative four. There are other categories of operators that we'll learn about soon but these are all the arithmetic operators in Python.


@@@
1
00:00:00,420 --> 00:00:11,814
Understanding how to perform arithmetic in Python is useful. But understanding how to use variables can turn Python into more than just a calculator.

2
00:00:11,814 --> 00:00:22,149
Using variables, as opposed to just raw numbers, has many advantages. Let's get started. Creating a new variable in Python is simple.

3
00:00:22,149 --> 00:00:30,910
Here's one which stores the population of mountain view. The variable name in this example is mv_population.

4
00:00:30,910 --> 00:00:47,504
The equal sign is the assignment operator and the value of the variable is 74,728. Notice, this isn't simply an expression of equality like in Math where x equals y, and y equals x mean the same thing.

5
00:00:47,505 --> 00:00:55,415
In Python, the equal sign is an operator that assigns the value on the right to the variable name on the left.

6
00:00:55,414 --> 00:01:02,344
In other words, whatever term is on the left side is now a name for whatever value is on the right side.

7
00:01:02,344 --> 00:01:15,145
Let's see another example. The first line here defines x as 2, and the second line defines y as the value of x which is printed in the third line.

8
00:01:15,144 --> 00:01:26,425
Notice we can use a variable's name to access its value. In this line, we only needed the name of the variable x to define y as its value two.

9
00:01:26,424 --> 00:01:39,474
Similarly, in this next line, we were able to print the value of y just by using the name y. If you try to access the value of a variable that was never defined, you'll get this error.


@@@
1
00:00:03,850 --> 00:00:21,635
Python also has a useful way to assign multiple variables at once. These three assignments can be abbreviated using multiple assignments like this: x is still assigned to two, y to three, and z to five.

2
00:00:21,635 --> 00:00:30,495
You can use this when you're assigning closely related variables like the width and height of an object or an object coordinates.

3
00:00:30,495 --> 00:00:39,409
In this example, we used x, y, and z as variable names. However, normally, we'd want something more descriptive.

4
00:00:39,409 --> 00:00:49,539
x, y, and z would be appropriate variable names for something like coordinates in a 3D space. But for most other situations, we can do better.

5
00:00:49,539 --> 00:01:01,335
For example, if you were computing the population density of Mountain View by dividing the population by the land area, this is one way to compute the correct answer.

6
00:01:01,335 --> 00:01:08,734
But it's much clear with these variable names, which are actually descriptive of the values they represent.

7
00:01:08,734 --> 00:01:15,444
In addition to being descriptive, there are a few things to watch out for when naming variables in Python.

8
00:01:15,444 --> 00:01:27,734
First, only use ordinary letters, numbers, or underscores in your variable names. They can't have spaces and need to start with a letter or underscore.

9
00:01:27,734 --> 00:01:37,560
Second, you can't use reserved words or built-in identifiers that have important purposes in Python, which you'll learn about throughout this course.

10
00:01:37,560 --> 00:01:47,950
There are also links below. Using them as variable names will lead to errors or problematic situations when you try to use that word for its intended purpose.

11
00:01:47,950 --> 00:01:55,359
Third, the pythonic way to name variables is to use all lowercase letters and underscores to separate words.


@@@
1
00:00:02,950 --> 00:00:11,509
We've already set mv_ population to this value. But what if we want to update it now that the population changed?

2
00:00:11,509 --> 00:00:30,920
We can just assign this variable again to its new value which we found to be 78,128 or if we got this new value because we knew 4,000 moved to Mountain View and 600 moved away, we could just apply those changes directly to this variable.

3
00:00:30,920 --> 00:00:42,089
In this line, the variable mv_population is being assigned to itself plus 4000 minus 600 which results in 78,128.

4
00:00:42,090 --> 00:00:51,375
Because this kind of increment and reassign operation is very common, Python actually has special assignment operators for this.

5
00:00:51,375 --> 00:01:03,965
Instead of using mv_population twice in one line, we can actually use this plus equals operator to tell Python we are incrementing the variable on the left by the value on the right.

6
00:01:03,965 --> 00:01:13,545
Plus equals is one example of another assignment operator in Python, minus equals is another, and there are a bunch more.

7
00:01:13,545 --> 00:01:29,959
These are actually just all the arithmetic operators followed by an equal sign. All of these operators just apply the arithmetic operation to the variable on the left with the value on the right and makes your code more concise which is good.


@@@
1
00:00:01,260 --> 00:00:13,390
So far, most of the numbers we've been working with have been whole numbers or integers. However, as you may have noticed other types of numbers do exist in Python.

2
00:00:13,390 --> 00:00:30,385
Here, dividing one integer by another gives us a number that isn't an integer 0.75. In Python and computing in general, we represent such a number as a float, which is short for floating point number.

3
00:00:30,385 --> 00:00:43,429
A float is a real number that uses a decimal point to allow numbers with fractional values. Even if one integer divides another exactly, the result will be a float.

4
00:00:43,429 --> 00:00:52,215
The int and float are actually two kinds of data types. In Python, every object you encounter will have a type.

5
00:00:52,215 --> 00:01:05,445
An object's type defines which operators and functions will work on that object and how they work. You can check the type of any object directly using the built-in function type.

6
00:01:05,444 --> 00:01:14,222
Here you can see that the type of a number without a decimal and a number with a decimal are different in Python.

7
00:01:14,222 --> 00:01:23,805
To make an int, just give a whole number without a decimal point. Here is an int. To make a float, include a decimal point.

8
00:01:23,805 --> 00:01:32,070
If the number itself is actually a whole number, that's okay, you don't even have to put anything after the decimal point.

9
00:01:32,069 --> 00:01:46,170
These are both floats. An operation involving an int and a float always produces a float. Sometimes, you might need to manually convert one numeric type to another.

10
00:01:46,170 --> 00:02:01,085
And you can do that by constructing new objects of those types with int and float. When we convert a float to an int, the part of the number after the decimal point is cut off which means that no rounding occurs.

11
00:02:01,084 --> 00:02:16,820
49.7 is cut to 49. 4.0 calculated from 16 divided by 4 is cut to 4. Converting from int to float adds decimal zero to the end of the number.

12
00:02:16,819 --> 00:02:25,405
So, we've seen Python's two main numeric types: integers and floating point numbers. What are they good for?

13
00:02:25,405 --> 00:02:32,830
There are many times when you might need to count items or need to rely on the result of a computation being an integer.

14
00:02:32,830 --> 00:02:44,050
For example, let's say you want to count how many people showed up at your dinner party. You can't count 0.47 of a person, so you use an integer.

15
00:02:44,050 --> 00:02:57,162
If what you're working on isn't necessarily a whole number, a float is the type you're looking for. For example, let's say you made five pies for your dinner party and you want to keep track of the amount of pie left.

16
00:02:57,163 --> 00:03:06,682
People usually take one-sixth of a slice. Once one slice is taken, you're down to 4.83 pies, so you use a float.

17
00:03:06,682 --> 00:03:16,344
Floating point numbers are approximations of the numbers they're opposed to represent. This is necessary because floats can represent an enormous range of numbers.

18
00:03:16,344 --> 00:03:26,395
So, in order to fit numbers in computer memory, Python must use approximations. This trade off can sometimes have surprising results.

19
00:03:26,395 --> 00:03:42,995
Because the float or approximation for 0.1 is actually slightly more than zero 0.1, when add several of them together we can see a difference between the mathematically correct answer and the one that Python creates.

20
00:03:42,995 --> 00:03:50,189
In most contexts, these small differences are irrelevant but it's important to know that they're there.


@@@
1
00:00:02,899 --> 00:00:11,550
One thing you might have noticed is that in a single line of Python, whitespace doesn't really affect how your code works.

2
00:00:11,550 --> 00:00:22,890
For example, this will give exactly the same output as this, however. That doesn't mean that these lines are equally good lines of code.

3
00:00:22,890 --> 00:00:30,945
Learning how to write clear and readable code is critical for others in your company and future you to understand.

4
00:00:30,945 --> 00:00:45,255
Here are some best practices for code style in Python. When you call a function like print, put the opening parentheses straight after the name of the function, like here. Not like this.

5
00:00:45,255 --> 00:01:07,754
Don't put extra spaces immediately inside the parentheses either. So, this is good. But this isn't. If you are mixing operators with different priorities like multiplication and subtraction, then you might like to add a space around the lower priority, in this case subtraction, to make the code easier to read.

6
00:01:07,754 --> 00:01:16,914
See how this is slightly harder to comprehend for operation order. Don't write extremely long lines of code.

7
00:01:16,915 --> 00:01:25,704
They're hard to understand. People commonly limit themselves to lines that are 79 or 99 characters long.

8
00:01:25,704 --> 00:01:34,735
If you feel like you need to write longer lines, consider rewriting, simplifying, or separating your code into multiple lines.

9
00:01:34,734 --> 00:01:43,209
These conventions come from the Python developers guide, which has a style guide called PEP 8, which is linked below.

10
00:01:43,209 --> 00:01:51,620
There's a lot in there. Don't worry, you don't need to digest it all now. Why are these conventions important?

11
00:01:51,620 --> 00:02:03,694
Although how you format the code doesn't affect how it runs, following standard style guidelines makes code easier to read and consistent among different developers on a team.

12
00:02:03,694 --> 00:02:15,560
So, it's a good idea to follow the guidelines, even with one line expressions. It will be useful to refer back to PEP 8 once in a while to get your Python style right.

13
00:02:15,560 --> 00:02:24,260
Later, we'll also learn about tools that can check your code for you, and provide suggestions based on PEP 8 guidelines.


@@@
1
00:00:01,790 --> 00:00:16,044
We've seen two kinds of Python data types so far, ints and floats and we've used arithmetic operators like addition and exponentiation to work with these values.

2
00:00:16,045 --> 00:00:26,085
Another type is Bool which is used to represent the values true and false. Bool is an abbreviation of boolean.

3
00:00:26,085 --> 00:00:37,685
Boolean Algebra is a branch of algebra dealing with variables whose values are true or false. Boolean algebra is named for its inventor George Bool.

4
00:00:37,685 --> 00:00:50,854
Boolean logic underpins all digital devices existing in almost every line of computer code and has transformed the way we live our lives which you can read more about in the instructor notes below.

5
00:00:50,854 --> 00:01:04,674
We can assign boolean values like this. It's not very useful on its own though. We can use comparison operators like less than and greater than to compare values and produce a boolean result.

6
00:01:04,674 --> 00:01:27,659
Here, 42 is not greater than 43. So printing the result provides the boolean false. Here, you can see a full list of the comparison operators in Python: less than, greater than, less than or equal to, greater than or equal to, equal to, and not equal to.

7
00:01:27,659 --> 00:01:36,420
Notice that evaluating equality is performed with two equal signs and a not equal uses an exclamation point.

8
00:01:36,420 --> 00:01:58,185
This is a bit different than Excel or SQL. In addition to comparison operators, these logical operators are very useful when working with booleans and evaluates if both sides are true or evaluates if at least one side is true and not inverse as a boolean type.

9
00:01:58,185 --> 00:02:09,629
Here's an example that evaluates whether age is within the range of a teenager. Here, you can imagine the 14 being placed in these two spots.

10
00:02:09,629 --> 00:02:23,044
Then, if both are true, true will be assigned to the variable is_teen. In other words if the person is older than 12 and younger than 20 this person is a teen.


@@@
1
00:00:03,290 --> 00:00:12,615
Often programming involves more than numbers and arithmetic. There may be situations where you need to work with text.

2
00:00:12,615 --> 00:00:20,835
To work with text in Python, you will need to use a string, which is an immutable ordered series of characters.

3
00:00:20,835 --> 00:00:34,410
More on the immutable ordered part later. You can create a string by using quotes. Single or double quotes work equally well, although there are some edge cases, which we will work through.

4
00:00:34,409 --> 00:00:46,974
In each of these cases I printed the string "hello" and got the output "hello". We can set a variable to be a string the same way we did with numbers.

5
00:00:46,975 --> 00:00:58,859
Strings can include any characters, even spaces, punctuation and numbers. However, what do we do when we want quotation marks in our string?

6
00:00:58,859 --> 00:01:08,325
Since we use quotation marks to define our strings, this presents a small problem. Here the code doesn't work the way we want it to.

7
00:01:08,325 --> 00:01:19,140
Python offers two solutions to this problem. The first is to place the string in single quotes rather than double quotes, like this.

8
00:01:19,140 --> 00:01:30,750
You can use either type of quote to define strings, but sometimes you'll need to define a string that includes both single and double quotes.

9
00:01:30,750 --> 00:01:40,959
What then? In that case, you can use the backslash to escape quotes. Here the string is delimited by single quotes.

10
00:01:40,959 --> 00:01:53,123
The single quote within the string is preceded by a backslash so that Python knows that it should be interpreted as part of the string rather than the quote that ends the string.

11
00:01:53,123 --> 00:02:03,319
Once our strings are defined, there are a few operations that are used for integers and floats that we can also use for strings.

12
00:02:03,319 --> 00:02:11,509
For example, we can use the plus sign to put strings together, and we can use multiplication to repeat strings.

13
00:02:11,509 --> 00:02:24,955
Let's look at an example of each. Here our variables are holding two words. We can use the plus sign to concatenate the two strings together and print the result.

14
00:02:24,955 --> 00:02:32,870
This is fundamentally different from numeric addition. However, notice the two names have been squished together.

15
00:02:32,870 --> 00:02:45,875
We're missing a space. Python is completely literal when working with strings. We need to explicitly include spaces and punctuation if we want what we write to make sense.

16
00:02:45,875 --> 00:03:01,936
This time we got a string that makes sense, putting the two words together with a space in between. Note that previously I said a white space doesn't matter in between parentheses in bits of code, like print statements.

17
00:03:01,936 --> 00:03:12,794
Here with strings, you can see that spaces do matter in between the quotation marks. Let's try another mathematical operation.

18
00:03:12,794 --> 00:03:20,375
Turns out we can use the multiplication operator as well. It repeats the string as many times as you multiply it.

19
00:03:20,375 --> 00:03:30,530
Here five times. Although addition and multiplication have different applications for strings, subtraction and division do not.

20
00:03:30,530 --> 00:03:44,579
Here we get an error that a string is an unsupported type for the division operator. A useful function that's built into Python is Len which can tell us the length of a string.

21
00:03:44,580 --> 00:03:56,750
This is just the number of characters in the string. Len is like print in that it's a built-in function that takes in a value in parentheses to perform an action.

22
00:03:56,750 --> 00:04:11,535
Len differs from print in that it returns a value that can be stored in a variable. In this example, the Len function outputs to number seven, which is stored in the Udacity length variable.

23
00:04:11,534 --> 00:04:19,519
Built-in just means Python provides these functions for us. Later, we'll learn how to define our own functions.


@@@
1
00:00:00,540 --> 00:00:18,740
Up until now, we've discussed four data types in Python: Int, float, bool, and string. If you recall from our previous video, you can check the type of any object directly using the built-in function type.

2
00:00:18,739 --> 00:00:27,509
Using type, we can observe that the same number can be coded in different types, each with their own set of behaviors.

3
00:00:27,510 --> 00:00:39,670
As a side note, here, we're calling a function print on the output of another function type. We use parenthesis to define the order in which functions get run.

4
00:00:39,670 --> 00:00:48,435
What's contained in one set of parenthesis needs to be evaluated first before being given as input to the next function.

5
00:00:48,435 --> 00:00:57,229
Here, the type function is run first and then its output is printed. Different types have different properties.

6
00:00:57,229 --> 00:01:05,075
And when you're designing on computer program, you'll need to choose the types for your data based on how you're going to use them.

7
00:01:05,075 --> 00:01:13,525
For example, if you want to use a number as a part of a sentence, it'll be easiest if that number is a string.

8
00:01:13,525 --> 00:01:20,850
If you want to encode a true-false value, it'll be much easier to manipulate as a boolean than a string.

9
00:01:20,849 --> 00:01:28,804
Why is it easier? There are specially designed functions for working with each data type. You'll learn about these soon.

10
00:01:28,805 --> 00:01:36,429
You might also have situations where you don't control the type of data that you receive, but you still need to use it.

11
00:01:36,430 --> 00:01:46,135
The good news is that you can create new objects from old and change a type in the process. We went over this in the integers and floats video.

12
00:01:46,135 --> 00:02:00,944
For example, here, we create an int from a float and assign it to a variable count. Here, we create a string out of the house_number and use that to build a larger address string.

13
00:02:00,944 --> 00:02:09,344
First, we have the house_number, as well as the street and town_name. You can see that the house_number is currently an int.

14
00:02:09,344 --> 00:02:20,099
We can change it to a string, like this, and use the plus operator to create the full address. You can also build a number from a string like this.

15
00:02:20,099 --> 00:02:28,009
Here, we start with a string of 35. But by wrapping it in a float function, we can see the type has now changed.


@@@
1
00:00:00,000 --> 00:00:14,904
So far, we've seen two distinct ways to process data with Python, operators and functions. We've used operators like these, which process the two values on either side of the operator.

2
00:00:14,904 --> 00:00:25,400
We've also used functions like print and len. Functions are very similar to operators. In fact, the only real difference is in how they look.

3
00:00:25,399 --> 00:00:35,525
Function Inputs are put in parentheses rather than being placed next to an operator, and functions have descriptive names rather than short symbols.

4
00:00:35,524 --> 00:00:44,695
There is a third technique for operating on values, methods. The best way to learn about methods is with an example.

5
00:00:44,695 --> 00:00:55,024
Consider this title method. Methods are related to functions, but unlike functions, methods are associated with specific types of objects.

6
00:00:55,024 --> 00:01:08,694
That is, there are different methods depending on the type of data you're working with. In this example, the object is a string with the value Sebastian Thrun and we are calling its title method.

7
00:01:08,694 --> 00:01:24,799
The method returns a string in title case, meaning the first letter of each word is capitalized. So methods are functions that belong to an object, an object, for example, being a string.

8
00:01:24,799 --> 00:01:33,719
Let's try another string method, islower(). The lower method checks one of the characters in a string or lowercase.

9
00:01:33,719 --> 00:01:53,079
In this case, the string object is Sebastian Thrun. Is lower returns true since there are no uppercase letters you've probably noticed that when we call the is lower and title methods, we use parentheses, but we haven't put anything in them like we did when calling functions.

10
00:01:53,079 --> 00:02:06,090
Those inputs in the parentheses are called arguments. Since methods or special types of functions that belong to an object, the object is always the first argument to a method.

11
00:02:06,090 --> 00:02:17,250
So, is lower and title actually did have an argument although there was nothing in the parentheses. The argument is disguised as the string object.

12
00:02:17,250 --> 00:02:30,064
Let's try a method that takes more arguments than just the object, count. Here, the count method returns how many times the substring fish occurs in the string.

13
00:02:30,064 --> 00:02:43,000
The object is the string, one fish, two fish, red fish, blue fish, and the method is dot count, and four occurrences of the word fish exist in the string.


@@@
1
00:00:00,167 --> 00:00:06,900
MEMBERSHIP LISTS AND OPERATORS, PART 1 We looked at individual pieces of data like the string and the number.

2
00:00:06,942 --> 00:00:19,700
They are good, but Python's ability to compose powerful programs is amplified when we work with data containers, which contain other types of data and even other containers.

3
00:00:19,742 --> 00:00:30,533
Let's look at our first Python container, list. It is a data structure in Python that is a mutable ordered sequence of elements.

4
00:00:30,575 --> 00:00:40,299
We'll talk more about the changeable part later. This code defines the months variable, which contains a list of strings.

5
00:00:40,341 --> 00:00:51,132
Each element in the list is a month of the year. A list is defined with square brackets and always contains data separated by commas.

6
00:00:51,174 --> 00:01:00,366
The data can be a mixture of any data type. The list has an order or, In other words, it is ordered.

7
00:01:00,408 --> 00:01:15,933
We can search for individual elements in the list by their index. We can search for values ​​this way: we type the name of the list followed by square brackets with an integer indicating its position or index.

8
00:01:15,975 --> 00:01:26,800
Note that the first element in the list, "January", is located at index zero, and not at index one. Many programming languages ​​follow this convention.

9
00:01:26,842 --> 00:01:39,066
We call this “indexing from scratch”. If this is confusing, think of it this way: an element's index describes its distance from the beginning of the list.

10
00:01:39,108 --> 00:01:53,333
The first is zero elements away from the start, the second is one element away, and so on. We can also index from the end of the list, not the beginning, as we have done so far.

11
00:01:53,375 --> 00:02:08,099
For this, we use negative indices. For example, we can take the last month of the year. Index -1 refers to the last element in the list, -2 to the penultimate, and so on.

12
00:02:08,141 --> 00:02:15,900
Note that although zero is the first index in the list, negative zero will not be the last, and this can be a little confusing.

13
00:02:15,942 --> 00:02:23,900
When you try to access an index in a list that doesn't exist, you will receive a list of index exceptions.

14
00:02:23,942 --> 00:02:35,501
This is how Python tells us that we are trying to access an index that is not in the list. Since index 25 does not exist, we receive this error.

15
00:02:36,167 --> 00:02:44,733
Index errors are common, especially when we are not used to doing this. We can see a lot of mistakes in the beginning.

16
00:02:44,775 --> 00:02:54,299
The most common reason for an index error is using an index that loses by one, but there may be other reasons for an error.

17
00:02:54,341 --> 00:03:03,667
It's always good to use the print function and debug with a small example. This will tell you if the index is out and by what amount.


@@@
1
00:00:01,667 --> 00:00:12,634
LISTS AND MEMBERSHIP OPERATORS, PART 2 In addition to accessing individual elements in a list, we can use Python slicing notation to access a subsequence of a list.

2
00:00:12,667 --> 00:00:22,000
This means using indexes to slice parts of an object like a string or a list. An example makes this very clear.

3
00:00:22,033 --> 00:00:35,901
See this list of months. We can slice the third quarter of the year this way. The index to the left of the colon, six, is where the slicing begins.

4
00:00:35,968 --> 00:00:45,834
It continues until the second index, nine. Note that six is ​​the July index, but nine is the October index.

5
00:00:45,901 --> 00:00:56,167
The lower bound is inclusive, but the upper bound is exclusive. There are some slicing shortcuts that simplify common situations.

6
00:00:56,200 --> 00:01:03,701
If we want to slice something that starts from the beginning of the list, we can omit the starting index like this.

7
00:01:04,367 --> 00:01:17,167
If we want to slice something to the end of the list, we can omit the end index like this. List is a type, just like string, float and int.

8
00:01:17,234 --> 00:01:27,234
Of the types we've seen, "list" looks most like "string". Both support the "LEN" function, indexing and slicing.

9
00:01:27,267 --> 00:01:36,067
We see here that the length of a string is the number of characters, and that of a list is the number of elements.

10
00:01:36,100 --> 00:01:47,434
Indexing from scratch serves both, as does index-based slicing. Other things they support are association operators "in" and "not in".

11
00:01:47,501 --> 00:02:00,133
The "in" evaluates whether the object on the left is included in the object on the right. "not in" evaluates whether the object on the left is not included in the object on the right.

12
00:02:00,200 --> 00:02:10,701
Here we see if the string "her" it's in this string, which is right here. However, "Sunday" is not in the list, as it only shows the months.


@@@
1
00:00:00,601 --> 00:00:10,933
MEMBERSHIP LISTS AND OPERATORS, PART 3 How are lists different from strings? Both support indexing, slicing and the in operator.

2
00:00:10,975 --> 00:00:18,832
The most obvious difference is that strings are sequences of letters, while list elements can be any type of object.

3
00:00:18,874 --> 00:00:31,233
A more subtle difference is that lists can be modified, but strings cannot. Here we can change the fourth element, index three, to Friday if we want.

4
00:00:31,275 --> 00:00:43,066
But here we get an error if we try to change the third element of a string. The technical term that describes an object that can be modified after creation is mutability.

5
00:00:43,108 --> 00:00:53,234
We can modify the values ​​of lists, therefore list is a mutable data type. We cannot modify strings, so they are immutable data types.

6
00:00:53,868 --> 00:01:03,933
As we see more types of data, there are two factors we will continue to consider: one is mutability and the other is order.

7
00:01:03,975 --> 00:01:13,299
"Order" is whether the order of an object's contents matters and whether part of the object can be accessed using "order".

8
00:01:13,341 --> 00:01:23,099
Strings and lists are ordered, which is why indexing works well with them. However, one is changeable and the other is not.

9
00:01:23,141 --> 00:01:37,267
We will see more data types from different sections of this grid. For now, we will explore methods and functions that work with lists and use list mutability in our programs.


@@@
1
00:00:00,000 --> 00:00:09,010
Previously, when we created a variable, that held an immutable object, the value of that immutable object was saved in memory.

2
00:00:09,010 --> 00:00:19,145
Here, we create a name with value Jim, and assign it to another variable, called Student. It is the value Jim, that is assigned to student.

3
00:00:19,145 --> 00:00:26,470
So, when we reassign name, to update it to Tim, this change is not reflected in the value of student.

4
00:00:26,470 --> 00:00:41,940
Lists are different from strings, as they are mutable. Here, we create a list of scores, and assign the same list to the variable grades: B, C, A, D, B, A, six of them.

5
00:00:41,940 --> 00:00:52,345
When we change, or mutate the score's list, making the fourth grade B instead of D, this affects both scores and grades.

6
00:00:52,345 --> 00:01:01,750
Both scores and grades are variable names for the same underlying list, and either name can be used to access and change that lists.

7
00:01:01,750 --> 00:01:10,400
The behavior of variables containing mutable and immutable objects, are very different and might even seem surprising at times.

8
00:01:10,400 --> 00:01:19,520
Experiment used to print functions and double check your work where you can, to make sure that your programs correctly keep track of their data.

9
00:01:19,520 --> 00:01:28,960
While you experiment with lists, there are some useful functions you should get familiar with. Len, retains how many elements are in a list.

10
00:01:28,960 --> 00:01:38,545
Max, retains the greatest element of a list. How the greatest element is determined, depends on what type of objects are in your list.

11
00:01:38,545 --> 00:01:51,275
The maximum element in a list of numbers, is the largest number. The maximum element in a list of strings, is the element that would occur last, if the list was sorted alphabetically.

12
00:01:51,275 --> 00:02:02,475
That's reticulated python for this list. R is the largest letter alphabetically. In other words, greater than B, A, B, and A.

13
00:02:02,475 --> 00:02:14,605
This works because the max function, is defined in terms of the greater than comparison operator. The greater than comparison operator, is defined for many non-numeric types.

14
00:02:14,605 --> 00:02:21,160
If you're working with objects that can be compared with this, then you can use max on a list of the objects.

15
00:02:21,160 --> 00:02:29,875
For strings, the standard comparison is alphabetical. So the maximum of this list, is the element that appears last alphabetically.

16
00:02:29,875 --> 00:02:36,920
Although you can create lists, that hold a mix of elements of many types as you see here, integers and texts.

17
00:02:36,920 --> 00:02:47,280
The max function is undefined for lists, that contain elements from different incomparable types. Here, you can see it breaks, with this mix of datatypes.

18
00:02:47,280 --> 00:03:00,495
Min is the opposite of max, and returns the smallest element in a list. Sorted returns a copy of a list, in order from smallest to largest, leaving the original lists unchanged.

19
00:03:00,495 --> 00:03:11,195
Here, for a list called sizes, the order is ascending. You can sort from largest to smallest, by adding the optional argument, reverse equals true.


@@@
1
00:00:02,240 --> 00:00:15,200
Let's introduce a new string method that works with lists. Join. Join takes a list as an argument, and returns a string consisting of the list elements, joined by separator string.

2
00:00:15,200 --> 00:00:22,965
In this example, we use a string backslash and, as a separator, so that there's a new line between each element.

3
00:00:22,965 --> 00:00:32,885
Fore newline, aft newline, starboard newline, port. We can also use other strings as separators would join.

4
00:00:32,885 --> 00:00:41,000
Here, we use a hyphen. It's important to remember to separate each of the items in the list you're joining, with a comma.

5
00:00:41,000 --> 00:01:01,655
Forgetting to do so, will not trigger an error, but will also give you unexpected results. In the example below, omitting a comma between Garcia and Okelly, results in the following: Notice how the hyphen separator is missing between Garcia and Okelly, and instead, the two strings were appended.

6
00:01:01,655 --> 00:01:11,890
This happens because of Python's default string literal appending. If join returns different results than expected, check for missing commas.

7
00:01:11,890 --> 00:01:23,020
Also, note that join will trigger an error, if we try to join anything other than strings. We get an error here, because an integer was included in the list.

8
00:01:23,020 --> 00:01:33,520
Lastly, a helpful method called append, adds an element to the end of the list. Next, you'll practice working with lists, and explore more list methods.


@@@
1
00:00:01,760 --> 00:00:11,370
Python provides another useful container, tuples. Tuples are used to store related pieces of information.

2
00:00:11,370 --> 00:00:22,410
A tuple is a data structure in Python that is an immutable ordered sequence of elements. Consider this example involving latitude and longitude.

3
00:00:22,410 --> 00:00:30,240
Tuples are similar to lists in that they store an ordered collection of objects which can be accessed by their indices.

4
00:00:30,240 --> 00:00:43,565
For example, location zero and location one. Unlike lists however, tuples are immutable. You cannot add or remove items from tuples or sort them in place.

5
00:00:43,565 --> 00:00:58,090
Why do we have tuples if they're like lists with fewer features? Tuples are useful when you have two or more values that are so closely related that they will always be used together, like latitude and longitude coordinates.

6
00:00:58,090 --> 00:01:10,885
Tuples can also be used to assign multiple variables in a compact way. Notice that the values assigned to the tuple dimensions aren't surrounded with parentheses as previous examples were.

7
00:01:10,885 --> 00:01:19,095
The parentheses are optional when making tuples and programmers frequently omit them if parentheses don't clarify the code.

8
00:01:19,095 --> 00:01:27,365
In the second line, three variables are assigned from the content of the tuple dimensions, this is called tuple unpacking.

9
00:01:27,365 --> 00:01:38,875
You can use tuple unpacking to assign the information from a tuple into multiple variables without having to access them one by one, and make multiple assignment statements.

10
00:01:38,875 --> 00:01:50,430
In this example, if we won't need to use dimensions directly, we could shorten those two lines of code into a single line that assigns three variables in one go.


@@@
1
00:00:02,720 --> 00:00:09,815
Imagine that you run a popular search engine, and you've surveyed your users to see where they're browsing from.

2
00:00:09,815 --> 00:00:22,645
You've collected the 785 responses and have assembled them into a list of countries. There aren't 785 countries in the world, which means that there are duplicate entries in the country's list.

3
00:00:22,645 --> 00:00:33,760
Slicing the list to see the first few elements confirms this. It will be useful to remove the duplicates to produce a list of all the countries that users browse from.

4
00:00:33,760 --> 00:00:41,875
Well, a set in Python does exactly that. Sets our containers of unique elements without any particular ordering.

5
00:00:41,875 --> 00:00:53,245
We can create a set from a list like this. Set removes the duplicates and the print function prints the unique values of which there are 196 countries.

6
00:00:53,245 --> 00:01:08,455
Sets support the in operator the same way lists do. India is in this set. You can add elements to sets where you don't use the append method like you do with lists, instead, sets have the add method.

7
00:01:08,455 --> 00:01:18,725
Here, Italy is added. Sets also have a pop method just like lists. When you pop an element from a set, a random element is removed.

8
00:01:18,725 --> 00:01:30,415
Remember that sets, unlike lists, are unordered, so there is no last element. Other operations you can perform with sets, include those of mathematical sets.

9
00:01:30,415 --> 00:01:40,760
Methods like union, intersection, and difference are easy to perform with sets and are much faster than such operators with other containers.


@@@
1
00:00:03,630 --> 00:00:14,355
Sets, are simple data structures, and they have one main use, collecting unique elements. Our next data structures, dictionaries, are more flexible.

2
00:00:14,355 --> 00:00:23,295
Rather than storing single objects like lists and sets do, dictionaries store pairs of elements, keys and values.

3
00:00:23,295 --> 00:00:31,440
In this example, we define a dictionary, where the keys are element names, and their values are their corresponding atomic numbers.

4
00:00:31,440 --> 00:00:41,490
We can look up values in the dictionary, by using square brackets enclosing a key. We can also insert new values into the dictionary, with square brackets.

5
00:00:41,490 --> 00:00:49,030
Here, we are adding lithium, and giving it a value of three. Dictionary keys are similar to list indices.

6
00:00:49,030 --> 00:01:00,290
We can select elements from the data structure, by putting the key in square brackets. Unlike lists, dictionaries can have keys of any immutable type, not just integers.

7
00:01:00,290 --> 00:01:08,685
The element dictionary uses strings for its keys. However, it's not even necessary for every key to have the same type.

8
00:01:08,685 --> 00:01:17,295
We can check whether a value is in a dictionary, the same way we check whether a value is in a list or set, with the in keyword.

9
00:01:17,295 --> 00:01:26,165
We can use in to verify, whether a key is in the dictionary, before looking it up. If there's a possibility, that the key is not there.

10
00:01:26,165 --> 00:01:35,960
Mithril, was not part of our elements dictionary, so false is printed. Dictionaries have a related method, that's also useful, "get".

11
00:01:35,960 --> 00:01:45,770
"Get" looks up values in a dictionary, but unlike square brackets, "get" returns none, or a default value of your choice, if the key is not found.

12
00:01:45,770 --> 00:02:00,550
The dilithium is not in our dictionary, so none is returned, and then printed. If you expect lookups to sometimes fail, Get might be a better tool than normal square bracket lookups, because errors can crash your program, which isn't good.

13
00:02:00,550 --> 00:02:08,505
You can check if a key return none with the "is" operator, or you can check for the opposite using "is not".

14
00:02:08,505 --> 00:02:21,720
These are called identity operators. You will learn more about identity operators, and how they differ from using these equals to, or not equals to, comparison operators in the quizzes that follow.


@@@
1
00:00:02,210 --> 00:00:11,675
In the elements dictionary we saw earlier, element names which are strings are mapped to their atomic numbers which are integers.

2
00:00:11,675 --> 00:00:26,360
But what if we wanted to store more information about each element, like their weight and symbol. We can do that by adjusting this dictionary so that it maps the element names to another dictionary that stores that collection of data.

3
00:00:26,360 --> 00:00:35,775
We can look up information about an entry in this nested dictionary in the same ways we did before, with square brackets or the GET method.

4
00:00:35,775 --> 00:00:48,245
We can look up specific information from the helium dictionary like this. This code is first looking up the key helium in the elements dictionary, producing the helium dictionary.

5
00:00:48,245 --> 00:00:56,160
The second lookup weight, then looks up the weight key in that helium dictionary to find helium's atomic weight.


@@@
1
00:00:00,170 --> 00:00:09,044
Congratulations on completing this lesson on data types and operators. You started your journey in Python with a great foundation.

2
00:00:09,044 --> 00:00:17,640
Next, you'll learn how to piece together the building blocks you just learned to write cooler and more complex programs. Great job.


@@@
1
00:00:01,730 --> 00:00:12,554
Welcome to this lesson on Control Flow. Here, you'll learn how to add more functionality to your code by being able to use conditional statements and loops.

2
00:00:12,554 --> 00:00:30,240
You'll learn how to implement decision-making with if statements, repeat code with for and while loops, exit or skip loops with break and continue, use helpful built-in functions like zip and enumerate, and combine some of these concepts in concise list comprehensions.

3
00:00:30,239 --> 00:00:40,379
With these tools, you'll be able to bring together the data types and operators you learned from the previous lesson in more complex and creative ways. Let's get started.


@@@
1
00:00:02,120 --> 00:00:14,339
We've been running code that simply executes every line one by one from the top down. Many times however, we want to run code only if a particular condition holds.

2
00:00:14,339 --> 00:00:20,894
To demonstrate this concept, let's take a look at this billing system for a pay-as-you-go mobile phone.

3
00:00:20,894 --> 00:00:41,689
Say a customer has a credit balance for their phone which they can use for calls and messages. The customer can then set up a link to their bank account so that if their phone credit balance goes below a threshold amount, in this case five dollars, then 10 more dollars of credit are added and their bank balance is build.

4
00:00:41,689 --> 00:00:53,629
Here is a simple way of representing this billing system in code. If the phone balance is below five, add 10 to phone balance and subtract 10 from bank balance.

5
00:00:53,630 --> 00:01:05,465
This is an example of an if statement. An if statement is a conditional statement that runs or skips code based on whether a condition is true or false.

6
00:01:05,465 --> 00:01:15,380
In an if statement, the if keyword is followed by the condition to be checked, in this case, phone balanced less than five, and then a colon.

7
00:01:15,379 --> 00:01:27,815
The condition is specified in a Boolean expression that evaluates to either true or false. After this line is an indented block of code to be executed if the condition is true.

8
00:01:27,814 --> 00:01:42,859
So in this case, these lines will only execute if it is true that phone balance is less than five. If phone balance is three, this condition evaluates to true and these indented lines of code are executed.

9
00:01:42,859 --> 00:01:53,370
You can observe these changes in the output. If phone balance is eight however, this condition evaluates to false and these lines are not executed.

10
00:01:53,370 --> 00:02:03,200
As you see in the output, there were no changes to phone or bank balance, which is what we expect to happen since the condition in this if statement was false.


@@@
1
00:00:02,509 --> 00:00:14,810
Now you know how to execute a block of code if a condition is true. But what if you have a different block of code that you want to execute when that condition is false?

2
00:00:14,810 --> 00:00:25,050
You can use the 'else' keyword to do so. Consider this code, which prints a message indicating whether an integer N is even or odd.

3
00:00:25,050 --> 00:00:37,424
If N is even, this line is run. Otherwise, this line is run. Code indented under the 'else' is what happens when this condition evaluates to false.

4
00:00:37,424 --> 00:00:52,820
The 'else' keyword is always followed by a colon and doesn't need a Boolean expression. So, if we set N to be 4 like this, the if statement here would evaluate to true and we would print the following string.

5
00:00:52,820 --> 00:01:03,704
Alternatively, if we set N to be 15, this condition would evaluate to false so we would skip this block and execute the code under the else.

6
00:01:03,704 --> 00:01:12,424
If you have more than two possible cases, you can also use elif, short for 'else if', to deal with the situation.

7
00:01:12,424 --> 00:01:20,234
This saves the multiple indentation that would be needed if we used 'else' and then another 'if' statement inside the 'else' block.

8
00:01:20,234 --> 00:01:32,084
Like 'if', an elif statement always requires a conditional expression. For example, let's say we wanted to print what to do with the garden based on the current season.

9
00:01:32,084 --> 00:01:42,059
If we set season equal to spring, like this, then we can see that plant the garden is printed as its first condition evaluates to true.

10
00:01:42,060 --> 00:01:54,945
Alternatively, if we set season to winter, then each of these conditions will evaluate to false until we had this condition which will evaluate as true and print stay indoors.

11
00:01:54,944 --> 00:02:07,150
Notice here we're using double equal sign again. Remember, a single equal sign is for assignment like we did here when we were setting a season as a particular string.


@@@
1
00:00:02,419 --> 00:00:13,695
As you have just seen, indentation is important. It's how we tell Python what code is in the body of an if statement, and what code is outside of it.

2
00:00:13,695 --> 00:00:26,100
Indentation doesn't just matter in if statements. Soon, you'll see it used in other contexts. Some other languages use braces to show where blocks of code begin and end.

3
00:00:26,100 --> 00:00:36,329
In Python, we use indentation to enclose blocks of code. This indentation conventionally comes in multiples of four spaces.

4
00:00:36,329 --> 00:00:44,780
It's important to be strict about following this convention because changing the indentation can completely change the meaning of the code.

5
00:00:44,780 --> 00:00:52,059
If you are working on a team of Python programmers, it's important that everyone follows the same indentation convention.


@@@
1
00:00:02,919 --> 00:00:12,714
All the if and elif statements we've seen so far have been followed by a single Boolean expression that checks only one condition.

2
00:00:12,714 --> 00:00:28,704
However, more complicated Boolean expressions can be useful as well. If the condition is working with the numerical variable, you might want to check whether a value lies in a certain range or even do some calculation in order to make a comparison.

3
00:00:28,704 --> 00:00:41,395
Notice this condition uses logic and algebra, and will still run correctly in Python. Storing values for height and weight, we can quickly print the result for any individual.

4
00:00:41,395 --> 00:00:49,414
Some situations may call for logical operators. If it's rainy and sunny, I might look for a rainbow.

5
00:00:49,414 --> 00:00:57,815
Notice that the if statement here requires that both of these individual variables hold true to evaluate as true.

6
00:00:57,814 --> 00:01:16,009
If either is false, then this line will evaluate as false, and our print statement will not run. Let's say I want to send a promotional email to a customer, if they have not requested to be taken off the email list, and they're in a location where they'll be able to redeem the offer.

7
00:01:16,010 --> 00:01:27,250
For really complicated conditions, you might need to combine some ands, ors, and nots together. Use parentheses if you need to make the combinations clear.

8
00:01:27,250 --> 00:01:43,000
However, simple or complex, the entire line in an if statement must be a Boolean expression that evaluates to either true or false, and it is this value that decides whether the indented block in an if statement executes or not.


@@@
1
00:00:03,209 --> 00:00:18,769
We'll go over some good and bad examples of Boolean expressions used in if statements. While true is a valid Boolean expression, it's not exactly useful as a condition as it always evaluates to true.

2
00:00:18,769 --> 00:00:29,460
So the intended code will always get run. Similarly, if false is not a condition you should run either, the condition and the if statement would never occur.

3
00:00:29,460 --> 00:00:39,339
In this example, is cold or not is cold, will always evaluate to true. If it is cold, then is cold will be true.

4
00:00:39,340 --> 00:00:48,034
If it is not cold, then not is code will be true. This has no use as a condition because the indented code will always get run.

5
00:00:48,034 --> 00:01:01,505
This code is valid in Python, but it is not a Boolean expression, although it reads like one. The reason is that the expression to the right of or is not a Boolean expression, it's a string.

6
00:01:01,505 --> 00:01:14,695
Later, we'll discuss what happens when you use a non-boolean type object in place of Boolean. It takes a few more characters, but this is now unambiguously a Boolean expression.

7
00:01:14,694 --> 00:01:26,589
The expressions on both sides of the logical operator or are each checking something. This is a valid condition, but we could express it more briefly and clearly.

8
00:01:26,590 --> 00:01:35,525
Is cold is a Booolean expression on its own right, so we can make the code more readable by using the variable itself instead.

9
00:01:35,525 --> 00:01:45,370
If you want to check whether a Boolean is false, you can use the "not" operator. Let's summarize the lessons from these examples.

10
00:01:45,370 --> 00:02:01,239
True and False are both Booleans, but it doesn't make sense to use if True or if False. Logical operators and, or, and not have specific meanings that aren't quite the same as their meanings in plain English.

11
00:02:01,239 --> 00:02:12,129
Make sure your Boolean expressions are being evaluated the way you expect them to. Don't compare a variable that is a Boolean with equals true or equals false.


@@@
1
00:00:02,629 --> 00:00:12,955
So far, the conditions we've used were Boolean expressions that evaluate to a Boolean object, either true or false.

2
00:00:12,955 --> 00:00:24,515
If we put some other object that is not a Boolean in the IF statement, Python will check for its truth value and use that to decide whether or not to run the indented code.

3
00:00:24,515 --> 00:00:36,450
Here, the condition is an integer variable and evaluates to true. The Python documentation lists all the objects that are considered false in this situation.

4
00:00:36,450 --> 00:00:48,424
Anything that isn't listed as having a truth value false will evaluate as true. In this code, errors has a truth value true because it's a non-zero number.

5
00:00:48,424 --> 00:01:03,984
So the error message is printed. This is a nice way of writing a succinct IF statement. Similarly, in this example you saw earlier, the string in the second part of this condition will evaluate to true.

6
00:01:03,984 --> 00:01:22,099
So this statement will always evaluate to true regardless of what value is stored in weather. Alternatively, this will evaluate to true If weather is the string snow, and will evaluate to false if weather is any other string.


@@@
1
00:00:02,069 --> 00:00:10,150
Now that you've learned about conditional statements, let's move on to loops, which allow us to repeat blocks of code.

2
00:00:10,150 --> 00:00:21,542
Python has two kinds, for loops and while loops. Let's first take a look at the for loop, which we can use to iterate over an iterable.

3
00:00:21,542 --> 00:00:36,820
An iterable is an object that can return one of its elements at a time. This can include sequence types such as strings, lists, and tuples as well as non-sequenced types such as dictionaries and files.

4
00:00:36,820 --> 00:00:46,515
You can define objects within iter method to allow them to be used as an iterable, which you can find more information on in the notes below.

5
00:00:46,515 --> 00:00:57,420
Consider this for loop that iterates through a list of cities, capitalizes each one, and prints it. The for keyword signals that this is a for loop.

6
00:00:57,420 --> 00:01:10,314
Cities is the iterable and city is the loop's iteration variable. That is the variable that represents the element in the iterable that the loop is currently processing.

7
00:01:10,314 --> 00:01:19,734
So, in the first iteration, city would be New York City. In the second iteration, it would be Mountain View,and so on.

8
00:01:19,734 --> 00:01:27,640
We can use the city variable to refer to an element within the indented body of a for loop during any iteration.

9
00:01:27,640 --> 00:01:39,909
This indented body is executed once for each element in cities. You can name iteration variables however you like though this example demonstrates a common pattern.

10
00:01:39,909 --> 00:01:54,849
The name of the list cities is the plural form of city, the name of the iteration variable. Naming lists and iteration variables in this style makes it easier for other programmers to understand the purpose of each variable.

11
00:01:54,849 --> 00:02:04,135
So far, the loops we've written extract information from lists. We can also use for loops to create lists and to modify lists.

12
00:02:04,135 --> 00:02:11,405
To create a new list, we can start with an empty list, and then use the append method to add new items.

13
00:02:11,405 --> 00:02:25,034
This for loop iterates through each city and cities and appends it to capitalized cities. Modifying a list is a bit more involved and requires a use of a new function, range.

14
00:02:25,034 --> 00:02:37,194
Range is a built in function used to create immutable sequences of numbers. It has three arguments which must all be integers; start, stop, and step.

15
00:02:37,194 --> 00:02:47,284
Start is the first number of the sequence. Stop is one above the last number of the sequence. And step is the difference between the numbers in the sequence.

16
00:02:47,284 --> 00:03:02,405
If unspecified, start defaults to zero and step defaults to one. Calling range with one integer will make that the stop argument and return a sequence of numbers from zero to that integer minus one.

17
00:03:02,405 --> 00:03:18,299
So, range four returns zero through four minus one, which is three. Calling range with two integers will make those the start and stop, and return a sequence of numbers from the first number to the second number minus one.

18
00:03:18,300 --> 00:03:33,585
Range two, six returns a sequence from two to five. Calling range with three integers will return a sequence of numbers from the first to the second minus one separated by the third.

19
00:03:33,585 --> 00:03:46,650
So, range 1, 10, 2 returns a sequence from one to nine incremented by two. Notice in these examples, we adopt range in a list before printing it.

20
00:03:46,650 --> 00:03:59,984
This is because printing just the output of range only shows you a range object. You can view the values in the range object by converting it to a list or iterating through it in a for loop.

21
00:03:59,985 --> 00:04:08,775
Back in our cities example, we can use the range function to generate the indices of each value in the cities list.

22
00:04:08,775 --> 00:04:18,204
This let's us access the elements of the list with cities bracket index. So, that we can modify the values in the cities list in place.

23
00:04:18,204 --> 00:04:31,280
Let's go through one iteration of this to show exactly how all these pieces work together. Len cities provides four, so the list will be a range from zero to three.

24
00:04:31,279 --> 00:04:39,519
The index will take on each of these values one at a time. So, in this first iteration, we'll have an index of zero.

25
00:04:39,519 --> 00:04:51,455
This part here will then index the first city, New York City, capitalize it using title, then place it back in place of the original New York City spot.

26
00:04:51,454 --> 00:05:01,689
This same process would then occur for each of the additional cities. We can use print to see the change in the cities list at each iteration.

27
00:05:01,689 --> 00:05:11,480
By getting a list of indices with the range function, we were able to index into each element of a list in a for loop to apply a change.

28
00:05:11,480 --> 00:05:18,285
While this modification is one application of the range function, that isn't the only thing it's useful for.


@@@
1
00:00:01,460 --> 00:00:10,560
"For loops" are an example of definite iteration, meaning that the loop's body is run a predefined number of times.

2
00:00:10,560 --> 00:00:23,070
A "for loop" over a list, executes the body once for each element in the list. A "for loop" using the range function will execute the number of times specified by the range function.

3
00:00:23,070 --> 00:00:32,175
This differs from indefinite iteration, which is when a loop repeats an unknown number of times and ends when some condition is met.

4
00:00:32,175 --> 00:00:45,440
Consider this "while loop" that simulates a blackjack dealer by drawing cards from a deck list into a hand list stopping when the value of the cards in the hand is 17 or more.

5
00:00:45,440 --> 00:00:56,780
This example features a new function "sum" and a new list method "pop". Sum is pretty intuitive, it returns the sum of the elements in a list.

6
00:00:56,780 --> 00:01:07,415
Pop is the inverse of the append method, it removes the last element from a list and returns it. You can read more about this in the official documentation.

7
00:01:07,415 --> 00:01:15,255
In this line, we are computing the sum of the list hand and checking if that is less than or equal to 17.

8
00:01:15,255 --> 00:01:25,825
In this line, we're popping the last element from card deck and appending that to the hand list. Let's talk about how this "while loop" works.

9
00:01:25,825 --> 00:01:36,910
The while keyword indicates that this is a while loop. Next is the condition. In this example, sum hand is less than or equal to 17.

10
00:01:36,910 --> 00:01:46,825
If this condition is true, the loop's body will be executed. Each time the loop's body runs, the condition is evaluated again.

11
00:01:46,825 --> 00:01:54,100
This process of checking the condition and then running the loop repeats until the expression becomes false.

12
00:01:54,100 --> 00:02:05,410
The indented body of the loop should modify at least one variable in the test expression. If the value of the test expression never changes, the result is an infinite loop.

13
00:02:05,410 --> 00:02:12,610
In this example, the loop's body appends numbers to the hand lists, which increases the value of sum hand.


@@@
1
00:00:02,209 --> 00:00:11,693
For loops iterate over every element in a sequence, and while loops iterate until they're stopping condition is met.

2
00:00:11,694 --> 00:00:18,314
This is sufficient for most purposes but we sometimes need more precise control over when a loop should end.

3
00:00:18,315 --> 00:00:26,990
In these cases, we use the break keyword. A loop will terminate immediately if it encounters a break statement.

4
00:00:26,989 --> 00:00:36,359
We can use this to end the loop if we detect that some condition has been met. The break keyword can be used in both for and while loops.

5
00:00:36,359 --> 00:00:46,664
For example, suppose you want to load a cargo ship with a list of items called manifest. This list contains tuples of items and their weights.

6
00:00:46,664 --> 00:00:53,579
Ideally, you would like to load all the items on the ship but the ship has a maximum weight capacity.

7
00:00:53,579 --> 00:01:07,594
Therefore, when the ship's capacity is reached, you want to stop loading. To accomplish this, let's use a for loop loading each item and keeping track of the weight of all the items we have loaded so far.

8
00:01:07,594 --> 00:01:15,605
Here, we check if the ship's total cargo weight reaches its maximum capacity of 100 with each addition of cargo.

9
00:01:15,605 --> 00:01:23,299
If it does, we use a break statement to stop loading. If not, we load the next item and add on its weight.

10
00:01:23,299 --> 00:01:38,129
Here's what we end up loading. That's not good. The boat is severely over its weight limit of 100. The break statement did prevent us from putting every item on the boat but we still exceeded the limit by 111.

11
00:01:38,129 --> 00:01:46,060
It's difficult to see what's gone wrong. One strategy we can use is to add print statements in the code.

12
00:01:46,060 --> 00:01:53,770
This is a really handy technique as it can give us some insight into what happens in the code as it's running step-by-step.

13
00:01:53,769 --> 00:02:01,430
Having print statements frequently that give context can really assist us in understanding what went wrong here.

14
00:02:01,430 --> 00:02:09,372
Here's the loop with debugging statements added. Debugging is the process of identifying and removing errors in your code.

15
00:02:09,372 --> 00:02:17,414
Here, we can see that we didn't break until the weight exceeded 100, when really, we should break before that item is added.

16
00:02:17,414 --> 00:02:26,159
Additionally, we can see that the cheeses still could have fit if the machine wasn't added. This brings us to another statement.

17
00:02:26,159 --> 00:02:34,680
Sometimes, rather than breaking out of the loop completely, there will be times we want to skip simply one iteration of the loop.

18
00:02:34,680 --> 00:02:45,115
In this case, we would use the continue keyword. In this example, we iterate through a list of foods and increment account if the food is a fruit.

19
00:02:45,115 --> 00:02:53,995
Here, we terminate an iteration if the food is not found in fruit. Otherwise, we add one to fruit count.


@@@
1
00:00:00,000 --> 00:00:11,445
Looking back at our list of cargo, notice each element is a tuple of size two. Iterating through a list with multiple values, can be pretty helpful.

2
00:00:11,445 --> 00:00:28,710
It's actually really easy to combine and split lists like this. If we originally started, with these two separate lists, items and weights, and wanted to combine them to create a list, like manifest, we can use a built-in function called zip.

3
00:00:28,710 --> 00:00:40,815
Zip, returns an iterator. So, we need to convert it to a list to see the elements. Or, iterate through it with a for loop, if we want to print the values, similar to range.

4
00:00:40,814 --> 00:00:51,479
You could also unpack each tuple in a for loop like this. In addition to zipping two lists together, you can also unzip a list using an asterisk.

5
00:00:51,479 --> 00:00:59,564
For example, using the manifest lists like this, you can separate it into an items and weights list, like this.

6
00:00:59,564 --> 00:01:09,000
The next function we'll look at, is enumerate. Many times, you'll find it useful to iterate through the values of a list, along with the index.

7
00:01:09,000 --> 00:01:19,379
This is one way you could do it. This uses a for loop to iterate through a list of tuples containing the index, and value of each item in the list.

8
00:01:19,379 --> 00:01:29,144
The indices are created by getting a range object from zero, to the length of items minus one, and zipping that with the values in items.

9
00:01:29,144 --> 00:01:41,100
Enumerate, is a special built-in function that makes this a lot simpler. Enumerate returns these tuples, containing the indices and values of a list, in an iterable for you.

10
00:01:41,099 --> 00:01:48,120
You'll be getting some practice using Zip and Enumerate and see how helpful they can be, in the following quiz section.


@@@
1
00:00:01,229 --> 00:00:09,910
In Python, you can create lists really quickly and concisely with a cool tool called List Comprehensions.

2
00:00:09,910 --> 00:00:17,469
In the cities example from earlier, we created a list of capitalized cities from the cities list in a for loop.

3
00:00:17,469 --> 00:00:28,570
With a list comprehension, we can get the same result like this. List comprehensions allow us to create a list using a for loop in one step.

4
00:00:28,570 --> 00:00:37,225
You create a list comprehension with brackets including an expression to evaluate for each element in an iterable.

5
00:00:37,225 --> 00:00:58,155
This line called city.title for each element in cities to create each element in the new list. Notice this part looks just like the first line of a for loop without a colon, and the action you want to take on each element is taking on the element here, and append it to this new list.

6
00:00:58,155 --> 00:01:05,844
In the list comprehension, we don't need to create a new list beforehand and append to it like we would in a for loop.

7
00:01:05,844 --> 00:01:18,314
List comprehensions are not found in other languages, but are very common in Python. Here's another example that creates a list of squares from 0 to 64.

8
00:01:18,314 --> 00:01:28,094
This line called X to the power of two for every element in range nine to create each element in the new list, squares.

9
00:01:28,094 --> 00:01:41,659
We can write this as a list comprehension like this. Again, we are just looping through each element in this iterable and evaluating this expression to get each new element in our list.

10
00:01:41,659 --> 00:01:51,725
You can also add conditionals to list comprehensions. After the iterable, you can use the If keyword to check a condition in each iteration.

11
00:01:51,724 --> 00:02:03,405
In this example, X to the power of two is only evaluated if X is even. This gives us a list only including squares of even numbers.

12
00:02:03,405 --> 00:02:17,858
If you want to add an Else, you will get a syntax error doing this. If you'd like to add Else, you have to move the conditionals to the beginning of the list comprehension right after the expression like this.


@@@
1
00:00:02,390 --> 00:00:15,719
Congratulations on completing this lesson on Control Flow. You learned how to use conditional statements, loops, useful built-in functions, and list comprehensions to add more functionality to your code.


@@@
1
00:00:00,480 --> 00:00:10,900
Welcome to this lesson on functions. Previously, we used several of Python's built-in functions. Here, we will write functions of our own.

2
00:00:10,900 --> 00:00:22,239
Functions are useful chunks of code that allow you to encapsulate a task. Encapsulation is a way to carry out a whole series of steps with one simple command.

3
00:00:22,239 --> 00:00:33,894
For example, imagine you want to bake a cake. You would need to buy the ingredients, mix them together in a certain way for specific amounts of time, put it all in the oven, and then let it cool.

4
00:00:33,895 --> 00:00:47,875
In computer programming, functions encapsulate all the steps of a process into one command. In this case, we might just have a bake cake function and throw all of these directions into this one function.

5
00:00:47,875 --> 00:00:54,969
Now any, time you want to bake a cake, we just use this function without worrying about all the specifics.


@@@
1
00:00:01,270 --> 00:00:15,100
As our first function, we'll write a function that calculates the volume of a cylinder. The formula for this is the cylinder's height, multiplied by the square of its radius, multiplied by pi.

2
00:00:15,099 --> 00:00:25,489
Here's what that formula looks like when defined in a function called cylinder volume. After defining the cylinder volume function, we can use it like this.

3
00:00:25,489 --> 00:00:34,619
This value is the volume of a cylinder 10 inches tall with a radius of three inches. Let's see how this works behind the scenes.

4
00:00:34,619 --> 00:00:52,783
A function definition includes several important parts. First, let's look at the function header. Defining a function always starts with the def keyword to indicate that the code that follows is a function definition, and ends with a colon.

5
00:00:52,783 --> 00:01:01,604
Following def is the name of the function. In this case, cylinder_volume. This needs to be a one word with no gaps.

6
00:01:01,604 --> 00:01:08,944
That's why this has an underscore. The rules for function names are the same as those for variable names.

7
00:01:08,944 --> 00:01:21,320
If you need a reminder on this, see the text below this video. After the function name are parentheses that includes the arguments that the function expects, separated by commas.

8
00:01:21,319 --> 00:01:29,034
These arguments are values that are passed in as inputs when the function is called and are used in this body.

9
00:01:29,034 --> 00:01:39,000
If you write a function that doesn't take arguments, then just leave these parentheses empty. Here is an example of a function that takes no arguments.

10
00:01:39,000 --> 00:01:47,369
In this case, there is no input data we want to work with in the body of the function, so no arguments are necessary.

11
00:01:47,370 --> 00:01:56,814
Next, let's discuss the body of a function. The body of the function is indented after the header and is where the function does its work.

12
00:01:56,814 --> 00:02:07,135
Within this body, we can refer to the argument variables and define new variables. The pi variable that we define here is a local variable.

13
00:02:07,135 --> 00:02:17,050
Meaning; it can only be used within the body of this function. Attempting to access this variable outside the function isn't possible.

14
00:02:17,050 --> 00:02:24,914
This is due to what is called variable scope, which determines which variables you have access to in Python.

15
00:02:24,914 --> 00:02:37,629
We'll discuss this in detail later in the lesson. Often, the body of a function will include this return key word which is used to give back an output value when the function is called.

16
00:02:37,629 --> 00:02:51,033
The value of the expression that follows return is the output of the function. In this example, we return the value evaluated from this formula for the volume of a cylinder.

17
00:02:51,032 --> 00:03:02,930
Rather than returning the value as it is calculated, an alternative technique would be to calculate the volume earlier in the function body and then store it in a variable named volume.

18
00:03:02,930 --> 00:03:16,509
In this case, we would return volume like this. Functions like this can be imagined as little machines that take inputs or arguments and process them into output or return values.

19
00:03:16,509 --> 00:03:30,080
This is a good image but it's incomplete. Some functions like print don't return anything at all. Print displays text on the output window but as we see here, the value it returns is none.

20
00:03:30,080 --> 00:03:41,669
None is when a function will return by default if it doesn't explicitly return anything else. The difference between print and return is often confused.

21
00:03:41,669 --> 00:03:49,676
Print provides output to the console while return provides the value that you can store and work with and code later.

22
00:03:49,676 --> 00:03:58,395
You'll get some practice with this in the quizzes that follow. It's not necessary that every function has a return statement.

23
00:03:58,395 --> 00:04:04,159
Notice this function, print_greeting, doesn't have a return statement, but it's still a valid function.


@@@
1
00:00:00,470 --> 00:00:08,865
Let's revisit the cylinder volume function, but with one modification. This function includes a default argument.

2
00:00:08,865 --> 00:00:16,125
If radius is not specified, then the variable radius will default to five when used in the function body.

3
00:00:16,125 --> 00:00:26,559
Calling the function like this would be the same thing as calling it like this, because radius is set to five if not specified as another value.

4
00:00:26,559 --> 00:00:43,914
Default arguments allow functions to use default values when those arguments are omitted. This is helpful because it can make your code more concise in scenarios where there is a common value you can use for a variable, although you still want it to be customizable.

5
00:00:43,914 --> 00:01:01,574
In this example, the default radius is five but we can still change this. This time when we call the function, we are overriding the default value of five and calculating the cylinder volume for a cylinder with a radius of seven instead.

6
00:01:01,575 --> 00:01:13,080
Also notice here, we're passing values to our arguments by position. It's possible to pass values in two ways, by position and by name.

7
00:01:13,079 --> 00:01:24,435
Each of these function is evaluated in the same way. This one is simply taking the arguments by position while this function is accepting arguments by name.

8
00:01:24,435 --> 00:01:34,250
There are some nuances for when you can perform the passing of values in each of these ways. You'll see more applications of this in the following quiz section.


@@@
1
00:00:03,660 --> 00:00:17,390
Scope refers to which parts of a program a variable can be referenced or used from. If a variable is created inside a function, it can only be used within that function.

2
00:00:17,390 --> 00:00:27,300
Consider these two functions, word count and nearest square. The first function uses answer to count words in a document.

3
00:00:27,300 --> 00:00:43,115
The second function uses answer to check values while searching for the nearest square. Both functions include this answer variable, but they are distinct variables that can only be referenced within their respective functions.

4
00:00:43,115 --> 00:00:53,625
Alternatively, we might have a variable defined outside of functions, like this. And we can access it anywhere outside or within these functions.

5
00:00:53,625 --> 00:01:02,000
Scope is essential to understanding how information is passed throughout programs, in Python, and really any programming language.


@@@
1
00:00:00,000 --> 00:00:07,560
One of the key advantages of functions is that they can help break a program down into smaller chunks.

2
00:00:07,559 --> 00:00:16,649
This makes code easier to write and also easier to read because the pieces of your code, the functions, are reusable.

3
00:00:16,649 --> 00:00:29,070
If a program needs to calculate volumes of multiple cylinders, it can call the cylinder volume function multiple times, which is much cleaner than writing out the formula over and over again.

4
00:00:29,070 --> 00:00:43,410
Functions make code easier to read because they give human readable names to processes. While the cylinder volume formula isn't that complicated, it is still harder to recognize than a precisely-named function.

5
00:00:43,409 --> 00:00:51,100
There is an additional technique that makes functions more readable, documentation strings or docstrings.

6
00:00:51,100 --> 00:00:58,274
Docstrings are a type of comment used to explain the purpose of a function and how it should be used.

7
00:00:58,274 --> 00:01:06,195
Here is a function for population density with a docstring. Docstrings are surrounded by triple quotes.

8
00:01:06,194 --> 00:01:16,780
The first line of the docstring is a brief explanation of the function's purpose. If you feel that this is sufficient documentation, you can end the docstring here.

9
00:01:16,780 --> 00:01:29,990
Single line docstrings are perfectly acceptable. If you think that the function is complicated enough to want a longer description, you can add a more thorough paragraph after the one-line summary.

10
00:01:29,989 --> 00:01:42,359
The next element of a docstring is an explanation of the function's arguments. Here, you list the arguments, state their purpose and what types the arguments should be.

11
00:01:42,359 --> 00:01:51,230
Finally, it's common to provide some description of the output of the function. Every piece of the docstring is optional.

12
00:01:51,230 --> 00:02:00,140
However, docstrings are part of good coding practice. They assist you and future users in understanding the code you produced.


@@@
1
00:00:02,670 --> 00:00:11,860
In Python, you can use lambda expressions to create anonymous functions. That is, functions that don't have a name.

2
00:00:11,860 --> 00:00:21,280
They are very helpful for creating quick functions that aren't really needed later in your code. Later, we'll learn about higher-order functions.

3
00:00:21,280 --> 00:00:28,510
Are functions that take in other functions as arguments, where lambda expressions become especially useful.

4
00:00:28,510 --> 00:00:37,390
Let's compare the structure of a function and a lambda expression. Here, is a simple function that doubles a number.

5
00:00:37,390 --> 00:00:49,925
It takes in a number x and returns x multiplied by two, calling double three, would return six. Here is the equivalent in a lambda expression.

6
00:00:49,925 --> 00:01:01,850
The lambda keyword is used to indicate that this is a lambda expression. Following lambda or one or more arguments for the anonymous function and then a colon.

7
00:01:01,850 --> 00:01:14,265
These are equivalent, similar to the way argument names in a function are arbitrary. Last is an expression that is evaluated and returned in this function.

8
00:01:14,265 --> 00:01:28,555
This is a lot like the expression you might see as a return statement in a function. With this structure, lambda expressions aren't ideal for complex functions, but can be very useful for short symbol functions.

9
00:01:28,555 --> 00:01:36,790
If you want to specify multiple arguments in a lambda function, you can include them before the colon, separated by commas.

10
00:01:36,790 --> 00:01:48,570
Here's a lambda function that multiplies two numbers together. In the following quizzes, you'll get some practice using lambda functions and see how useful they can be.


@@@
1
00:00:00,860 --> 00:00:09,500
If you recall from the previous lesson, iterables are objects that can return one of its elements at a time.

2
00:00:09,500 --> 00:00:22,567
Lists are one of the most common iterables you have used. It turns out many of the built-in functions we've used so far, such as enumerate, returns something called an iterator.

3
00:00:22,568 --> 00:00:34,950
An iterator is an object that represents a stream of data. This is different from a list which is also an iterable but not an iterator since it is not a stream of data.

4
00:00:34,950 --> 00:00:46,155
Soon, you'll see some reasons that iterators are favored in different situations. Here we will learn how to create iterators using generators.

5
00:00:46,155 --> 00:00:54,539
Generators are a simple way to create iterators using functions. However, it's not the only way to create iterators.

6
00:00:54,539 --> 00:01:09,935
You can read more about this in the notes below. These terms may be a little confusing. The term generator is often used to refer to the generator function, but it's also used to refer to the iterator object produced by the function.

7
00:01:09,935 --> 00:01:19,015
Here I'll differentiate these by referring to the function as a generator function, and what it produces as the iterator.

8
00:01:19,015 --> 00:01:27,640
Here is a generator function called 'my range' that produces a stream of numbers from zero to x minus one.

9
00:01:27,640 --> 00:01:41,481
Notice that instead of using the return keyword, this uses yield. This allows the function to return values one at a time and start where it left off each time it's called.

10
00:01:41,481 --> 00:01:55,454
This yield keyword is what differentiates a generator from a typical function. As you can see here, calling my range four returns an iterator that we can then iterate through.

11
00:01:55,454 --> 00:02:04,590
Using a for loop, we can print values from this stream of data. Here this prints zero, one, two and three.


@@@
1
00:00:00,000 --> 00:00:13,439
Congratulations on completing this lesson on functions. You learned about functions, variable scope, documentation, lambda expressions, iterators, and generators.

2
00:00:13,439 --> 00:00:21,355
Next, you'll learn how to write scripts in local environments with many functions pieced together. That's a lot of stuff.

3
00:00:21,355 --> 00:00:27,855
It's okay if you haven't mastered everything yet. You've already accomplished a ton by getting this far.


@@@
1
00:00:00,000 --> 00:00:12,464
Welcome to this lesson on scripting. Here, you'll learn how to combine all the concepts you've learned throughout this course to write and run your own scripts locally on your computer.

2
00:00:12,464 --> 00:00:28,660
You'll learn about good scripting practices, working with raw input from users, reading and writing files, handling errors, importing local scripts, and working with different libraries. Let's get started.


@@@
1
00:00:02,859 --> 00:00:13,844
In all of the previous lessons, we've been using these boxes to represent a code editor where we wrote our Python code and output window where we viewed results.

2
00:00:13,845 --> 00:00:29,989
And in the classroom, you use these programming quizzes to write Python and run your code but instead of running your code online in the classroom, you can run Python on your own computer by installing Python and useful tools for your programming environment.

3
00:00:29,989 --> 00:00:44,150
Since Python is a popular open-source programming language, it's free to download, change and use and the fact that it's already popular means there are online instructions to help you get it installed for most common operating systems.

4
00:00:44,149 --> 00:00:50,660
Because it's popular and open-source, there's a large global community working on developing Python.

5
00:00:50,659 --> 00:00:57,760
For you, as a Python user, that means there'll be new versions coming out as Python is improved by the community.

6
00:00:57,759 --> 00:01:08,534
You might find yourself needing to update Python from time to time especially if you're collaborating with other people as you write Python programs that are compatible with the version they're using.


@@@
1
00:00:00,000 --> 00:00:14,334
Now that you've successfully installed Python, let's try to run a script in your terminal. First, download the file first_script.py attached at the bottom of this page and move it to an appropriate directory.

2
00:00:14,335 --> 00:00:24,730
I put my file here by dragging it from the downloads folder to this folder. This might be a good time to set up a new directory for your learning if you don't have one already.

3
00:00:24,730 --> 00:00:38,070
Next, open your terminal and use CD to navigate to the directory containing the downloaded file. I can double check that the file is located here by typing ls or dir on Windows.

4
00:00:38,070 --> 00:00:47,304
Now that you're in the directory with the file, you can run it by typing python followed by the name of the file like this. Then press enter.

5
00:00:47,304 --> 00:01:00,664
You'll know you've run the scripts successfully if you see this message. Whenever you type Python followed by the file name for a Python script, that script is run and the output is shown immediately afterwards.

6
00:01:00,664 --> 00:01:07,074
Once the codes finished running, and all the output has been shown, your command line prompt reappears.

7
00:01:07,075 --> 00:01:14,340
Note if you typed and entered just Python without a script to run, it'll start an interactive interpreter for Python.

8
00:01:14,340 --> 00:01:27,719
You'll learn more about this later. For now, you can just type in exit with parentheses to get out or alternatively, control D on Mac and Linux or control Z and enter on Windows.

9
00:01:27,719 --> 00:01:35,000
Now, I'm back to the command line prompt. It's your turn. Download the file below and run the script in your terminal.


@@@
1
00:00:00,000 --> 00:00:06,914
There are many ways to setup a programming environment and there isn't a right or wrong way to go about it.

2
00:00:06,915 --> 00:00:14,550
However, if you're just starting out programming in Python, you might like to see this setup to get some ideas.

3
00:00:14,550 --> 00:00:23,440
Over here is a text editor. This particular editor is called atom. But there are many additional editors I recommend in the notes below.

4
00:00:23,440 --> 00:00:38,004
A text editor is really different from a word processor, so don't mix them up. Code needs to be written line-by-line with intentional line breaks and indentations that fundamentally change how the code is read and run.

5
00:00:38,005 --> 00:00:52,965
Good text editors have simple but vital features such as line numbers and syntax highlighting. You can see the different ways atom highlights a text based on the types of words and objects it recognizes from the Python language.

6
00:00:52,965 --> 00:01:01,483
Syntax highlighting really helps make code more readable and organized. It can also prevent us from making some syntax errors.

7
00:01:01,482 --> 00:01:09,395
For example, if I forgot a quote at the end of the string, I'll know I missed it when the rest of the line is highlighted like a string.

8
00:01:09,394 --> 00:01:19,000
Atom knows to use syntax highlighting for Python because of this.py at the end of the file name. This tells atom it's dealing with a Python file.

9
00:01:19,000 --> 00:01:27,609
Editors are highly customizable to suit your style. One setting that I use is soft tabs with the top length of four.

10
00:01:27,609 --> 00:01:39,910
Which means that when I press tab it is converted into four spaces. You can also add cool add-ons. Like this Python linter, that uses Pylint to point out helpful things in my code.

11
00:01:39,909 --> 00:01:49,989
For example, in this line, pylint gives me a warning because I'm not following PEP 8 guidelines by including a space before my closing bracket.

12
00:01:49,989 --> 00:01:58,349
I also have this cool add-on which let's me use my terminal in my editor. Atom isn't the only thing on my screen right now.

13
00:01:58,349 --> 00:02:06,719
I do use this terminal add-on from time to time but generally I like to have a separate window for my terminal where I can run my scripts.

14
00:02:06,719 --> 00:02:14,862
Generally, I can't write a whole lot of code in one go because something will go wrong and I'll need to find the issue and debug it.

15
00:02:14,861 --> 00:02:23,759
When there's an error it will show up here when I run the script. I make small changes to the code in a text editor and then run the script again to see the output.

16
00:02:23,759 --> 00:02:34,519
If I see a problem I'll try to fix it or if things seem to be working I'll move on. I've also got the Udacity classroom and results from a question I googled in my browser.

17
00:02:34,520 --> 00:02:42,367
Normally, I'll need a browser open to look up documentation or search for the solution to a bug I encounter along the way.

18
00:02:42,367 --> 00:02:52,250
From now on, we recommend that you try out all your code in this way. Put code into a new file in your text editor and don't forget the.py extension.


@@@
1
00:00:00,000 --> 00:00:11,789
Now that you have your local program environment all set up, it's time to try out different inputs. Programs can get a lot more interesting when they can interact with external information.

2
00:00:11,789 --> 00:00:25,519
First, let's try getting raw input from the user. Here's what I mean. This program prompts the user to enter a name, and picks in that input as a string to use in its code.

3
00:00:26,850 --> 00:00:38,859
This input function is what takes in the user's input as a string. It has an optional argument that you can use to specify a prompt shown to the user.

4
00:00:38,859 --> 00:00:53,320
Since the input function interprets input as a string, you'll need to wrap the result with int or float if you want to use it as a number.

5
00:00:53,320 --> 00:01:04,049
If you don't do this and try to use the input as a number, you'll get an error. Here, I wrap the input with int and added 20 to it.

6
00:01:04,049 --> 00:01:14,484
But what if the user inputs a non integer number? You can just wrap it as a float to catch non integer numbers as well.

7
00:01:14,484 --> 00:01:21,154
But what if you need an integer, like if you're multiplying a string by it to repeat it a certain number of times?

8
00:01:21,155 --> 00:01:30,170
This wouldn't work with the float even if it's an integer number. We can actually wrap this int with float, and then convert it into an int like this.

9
00:01:30,170 --> 00:01:39,019
Okay, that works. Clearly, you're trying to imagine and handle every possible case with user input can quickly get complicated.

10
00:01:39,019 --> 00:01:46,644
We addressed some cases here, but there are also loads of other cases we didn't address that could lead to more errors.

11
00:01:46,644 --> 00:01:55,680
We'll learn a better way to handle these scenarios in the next section. Before we move on to practice, here's another way we can interpret user input.

12
00:01:55,680 --> 00:02:05,099
Eval is a built-in function that evaluates a string as a line of Python. You can even include variables in the string like this.


@@@
1
00:00:00,000 --> 00:00:09,330
As you saw in the last video, trying to handle every kind of scenario when dealing with unexpected input can be a bit much.

2
00:00:09,330 --> 00:00:18,570
There's actually a much simpler way of dealing with this in Python. Here, you learn how to handle errors with try and except blocks.

3
00:00:18,570 --> 00:00:32,075
First, let's learn more about what errors in Python are. We've seen error messages due to several reasons in this course, and we can separate them into two types, syntax errors and exceptions.

4
00:00:32,075 --> 00:00:39,850
Syntax errors occur when Python can't interpret our code since we didn't follow the correct syntax for Python.

5
00:00:39,850 --> 00:00:45,585
These are errors you're likely to get when you make a typo, or you're first starting to learn Python.

6
00:00:45,585 --> 00:00:57,680
Here, I get a syntax error because I missed the ending quotation mark at the end of the string. I can see this in the error message, SyntaxError: End of line when scanning string literal.

7
00:00:57,679 --> 00:01:04,409
The other kind of error, exceptions occur when unexpected things happen during the execution of the code.

8
00:01:04,409 --> 00:01:14,899
Here, although our code is syntactically correct, we run into an exception because a string spelling out 10 isn't a valid argument for the int function.

9
00:01:14,900 --> 00:01:22,769
There are different types of built-in exceptions in Python, and you can see which exception is being thrown here in the error message.

10
00:01:22,769 --> 00:01:34,539
Value error is one type of exception. It occurs when a built-in operation or function is given an argument with the correct type but an inappropriate value.

11
00:01:34,540 --> 00:01:41,489
Here's another example that there was an exception. Here, we try to reference a variable name that we didn't define yet.

12
00:01:41,489 --> 00:01:53,590
So, we get a name error exception. Again, this code is syntactically correct, but when Python gets to this line of code, it can't find the value for this variable when it tries to run it.


@@@
1
00:00:00,000 --> 00:00:06,504
In Python, there are ways to handle exceptions so they don't always crush our program when they occur.

2
00:00:06,504 --> 00:00:17,740
Let's look back at this example that takes an input from the user. We saw that when we ran this code, it got an error when the user input something that can't be converted to an int.

3
00:00:17,739 --> 00:00:33,323
We can actually handle this error using a try statement. In a try statement, the code inside the try block is first run and if Python runs into any exceptions while it's running this block, it will jump to the code in the except block.

4
00:00:33,323 --> 00:00:41,475
Here if I run this code and enter the word ten, it prints, that's not a valid number and moves on to the rest of the code.

5
00:00:41,475 --> 00:00:55,859
The program continues to run whether or not it runs into an exception during this try block. For example, if I have a line after this try statement that prints attempted input, you'll see this is printed in both cases.

6
00:00:55,859 --> 00:01:07,283
If we want this code to keep running until the user inputs a valid number, we can use a while loop and break the loop if all the code in the try block successfully executes.

7
00:01:07,284 --> 00:01:17,844
Here, the program keeps taking in input until I enter a valid number. If I enter a valid number, this statement in the try block doesn't raise an exception.

8
00:01:17,844 --> 00:01:25,670
So it moves on to the next line where it breaks from the loop. However, since it breaks the loop, it never prints attempted input.

9
00:01:25,670 --> 00:01:33,830
If we want this last line to always run after the try statement no matter what, there is an optional component of the statement we can use.

10
00:01:33,829 --> 00:01:48,150
Finally, now, attempted input will be printed when the program is exiting this try statement under any conditions, whether there is a break statement, return statement or error that causes this program to crash within except block.

11
00:01:48,150 --> 00:01:55,765
You can read more about this in the notes below. The finally block is useful for cleaning up actions in your code.

12
00:01:55,765 --> 00:02:01,310
Later in the lesson, we will use this to close a file after attempting to open one in a try statement.


@@@
1
00:00:00,000 --> 00:00:07,264
Here, the code in the except block occurs when any kind of exception occurs while executing the try block.

2
00:00:07,264 --> 00:00:21,664
Really though we just want to adjust the ValueError exception. If I try to exit the program with Ctrl C, it doesn't stop the program because the except handles any error, including the KeyboardInterrupt error.

3
00:00:21,664 --> 00:00:31,745
We can actually specify which error we want to handle like this. Now it catches the ValueError exception, but not other exceptions like KeyboardInterrupt.

4
00:00:31,745 --> 00:00:43,350
Notice in this case when I press Ctrl C, it ends the program. This is because a KeyboardInterrupt exception was raised, which isn't handled by this try statement.

5
00:00:43,350 --> 00:00:57,945
Also notice that although the program crashed, this code in the finally block was still executed. Attempted Input is printed no matter what as the program is exiting this try statement, even if that means exiting the program.

6
00:00:57,945 --> 00:01:06,915
If we want this handler to address more than one type of exception, we can include a tuple after the except with the exception like this.

7
00:01:06,915 --> 00:01:15,015
Or if we want to execute different blocks of code depending on the exception, you can have multiple except blocks like this.

8
00:01:15,015 --> 00:01:26,890
Here, I have two handlers, one for ValueError and one for KeyboardInterrupt. When I press Ctrl C, it says No input taken and breaks from the while loop.


@@@
1
00:00:00,000 --> 00:00:17,925
In order for a program to be really useful, it needs to interact with real-world data. Images, web pages, and databases are all examples of files, and we routinely create, move, manipulate, and read these files in our daily digital lives.

2
00:00:17,925 --> 00:00:26,820
All the data we've used so far has been defined inside the Python script or raw input from the user during execution of the script.

3
00:00:26,820 --> 00:00:37,155
Next, we're going to massively increase the variety of what we can achieve in our Python programming by introducing how to read and write files in Python.

4
00:00:37,155 --> 00:00:44,070
This will allow us to interact with and process larger amounts of information from many more sources.

5
00:00:44,070 --> 00:00:51,930
All kinds of files have a similar structure on a computer. There are strings of characters that encode some information.

6
00:00:51,929 --> 00:01:06,224
The specific file format, often indicated by the extension of the file name such as.py, TXT, HTM, CSV, and many more, will indicate how those characters are organized.

7
00:01:06,224 --> 00:01:19,620
The characters in a file are interpreted by the various programs we use to interact with them. For example, an image editing program will interpret the information of a digital photograph file and display the image.

8
00:01:19,620 --> 00:01:26,535
If we then edit the image in the program, were using this program to make changes to the characters in the file.

9
00:01:26,534 --> 00:01:35,344
In Python, we can read those file characters directly. The experience will seem quite different from opening a file in a desktop application.

10
00:01:35,344 --> 00:01:49,500
Opening files in Python gives us a common programmatic interface to all kinds of files without the need for a graphical user interface, which means we can automate tasks involving files with Python programs.


@@@
1
00:00:00,000 --> 00:00:12,240
Let's see how we read information from a file into Python. To read from a file, we first need to open it, which we can accomplish with the built-in function open.

2
00:00:12,240 --> 00:00:19,414
We include a string with the path to the file along with any optional parameters we want to specify.

3
00:00:19,414 --> 00:00:27,469
The open function returns a file object which is a Python object through which Python interacts with the file itself.

4
00:00:27,469 --> 00:00:37,429
Here we assigned this file object to the variable f. This second optional parameter specifies the mode in which we open the file.

5
00:00:37,429 --> 00:00:47,554
In this case r, or read only. We are using this mode since we only want to read from the file. We don't want to change any of its contents.

6
00:00:47,554 --> 00:00:56,054
This parameter isn't really necessary for us to include here, though, since the mode defaults to read only if unspecified.

7
00:00:56,054 --> 00:01:04,140
Once we opened a file and created a file object, we can use the read method to access the contents of this file.

8
00:01:04,140 --> 00:01:15,810
This read method takes the text contained in a file and puts it into a string. Here we assign the string returned from this method into the variable file_data.

9
00:01:15,810 --> 00:01:23,850
When we are finished with the file f, we should close it. This will free up any system resources taken up by the file.

10
00:01:23,849 --> 00:01:34,584
Here's an example that uses this file, some_file.txt. It's important to remember to always close files we have opened once we no longer need them.

11
00:01:34,584 --> 00:01:43,059
If we open a lot of files without closing them, we can run out of file handles and we won't be able to open any new files.

12
00:01:43,060 --> 00:01:49,795
Exactly how many files you can open before running out of handles will depend on your operating system.

13
00:01:49,795 --> 00:02:00,700
To convince yourself, you can try running the following script in Python. At some point for a large enough number, you will receive an error.

14
00:02:00,700 --> 00:02:09,865
We can see that at 7164 open files, the system no longer had available resources to open any new files.

15
00:02:09,865 --> 00:02:19,569
To avoid this, it is always a good idea to close any files you no longer need. Opening a file object is like opening a window to look into a file.

16
00:02:19,569 --> 00:02:27,474
To be more precise, it's a window that's only one character wide and it always starts off at the very start of the file.

17
00:02:27,474 --> 00:02:34,309
This is very different from reading a book or a document, where you can look at multiple words or even pages at once.

18
00:02:34,310 --> 00:02:43,205
Think instead of the file as a long stream of characters. The file object can look at just one character at a time and order.

19
00:02:43,205 --> 00:02:51,245
In addition to reading from a file, you can also write to a file, in which case you will change the content of the file.

20
00:02:51,245 --> 00:03:02,224
To do so, you must open the file in writing mode. Be careful, once you open a file in writing mode, anything that it contained previously will be deleted.

21
00:03:02,224 --> 00:03:09,539
If you're interested in adding to an existing file without deleting its content, you should use append instead of write.

22
00:03:09,539 --> 00:03:16,170
You can visit the Python documentation for more information on the different modes in which you can open a file.

23
00:03:16,169 --> 00:03:26,504
Since we're in write mode and I don't want to delete what's in this file, let's use another one. If the file does not exist, Python will create it for you.


@@@
1
00:00:00,000 --> 00:00:08,214
It could be easy to forget to close a file when you're done using it. So Python provides a special syntax that auto closes it.

2
00:00:08,214 --> 00:00:20,549
This with keyword allows you to open a file, do operations on it, and automatically close it after the indented code is executed, in this case, reading from the file.

3
00:00:20,550 --> 00:00:40,275
Now, we don't have to call f.close. This code, as f assigns the file object created by the open function to the variable name, f. This line of code is basically this line of code, except you can only access the file object, f within this block.

4
00:00:40,274 --> 00:00:48,594
This is another kind of scope. Once you leave this indented block, the file is closed and you are no longer able to interact with it.

5
00:00:48,594 --> 00:01:00,210
For example, trying to call f.read outside this block like this would return an error. However, just because you closed the file doesn't mean you lose the data.

6
00:01:00,210 --> 00:01:07,844
Here, we read in the file in this line and get this file data string that has the text contained in the file.

7
00:01:07,844 --> 00:01:17,000
Calling file data outside the block works fine. We can use all the usual string methods on this file data string to process its contents.


@@@
1
00:00:00,000 --> 00:00:17,339
In addition to reading in data from files, we can actually import Python code from other scripts. This is especially helpful if you're working on a bigger project where you want to organize your code into multiple files and reuse code in those files.

2
00:00:17,339 --> 00:00:28,054
If the Python script you want to import is in the same directory as your current script, you just type import, followed by the name of the file without the dot PY extension.

3
00:00:28,054 --> 00:00:41,920
Imports statements are written at the top of a Python script, each one on a separate line. Python will still run if imports statements are included later in the script, but it's common practice to always have these at the top.

4
00:00:41,920 --> 00:00:48,864
You're only able to access what you've imported after this statement so, it's just less confusing to have these written first.

5
00:00:48,865 --> 00:00:56,905
It's also nice for readers to see what a script depends on before reading the rest of the code. Let's start with a really simple example.

6
00:00:56,905 --> 00:01:09,114
Here, I have a Python file called other_script.py that just prints two plus three. In this file demo.py, I import other_script and then print four.

7
00:01:09,114 --> 00:01:17,890
When we run demo.py, this import statement tells Python to run code from that file which prints five.

8
00:01:17,890 --> 00:01:35,373
It then continues to execute the rest of the code in this file printing four. If instead this file had the line num equals two plus three, and we try to access this in demo.py, referencing it with just the name of the variable would return an error.

9
00:01:35,373 --> 00:01:48,475
To access objects in other_script.py, we need to use the name of the file followed by a dot, followed by the object to tell Python to look for this object in the other_script file we imported.

10
00:01:48,474 --> 00:01:57,283
Now, it accesses the variables successfully. When Python runs the script. It only has direct access to objects defined in the script.

11
00:01:57,283 --> 00:02:04,570
One of these objects is a module called other_script. A module is just a file with Python definitions and statements.

12
00:02:04,569 --> 00:02:11,044
When we import a Python file like this, it creates an object called other_script with a type module.

13
00:02:11,044 --> 00:02:20,155
Let's see a more useful example of importing a module. Here, we have a Python file that contains useful functions we would like to use.

14
00:02:20,155 --> 00:02:32,560
One, that returns the mean of a list and one that adds five to each element of a list. We can import them into demo.py, and type useful_functions dot the name of the function to use them.

15
00:02:32,560 --> 00:02:46,180
You can imagine this would be very helpful if we had many functions used a lot in different files. Although, it seems a little annoying that we have to type out the whole name of the file each time we want to use a function from it.

16
00:02:46,180 --> 00:02:53,944
We can make this much simpler by adding an alias. Here, I made an alias for the useful_function module UF.

17
00:02:53,944 --> 00:03:04,810
Now, I can just type UF instead of the whole module name when calling functions from it. This is useful when we have objects we want to import from other Python scripts like functions.

18
00:03:04,810 --> 00:03:12,219
But, what if that script also includes executable statements in addition to function definitions that we don't want to import.

19
00:03:12,219 --> 00:03:21,000
For example, what if useful_functions.py has code at the bottom of the script that tests its functions and prints the results?

20
00:03:21,000 --> 00:03:31,550
This code is nice if we run useful_functions.py to test out these functions, but unnecessary if we're just trying to use these functions in another script.

21
00:03:31,550 --> 00:03:48,490
Here is where we can use this statement if name equals main. By including these executable statements inside this if main block, we tell Python to execute this code only when the main program being executed is this useful_functions.py.

22
00:03:48,490 --> 00:03:59,873
If we run this file the code inside this block is run. However, if we run another script that simply imports useful_function.py this code is not run.

23
00:03:59,873 --> 00:04:11,865
Generally, it's good practice to write executable statements inside an if name block or alternatively include them in a function called Main and call this in the if name block.

24
00:04:11,865 --> 00:04:26,305
You're probably wondering what this Name and Main stuff is. Whenever we run a script like this, Python actually sets a special built in variable called Name with two underscores before and after it for any module.

25
00:04:26,305 --> 00:04:38,824
Here, since we ran Python demo.py, Python recognizes this module as the main program and sets the name variable for this module to the string Main.

26
00:04:38,824 --> 00:04:46,110
For any modules that are important in the script, this built-in name variable is just set to the name of that module.


@@@
1
00:00:00,000 --> 00:00:10,470
You've seen how helpful it could be to import your own modules. But what if I told you there was an entire library of built-in modules that just come with Python?

2
00:00:10,470 --> 00:00:22,320
This is the Python standard library. Up here, you see built-in objects we use throughout this course; like built-in functions, data types, and exceptions.

3
00:00:22,320 --> 00:00:33,479
Below that though, are tons of useful modules that you can import. Think of this library as a very big set of tools that you can use to help you program in Python.

4
00:00:33,479 --> 00:00:46,103
It provides new types of objects and functions for a range of common and specialized tasks. Other people have already written this code and put it into useful modules for you to use.

5
00:00:46,103 --> 00:00:55,229
Using modules from the Python standard library to easily access and use existing code gives you a lot of programming power.

6
00:00:55,229 --> 00:01:05,250
The Python standard library is organized into modules. Many modules are simply Python files, like the Python scripts you've already written and imported.

7
00:01:05,250 --> 00:01:18,364
Let's import this math module. Again, an import statement runs the code in the module. Modules typically contain a lot of definitions and usually don't show any output.

8
00:01:18,364 --> 00:01:31,399
Running the code will make all the modules' functions and types of objects available to use. Math has a factorial function, which finds the product of a number and all the positive integers less than it.

9
00:01:31,400 --> 00:01:48,689
Four times three, times two, times one is 24, so math.factorial(4) for prints 24. The Python standard library has good documentation for each of its modules, and it's a good idea to read the relevant page whenever you use one.

10
00:01:48,689 --> 00:01:56,460
Here's a documentation for the math module. So far, you've seen only one module in the Python standard library.

11
00:01:56,459 --> 00:02:07,560
It's a good one, but there are many more. If you look at the documentation for the whole of the Python standard library, modules are listed in groups based on their uses.

12
00:02:07,560 --> 00:02:16,599
Clicking on a name takes you to the documentation for that module, which often includes example code that you should feel free to test out.


@@@
1
00:00:00,000 --> 00:00:12,059
So far, you've imported modules with import followed by the module name, which makes all the classes of objects and functions of that module available via.notation.

2
00:00:12,060 --> 00:00:26,609
There are some other variance of importing that are useful in other situations. You can import an individual function or class from a module like this, from, the module name, import, the object name.

3
00:00:26,609 --> 00:00:36,274
This gives access only to defaultdict from the collections module. Defaultdict will be accessible with its own name without the module name before it.

4
00:00:36,274 --> 00:00:52,465
Trying to access collections or even calling collections.defaultdict will give a name error. Importing individual objects from a module means you only take what you need and you don't need to use.notation to access them.

5
00:00:52,465 --> 00:01:03,819
You can import multiple individual objects from a module by separating them with commas. This technique is very common when importing pieces from large libraries.

6
00:01:03,820 --> 00:01:11,229
And as you saw before, you can import a module and give it a different, usually shorter name, like this.

7
00:01:11,230 --> 00:01:21,220
If the name of a module is particularly long or if there's a clash with something with the same or similar name, renaming the module like this can be helpful.

8
00:01:21,219 --> 00:01:29,185
Check code examples in the documentation as these will often include a standard abbreviation if one is used for this module.

9
00:01:29,185 --> 00:01:40,340
Using an abbreviation that is consistent with others will make your code more readable. For example, the standard abbreviation for the multiprocessing module is MP.

10
00:01:40,340 --> 00:01:47,630
You can combine the previous two pieces of syntax to input an item from a module and change its name.

11
00:01:47,629 --> 00:01:56,024
Again, you'll be able to access only that individual item directly with its newly specified name. No.notation is needed.

12
00:01:56,025 --> 00:02:02,729
This can be useful if you have multiple objects with similar names from different packages in your namespace.

13
00:02:02,730 --> 00:02:12,064
For example, perhaps you want a CSV reader and JSON reader. You could import them from their respective modules and give them descriptive names.

14
00:02:12,064 --> 00:02:18,685
Another way of importing that you may see in other people's code but that you should not use is this.

15
00:02:18,685 --> 00:02:28,150
Using this asterisk will import every object from a module individually and allow you to access each of them directly via its name.

16
00:02:28,150 --> 00:02:41,695
The real problem with this is that modules may contain many objects, each of which has a name. Including all these names may overwrite or maybe overwritten by other names you're using in your program.

17
00:02:41,694 --> 00:02:48,730
Import star also makes it impossible for collaborators to find where an important object was defined.

18
00:02:48,729 --> 00:02:57,594
A reader can search for a definition of a function and not find it and they won't know which import star statement introduced a function.

19
00:02:57,594 --> 00:03:14,669
These problems can lead to a lot of confusion. Do not use from module name import asterisk. If you really want to use all the objects from the Random module, use the standard import random instead and access each of the objects with the.notation.


@@@
1
00:00:00,000 --> 00:00:12,285
Some of the modules in the Python Standard Library have a lot in them. In order to manage the code better, they're split down into sub-modules that are contained within a package.

2
00:00:12,285 --> 00:00:21,614
A package is simply a module that contains sub-modules. A sub-module is specified with the usual dot notation.

3
00:00:21,614 --> 00:00:33,299
For example, the OS module, which is useful for dealing with file systems has a sub-module os.path, which is used specifically for dealing with path names.

4
00:00:33,299 --> 00:00:41,777
Modules that are sub-modules are specified by the package name and then the sub-module name separated by a dot.

5
00:00:41,777 --> 00:00:50,129
You can import the sub-module os.path like this. You can then use the objects from the sub-module in the usual way.

6
00:00:50,130 --> 00:00:58,920
However, this syntax for importing will only work for sub-modules. You cannot import a function from a module in this way.

7
00:00:58,920 --> 00:01:08,784
If you want to use other parts of the OS module two, you could import OS instead and everything in the os.path will still be accessible.

8
00:01:08,784 --> 00:01:19,790
Sometimes, naming can be a point of confusion when working with modules. For example, a module might be named after one of the important classes or functions within it.

9
00:01:19,790 --> 00:01:29,150
In this case, you will need to think carefully about your import statements. This imports the datetime class from the datetime module.


@@@
1
00:00:00,000 --> 00:00:12,195
Compared to most languages, Python has a large standard library. In fact, people say Python comes with batteries included because it comes with the libraries you need to get right to work.

2
00:00:12,195 --> 00:00:21,500
However, the standard library doesn't come with everything you might want. Some tasks are too specialized to be accommodated by the standard library.

3
00:00:21,500 --> 00:00:27,894
Fortunately, there are tens of thousands of third-party libraries written by independent developers.

4
00:00:27,894 --> 00:00:38,965
How do we get these packages though, if they aren't included with Python itself? We can install libraries using pip, a package manager that is included with Python 3.

5
00:00:38,965 --> 00:00:50,585
Pip is a standard package manager for Python, but it isn't the only one. One popular alternative is Anaconda, which is designed specifically for data science.

6
00:00:50,585 --> 00:01:01,755
Here, we'll use pip, which is a general standard. Let's use pip to install this library, which is used for working with time zones or remarkably complicated task.

7
00:01:01,755 --> 00:01:07,679
This command will download and install the package, so that it's available to import in our programs.

8
00:01:07,680 --> 00:01:15,435
Once installed, we can import third-party packages using the same syntax we used to import from the standard library.

9
00:01:15,435 --> 00:01:29,810
In this example, I import a time zone package along with a datetime from the standard library. It's standard practice to put the import statements for third-party libraries after imports from the standard library.

10
00:01:29,810 --> 00:01:38,930
This example stores the current time expressed in terms of Coordinated Universal Time or UTC in the variable now.

11
00:01:38,930 --> 00:01:46,759
It then translates this time into IST, Indian Standard Time and stores that in the variable ist_now.

12
00:01:46,760 --> 00:02:00,390
Larger Python programs might depend on dozens of third-party packages. To make it easier to share these programs, programmers often list of projects dependencies in a file called requirements.txt.

13
00:02:00,390 --> 00:02:09,810
This is an example of a requirements.txt file. Each line of the file includes the name of a package and it's version number.

14
00:02:09,810 --> 00:02:25,365
The version number is technically optional, but it usually should be included. Libraries can change subtly or dramatically between versions, so it's important to use the same library versions that the program's author used when they wrote the program.


@@@
1
00:00:00,000 --> 00:00:08,464
If you open your terminal and type python, you should see something like this. This is the python interactive interpreter.

2
00:00:08,464 --> 00:00:17,634
You can type here to interact with Python directly. You just type your code, press enter and the output will appear on the following line.

3
00:00:17,635 --> 00:00:26,234
This is a good place to experiment and try bits of Python code at a time. Notice here, I didn't have to print the type to see the output.

4
00:00:26,234 --> 00:00:36,825
In the interpreter, the value of the last line in a prompt will be outputted automatically. If you had multiple lines where you'd want output values, you'd still have to print.

5
00:00:36,825 --> 00:00:45,155
If you start to define a function, you will see a change in the prompt to signify that this is a continuation line.

6
00:00:45,155 --> 00:00:53,699
You'll have to include your own indentation as you defined the function. A drawback of the interpreter is that it's tricky to edit code.

7
00:00:53,700 --> 00:01:02,085
If you made a mistake when typing this function or forgot to indent the body of the function, you can't use a mouse to click your cursor where you want it.

8
00:01:02,085 --> 00:01:09,254
You have to navigate with the arrow keys to move the cursor forwards and backwards to the line itself for editing.

9
00:01:09,254 --> 00:01:15,474
It would be helpful for you to learn useful shortcuts for actions like moving to the beginning or end of the line.

10
00:01:15,474 --> 00:01:29,545
Notice, I can reference any objects I defined earlier in the interpreter. One useful trick is using the up and down arrow to cycle through your recent commands at the interactive prompt.

11
00:01:29,545 --> 00:01:46,379
This can be useful to rerun or adapt code you've already tried. To quit the python interactive interpreter, use the command exit with parentheses or hit control D on mac or linux or control Z and enter for Windows.

12
00:01:46,379 --> 00:02:02,590
There's actually a really good alternative to the default python interpreter. IPython which comes as many additional features like tab completion which completes words for you or shows what options are available if there are multiple.

13
00:02:02,590 --> 00:02:12,715
This can be useful if you want to see what methods are available for an object. Another useful check is this question mark to get details about a particular object.

14
00:02:12,720 --> 00:02:26,040
I can quickly see what this function does without looking up the documentation. You can also execute system shell commands using an exclamation point and some common ones don't even require it.

15
00:02:26,050 --> 00:02:35,045
You can learn more about IPython in the notes below. Using an interpreter can be really helpful for experimenting and testing python code.

16
00:02:35,044 --> 00:02:43,000
Those not necessarily just for experimentation. I also used an interpreter when I want to quickly investigate or modify files using Python.


@@@
1
00:00:00,080 --> 00:00:16,214
Awesome job completing this course. After covering lessons on Python data types and operators, control flow, functions, and scripting, you've developed the understanding and skills necessary to tackle projects and courses in Python.


@@@
1
00:00:07,030 --> 00:00:25,219
Hey everyone! So, I'm pretty excited about the next sequence of videos that I'm doing. They'll be about linear algebra, which as a lot of you know is one of those subjects that's required knowledge for just about any technical discipline, but it's also I've noticed generally poorly understood by students taking it for the first time.

2
00:00:25,219 --> 00:00:36,140
A student might go through a class and learn how to compute lots of things like matrix multiplication or the determinant or cross products, which use the determinant or eigenvalues.

3
00:00:36,140 --> 00:00:42,204
But they might come out without really understanding why matrix multiplication is defined the way that it is.

4
00:00:42,204 --> 00:00:48,065
Why the cross product has anything to do with the determinant or what an eigenvalue really represents.

5
00:00:48,064 --> 00:00:56,704
Oftentimes, students end up well practiced in the numerical operations of matrices but are only vaguely aware of the geometric intuitions underlying it all.

6
00:00:56,704 --> 00:01:03,450
But there's a fundamental difference between understanding linear algebra on a numeric level and understanding it on a geometric level.

7
00:01:03,450 --> 00:01:15,340
Each has its place but roughly speaking, the geometric understanding is what let's you judge what tools to use to solve specific problems, feel why they work, and know how to interpret the results.

8
00:01:15,340 --> 00:01:20,839
And the numeric understanding is what let's you actually carry through the application of those tools.

9
00:01:20,840 --> 00:01:31,819
Now, if you learn linear algebra without getting a solid foundation in that geometric understanding, the problems can go unnoticed for a while until you've gone deeper into whatever field you happen to pursue.

10
00:01:31,819 --> 00:01:42,125
Once you're in a class or a job for that matter that assumes fluency with linear algebra, the way that your professors or your co-workers apply that field could seem like utter magic.

11
00:01:42,125 --> 00:01:53,090
They'll very quickly know what the right tool to use is and what the answer roughly looks like in a way that would seem like computational wizardry if you assume that they're actually crunching all the numbers in their head.

12
00:01:53,090 --> 00:02:01,400
Here, as an analogy, imagine that when you first learned about the sine function in trigonometry you were shown this infinite polynomial.

13
00:02:01,400 --> 00:02:14,230
This by the way is how your calculator evaluates the sine function. For homework, you might be asked to practice computing approximations of the sine function by plugging in various numbers to the formula and cutting it off at a reasonable point.

14
00:02:14,229 --> 00:02:23,784
And in fairness, let's say you had a vague idea that this was supposed to be related to triangles but exactly how had never really been clear and was not the focus of the course.

15
00:02:23,784 --> 00:02:37,045
Later on, if you took a physics course where sines and cosines are thrown around left and right and people are able to tell pretty immediately how to apply them and roughly what the sine of a certain value will be, it would be pretty intimidating, wouldn't it?

16
00:02:37,044 --> 00:02:46,469
It would make it seem like the only people who are cut out for physics are those with computers for brains and you would feel unduly slow or dumb for taking so long on each problem.

17
00:02:46,469 --> 00:02:56,245
It's not that different with linear algebra. And luckily, just as with trigonometry, there are a handful of intuitions, visual intuitions, underlying much of the subject.

18
00:02:56,245 --> 00:03:03,659
And unlike the trig example, the connection between the computation and these visual intuitions is typically pretty straightforward.

19
00:03:03,659 --> 00:03:14,750
And when you digest these and really understand the relationship between the geometry and the numbers, the details of the subject as well as how it's used in practice start to feel a lot more reasonable.

20
00:03:14,750 --> 00:03:21,009
In fairness, most professors do make an effort to convey that geometric understanding. The sine example is a little extreme.

21
00:03:21,009 --> 00:03:35,569
But I do think that a lot of courses have students spending a disproportionate amount of time on the numerical side of things, especially given that in this day and age we almost always get computers to handle that half while in practice humans worry about the conceptual half.

22
00:03:35,569 --> 00:03:47,530
So, this brings me to the upcoming videos. The goal is to create a short, watchable series animating those intuitions from the basics of vectors up through the core topics that make up the essence of linear algebra.

23
00:03:47,530 --> 00:03:59,719
I'll do what I can to keep things well paced throughout but it's hard to simultaneously account for different people's different backgrounds and levels of comfort so I do encourage you to readily pause and ponder if you feel that it's necessary.

24
00:03:59,719 --> 00:04:09,564
Actually, I'd give that same advice for watching any math video even if it doesn't feel too quick since the thinking that you do on your own time is where all the learning really happens, don't you think?


@@@
1
00:00:10,330 --> 00:00:20,160
The fundamental root of it all building block for linear algebra is the vector. So, it's worth making sure that we're all on the same page about what exactly a vector is.

2
00:00:20,160 --> 00:00:30,500
You see, broadly speaking, there are three distinct but related ideas about vectors, which I'll call the physics student perspective, the computer science student perspective and the mathematician perspective.

3
00:00:30,500 --> 00:00:39,030
The physics student perspective is that vectors are arrows pointing in space. What defines a given vector, is its length and the direction it's pointing.

4
00:00:39,030 --> 00:00:43,530
But as long as those two facts are the same, you can move it all around and it's still the same vector.

5
00:00:43,530 --> 00:00:50,924
Vectors that live in the flat plane, are two-dimensional and those sitting in broader space, that you and I live in, are three-dimensional.

6
00:00:50,924 --> 00:01:03,075
The computer science perspective is that vectors are ordered lists of numbers. For example, let's say you were doing some analytics about house prices, and the only features you cared about were square footage and price.

7
00:01:03,075 --> 00:01:09,040
You might model each house with a pair of numbers, the first indicating square footage and the second indicating price.

8
00:01:09,040 --> 00:01:24,920
Notice, the order matters here. In the lingo, you'd be modeling houses as two-dimensional vectors, where in this context vector is pretty much just a fancy word for list, and what makes it two-dimensional, is the fact that the length of that list is two.

9
00:01:24,920 --> 00:01:39,185
The mathematician, on the other hand, seeks to generalize both these views, basically saying that a vector can be anything where there's a sensible notion of adding two vectors, and multiplying a vector by a number, operations that I'll talk about later on in this video.

10
00:01:39,185 --> 00:01:50,870
The details of this view are rather abstract, but the reason they bring it up here, is that it hints at the fact that the ideas of vector addition and multiplication by numbers, will play an important role throughout linear algebra.

11
00:01:50,870 --> 00:01:57,670
But before we talk about those operations, let's just settle in on a specific thought to have in mind, when I say the word vector.

12
00:01:57,670 --> 00:02:12,620
Given the geometric focus that I'm shooting for here, whenever we introduce a new topic involving vectors, I want you to first think about an arrow, and specifically, think about that arrow inside a coordinate system, like the x, y plane, with its tail sitting at the origin.

13
00:02:12,620 --> 00:02:18,514
This is a little bit different from the physics student perspective, where vectors can freely sit anywhere they want in space.

14
00:02:18,514 --> 00:02:34,334
In linear algebra, it's almost always the case that your vector will be rooted at the origin. Then, once you understand a new concept in the context of arrows in space, we'll translate it over to the list of numbers point of view, which we can do by considering the coordinates of the vector.

15
00:02:34,335 --> 00:02:45,280
Now, while I'm sure that many of you are already familiar with this coordinate system, it's worth walking through explicitly, since this is where all of the important back and forth happens between the two perspectives of linear algebra.


@@@
1
00:00:00,000 --> 00:00:07,950
Focusing our attention on two dimensions for the moment, you have a horizontal line called the x-axis and a vertical line called the y-axis.

2
00:00:07,950 --> 00:00:13,954
The place where they intersect is called the origin, which you should think of as the center of space and the root of all vectors.

3
00:00:13,955 --> 00:00:19,850
After choosing an arbitrary length represent one, you make tick marks on each axis to represent this distance.

4
00:00:19,850 --> 00:00:29,600
When I want to convey the idea of 2D space as a whole, which you'll see comes up a lot in these videos, I'll extend these tick marks to make grid lines, but right now they'll actually get a little bit in the way.

5
00:00:29,600 --> 00:00:38,494
The coordinates of a vector is a pair of numbers that basically gives instructions for how to get from the tail of that vector at the origin to it's tip.

6
00:00:38,494 --> 00:00:51,089
The first number tells you how far to walk along the x-axis. Positive numbers indicating rightward motion, negative numbers indicating leftward motion, and the second number tells you how far to walk parallel to the y-axis after that.

7
00:00:51,090 --> 00:01:02,780
Positive numbers indicating upward motion, and negative numbers indicating downward motion. To distinguish vectors from points, the convention is to write this pair of numbers vertically with square brackets around them.

8
00:01:03,250 --> 00:01:12,129
Every pair of numbers gives you one and only one vector, and every vector is associated with one and only one pair of numbers.

9
00:01:12,129 --> 00:01:24,545
What about in 3-dimensions? Well, you add a third axis called the z-axis which is perpendicular to both the x and y axis and in this case, each vector is associated with an ordered triplet of numbers.

10
00:01:24,545 --> 00:01:36,029
The first tells you how far to move along the x-axis, the second tells you how far to move parallel to the y-axis and the third one tells you how far to then move parallel to this new z-axis.

11
00:01:36,030 --> 00:01:43,719
Every triplet of numbers gives you one unique vector in space. And every vector in space gives you exactly one triplet of numbers.


@@@
1
00:00:00,000 --> 00:00:08,544
All right. So, back to vector addition and multiplication by numbers. After all, every topic in linear algebra is going to center around these two operations.

2
00:00:08,544 --> 00:00:17,224
Luckily, each one's pretty straightforward to define. Let's say we have two vectors, one pointing up and a little to the right and the other one pointing right and down a bit.

3
00:00:17,225 --> 00:00:32,395
To add these two vectors, move the second one so that its tail sits at the tip of the first one. Then, if you draw a new vector from the tail of the first one to where the tip of the second one now sits, that new vector is their sum.

4
00:00:32,395 --> 00:00:39,859
This definition of addition by the way, is pretty much the only time in linear algebra where we let vectors stray away from the origin.

5
00:00:39,859 --> 00:00:53,950
Now, why is this a reasonable thing to do? Why this definition of addition and not some other one? Well, the way I like to think about it is that each vector represents a certain movement, a step with a certain distance and direction in space.

6
00:00:53,950 --> 00:01:05,694
If you take a step along the first vector, then take a step in the direction and distance described by the second vector, the overall effect is just the same as if you move along the sum of those two vectors to start with.

7
00:01:05,694 --> 00:01:18,715
You can think about this as an extension of how we think about adding numbers on a number line. One way that we teach kids to think about this, say with 2 + 5 is to think of moving two steps to the right followed by another five steps to the right.

8
00:01:18,715 --> 00:01:26,399
The overall effect is the same as if you just took seven steps to the right. In fact, let's see how vector addition looks numerically.

9
00:01:26,400 --> 00:01:42,189
The first vector here has coordinates 1,2. And the second one has coordinates 3,-1. When you take the vector sum using this tip to tail method, you can think of a four-step path from the origin to the tip of the second vector.

10
00:01:42,189 --> 00:01:59,739
Walk one to the right then two up then three to the right then one down. Reorganizing these steps so that you first do all of the rightward motion, then do all the vertical motion, you can read it saying first move 1 + 3 to the right, then move 2 - 1 up.

11
00:02:00,019 --> 00:02:13,830
So, the new vector has coordinates 1 + 3 and 2 + -1. In general, vector addition in this list of numbers conception looks like matching up their terms and adding each one together.

12
00:02:14,580 --> 00:02:22,299
The other fundamental vector operation is multiplication by a number. Now this is best understood just by looking at a few examples.

13
00:02:22,300 --> 00:02:30,620
If you take the number two and multiply it by a given vector, it means you stretch out that vector so that it's two times as long as when you started.

14
00:02:30,620 --> 00:02:37,805
If you multiply that vector by say one third, it means you squish it down and so that it's one third the original length.

15
00:02:37,805 --> 00:02:47,340
When you multiply it by a negative number like negative 1.8, then the vector first gets flipped around then stretched out by that factor of 1.8.

16
00:02:47,340 --> 00:02:53,819
This process of stretching or squishing or sometimes reversing the direction of a vector is called scaling.

17
00:02:53,819 --> 00:03:02,110
And whenever you catch a number like two or one third or negative 1.8 acting like this scaling some vector, you call it a scalar.

18
00:03:02,110 --> 00:03:12,125
In fact, throughout Linear Algebra, one of the main things that numbers do is scale vectors. So, it's common to use the word scalar pretty much interchangeably with the word number.

19
00:03:12,125 --> 00:03:20,500
Numerically, stretching out a vector by a factor of say, two corresponds with multiplying each of its components by that factor two.

20
00:03:20,500 --> 00:03:29,959
So, in the conception of vectors as lists of numbers, multiplying a given vector by a scalar means multiplying each one of those components by that scalar.

21
00:03:30,110 --> 00:03:40,710
You'll see in the following videos what I mean when I say that linear algebra topics tend to revolve around these two fundamental operations, vector addition and scalar multiplication.

22
00:03:40,710 --> 00:03:50,550
So, they're your vector basics and in the next video, I'll start getting into some pretty neat concepts surrounding vectors like span, basis and linear dependence. See you then.


@@@
1
00:00:11,289 --> 00:00:23,359
In the last video, along with the ideas of vector addition and scalar multiplication, I described vector coordinates, where there's this back and forth between, for example, pairs of numbers and two-dimensional vectors.

2
00:00:23,359 --> 00:00:32,454
Now, I imagined the vector coordinates were already familiar to a lot of you, but there's another kind of interesting way to think about these coordinates, which is pretty central to linear algebra.

3
00:00:32,454 --> 00:00:40,500
When you have a pair of numbers that's meant to describe a vector, like three negative two, I want you to think about each coordinate as a scalar.

4
00:00:40,500 --> 00:01:01,795
Meaning, think about how each one stretches or squishes vectors. In the x, y coordinate system, there are two very special vectors, the one pointing to the right with length one, commonly called i hat or the unit vector in the x-direction, and the one pointing straight up with length one, commonly called j hat or the univector in the y-direction.

5
00:01:01,795 --> 00:01:14,584
Now, think of the x-coordinate of our vector as a scalar that scales i hat, stretching it by a factor of three and the y-coordinate as a scalar that scales j hat, flipping it and stretching it by a factor of two.

6
00:01:14,584 --> 00:01:26,454
In this sense, the vector that these coordinates describe is the sum of two scaled vectors. That's a surprisingly important concept, this idea of adding together two scaled vectors.

7
00:01:26,454 --> 00:01:33,905
Those two vectors, i hat and j hat, have a special name by the way. Together, they're called the basis of a coordinate system.

8
00:01:33,905 --> 00:01:42,060
What this means basically is that when you think about coordinates as scalars, the basis vectors are what those scalars actually scale.

9
00:01:42,060 --> 00:01:54,149
There's also a more technical definition, but I'll get to that later. By framing our coordinate system in terms of these two special basis vectors, it raises a pretty interesting and subtle point.

10
00:01:54,150 --> 00:02:00,829
We could have chosen different basis vectors and gotten a completely reasonable new coordinate system.

11
00:02:00,829 --> 00:02:07,254
For example, take some vector pointing up and to the right along with some other vector pointing down and to the right in some way.

12
00:02:07,254 --> 00:02:17,450
Take a moment to think about all the different vectors that you can get by choosing two scalars, using each one to scale one of the vectors, then adding together what you get.

13
00:02:17,449 --> 00:02:28,295
Which two-dimensional vectors can you reach by altering the choices of scalars? The answer is that you can reach every possible two-dimensional vector.

14
00:02:28,294 --> 00:02:46,030
And I think it's a good puzzle to contemplate why. A new pair of bases vectors like this still gives us a valid way to go back and forth between pairs of numbers and two-dimensional vectors, but the association is definitely different from the one that you get using the more standard basis of i hat and j hat.

15
00:02:46,030 --> 00:02:52,834
That anytime we described vectors numerically, it depends on an implicit choice of what basis vectors we're using.

16
00:02:52,835 --> 00:03:00,710
So, anytime that you're scaling two vectors and adding them like this, it's called a linear combination of those two vectors.

17
00:03:01,300 --> 00:03:17,490
Where does this word linear come from? Why does this have anything to do with lines? Well, this isn't the etymology, but one way I like to think about it is that if you fix one of those scalars and let the other one change its value freely, the tip of the resulting vector draws a straight line.

18
00:03:19,020 --> 00:03:26,860
Now, if you let both scalars range freely and consider every possible vector that you can get, there are two things that can happen.

19
00:03:26,860 --> 00:03:34,144
For most pairs of vectors, you'll be able to reach every possible point in the plane. Every two-dimensional vector is within your grasp.

20
00:03:34,145 --> 00:03:44,094
However, in the unlucky case where your two original vectors happen to line up, the tip of the resulting vector is limited to just the single line passing through the origin.

21
00:03:44,094 --> 00:03:51,710
Actually, technically, there's a third possibility too, both your vectors could be zero in which case you'd just be stuck at the origin.

22
00:03:51,710 --> 00:04:02,879
Here's some more terminology. The set of all possible vectors that you can reach with a linear combination of a given pair of vectors is called the span of those two vectors.

23
00:04:04,490 --> 00:04:11,934
So, restating what we just saw in this lingo, the span of most pairs of 2D vectors is all vectors of 2D space.

24
00:04:11,935 --> 00:04:22,800
But when they line up, their span is all vectors whose tip sits on a certain line. Remember how I said that linear algebra revolves around vector addition and scalar multiplication?

25
00:04:22,800 --> 00:04:34,439
Well, the span of two vectors is basically a way of asking what are all the possible vectors you can reach using only these two fundamental operations, vector addition and scalar multiplication?


@@@
1
00:00:00,000 --> 00:00:08,385
This is a good time to talk about how people commonly think about vectors as points. It gets really crowded to think about a whole collection of vectors, sitting on a line.

2
00:00:08,384 --> 00:00:13,474
And more crowded still, to think about all two-dimensional vectors all at once filling up the plane.

3
00:00:13,474 --> 00:00:20,170
So when dealing with collections of vectors like this, it is common to represent each one with just a point in space.

4
00:00:20,170 --> 00:00:26,750
The point at the tip of that vector, where as usual, I want you thinking about that vector with its tail on the origin.

5
00:00:26,750 --> 00:00:34,950
That way, if you want to think about every possible vector whose tip sits on a certain line, just think about the line itself.

6
00:00:35,719 --> 00:00:44,140
Likewise, to think about all possible two-dimensional vectors all at once, conceptualize each one as the point where its tip sits.

7
00:00:44,140 --> 00:00:51,755
So, in effect what you will be thinking about is the infinite flat sheet of two-dimensional space itself, leaving the arrows out of it.

8
00:00:51,755 --> 00:01:01,464
In general, if you are thinking about a vector on its own, think of it as an arrow. And if you are dealing with a collection of vectors, it's convenient to think of them all as points.

9
00:01:01,465 --> 00:01:08,670
So, for our span example, the span of most pairs of vectors ends up being the entire infinite sheet of two-dimensional space.

10
00:01:08,670 --> 00:01:20,260
But if they line up, their span is just a line. The idea of span gets a lot more interesting if we start thinking about vectors in three-dimensional space.

11
00:01:20,260 --> 00:01:28,449
For example, if you take two vectors in 3D space, that are not pointing in the same direction. What does it mean to take their span?

12
00:01:28,909 --> 00:01:42,109
Well, their span is the collection of all possible linear combinations, of those two vectors. Meaning all possible vectors you get by scaling each of the two of them in some way and then adding them together.

13
00:01:42,109 --> 00:01:52,185
You can kind of imagine turning two different knobs to change the two scalars defining the linear combination, adding the scaled vectors and following the tip of the resulting vector.

14
00:01:52,185 --> 00:01:58,109
That tip will trace out some kind of flat sheet cutting through the origin of three-dimensional space.

15
00:01:58,109 --> 00:02:08,185
This flat sheet is the span of the two vectors, or more precisely, the set of all possible vectors whose tips sit on that flat sheet is the span of your two vectors.

16
00:02:08,185 --> 00:02:16,674
Isn't that a beautiful mental image? So, what happens if we add a third vector and consider the span of all three of those guys?

17
00:02:16,675 --> 00:02:27,979
A linear combination of three vectors is defined pretty much the same way as it is for two. You'll choose three different scalars, scale each of those vectors and then add them all together.

18
00:02:31,979 --> 00:02:42,704
And again, the span of these vectors is the set of all possible linear combinations. Two different things could happen here.

19
00:02:42,705 --> 00:02:48,314
If your third vector happens to be sitting on the span of the first two, then the span doesn't change.

20
00:02:48,314 --> 00:02:58,614
You're sort of trapped, on that same flat sheet. In other words, adding a scaled version of that third vector to the linear combination doesn't really give you access to any new vectors.

21
00:02:58,615 --> 00:03:04,995
But if you just randomly choose a third vector, it's almost certainly not sitting on the span of those first two.

22
00:03:04,995 --> 00:03:11,524
Then, since it is pointing in a separate direction, it unlocks access to every possible three-dimensional vector.

23
00:03:11,525 --> 00:03:21,849
One way I like to think about this, is that as you scale that new third vector, it moves around that span sheet of the first two sweeping it through all of space.

24
00:03:21,849 --> 00:03:31,759
Another way to think about it, is that you are making full use of the three freely changing scalars that you have at your disposal to access the full three dimensions of space.

25
00:03:32,530 --> 00:03:44,670
Now, in the case where the third vector was already sitting on the span of the first two or the case where two vectors happen to line up, we want some terminology to describe the fact that at least one of these vectors is redundant.

26
00:03:44,669 --> 00:03:56,574
Not adding anything to our span. Whenever this happens, where you have multiple vectors and you could remove one without reducing the span, the relevant terminology is to say that they are linearly dependent.

27
00:03:56,574 --> 00:04:06,240
Another way of phrasing that would be to say that one of the vectors can be expressed as a linear combination of the others, since it is already in the span of the others.

28
00:04:08,569 --> 00:04:17,250
On the other hand, if each vector really does add another dimension to the span, they are said to be linearly independent.

29
00:04:22,060 --> 00:04:29,089
So with all of that terminology and hopefully with some good mental images to go with it, let me leave you with a puzzle before we go.

30
00:04:29,089 --> 00:04:37,714
The technical definition of a basis of a space is a set of linearly independent vectors that span that space.

31
00:04:37,714 --> 00:04:49,349
Now given how I described a basis earlier and given your current understanding of the words span and linearly independent, think about why this definition would make sense.


@@@
1
00:00:11,519 --> 00:00:22,454
Hey everyone, if I had to choose just one topic that makes all of the others in linear algebra start to click and which too often goes unlearned the first time a student takes linear algebra, it will be this one.

2
00:00:22,454 --> 00:00:35,524
The idea of a linear transformation, and its relation to matrices. For this video, I'm just going to focus on what these transformations look like in the case of two dimensions, and how they relate to the idea of matrix-vector multiplication.

3
00:00:35,524 --> 00:00:42,649
In particular, I want to show you a way to think about matrix-vector multiplication, that doesn't rely on memorization.

4
00:00:42,649 --> 00:01:01,799
To start, let's just parse this term, linear transformation. Transformation is essentially a fancy word for function, it's something that takes in inputs, and spits out an output for each one Specifically in the context of linear algebra, we like to think about transformations that take in some vector, and spit out another vector.

5
00:01:01,799 --> 00:01:11,725
So, why use the word transformation instead of function, if they mean the same thing? Well, it's to be suggestive of a certain way to visualize this input-output relation.

6
00:01:11,724 --> 00:01:25,295
A great way to understand functions of vectors, is to use movement. If a transformation takes some input vector, to some output vector, we imagine that input vector moving over to the output vector.

7
00:01:25,295 --> 00:01:34,605
Then to understand the transformation as a whole, we might imagine watching every possible input vector, move over to its corresponding output vector.

8
00:01:34,605 --> 00:01:45,959
It gets really crowded to think about all of the vectors, all at once, each one has an arrow. So, as I mentioned in the last video, a nice trick is to conceptualize each vector not as an arrow, but as a single point.

9
00:01:45,959 --> 00:01:56,844
The point where it's tip sets That way, to think about a transformation taking every possible input vector to some output vector, we watch every point in space moving to some other point.

10
00:01:56,844 --> 00:02:06,194
In the case of transformations in two dimensions, to get a better feel for the whole shape of the transformation, I like to do this with all of the points on an infinite grid.

11
00:02:06,194 --> 00:02:13,900
I also sometimes like to keep a copy of the grid in the background, just to help keep track of where everything ends up, relative to where it starts.

12
00:02:13,960 --> 00:02:21,534
The effect for various transformations moving around all of the points in space is, and you've got to admit, beautiful.

13
00:02:21,534 --> 00:02:30,229
It gives the feeling of squishing, and morphing space itself. As you can imagine though arbitrary transformations can look pretty complicated.

14
00:02:30,229 --> 00:02:38,764
But luckily, linear algebra limits itself to a special type of transformation. Ones that are easier to understand, called linear transformations.

15
00:02:38,764 --> 00:02:50,180
Visually speaking, a transformation is linear if it has two properties. All lines must remain lines without getting curved, and the origin must remain fixed in place.

16
00:02:50,180 --> 00:03:02,295
For example, this right here would not be a linear transformation, since the lines get all curvy. And this one right here although it keeps the line straight, is not a linear transformation, because it moves the origin.

17
00:03:02,294 --> 00:03:09,430
This one here fixes the origin, and it might look like it keeps lines straight, but that's just because I'm only showing the horizontal, and vertical grid lines.

18
00:03:09,430 --> 00:03:16,079
When you see what it does to a diagonal line, it becomes clear that it's not at all linear, since it turns that line all curvy.

19
00:03:16,080 --> 00:03:22,899
In general, you should think of linear transformations as keeping grid lines parallel, and evenly spaced.

20
00:03:22,900 --> 00:03:31,119
Some linear transformations are simple to think about, like rotations about the origin. Others are a little trickier to describe with words.


@@@
1
00:00:00,000 --> 00:00:07,804
So how do you think you could describe these transformations numerically, if you're say, programming some animations to make a video teaching the topic?

2
00:00:07,804 --> 00:00:15,814
What formula do you give the computer, so that if you give it the coordinates of a vector, it can give you the coordinates of where that vector lands?

3
00:00:15,814 --> 00:00:24,900
It turns out that you only need to record where the two bases vectors, i hat and j hat, each land, and everything else will follow from that.

4
00:00:24,899 --> 00:00:34,579
For example, consider the vector V with coordinates negative one, two. Meaning that it equals negative one times i hat, plus two times j hat.

5
00:00:35,990 --> 00:00:46,679
If we play some transformation and follow where all three of these vectors go, the property that grid lines remain parallel and evenly spaced has a really important consequence.

6
00:00:46,679 --> 00:00:53,664
The place where V lands, will be negative one times the vector where i hat landed, plus two times the vector where j hat landed.

7
00:00:53,664 --> 00:01:03,020
In other words, it started off as a certain linear combination of i hat and j hat, and it ends up as that same linear combination of where those two vectors landed.

8
00:01:03,020 --> 00:01:12,790
This means you can deduce where V must go based only on where i hat and j hat each land. This is why I like keeping a copy of the original grid in the background.

9
00:01:12,790 --> 00:01:19,109
For the transformations shown here, we can read off that i hat lands on the coordinates one, negative two.

10
00:01:19,109 --> 00:01:34,560
And j hat lands on the x axis over at the coordinates three, zero. This means that the vector represented by negative one i hat plus two times j hat, ends up at negative one times the vector one negative two, plus two times the vector three, zero.

11
00:01:34,560 --> 00:01:45,794
Adding that all together you can deduce that it has to land on the vector five, two. This is a good point to pause and ponder because it's pretty important.

12
00:01:45,795 --> 00:01:53,474
Now, given that I'm actually showing you the full transformation, you could have just looked to see that V has the coordinates five, two.

13
00:01:53,474 --> 00:02:05,859
But the cool part here is that this gives us a technique to deduce where any vectors land, so long as we have a record of where i hat and j hat each land without needing to watch the transformation itself.

14
00:02:05,859 --> 00:02:19,155
Write the vector with more general coordinates x and y, and it will land on x times the vector where i hat lands, one negative two, plus y times the vector where j hat lands, three, zero.

15
00:02:19,155 --> 00:02:32,125
Carrying out that sum, you see that it lands at one x plus three y, negative two x plus zero y. I give you any vector and you can tell me where that vector lands using this formula.

16
00:02:32,125 --> 00:02:44,719
What all of this is saying is that a two dimensional linear transformation is completely described by just four numbers, the two coordinates for where i hat lands and the two coordinates for where j hat lands.

17
00:02:44,719 --> 00:02:51,840
Isn't that cool? It's common to package these coordinates into a two by two grid of numbers called a two by two matrix.

18
00:02:51,840 --> 00:03:15,615
Where you can interpret the columns as the two special vectors where i hat and j hat each land. If you're given the two by two matrix describing a linear transformation and some specific vector and you want to know where that linear transformation takes that vector, you can take the coordinates of the vector, multiply them by the corresponding columns of the matrix, then add together what you get.


@@@
1
00:00:00,000 --> 00:00:12,019
Let's see what this looks like in the most general case, where your matrix has entries a, b, c, d. And remember, this matrix is just a way of packaging the information needed to describe a linear transformation.

2
00:00:12,019 --> 00:00:22,530
Always remember to interpret that first column, a, c, as the place where the first basis vector lands and that second column, b,d, as the place where the second basis vector lands.

3
00:00:22,530 --> 00:00:38,570
When we apply this transformation to some vector x, y, what do you get? Well, it will be x times a, c, plus, y times b, d. Putting this together, you get a vector ax + by, cx + dy.

4
00:00:38,570 --> 00:00:46,780
You could even define this as matrix vector multiplication when you put the matrix on the left of the vector like it's a function.

5
00:00:46,780 --> 00:00:53,054
Then you could make high schoolers memorize this without showing them the crucial part that makes it feel intuitive.

6
00:00:53,054 --> 00:01:04,579
But isn't it more fun to think about these columns as the transformed versions of your basis vectors, and to think about the results as the appropriate linear combination of those vectors?

7
00:01:05,540 --> 00:01:23,170
Let's practice describing a few linear transformations with matrices. For example, if we rotate all of space 90 degrees counter-clockwise, then i_hat lands on the coordinates (0, 1,) and j_hat lands on the coordinates (-1, 0).

8
00:01:23,170 --> 00:01:32,165
So the matrix we end up with has columns (0, 1) (-1, 0) to figure out what happens to any vector after a 90 degree rotation.

9
00:01:32,165 --> 00:01:40,204
You could just multiply its coordinates by this matrix. Here's a fun transformation with a special name called a shear.

10
00:01:40,204 --> 00:01:51,324
In it, i_hat remains fixed. So, the first column of The Matrix is 1, 0 but j_hat moves over to the coordinates (1, 1) which become the second column of The Matrix.

11
00:01:51,325 --> 00:02:00,500
And at the risk of being redundant here, figuring out how a shear transforms a given vector comes down to multiplying this matrix by that vector.

12
00:02:00,500 --> 00:02:10,129
Let's say we want to go the other way around. Starting with a matrix, say with columns 1, 2 and 3, 1, and we want to deduce what its transformation looks like.

13
00:02:10,129 --> 00:02:20,724
Pause and take a moment to see if you can imagine it. One way to do this is to first move i_hat to 1, 2, then move j_hat to 3, 1.

14
00:02:20,724 --> 00:02:49,039
Always moving the rest of space in such a way that keeps grid lines parallel and evenly spaced. If the vectors that i_hat and j_hat land on are linearly dependent, which if you recall from last video means that one is a scaled version of the other, it means that the linear transformation squishes all of 2D space onto the line where those two vectors sit, also known as the one dimensional span of those two linearly dependent vectors.

15
00:02:49,490 --> 00:02:59,709
To sum up, linear transformations are a way to move around space such that grid lines remain parallel and evenly spaced, and such that the origin remains fixed.

16
00:02:59,710 --> 00:03:07,855
Delightfully, these transformations can be described using only a handful of numbers, the coordinates of where each basis vector lands.

17
00:03:07,854 --> 00:03:20,535
Matrices give us a language to describe these transformations where the columns represent those coordinates and matrix vector multiplication is just a way to compute what that transformation does to a given vector.

18
00:03:20,534 --> 00:03:27,800
The important takeaway here is that every time you see a matrix you can interpret it as a certain transformation of space.


@@@
1
00:00:00,000 --> 00:00:09,840
Now I'm going to introduce you to Jupyter notebooks. Notebooks are an amazing tool for data analysis, where text, code, and images all sit in one document in your browser.

2
00:00:09,840 --> 00:00:19,980
Here's an example notebook where I explored predicting body fat percentage with various regression models. Up top here you see what's called a text cell.

3
00:00:19,980 --> 00:00:43,649
Cells are these guys and they can contain text or code. If I double-click on the text cell, I can edit the text in here. It's written in markdown, a text format with syntax that renders to HTML. So for instance, if I want to write a link, this is the syntax for it, and if I render the text, it's a link. This is a code cell.

4
00:00:43,649 --> 00:01:04,739
You can see here I'm importing some packages like Numpy and Pandas. I can run this cell and the code is executed the same way as it is in the terminal or a Python script. This command here "%matplotlib inline" will render images generated with matplotlib in the notebook instead of a separate window.

5
00:01:04,739 --> 00:01:26,759
If the cell returns some output, you see here. For instance, data. head() returns an HTML table displaying some of the data. Right below here you see the data visualized with a grid of scatter plots and histograms. Notebooks even render math in a text cells.

6
00:01:26,759 --> 00:01:34,560
This is just an example of what you can do with notebooks. You have your code, documentation, visualizations, math, all in one place.


@@@
1
00:00:00,000 --> 00:00:13,919
NumPy is short for Numerical Python and is a library designed for efficient Scientific Computation. It's built on top of the programming language C, which works at a lower level on our computer.

2
00:00:13,919 --> 00:00:24,740
To understand what this means for the speed of our code, see the link in the instructor notes. At the core of NumPy, is its N-dimensional array object.

3
00:00:24,739 --> 00:00:31,855
This is just a multi-dimensional array that holds a group of elements that all have the same data type.

4
00:00:31,855 --> 00:00:45,405
In other words, it's like a grid that can take on many shapes and enforces every element in that grid to have the same type, whether that's string, float, boolean, or something else.

5
00:00:45,405 --> 00:00:54,695
Making arrays only able to hold one data type at a time helps NumPy make very quick computations with vector operations.

6
00:00:54,695 --> 00:01:03,509
These arrays, along with many useful functions in NumPy, can significantly optimize and simplify operations on data.

7
00:01:03,509 --> 00:01:12,765
Here's a simple example that demonstrates this. First, let's import NumPy with the standard alias for this library, NP.

8
00:01:12,765 --> 00:01:25,359
This generates an array of 100 million floats between zero and one. Let's compare the time it takes playing Python versus NumPy to calculate the mean of this array.

9
00:01:25,359 --> 00:01:33,349
Instead of Python, we do this by getting the sum of X and dividing that by the length of X, pretty straightforward.

10
00:01:33,349 --> 00:01:42,420
Using the time package, we can check how long this line of code takes to run. That took about 9.3 seconds.

11
00:01:42,420 --> 00:01:52,140
Makes sense that it took a while with 100 million values. Now, let's see how long it takes NumPy. That was ridiculously faster.

12
00:01:52,140 --> 00:02:05,119
NumPy took only 0.092 seconds, while plain Python took 9.31. As you saw, NumPy can make a difference of orders of magnitude in computation time.

13
00:02:05,120 --> 00:02:12,604
Imagine how much this speeds up the process for more complex situations that require many more calculations.

14
00:02:12,604 --> 00:02:26,105
For example, let's say we're engineering a whole new feature or column in a dataset, which you compute by multiplying the values and two columns together and dividing that by the values in another for each row.

15
00:02:26,104 --> 00:02:35,310
Unlike NumPy, plain Python would require a massively long loop through all of the rows to compute the new value for each individual row.

16
00:02:35,310 --> 00:02:41,710
In many machine learning problems, you'll often find yourself using NumPy arrays in many situations.

17
00:02:41,710 --> 00:02:50,025
For instance, you might use a NumPy array to hold the pixel values of an image that will be fed into a model for image classification.

18
00:02:50,025 --> 00:02:58,134
Later, we'll also learn about a popular data science package, Pandas, which is very useful for manipulating datasets.

19
00:02:58,134 --> 00:03:08,000
It's actually built on top of NumPy, which is why its computations are so fast. Now that you've learned a bit about the power of NumPy, let's get started.


@@@
1
00:00:00,000 --> 00:00:12,955
Generally, there are two ways to create Numpy arrays. First, using Numpy's array function to create them from other array-like objects such as regular Python lists.

2
00:00:12,955 --> 00:00:20,820
And second, using a variety of built-in Numpy functions that quickly generate specific types of arrays.

3
00:00:20,820 --> 00:00:32,924
In this section, we will start with the first way. Let's import Numpy and create our first array. Here's a one-dimensional array that contains integers.

4
00:00:32,924 --> 00:00:44,710
Note that for clarity, the examples throughout these lessons will use small, simple arrays. We'll start by creating one-dimensional or 1D Numpy arrays.

5
00:00:44,710 --> 00:00:54,359
Let's print the array we just created, as well as, it's type. You can see that the type is Numpy's ndarray or n-dimensional array.

6
00:00:54,359 --> 00:01:03,740
Numpy arrays have useful attributes that provide us information about them in a very intuitive way. For example, this dtype attribute.

7
00:01:03,740 --> 00:01:13,689
Dtype returns the data type of the elements in that array. Notice, dtype is different from the datatype of the array itself.

8
00:01:13,689 --> 00:01:27,365
This d type let's us know that the elements of X are stored in memory a signed 64-bit integers. An additional advantage of Numpy is that it handles more datatypes than Python.

9
00:01:27,364 --> 00:01:35,479
You can check out all the different datatypes supported by Numpy in it's documentation. Another useful attribute is shape.

10
00:01:35,480 --> 00:01:45,795
This returns a tuple of n positive integers that specifies the sizes of each dimension n being the number of dimensions in the array.

11
00:01:45,795 --> 00:02:05,250
X has one dimension. So, shape returns an integer indicating the length of the array, five. If we had a two-dimensional array, this shape attribute would return a tuple with two values, one for the number of rows and one for the number of columns.

12
00:02:05,250 --> 00:02:13,079
To see this, let's create a two-dimensional array from a nested Python list. Here is one that contains integers.

13
00:02:13,080 --> 00:02:23,554
And let's print an additional attribute size. Looking at the tuple returned by shape, we know that Y has two dimensions since there are two elements.

14
00:02:23,555 --> 00:02:35,254
One is for the size of the first dimension which is the number of rows, four, and the other is for the second dimension, which is the number of columns, three.

15
00:02:35,254 --> 00:02:44,985
The size attribute gives us the total number of elements in Y which is 12. Let's pause for a second to introduce some useful terminology.

16
00:02:44,985 --> 00:03:02,079
In general, we say that an array with n dimensions has a rank n. So, we refer to the 1D array we created earlier as a rank one-array and we refer to the 2D array we just created as a rank two-array.

17
00:03:02,080 --> 00:03:11,224
For our next example, let's create a rank one-array that contains strings, and let's print those same attributes.

18
00:03:11,224 --> 00:03:20,984
The type of the array object itself isn't any different, it's still just a Numpy array. However, the dtype of this array is different.

19
00:03:20,985 --> 00:03:35,400
Here elements are stored as unicode strings of five characters. Notice that when Numpy creates an array, it automatically assigns it's dtype based on the type of the elements you used to create the array.

20
00:03:35,400 --> 00:03:42,824
But what happens when we try to create a Numpy array with a list that contains both integers and strings?

21
00:03:42,824 --> 00:03:57,354
We can see that even though the Python list had mixed datatypes, the array function created a Numpy array with elements of all the same datatype namely, unicode strings of 21 characters.

22
00:03:57,354 --> 00:04:08,500
Remember, unlike Python lists, Numpy arrays must contain elements of the same type. Up until now, we've only used elements that were integers or strings.

23
00:04:08,500 --> 00:04:25,720
Let's try another example with mixed datatypes using integers and floats. When we input a list with both integers and floats, Numpy assigns all elements, the float 64 dtype, this is called upcasting.

24
00:04:25,720 --> 00:04:38,740
Since all the elements of a Numpy array must be of the same type, Numpy upcasts the integers in the array to floats in order to avoid losing precision in numerical computations.

25
00:04:38,740 --> 00:04:50,699
Numpy also allows you to specify a particular dtype you want to assign to the elements of an array, you can do this using the keyword dtype in the array function.

26
00:04:50,699 --> 00:05:13,379
Here, you can see Numpy created an array of ints even though we passed it a list of floats. Specifying the datatype of the elements in a Numpy array can be useful in cases where you don't want to accidentally choose the wrong datatype or when you only need a certain amount of precision in your calculations and want to save memory.

27
00:05:13,379 --> 00:05:21,279
Once you create a Numpy array, you may want to save it to a file to be read later or to be used by another program.

28
00:05:21,279 --> 00:05:30,545
Numpy provides a way to save the arrays into files for later use. We can save X into the current directory like this.

29
00:05:30,545 --> 00:05:43,080
This saves the array into a file named my array dot npy. You can later load this file into a variable by using the load function like this.


@@@
1
00:00:00,000 --> 00:00:11,019
In the last video, we learnt how to create NumPy arrays by converting existing array like objects such as Python lists, using NumPy's array function.

2
00:00:11,019 --> 00:00:21,420
But one great time saving feature of NumPy is its ability to generate specific kinds of NumPy arrays from nothing, using just one line of code.

3
00:00:21,420 --> 00:00:33,689
Here, we will see a few of the most useful builtin functions for generating Numpy arrays. Let's start by creating a NumPy array of zeroes with a shape that we specify.

4
00:00:33,689 --> 00:00:42,929
We can do this by using NumPy's zeros function. This function takes as an argument the shape of the array you want to create.

5
00:00:42,929 --> 00:00:54,494
Passing in the tuple (3,4) gives us a 3 by 4 array of zeros. By default, this creates an array with the data type float 64.

6
00:00:54,494 --> 00:01:08,760
If you want to use a different data type, you can change this with the keyword dtype. Similarly, we can create a NumPy array of ones using this function, which also takes shape as an argument.

7
00:01:08,760 --> 00:01:17,649
In addition to ones and zeros, you can create an array filled with any constant value using the full function.

8
00:01:17,650 --> 00:01:31,364
This takes two arguments, a shape for the array and the constant you want to fill it with. The full function by default creates an array with a data type of the value you inputted as a constant.

9
00:01:31,364 --> 00:01:44,144
Here, since we used the Integer 5 as our constant, this generated an array with the dtype int64. Use the key word dtype to specify otherwise.

10
00:01:44,144 --> 00:01:56,665
A fundamental array in Linear Algebra is the identity matrix. A matrix is just another term used to describe a two dimensional array with rows and columns.

11
00:01:56,665 --> 00:02:05,730
And, an identity matrix is just a square shaped matrix that has ones along its main diagonal, and zeros everywhere else.

12
00:02:05,730 --> 00:02:16,490
NumPy's eye function can be used to create this. Since all identity matrices are square, this only takes a single integer as an argument.

13
00:02:16,490 --> 00:02:27,145
Using 5 gives us a 5 x 5 identity matrix. And its main diagonal which goes from the top left to the bottom right, is filled with ones.

14
00:02:27,145 --> 00:02:43,064
We can also use Numpy's diag function to create a diagonal matrix. This function takes in as input a sequence of values to use as the main diagonal of a square matrix, and fills in the rest with zeros.

15
00:02:43,064 --> 00:02:58,415
NumPy also has useful functions to generate arrays with specific numerical ranges. One useful one is arange, which creates a one dimensional array of evenly spaced values within a given interval.

16
00:02:58,414 --> 00:03:09,150
This takes three arguments: start, stop, and step. But we can still use this whether we want to specify one, two or three arguments.

17
00:03:09,150 --> 00:03:22,405
Let's see each case. When only one integer is specified, arange uses this as a stop argument, and generates an array of integers from zero to that integer minus one.

18
00:03:22,405 --> 00:03:34,280
The stop argument is exclusive, which is why we need to subtract one. For example, arange 10 gives us an array from zero to 10 minus one which is nine.

19
00:03:34,280 --> 00:03:43,025
When used with two arguments, arange uses the first as a start argument, and the second as a stop argument.

20
00:03:43,025 --> 00:03:53,110
The start is inclusive, and the stop is exclusive. Arange (4,10) gives us an array from four to nine.

21
00:03:53,110 --> 00:04:03,260
When you use the three arguments, arange generates an array from the first integer to the second minus one, evenly spaced by the third.

22
00:04:03,259 --> 00:04:17,444
This third argument, the step, is the distance between any two values in this array. When we specified only one or two arguments in arange earlier, step defaulted to one.

23
00:04:17,444 --> 00:04:28,775
Even though NumPy's arange function allows for a non integer steps such as 0.3, the output is usually inconsistent due to finite floating point precision.

24
00:04:28,774 --> 00:04:37,004
For this reason, when we want non integer steps, it's usually better to use a different NumPy function, linspace.

25
00:04:37,004 --> 00:04:51,734
This takes three arguments, start, stop, and n. This returns n evenly spaced numbers from start to stop, both start and stop being inclusive.

26
00:04:51,735 --> 00:05:01,854
Unlike arange, Linspace requires at least two arguments for start and stop. If n is not specified, it defaults to 50.

27
00:05:01,855 --> 00:05:15,665
Let's see some examples. Here's a rank one array that has 10 numbers evenly spaced from zero to 25. Again, note that both the start and stop points are inclusive.

28
00:05:15,665 --> 00:05:26,819
However, you can let the stop endpoint of the interval be excluded just like it is in the arange function, if you set the keyword endpoint to false like this.

29
00:05:26,819 --> 00:05:38,850
As you can see, because we have excluded the endpoint 25, the spacing between the values had to change in order to fit 10 evenly spaced numbers in the given interval.

30
00:05:38,850 --> 00:05:54,854
So far, we've only used the functions arange and linspace to create rank 1 arrays. However, we can use these functions to create rank 2 arrays of any shape by combining them with NumPy's reshape function.

31
00:05:54,855 --> 00:06:07,195
This function converts any NumPy array into a specified shape. It's important to note that the new shape specified here, should be compatible with a number of elements in the array.

32
00:06:07,194 --> 00:06:23,014
For example, you can convert a rank one array with 20 elements into a 4 by 5 rank two array, or a 10 by 2 array, since both of these rank two arrays still have 20 elements.

33
00:06:23,014 --> 00:06:36,384
However, you can't reshape this to a 5 by 5 array, since this rank two array would have 25 elements, which is greater than the number of elements in the original NumPy array.

34
00:06:36,384 --> 00:06:47,935
One great feature about NumPy is that some functions can also be applied as methods. This allows us to apply different functions in sequence, in just one line of code.

35
00:06:47,935 --> 00:07:01,455
NumPy array methods are similar to its attributes in that they are both applied using dot notation. Let's see how we can accomplish the same result as this example using one line of code.

36
00:07:01,454 --> 00:07:10,204
This gives us the same result. Notice that when using reshape as a method, we don't need to pas in the array as an argument.

37
00:07:10,204 --> 00:07:23,589
Similarly, we can also use reshape to create rank two arrays with the linspace function. Lastly, we are going to create NumPy arrays that contain random numbers.

38
00:07:23,589 --> 00:07:38,564
Often in machine learning, you need to create random matrices. For example, when initializing the weights of a neural network, NumPy offers a variety of random functions to help us create random NumPy arrays of any shape.

39
00:07:38,564 --> 00:07:51,314
Let's start by using NumPy's random function to create an array of a given shape, with random floats between zero and one, where zero is inclusive, and one is exclusive.

40
00:07:51,314 --> 00:08:06,655
The following functions including this random function, are contained in NumPy's random module. So we type np.random to access this module, and then.random to access the function in the module.

41
00:08:06,654 --> 00:08:17,464
NumPy also let's us create NumPy arrays that contain random integers within a particular interval. We can use the function randint to do this.

42
00:08:17,464 --> 00:08:25,794
This takes three arguments, the lower bound, inclusive, and the upper bound, exclusive, and the shape.

43
00:08:25,795 --> 00:08:33,960
In some cases, you may need to create NumPy arrays with random numbers that satisfy certain statistical properties.

44
00:08:33,960 --> 00:08:46,350
For example, you may want the random numbers in the array to have an average of zero. NumPy allows you to create random arrays with numbers drawn from various probability distributions.

45
00:08:46,350 --> 00:08:59,750
The function np.random.normal for example, creates an array with the given shape that contains random numbers picked from a normal distribution, with a given mean and standard deviation.

46
00:08:59,750 --> 00:09:11,659
This creates a 1000 by 1000 array of random floats drawn from a normal distribution with the mean of zero, and a standard deviation of 0.1.

47
00:09:11,659 --> 00:09:22,504
As we can see, the average of the random numbers in the array is very close to zeo, and the standard deviation is also very close to 0.1.

48
00:09:22,504 --> 00:09:32,730
Both the maximum and minimum values next are symmetric about zero, the average. And we have about the same number of positive and negative integers.


@@@
1
00:00:00,000 --> 00:00:09,320
Now that you know how to create a variety of NumPy arrays, let's see how NumPy allows us to effectively manipulate the data within them.

2
00:00:09,320 --> 00:00:16,899
NumPy arrays are mutable, meaning the elements in them can be changed after the array has been created.

3
00:00:16,899 --> 00:00:24,844
NumPy arrays can also be sliced in many different ways. This allows us to achieve any subset of the array we want.

4
00:00:24,844 --> 00:00:33,765
You'll often use slicing to separate data. For example, when dividing a dataset into training, cross validation, and testing sets.

5
00:00:33,765 --> 00:00:46,344
We will start by looking how the elements of a NumPy array can be accessed or modified by indexing. Let's create a rank one array that contains integers from one to five.

6
00:00:46,344 --> 00:00:57,920
Elements can be accessed by specifying their positions using indices inside square brackets. Positive indices are used to specify positions from the beginning of the array.

7
00:00:57,920 --> 00:01:10,620
Notice that to access the first element in the array, we have to use index zero not one. We can also use negative indices to specify positions from the end of the array.

8
00:01:10,620 --> 00:01:28,055
Notice that the same element can be accessed using both negative and positive integers. As mentioned earlier, positive indices are used to access elements from the beginning of the array, while negative indices are used to access elements from the end of the array.

9
00:01:28,055 --> 00:01:39,195
Now, let's see how we can modify the elements in an array. We can do this by accessing the element we want to change, and then reassigning it using the equal sign.

10
00:01:39,194 --> 00:01:53,369
Let's modify the array X we just created. We can change the fourth element in X from four to 20. We can also access and modify specific elements of rank two in NumPy arrays.

11
00:01:53,370 --> 00:02:00,424
The only difference is that we need to provide two indices separated by a comma inside the square brackets.

12
00:02:00,424 --> 00:02:07,675
Here's a three by three array containing the integers from one to nine. Let's access some elements in that array.

13
00:02:07,674 --> 00:02:16,169
Remember that zero, comma, zero refers to the element in the first row and the first column, which is one.

14
00:02:16,169 --> 00:02:28,574
Elements in rank two arrays can also be modified in the same way. We can change the zero, comma, zero element, in X from one to 20 like this.

15
00:02:28,574 --> 00:02:37,035
Now that we know how to access and modify elements in an array, let's take a look at how we can add and delete elements.

16
00:02:37,034 --> 00:02:47,445
We can delete elements using NumPy's delete function. This function takes in an array, list of indices to delete, and an axis to delete from.

17
00:02:47,444 --> 00:02:59,830
For rank one arrays, the axis keyword is not required. For rank two arrays, axis zero is used to select rows and axis one is used to select columns.

18
00:02:59,830 --> 00:03:08,015
Let's see some examples. Here is a rank one array, we can delete the first and last element of X like this.

19
00:03:08,014 --> 00:03:19,939
And here is a rank 2 array, and delete the first and last column of Y like this. We can add values to NumPy arrays using the append function.

20
00:03:19,939 --> 00:03:29,724
This function takes in an array, a list of elements to append, and the axis to append it on. Let's see some examples.

21
00:03:29,724 --> 00:03:41,155
We can append an element six to this rank one array like this. And we can append multiple elements, say seven and eight in a list like this.

22
00:03:41,155 --> 00:03:54,490
In a rank two array, we can append a new row containing 10, 11, and 12 like this. And we can append a new column containing 10, 11, 12 like this.

23
00:03:54,490 --> 00:04:04,765
Notice that when appending rows or columns to rank two NumPy arrays, the rows and columns must have the correct shape to match the shape of the array.

24
00:04:04,764 --> 00:04:16,439
Now, let's see how we can insert values into NumPy arrays. We can use the insert function for this, which takes in an array, index, elements, and axis.

25
00:04:16,439 --> 00:04:25,654
This inserts the given list of elements to the array right before the given index along the specified axis. Here's a rank one array.

26
00:04:25,654 --> 00:04:41,555
We can insert the integers three and four between these elements two and five like this. In this rank two array, we can insert a row between this first and last row like this.

27
00:04:41,555 --> 00:04:56,990
And insert a column full of fives between the first and second column like this. NumPy also allows us to stack NumPy arrays on top of each other or side by side.

28
00:04:56,990 --> 00:05:06,250
The stacking is done using either NumPy's Vstack for vertical stacking or Hstack function for horizontal stacking.

29
00:05:06,250 --> 00:05:17,675
It's important to note that in order to stack arrays, the shape of the arrays must match. Consider this rank one and rank two array X and Y.

30
00:05:17,675 --> 00:05:30,700
We can stack X on top of Y with Vstack like this. And we can stack X on the right of Y with Hstack like this.


@@@
1
00:00:00,000 --> 00:00:11,230
As we mentioned earlier, in addition to accessing individual elements one at a time, we can access subsets of NumPy arrays with slicing.

2
00:00:11,230 --> 00:00:20,455
Slicing is performed by combining indices with a colon inside the brackets. In general, you will come across three ways of slicing.

3
00:00:20,454 --> 00:00:37,164
First, slicing from a starting index to an ending index. Second, slicing from a starting index to the end of the array, by leaving this blank, or slicing from the beginning of the array to an ending index, by leaving this part blank.

4
00:00:37,164 --> 00:00:45,604
In the first and third methods, the ending index is always excluded. While here, the starting index is always included.

5
00:00:45,604 --> 00:00:54,619
NumPy arrays can be multidimensional. So, when slicing, you usually have to specify a slice for each dimension of the array.

6
00:00:54,619 --> 00:01:03,354
Let's see some examples of slicing, with a rank two array. Here is a four by five array that contains integers from one to 20.

7
00:01:03,354 --> 00:01:11,924
Let's say, you wanted to grab this subset from the array, these nine values. Here is one way we can do this.

8
00:01:11,924 --> 00:01:24,034
The part before the comma, specifies what indices you want to grab from the rows, and the part after the comma specifies what indices you want to grab from the columns.

9
00:01:24,034 --> 00:01:32,965
Remember, the starting index is included and the ending index is excluded. And indexing always starts with zero.

10
00:01:32,965 --> 00:01:42,859
So, the first row is zero and the second column is one. So, this grabs the rows one, two, and three.

11
00:01:42,859 --> 00:01:51,505
And this part grabs the columns two, three, and four. Here is another way we can accomplish that same thing.

12
00:01:51,504 --> 00:01:58,460
Remember, if we don't include an ending index after the colon, it will just go all the way to the last index.

13
00:01:58,459 --> 00:02:06,630
Let's try to grab another subset that uses these same columns, but instead of the last three rows, grabs the first three rows.

14
00:02:06,629 --> 00:02:15,600
This grabs this subset. Remember, not including a starting index before the colon just makes it go all the way to the beginning of the array.

15
00:02:15,599 --> 00:02:25,305
So, this grabs the rows zero, one, and two. For our next example, let's try to select all the elements in the third row.

16
00:02:25,305 --> 00:02:37,664
If we leave both sides of the colon blank, this will just grab all the rows in the array. So, X, colon, comma, two will get all the rows in column two.

17
00:02:37,664 --> 00:02:49,419
So, zero, one, two it would grab this column. If we want to select all the elements in the third column but return it in a rank two array, we can do this.

18
00:02:49,419 --> 00:03:00,810
Notice that when we first selected all the elements in the third column this way, the slice returned a rank one array instead of a rank two array.

19
00:03:00,810 --> 00:03:19,490
But slicing X like this, in a slightly different way, can get us a rank two NumPy array. It's important to note that when we perform slices on NumPy arrays and save them into new variables, like we did here, the data is not actually copied into the new variable.

20
00:03:19,490 --> 00:03:26,180
This is one feature that often causes confusion for beginners. So, we'll look into this in more detail.

21
00:03:26,180 --> 00:03:39,115
In these examples, the slice of the original array X, is not copied into the variable Z. Rather, X and Z are now just two different names for the same array.

22
00:03:39,115 --> 00:03:50,270
We say that slicing only creates a view of the original array. This means, if you make any changes in Z, you'll also be changing the elements in X.

23
00:03:50,270 --> 00:03:59,224
Let's see an example with that four by five array. We'll select the elements over here again, and assign that to the variable Z.

24
00:03:59,224 --> 00:04:10,364
Now, we'll change the last element in Z to 555. Now, if we print X, we can see that it has also been affected by this change.

25
00:04:10,365 --> 00:04:18,870
If we want to create a new NumPy array that contains a copy of the values in the slice, we need to use NumPy's Copy function.

26
00:04:18,870 --> 00:04:27,935
This function can also be used as a method, as we saw before with the reshape function. Let's repeat this example with the copy command.

27
00:04:27,935 --> 00:04:37,435
Here's X again, we'll create a copy of the same slice using the copy function. We can also use Copy as a method like this.

28
00:04:37,435 --> 00:04:52,324
If we change the last element in Z to 555, we can see that X has not been changed. By using the Copy command, we are creating a new NumPy array, that is completely independent of the original.

29
00:04:52,324 --> 00:05:00,889
It's often useful to use an array as indices to make slices, select, or change elements in another NumPy array.

30
00:05:00,889 --> 00:05:09,300
Let's see some examples. Let's create a rank one array that will serve as indices to select elements from X.

31
00:05:09,300 --> 00:05:21,379
We'll use this indices NumPy array to select the second and fourth row of X. Now, we'll use the same array to select the second and fourth column of X.

32
00:05:21,379 --> 00:05:35,135
NumPy also offers built-in functions to select specific elements within NumPy arrays. For example, NumPy's diag function can extract elements along the diagonal of an array.

33
00:05:35,134 --> 00:05:47,204
Remember, X was this. We can also print the elements above the main diagonal of X by setting a parameter, K equal to one.

34
00:05:47,204 --> 00:05:59,474
This grabs the elements one, seven,13, and 19. If K is a negative number, this will grab the values below the main diagonal, five,11, and 17.

35
00:05:59,475 --> 00:06:10,069
By default, K is zero, which is why it gets the main diagonal. It's also often useful to extract only the unique elements in a NumPy array.

36
00:06:10,069 --> 00:06:19,970
We can find the unique elements in an array by using NumPy's Unique function. Here's a three by three array with repeated values.


@@@
1
00:00:00,000 --> 00:00:12,059
Up to now we've seen how to make slices and select elements of an NumPy array using indices. This is useful when we know the exact indices of the elements we want to select.

2
00:00:12,060 --> 00:00:29,640
However, there are many situations in which we don't know the indices of the elements we want. For example; Suppose we have a 10,000 by 10,000 array of random integers ranging from one to 15,000 and we only want to select integers that are less than 20.

3
00:00:29,640 --> 00:00:38,870
Boolean indexing can help us in these cases by helping us select elements using logical arguments instead of explicit indices.

4
00:00:38,869 --> 00:00:51,050
Let's see some examples. Consider this five by five array ranging from zero to 24. We can use boolean indexing to select elements greater than 10.

5
00:00:51,049 --> 00:00:59,899
Like this. Instead of indices, we are using a boolean expression. Let's also get the elements that are less than or equal to seven.

6
00:00:59,899 --> 00:01:12,524
And now both greater than seven and less than 17. We can use boolean indexing to assign the elements that are between 10 and 17 to the value of negative one.

7
00:01:12,525 --> 00:01:22,130
In addition to Boolean indexing, NumPy also allows for set operations. This is useful when comparing two NumPy arrays.

8
00:01:22,129 --> 00:01:33,109
For example, to find common elements. Consider these two rank one arrays. We can create arrays for the intersection, difference, and union.

9
00:01:33,109 --> 00:01:42,975
Like this. We can also sort NumPy arrays. Let's use NumPy sort function to sort rank one and rank two arrays in different ways.

10
00:01:42,974 --> 00:01:53,550
Like with other functions we saw before, the sort function can also be used as a method. However, there's a big difference on how the data is stored in memory in this case.

11
00:01:53,549 --> 00:02:02,189
When sort is used as a function, it sorts the NumPy array out of place, meaning they don't change the original array.

12
00:02:02,189 --> 00:02:10,049
However, when you use sort as a method, the array is sorted in place, meaning the original array is changed.

13
00:02:10,050 --> 00:02:20,980
Let's create an unsorted rank one array. We can sort x using sort as a function. This will sort x out of place and leave the original array as is.

14
00:02:20,979 --> 00:02:32,465
As you can see Numpy.sort did sort the x array, but x itself did not change. Notice that this sorts the array and leaves repeating values.

15
00:02:32,465 --> 00:02:38,620
If you want to sort only the unique elements in x, you can combine it with a unique function like this.

16
00:02:38,620 --> 00:02:50,975
Now, let's see how we can sort arrays in place by using sort as a method. Here's x again. If we sort x like this, we will see that this affects the original x and sorts it.

17
00:02:50,974 --> 00:02:57,754
When sorting rank two arrays, we need to tell the sort function whether we are sorting by rows or by columns.

18
00:02:57,754 --> 00:03:14,130
This is done by using the keyword axis. Here is an unsorted rank two array. We can sort x by rows like this, which you can see here or we can sort X by columns like this, which you can see up here.


@@@
1
00:00:00,000 --> 00:00:10,740
Let's see how NumPy does arithmetic operations on arrays. NumPy allows element-wise operations, as well as matrix operations.

2
00:00:10,740 --> 00:00:18,955
In this video, we will only be looking at element-wise operations. Consider these two rank one arrays.

3
00:00:18,954 --> 00:00:29,195
We can perform basic element-wise operations using arithmetic symbols or functions. Both of these forms will do the same operation.

4
00:00:29,195 --> 00:00:38,210
The only difference is that if you use this function approach, the functions usually have options that you can tweak using keywords and methods.

5
00:00:38,210 --> 00:00:46,065
Let's also try element-wise subtraction, multiplication, and division. Again, these also have the function approach.

6
00:00:46,064 --> 00:01:02,149
In order to complete these operations, NumPy sometimes uses something called broadcasting. Broadcasting is a term used to describe how NumPy handles element-wise arithmetic operations with arrays of different shapes.

7
00:01:02,149 --> 00:01:13,364
An important thing to note is that, since we are doing element-wise operations, the arrays being operated on must have the same shape or be broadcastable.

8
00:01:13,364 --> 00:01:21,475
We'll talk more about this in a minute. Let's perform the same element-wise arithmetic operations on rank two arrays.

9
00:01:21,474 --> 00:01:31,118
Again, remember that in order to do these operations, the arrays being operated on must have the same shape or be broadcastable.

10
00:01:31,118 --> 00:01:40,055
Here are two two-by-two matrices. Let's do the same arithmetic operations using the symbol notation.

11
00:01:40,055 --> 00:01:47,355
We can also apply mathematical functions such as square root to all the elements of an array at once.

12
00:01:47,355 --> 00:01:56,549
Here's that rank one array we were using before. We can get the square root of each element like this, and the exponential of each like this.

13
00:01:56,549 --> 00:02:04,590
And also, each element to the power of two. Another great feature of NumPy is its statistical functions.

14
00:02:04,590 --> 00:02:11,375
Statistical functions such as mean, provide us with statistical information about the elements in an array.

15
00:02:11,375 --> 00:02:22,704
Let's see some examples. Here is that rank two array again. We can get the average of the matrix like this, as well as the averages of individual rows and columns like this.

16
00:02:22,705 --> 00:02:31,629
We can do the same for statistics like the sum, and others like the standard deviation, median, maximum, and minimum.

17
00:02:31,629 --> 00:02:40,425
Finally, let's see how NumPy can add single numbers to all the elements of a NumPy array without the use of complicated loops.

18
00:02:40,425 --> 00:02:48,150
Here's our rank two array again. We can add three to each element like this, and subtract three from each like this.

19
00:02:48,150 --> 00:03:01,055
Multiply each by three, and divide each by three. In the examples above, NumPy is working behind the scenes to broadcast three along the X array so that they have the same shape.

20
00:03:01,055 --> 00:03:12,710
This allows us to add three to each element of X in just one line of code. Subject to certain constraints, NumPy can do the same for two NumPy arrays of different shapes.

21
00:03:12,710 --> 00:03:31,189
Consider this three-by-three array Y, and this one-by-three array X. If we do Y plus X, this adds zero, to the first column in Y, one, to the second column in Y, and two, to the third column in Y.

22
00:03:31,189 --> 00:03:44,179
As before, NumPy is able to add a one-by-three array to a three-by-three array by broadcasting the smaller array along the big array, so that they have compatible shapes.

23
00:03:44,180 --> 00:03:57,155
In general, NumPy can do this provided that the smaller array, such as the one-by-three array, can be expanded to fit the shape of the larger array, so that the resulting broadcast is unambiguous.

24
00:03:57,155 --> 00:04:15,530
We can do the same with a three-by-one array. This adds zero, the first row of Y, one, to the second row of Y, and two, to the third row of Y. Checkout the NumPy documentation for more information on broadcasting and its rules.


@@@
1
00:00:00,000 --> 00:00:12,234
Pandas is a powerful tool for data analysis and manipulation. If you remember from the last lesson, this package is built on top of NumPy which makes it very fast and efficient.

2
00:00:12,234 --> 00:00:20,420
In this lesson, we will go over the two main data structures in Pandas. The Pandas series and the Panda's dataframe.

3
00:00:20,420 --> 00:00:29,260
Let's start off by learning about the Pandas series object, and how to create one. When importing Pandas, use a standard alias, pd.

4
00:00:29,260 --> 00:00:37,924
Let's create a series containing grocery items. To access the series object, we just type pd.Series with a capital S.

5
00:00:37,924 --> 00:00:45,669
A panda series is a one-dimensional array-like object that can hold many data types, such as numbers and strings.

6
00:00:45,670 --> 00:00:59,204
This is different from a NumPy array which can only hold one data type. Another big difference between a Pandas series and a NumPy array is that you can assign an index label to each element in the Pandas series.

7
00:00:59,204 --> 00:01:11,369
Here, we pass two arguments, the data and the indices. For our grocery series and we will use food names as index labels and the quantities we need to buy as our data.

8
00:01:11,370 --> 00:01:19,525
We can see that a Pandas series is displayed with the indices in the first column, and the data in the second column.

9
00:01:19,525 --> 00:01:29,409
Notice that the data is not indexed zero to three, but rather with the names of the foods that we put in, eggs, apples, milk, and bread.

10
00:01:29,409 --> 00:01:36,805
Also notice that the data in our Pandas series has both integers and strings, just like NumPy arrays.

11
00:01:36,805 --> 00:01:43,355
Pandas series have attributes that allow us to get information from them in an easy way. Let's see some of them.

12
00:01:43,355 --> 00:01:54,849
Shape gives us the sizes of each dimension of the data, ndim gives us the number of dimensions of the data, and size gives us the total number of values in the array.

13
00:01:54,849 --> 00:02:04,414
We can also print the index labels and the data of the Pandas series separately. This is useful if you don't happen to know what the index labels of a series are.

14
00:02:04,415 --> 00:02:19,625
This gives us the index labels of the series object, and this gives us the data into series object. If you're dealing with a very large Pandas series, and you're unsure whether an index label exists, you can always check using the In command.

15
00:02:19,625 --> 00:02:28,130
This let's us know that bananas is not one of the index labels in the grocery series, and this tells us that bread is.


@@@
1
00:00:00,000 --> 00:00:08,875
In the last video, we created this Panda series of a grocery list. Now, how do we access or modify its elements?

2
00:00:08,875 --> 00:00:19,344
One great advantage of the series object, is that it allows us to access data in multiple ways. One way is accessing elements with their index labels.

3
00:00:19,344 --> 00:00:30,109
This accesses the quantity of eggs using the eggs label in square brackets. We can get multiple elements by providing a list of index labels.

4
00:00:30,109 --> 00:00:38,074
Another way to access elements is numeric indices, very similar to how we access elements in NumPy arrays.

5
00:00:38,075 --> 00:00:47,689
Here, we get the quantity of the first item, eggs, using zero as our index. Now, let's get the last element with the index negative one.

6
00:00:47,689 --> 00:01:07,745
And again, we can grab multiple items using a list of numerical indices. In order to remove any ambiguity from rather we're referring to an index label, or a numerical index, Panda series have two attributes loc, and iloc, to explicitly state what we mean.

7
00:01:07,745 --> 00:01:15,515
The attribute loc stands for a location, and it's used to explicitly state that we're using a labelled index.

8
00:01:15,515 --> 00:01:25,635
Similarly the attribute iloc, stands for integer location, and is used to explicitly state that we are using a numerical index.

9
00:01:25,635 --> 00:01:34,555
Panda series are also mutable like NumPy arrays, which means we can change the elements of a series after it's been created.

10
00:01:34,555 --> 00:01:40,795
Let's change the number of eggs we need to buy from our grocery list. Let's see our grocery list again.

11
00:01:40,795 --> 00:01:51,364
We can change the number of eggs from 30 to two by reassigning the element like this. Now, we can see that the data for eggs has been modified.

12
00:01:51,364 --> 00:02:01,834
We can also delete items from a Panda series using the drop method. This method removes an element with the given label from the Panda series.

13
00:02:01,834 --> 00:02:11,789
As you can see, apples is no longer included in the series returned by this method. However, this drops elements from the series out of place.

14
00:02:11,789 --> 00:02:20,194
Meaning, up here, this just returned the modified series, and didn't actually change the original one as you can see here.

15
00:02:20,194 --> 00:02:26,969
We can make this happen inplace, and change the original series by setting the parameter inplace to true.

16
00:02:26,969 --> 00:02:34,400
Now, notice the drop method modified the actual series instead of returning another series with the modification.


@@@
1
00:00:00,000 --> 00:00:07,620
Just like we did with NumPy arrays, we can perform element-wise arithmetic operations on Pandas series.

2
00:00:07,620 --> 00:00:18,934
In this video, we will look at arithmetic operations between Pandas series and single numbers. Let's create a new series that holds a grocery list of fruits.

3
00:00:18,934 --> 00:00:30,029
The first argument we pass in is the data, and the second argument is the index labels. We can modify the data in fruits by performing basic arithmetic operations.

4
00:00:30,030 --> 00:00:45,105
We can add two to each element in fruits, subtract two, multiply by two, and divide by two. We can also apply mathematical functions from NumPy such as square root to all the elements of a series.

5
00:00:45,104 --> 00:00:53,419
Let's import NumPy and take a look at our fruit series again. Using NumPy, we can get the square root of each element like this.

6
00:00:53,420 --> 00:01:05,275
And the exponential of each element, and each element to the power of two. Pandas also allows us to apply arithmetic operations on selected items in a series.

7
00:01:05,275 --> 00:01:15,819
Here's the fruit series again. We can add two to just the banana's item, like this. And let's subtract two from apples using its numerical index.

8
00:01:15,819 --> 00:01:34,160
We can double the apples and oranges like this, and divide apples and oranges by two like this. You can also apply arithmetic operations on a Pandas series of mixed data types provided that the arithmetic operation is defined for all data types in the series.

9
00:01:34,159 --> 00:01:46,579
To demonstrate this, let's go back to our grocery's list from the previous video, and let's multiply this series by two, since the multiplication operation is defined for both strings and numbers.

10
00:01:46,579 --> 00:02:01,105
This code doesn't return an error. Multiplying a string by two simply repeats it. If you were to apply an operation that was valid for numbers but not strings, for instance division, you would get an error.

11
00:02:01,105 --> 00:02:11,599
So, when you have mixed data types in your Pandas series, make sure the arithmetic operations you use are defined for all the data types in your series.


@@@
1
00:00:00,000 --> 00:00:11,974
The second main data structure in Pandas is a DataFrame, which is a two-dimensional object with labeled rows and columns and can also hold multiple data types.

2
00:00:11,974 --> 00:00:23,214
If you're familiar with Excel, you can think of a DataFrame as a really powerful spreadsheet. We can create Pandas DataFrames manually or by loading data from a file.

3
00:00:23,214 --> 00:00:34,340
We will start by creating a DataFrame manually from a dictionary, containing several pandas series. Let's create that dictionary and then pass it into Pandas DataFrame function.

4
00:00:34,340 --> 00:00:46,399
Here's one that contains the shopping carts of two people, Alice and Bob on an online store. Each series contains the price of the items and is labeled with the item names.

5
00:00:46,399 --> 00:00:56,609
And let's confirm the items is of the datatype dictionary. Now that we have a dictionary, we are ready to create a DataFrame by passing it to the DataFrame function.

6
00:00:56,609 --> 00:01:04,614
Remember, when using the DataFrame function, capitalize the D and F in DataFrame. There are several things to notice here.

7
00:01:04,614 --> 00:01:14,515
First, we see that DataFrames are displayed in a tabular form, much like a spreadsheet with the labels of the rows and columns in bold.

8
00:01:14,515 --> 00:01:28,379
Also notice that the row labels of the DataFrame are built from the union of the index labels we provided in the series, and the column labels of the DataFrame are taken from the keys of the dictionary.

9
00:01:28,379 --> 00:01:39,549
The columns are arranged alphabetically and not in the order given by the dictionary. Later, we will see that this is not the case when we load data into a DataFrame from a file.

10
00:01:39,549 --> 00:01:53,210
Lastly, notice the NaN values that appeared in a DataFrame. NaN stands for not a number, and is Pandas way of indicating that it doesn't have a value for this particular row and index.

11
00:01:53,209 --> 00:02:06,020
For example, if we look at the column Alice, we see that it has NaN in the watch index. This is because the dictionary over here didn't have an item for Alice called watch.

12
00:02:06,019 --> 00:02:14,485
Whenever a DataFrame is created, if a particular column doesn't have values for a particular index, Pandas will put a NaN there.

13
00:02:14,485 --> 00:02:21,104
If we were to feed this data into a machine-learning algorithm, we would have to remove these NaN values first.

14
00:02:21,104 --> 00:02:30,110
In a later video, we will learn how to deal with Nan values and clean our data. For now, we will leave these values in our dataframe.

15
00:02:30,110 --> 00:02:37,710
In this example, we created a Pandas DataFrame from a dictionary of pandas series that had clearly defined index labels.

16
00:02:37,710 --> 00:02:44,945
If we don't provide index labels however, Pandas will use numerical row indices when it creates the DataFrame.

17
00:02:44,944 --> 00:02:56,924
Let's create the same dictionary without the index labels. We can see that pandas indexes the rows of the DataFrame starting from zero, just like NumPy indexes its arrays.

18
00:02:56,925 --> 00:03:03,785
Like we did with the pandas series, we can also extract information from a DataFrame using attributes.

19
00:03:03,784 --> 00:03:18,599
Let's print some information on our shopping carts DataFrame from earlier. We can get the index labels, column labels and data from our dataframe with these attributes, and we can use the same attributes to get information about its shape.

20
00:03:18,599 --> 00:03:32,260
This dataframe has two dimensions with five rows and two columns, making a total size of 10. When creating the shopping carts DataFrame, we pass the whole items dictionary to the DataFrame function.

21
00:03:32,259 --> 00:03:44,544
However, there might be cases where you're only interested in a subset of the data. Pandas let's us select which data we want to put in our DataFrame with the keywords, column and index.

22
00:03:44,544 --> 00:03:55,164
Let's see some examples. Here's a DataFrame that only loads Bob's shopping cart, and here's one that only has selected items for both Alice and Bob.

23
00:03:55,164 --> 00:04:06,200
And this is one that only has selected items from Alice's shopping cart. You can also manually create DataFrames from a dictionary of lists or arrays.

24
00:04:06,199 --> 00:04:13,934
The procedure is the same as before, we start by creating the dictionary and then pass it into the DataFrame function.

25
00:04:13,935 --> 00:04:23,550
In this case however, all the lists or arrays in a dictionary must be of the same length. Here's a dictionary of integers and floats.

26
00:04:23,550 --> 00:04:34,050
Notice that since the data dictionary we created doesn't have index labels, Pandas automatically uses numerical row indices when it creates the DataFrame.

27
00:04:34,050 --> 00:04:47,034
We can however add these labels by using the index keyword in the DataFrame function. The last method we'll look at for manually creating Pandas DataFrames is using a list of Python dictionaries.

28
00:04:47,035 --> 00:04:56,884
Here's an example, again we don't have index labels. So Panda put numerical row indices here, and let's assume we're going to use this DataFrame.

29
00:04:56,884 --> 00:05:04,870
to hold the number of items a particular store has in stock. We'll rename the index labels to store 1 store 2.


@@@
1
00:00:00,000 --> 00:00:12,925
We can access elements in a DataFrame in different ways. In general, we can access rows, columns, or individual elements by using the row and column labels.

2
00:00:12,925 --> 00:00:27,269
Let's see some examples. Here's a DataFrame we created in the last video. We can access the bikes column using the column label, like this, and use a list of the column labels to access multiple columns.

3
00:00:27,269 --> 00:00:35,034
We can access a row using the row index label, and the value at a specific row and column like this.

4
00:00:35,034 --> 00:00:45,785
It's important to know that when accessing individual elements in a DataFrame, like in this last example, the column label always comes first, and then the row label.

5
00:00:45,784 --> 00:00:54,575
If you provide the row label first, you'll get an error. We can also modify our DataFrames by adding rows or columns.

6
00:00:54,575 --> 00:01:05,344
Suppose we decided to add a shirts column to our DataFrame, to do this, we can define our shirts column containing the quantity for each of the two store rows like this.

7
00:01:05,344 --> 00:01:15,159
This added a new column to the end of our DataFrame. We can also add new columns using arithmetic operations on other columns of our DataFrame.

8
00:01:15,159 --> 00:01:22,614
For example, we can create a new column called suits by computing the sum of the shirts and pants column.

9
00:01:22,614 --> 00:01:38,000
Suppose now, that you opened a new store and you want to add that as a row on your DataFrame. To add rows to our DataFrame, we first have to create a new DataFrame with those rows, and then append it to the original DataFrame.

10
00:01:38,000 --> 00:01:45,145
Let's see how this works. Here is a dictionary of the items in our new store that we will use to create a DataFrame.

11
00:01:45,144 --> 00:01:55,939
We can now add this row to our store items DataFrame using the append method. After appending to the DataFrame, the columns have been placed in alphabetical order.

12
00:01:55,939 --> 00:02:02,800
We can also add new columns to our DataFrame by selecting data that already exists in our DataFrame.

13
00:02:02,799 --> 00:02:19,479
For example, let's say you want to stock stores two and three with new watches. So you decide to add a new column called new watches to your DataFrame, and say you want the quantity of these new watches to be the same as the watches already in stock.

14
00:02:19,479 --> 00:02:30,185
Since we only want the stock stores two and three, we can index them like this. It is also possible to insert new columns into the DataFrame anywhere we want.

15
00:02:30,185 --> 00:02:42,290
The insert method allows us to specify the location, label, and data of the column we want to add. Let's use this to add a new column called shoes right before the suits column.

16
00:02:42,289 --> 00:02:54,780
The first argument is loc or location, we put five. So, 0, 1, 2, 3, 4, 5. So, we actually inserted the shoes column before the watches column.

17
00:02:54,780 --> 00:03:03,349
The second argument is the label, and the last is the data. In addition to adding rows and columns, we can also delete them.

18
00:03:03,349 --> 00:03:16,395
We can use the pop and drop methods to do this. The pop method allows us to delete columns while the drop method can be used to delete both rows and columns by using the axis keyword.

19
00:03:16,395 --> 00:03:24,060
Let's see some examples. We can delete the new watches column using pop which is used to remove columns.

20
00:03:24,060 --> 00:03:32,939
Now, we will move the watches and shoes columns with drop, which can be used to remove columns if we set axis to one.

21
00:03:32,939 --> 00:03:44,694
We'll use that same method to remove the rows store two and store one by setting axis to zero. Sometimes, we might need to change the row and column labels.

22
00:03:44,694 --> 00:03:56,750
Let's change the bikes column label to hats using the rename method. This takes in a dictionary with original label as keys and the new labels as values.

23
00:03:56,750 --> 00:04:05,060
We can see the bikes has been renamed to hats. Now, let's change the row label using the rename method.

24
00:04:05,060 --> 00:04:14,594
We can change the index label of store three to last store like this. Notice here, we set the index parameter to the dictionary.

25
00:04:14,594 --> 00:04:24,404
Whereas in the previous cell, we set columns to the dictionary. We can also set the index to be one of the existing columns in the DataFrame, like this.


@@@
1
00:00:00,000 --> 00:00:06,824
Before we can begin analyzing data or using it to train our learning algorithms, we need to clean it.

2
00:00:06,825 --> 00:00:22,960
This means we need a way to detect and correct errors in our data. While any given dataset can have many types of bad data such as outliers or incorrect values, the type of bad data we encounter almost always is missing values.

3
00:00:22,960 --> 00:00:34,259
As we saw earlier, Pandas assigns the value NaN, or not a number to missing data. In this section, we will learn how to detect and deal with these NaN values.

4
00:00:34,259 --> 00:00:53,819
In this DataFrame, we have three NaN values. One in store one, and two in store three. However, in cases where we load very large datasets into a DataFrame possibly with millions of items, the number of NaN values isn't easily visualized like this.

5
00:00:53,820 --> 00:01:07,935
For these cases, we can use a combination of methods to count the number of NaN values in our data. This combines to is null and sum methods to count the number of NaN values in our DataFrame. Let's break this down.

6
00:01:07,935 --> 00:01:16,500
The is null method returns a DataFrame with a Boolean for each value in the store items DataFrame. True, when the value is NaN.

7
00:01:16,500 --> 00:01:25,640
In Pandas, logical true values have the numerical value one and logical false values have the numerical value zero.

8
00:01:25,640 --> 00:01:32,609
Therefore, we can count the number of NaN values by counting the number of logical true values from this DataFrame.

9
00:01:32,609 --> 00:01:42,055
Using sum ones gives us a number of NaNs in each column. We use sum again to get the total number of NaNs for the entire DataFrame.

10
00:01:42,055 --> 00:01:51,085
Instead of counting the number of NaN values, we can also do the opposite and count the number of non-NaN values with the count method.

11
00:01:51,084 --> 00:01:58,469
Now that we know how to find out if our dataset has any NaN values, the next step is to decide what to do with them.

12
00:01:58,469 --> 00:02:07,424
In general, we have two options. We can either remove or replace missing values. To remove them, we can use the drop na method.

13
00:02:07,424 --> 00:02:19,465
We can set the axis parameter to zero to eliminate any rows with NaN values. We can also set axis to one to eliminate any columns with NaN values.

14
00:02:19,465 --> 00:02:27,025
The dropna method drops these rows or columns out of place. Meaning, the original DataFrame is not modified.

15
00:02:27,025 --> 00:02:37,960
You can remove them in-place by setting the keyword in-place to true like this. Now, instead of eliminating NaN values, let's replace them with actual values.

16
00:02:37,960 --> 00:02:46,164
As an example, we could choose to replace all NaNs with the value zero. The fillna method can be used for this.

17
00:02:46,164 --> 00:02:53,859
We can also replace NaNs with values from the previous row or column with something called forward filling.

18
00:02:53,860 --> 00:03:02,254
This replaces each NaN with the value from the previous value along the given axis. Here's the original DataFrame again.

19
00:03:02,254 --> 00:03:09,354
Notice that the two NaN values in store three have been replaced with the previous values in their column.

20
00:03:09,354 --> 00:03:20,805
However, notice that the NaN value in store one did not get replaced. That's because there were no previous values in this column since NaN is the first value in this column.

21
00:03:20,805 --> 00:03:32,804
However, if we do forward fill using the previous row values this won't happen. We see that in this case, all the NaN values have been replaced with the previous value in that row.

22
00:03:32,805 --> 00:03:41,869
Similarly, you can choose to replace the NaNs with the values that go after them in the DataFrame. This is known as backward filling.

23
00:03:41,870 --> 00:03:51,360
This replaces each NaN with the next value in that column. The NaN value in store one has been replaced by the next value in it's column.

24
00:03:51,360 --> 00:04:01,170
But the two NaN values in store three did not since they are the last values in their columns. The fillna method fills the NaN values out of place.

25
00:04:01,169 --> 00:04:11,560
So, set the parameter in place to true if you want to modify the original DataFrame. We can also choose to replace NaNs by using different interpolation methods.

26
00:04:11,560 --> 00:04:19,129
For example, this uses linear interpolation to replace NaN values using the values along the column axis.

27
00:04:19,129 --> 00:04:34,530
The two NaN values in store three have been replaced with linear interpolated values. However, NaN value in store one did not get replaced since there is no data before it to allow the interpolation function to calculate a value.

28
00:04:34,529 --> 00:04:45,600
We can replace NaN values with linear interpolation using row values like this. Like the other methods we saw, the interpolate method replaces NaN values out of place.


@@@
1
00:00:00,000 --> 00:00:10,765
When working with data you'll most likely use databases from many sources. Pandas allows us to load databases of different formats into DataFrames.

2
00:00:10,765 --> 00:00:23,659
One of the most popular formats used to store data is CSV or comma separated values. We can load CSV files into DataFrames using the read CSV function.

3
00:00:23,660 --> 00:00:37,230
Let's load Google stock data into a DataFrame. This file was taken from Yahoo finances and contains Google stock data from August 19th, 2004 to October 13th, 2017.

4
00:00:37,229 --> 00:00:52,184
The DataFrame consists of 3,313 rows and seven columns. Let's look at the stock data. We can see it's a pretty large dataset and that Pandas automatically assign numerical row indices to the DataFrame.

5
00:00:52,185 --> 00:01:06,670
Pandas also use the names that appeared in the CSV file to assign the column labels. When dealing with large data sets like this one, it's often useful to just take a look at the first few rows of data instead of the whole dataset.

6
00:01:06,670 --> 00:01:15,659
We can take a look at the first five rows using the head method. We can also take a look at the last five rows of data using the tail method.

7
00:01:15,659 --> 00:01:22,039
Both of these accept an integer as an argument if you want to specify a different number of rows to return.

8
00:01:22,040 --> 00:01:32,304
For example, I can get the last eight rows like this, or the first two rows like this. Let's do a quick check to see whether we have any none values in this data set.

9
00:01:32,305 --> 00:01:40,445
To do so, we will use the "Is no" method followed by the "Any" method to check whether any of the columns contain none values.

10
00:01:40,444 --> 00:01:48,750
This shows us that we have no missing data. When dealing with large data sets, it's often useful to get statistical information from them.

11
00:01:48,750 --> 00:01:55,655
Pandas provides a described method which returns descriptive statistics on each column of the DataFrame.

12
00:01:55,655 --> 00:02:07,815
If desired, we can apply the describe method on a single column like this. Similarly, you can also look at one statistic by using one of the many statistical functions that Pandas provides.

13
00:02:07,814 --> 00:02:18,704
Here are some examples. This gives us the maximum value in each column, and this gives us the mean of each column, and this gives us the minimum value in a specific column.

14
00:02:18,705 --> 00:02:26,879
Another important statistical measure is data correlation. We can use the core method to get the correlation between different columns.

15
00:02:26,879 --> 00:02:33,814
Lastly, let's look at the group by method, which allows us to group data to get different types of information.

16
00:02:33,814 --> 00:02:44,229
For the following examples, we are going to load data about a fake company. This data contains information for the years 1990 through 1992.

17
00:02:44,229 --> 00:02:51,790
And for each year, we see the name of the employees, the department they worked for, their age, and their annual salary.

18
00:02:51,789 --> 00:02:59,699
Now, let's use the group by method to get information. Let's calculate how much money the company spent on salaries each year.

19
00:02:59,699 --> 00:03:07,289
To do this, we will group the data by year and then add up the salaries of all of the employees with the sum method.

20
00:03:07,289 --> 00:03:25,809
The company spent a total of $150,000 in 1990, $162,000 in 1991, and $174,000 in 1992. If we want to know the average salary for each year, we can repeat the last step replacing the sum method with the mean method.

21
00:03:25,810 --> 00:03:37,504
Now, let's see how much in total each employee got paid during those five years. In this case, we will group the data by name and then add up the salaries for each year.

22
00:03:37,504 --> 00:03:49,020
Now let's see what the salary distribution per department was per year. In this case, we will group the data by year and by department and then add up the salaries for each group.

23
00:03:49,020 --> 00:03:59,325
We can see that in 1990, the Admin department paid $55,000 in salaries, HR $50,000 and R and D, $48,000.


@@@
1
00:00:00,000 --> 00:00:08,550
Statistics is at the core of analyzing data. For the stats portion of this class, you'll be learning from Sebastian Thrun and Josh Bernhard.

2
00:00:08,550 --> 00:00:15,705
Sebastian is a statistician and Stanford faculty member, as well as founder of Udacity and Google X.

3
00:00:15,705 --> 00:00:30,710
He'll be showcasing a number of examples for each of the statistical topics covered. Josh who is also a statistician teacher, has taught statistics at the University of Colorado and previously was working as a machine learning engineer for KPMG.

4
00:00:30,710 --> 00:00:36,780
He'll be working alongside Sebastian to assure you can implement the statistical applications in Python.


@@@
1
00:00:00,000 --> 00:00:09,605
A quick overview leading up to your first project. We will start with an overview of data types and the most common statistics used when analyzing data.

2
00:00:09,605 --> 00:00:17,910
We'll discuss measures of center and spread. The most common shapes that data takes on and how to handle outliers.

3
00:00:17,910 --> 00:00:32,310
You will take this farther by using spreadsheets to handle these calculations for you. You'll learn how to build visuals to better convey your message as well as how to use these features of spreadsheets to take your data game to a whole new level.


@@@
1
00:00:00,000 --> 00:00:11,004
The word "data" is defined as distinct pieces of information. You may think of data as simply numbers on a spreadsheet, but data can come in many forms.

2
00:00:11,005 --> 00:00:20,244
From text to video to spreadsheets and databases to images to audio, and I'm sure I'm forgetting many other forms.

3
00:00:20,245 --> 00:00:29,425
Utilizing data is the new way of the world. Data is used to understand and improve nearly every facet of our lives.

4
00:00:29,425 --> 00:00:36,020
From early disease detection to social networks that allow us to connect and communicate with people around the world.

5
00:00:36,020 --> 00:00:46,809
No matter what field you're in, from insurance and banking, to medicine, to education, to agriculture,, to automotive, to manufacturing, and so on.

6
00:00:46,810 --> 00:00:56,259
You can utilize data to make better decisions and accomplish your goals. We will be getting you started on the right foot to using your data in this course.


@@@
1
00:00:00,020 --> 00:00:06,224
In this video, we'll be taking a look at the different data types that exists in the world around us.

2
00:00:06,224 --> 00:00:12,814
When sitting at coffee shops, I enjoy watching the dogs pass. I often wonder, how many crossed my path?

3
00:00:12,814 --> 00:00:23,170
I wonder if more pass on weekdays or weekends. Maybe the number differs from Mondays to Tuesdays. I also pay attention to the breeds of the dogs.

4
00:00:23,170 --> 00:00:29,054
I wonder if more collies stopped by on Monday than on Wednesday. I wonder what's the most common breed.

5
00:00:29,054 --> 00:00:37,259
Is that breed the most common at all coffee shops? If I walked across the street to my favorite breed, would the most common breed change?

6
00:00:37,259 --> 00:00:45,870
This introduces two main data types: quantitative data, like the number of dogs, and categorical data, like the breed.

7
00:00:45,869 --> 00:00:56,050
Quantitative data takes on numeric values that allow us to perform mathematical operations. In the previous example, we saw this with the number of dogs.

8
00:00:56,049 --> 00:01:03,479
If I see five dogs on Monday, and six dogs on Tuesday, I've seen a total of 11 dogs so far this week.

9
00:01:03,479 --> 00:01:12,059
Alternatively, categorical data frequently are used to label a group or set of items. We saw this with the breeds of the dogs.


@@@
1
00:00:00,000 --> 00:00:09,870
We can divide categorical data types further into categorical ordinal and categorical nominal. First, let's look at categorical ordinal data.

2
00:00:09,869 --> 00:00:19,589
Remember those dogs at the coffee shop? Let's say I give each a rating of how nice it is to me. Sometimes I shake hands with the dog and we become best friends.

3
00:00:19,589 --> 00:00:31,000
Other times, it pees on my shoe. I rate these interactions from very positive to very negative. These ranked categories are known as categorical ordinal data.

4
00:00:31,000 --> 00:00:38,119
Notice, this is different than the breeds, which are known as categorical nominal data, and do not have a rank order.


@@@
1
00:00:00,000 --> 00:00:09,955
We can also divide quantitative data types further. I assume most of my positive interactions occur with older dogs, as they've had more time to train.

2
00:00:09,955 --> 00:00:18,744
The age of a dog is a continuous quantitative data type, while the number of dogs I interact with is a discrete quantitative data type.

3
00:00:18,745 --> 00:00:26,170
In the world, we kind of make all data discrete, so it can be difficult to see the difference between discrete and continuous.

4
00:00:26,170 --> 00:00:33,700
Continuous data can take on any numeric value including decimal values, and sometimes even negative numbers.

5
00:00:33,700 --> 00:00:44,524
The age of a dog in this situation is an example of continuous data, as we could split this variable into smaller and smaller pieces, and still something exists.

6
00:00:44,524 --> 00:00:55,405
For example, we could talk about age in terms of years, or months, or days, or hours, or minutes, or seconds, and still there are units that are smaller.

7
00:00:55,405 --> 00:01:03,220
This is true of continuous data. However, discrete data like the number of dogs, only takes on countable values.


@@@
1
00:00:00,000 --> 00:00:09,855
To summarize, we have two main data types, each with two subgroups. Quantitative data can be divided into continuous and discrete.

2
00:00:09,855 --> 00:00:23,054
Categorical can be split into nominal and ordinal. Identifying data types is important, as it allows us to understand the types of analyses that we can perform and the plots that we can build.

3
00:00:23,054 --> 00:00:31,800
Before we dive deeper, check your understanding. There are quizzes available in the next module to assure that you've mastered the concepts we've introduced.


@@@
1
00:00:00,000 --> 00:00:14,199
In the next lessons, we will discuss how to use statistics to describe quantitative data. You will gain insight into a process of how data is collected and how to answer questions using your data.

2
00:00:14,199 --> 00:00:22,979
Throughout this lesson, I hope you learn to be critical of your analysis that happened under the hood and what the numbers actually mean.

3
00:00:22,980 --> 00:00:31,765
As an example of an analysis that we do here at Udacity, we look at how long a nanodegree program takes students to complete.

4
00:00:31,765 --> 00:00:44,030
We try to provide an estimate of the number of months or hours that students will spend. One way we might start is by reporting the average amount of time it takes to complete the nanodegree program.

5
00:00:44,030 --> 00:00:53,149
But that doesn't tell the whole story. I'm sure there are differences in completion depending on what students knew before entering the program.

6
00:00:53,149 --> 00:01:01,714
The shortest amount of time needed to complete the nanodegree program might just be a few weeks. How did those people complete the course so fast?

7
00:01:01,715 --> 00:01:09,284
Well, the longest might be a couple of years. What proportion of students finish fast within two months?

8
00:01:09,284 --> 00:01:18,510
What proportion take longer than eight months? Using a variety of measures, like measures of center, give you an idea of the average student.

9
00:01:18,510 --> 00:01:30,689
Measures of spread give you an idea of how students differ. Visuals can provide us a more complete picture of how long it takes any student to complete a program.

10
00:01:30,689 --> 00:01:39,000
The material in the next sections will show you how to use these measures in a way that is informative and understandable to others.


@@@
1
00:00:00,420 --> 00:00:08,995
When analyzing both discrete and continuous quantitative data, we generally discuss four main aspects.

2
00:00:08,994 --> 00:00:18,219
The center, the spread, the shape and outliers. In this lesson, we will focus on measures of center.

3
00:00:18,219 --> 00:00:32,829
There are three widely accepted measures of center. The mean, the median and the mode. To illustrate how each of these measures is calculated, consider this table of the number of dogs I see at the coffee shop in a week.

4
00:00:32,829 --> 00:00:43,350
From the table, we can see that on Monday, I saw five dogs. On Tuesday, I saw three dogs. On Wednesday, I saw eight dogs and so on.

5
00:00:43,350 --> 00:00:51,979
A friend might ask you, how many dogs would you expect to see on any given day? We might choose to respond to this in a lot of different ways.

6
00:00:51,979 --> 00:01:02,750
Like, it depends on the day or it depends on the week. But commonly, the word expect is associated with the mean or the average of our data set.

7
00:01:02,750 --> 00:01:10,099
The mean is calculated as the sum of all of the values in our data set divided by how many data points we have.

8
00:01:10,099 --> 00:01:21,969
As you can see calculated here, this value is 12.57 dogs. That is, the sum of the number of dogs observed on each day divided by the number of days in a week.

9
00:01:21,969 --> 00:01:32,385
The mean isn't always the best measure of center. For this data set, you can see that the mean doesn't really seem like it's in the middle of the data at all.

10
00:01:32,385 --> 00:01:45,000
There are only two of the seven days that have recorded more dogs than the reported mean. It also is splitting our dogs into decimal values which will seem strange when we're reporting back to our friend.


@@@
1
00:00:00,000 --> 00:00:12,904
A more appropriate measure in this case, might be the median. The median is a value that divides our data set such that 50% of the values are larger while the remaining 50% are smaller.

2
00:00:12,904 --> 00:00:20,685
For our data set, we have a median of eight. This is a much better response than the 12 and a half dogs reported by the mean.

3
00:00:20,684 --> 00:00:26,445
Not only does eight sit in the middle of our data set, but it doesn't split any of our dogs in half.

4
00:00:26,445 --> 00:00:37,680
As a note on calculating the median, the actual calculation of the median depends on whether we're working with a data set with an even number of values or an odd number of values.

5
00:00:37,679 --> 00:00:46,195
Let's consider a couple of examples to illustrate this point. The first thing we should do is order the values from smallest to largest.

6
00:00:46,195 --> 00:01:01,489
In this top case, we are working with seven values. In notation we might say, n equals seven. Because this is an odd number, we have the median as the exact middle value, in this case, the number three.

7
00:01:01,490 --> 00:01:09,250
In a second case, let's consider n to be an even value. Let's find the median of the following list.

8
00:01:09,250 --> 00:01:21,490
Again, we should first put these values in order. Because there isn't an exact center value, we will, in this case,take the mean of the two values in the center as our median.

9
00:01:21,489 --> 00:01:34,359
Notice, this does not even need to be a number in our data set. Because n is equal to eight for this example, the mean of the fourth and the fifth values will provide to us the median for this data set.

10
00:01:34,359 --> 00:01:43,849
Moving four values from each of the sides, makes it so that we take the mean of three and five to obtain the median of four.


@@@
1
00:00:00,000 --> 00:00:09,580
The third measure of center, aims at providing us the most common value in the data set. In this data set, this is the value of three.

2
00:00:09,580 --> 00:00:16,940
The value that occurs most often is known as the Mode. These are all three potential measures of center.


@@@
1
00:00:00,000 --> 00:00:10,619
Previously, we listed the four main aspects of analyzing quantitative data; center, spread, shape and outliers.

2
00:00:10,619 --> 00:00:22,750
We also looked specifically at measures of center, by introducing means, medians and modes. Before we look at measures of spread, it's important to understand notation.

3
00:00:22,750 --> 00:00:30,119
You might not even know it, but you use notation all the time. Consider this example, of five plus three.

4
00:00:30,120 --> 00:00:41,314
Plus is an English word. This symbol is notation and its universal. Notation is a common math language used to communicate.

5
00:00:41,314 --> 00:00:52,519
Regardless, of whether you speak English, Spanish, Greek or any other language, you can work together using notation as a common language to solve problems.

6
00:00:52,520 --> 00:01:01,810
Like learning any new language, notation can be frightening at first, but it's an essential tool for communicating ideas associated with data.


@@@
1
00:00:00,000 --> 00:00:11,835
As a first example, let's apply this new idea of notation to something you've used before. Spreadsheets. Spreadsheets are a common way we hold data in the real world.

2
00:00:11,835 --> 00:00:21,779
In our spreadsheet, we have rows and columns. To better understand how we use spreadsheets to hold data, let's work through an example.

3
00:00:21,780 --> 00:00:33,585
Before even collecting data, we usually start with a question or many questions. Consider I run a small blog about my best and worst adventures with the dogs at the coffee shop.

4
00:00:33,585 --> 00:00:43,140
Which also sells trinkets related to those adventures. Everything from fetch toys to leashes, to doggie bags, and everything in between.

5
00:00:43,140 --> 00:00:50,505
Questions I might have are: How many people visit my site? Or how much time do visitors spend on my site?

6
00:00:50,505 --> 00:00:57,975
Are there differences in traffic, depending on the day of the week? How many visitors purchase an item through the blog?

7
00:00:57,975 --> 00:01:10,260
In order to answer these questions, say we keep track of the date of the visit, the day of the week of the visit, the amount of time spent on the site, and whether or not an individual buys an item.

8
00:01:10,260 --> 00:01:17,795
We can think of each of these as a column. A column in our dataset is associated with a random variable.

9
00:01:17,795 --> 00:01:31,829
Explaining what a random variable is in English is complicated. But in notation, it's simple. In English, a random variable is a placeholder for the possible values of some process.

10
00:01:31,829 --> 00:01:46,825
In notation, it's X. For our website, the date of the visit, the day of the week of the visit, the amount of time spent on the site, and whether or not an individual buys an item are all variables.

11
00:01:46,825 --> 00:01:56,130
Let's say we have a visitor on Thursday, June 15. The visitor stays on our site for five minutes and doesn't buy an item.

12
00:01:56,129 --> 00:02:03,659
Then a second visitor, visits the site on the exact same day for 10 minutes and they do buy an item.

13
00:02:03,659 --> 00:02:13,565
Notice how each of these individuals has been added to our spreadsheet. We might have many more visitors, and we could update our spreadsheet accordingly.

14
00:02:13,564 --> 00:02:25,709
When using spreadsheets, we frequently analyze a full column to answer our questions of interest. For example, to answer the question of how much time do visitors spend on our site?

15
00:02:25,710 --> 00:02:33,100
We need to look at this column. To answer the question of, Are there differences in traffic, depending on the day of the week?

16
00:02:33,099 --> 00:02:39,634
We need to look at this column. And to answer the question of, How many purchases occur through our blog?

17
00:02:39,634 --> 00:02:49,395
We need to look at this column. Mathematically, we usually consider a random variable or column using a capital letter.

18
00:02:49,395 --> 00:02:58,134
Commonly, we use the capital letter X, but we can just as easily use Y, Z or any other capital letter.

19
00:02:58,134 --> 00:03:05,655
We might say, consider the random variable X, which signifies the amount of time an individual spends on our website.

20
00:03:05,655 --> 00:03:20,250
Therefore, X relates to this entire column. Consider we also have a random variable Y, which signifies whether or not an individual purchases an item from the website, so Y relates to this entire column.


@@@
1
00:00:00,210 --> 00:00:09,304
Connecting this full circle, these capital letters X and Y relate to a random variable. This is an abstract idea.

2
00:00:09,304 --> 00:00:21,635
How much time an individual spends on our website, can take on lots of different values. So, this capital letter X is not a number, it's an entire set of possible values.

3
00:00:21,635 --> 00:00:34,359
We can think of capital X as a placeholder for any of these possible values. When we look at an individual outcome of our random variable, we signify this with a lower case letter.

4
00:00:34,359 --> 00:00:41,710
Often the lower case letter has a subscript, that helps us attach notation to each specific value in our dataset.

5
00:00:41,710 --> 00:00:52,095
For this dataset, we would say the amount of time an individual spends on our site is provided by an amount, which we would notate with a capital X.

6
00:00:52,094 --> 00:01:00,399
The first visitor spends five minutes on the site, which we would notate with x1. Notice, this is a lowercase x.

7
00:01:00,399 --> 00:01:10,359
The second visitor spends 10 minutes on the site, which we wouldn't notate as x2. Again, since this is an observed value this is a lowercase x.

8
00:01:10,359 --> 00:01:17,545
The labeling could continue until we reach the final visitor in our dataset, who we'd called the nth visitor.

9
00:01:17,545 --> 00:01:27,959
We would label this value as xn. Again, this is a lowercase value. Notation is an essential tool for us to communicate mathematical ideas.

10
00:01:27,959 --> 00:01:45,439
We have now discussed the idea that capital letters are used as notation for random variables. When we observe a particular value of the random variable we use a lowercase letter, with a subscript that signifies the specific value of the random variable we are considering.

11
00:01:45,439 --> 00:01:56,000
Notation can be tricky. Before we dive deeper, check your understanding. There are quizzes available in the next module to assure you've mastered the concepts we've introduced.


@@@
1
00:00:00,000 --> 00:00:07,544
In the next concepts, we are going to combine what we know about how to calculate the mean with notation.

2
00:00:07,544 --> 00:00:17,114
The purpose of this video is not to relearn how to calculate the mean but rather to introduce notation using a measure you already know.

3
00:00:17,114 --> 00:00:26,219
Let's consider the amount of time someone spends on our website in minutes. Imagine we collect the amount of time spent on our site for five individuals.

4
00:00:26,219 --> 00:00:38,660
From the last video, we saw that we could label the values in this way, x1 for the first value, x2 for the second value, x3 for the third value and so on.

5
00:00:38,659 --> 00:00:51,159
Imagine we want to add the first two numbers together. We could write this in notation as x1 plus x2 which in this example, would be the same as 15 plus 10.

6
00:00:51,159 --> 00:01:00,469
If we wanted to add more than just two values, it would be tedious to continue the same process. Imagine if we had 100 values.

7
00:01:00,469 --> 00:01:14,150
We would have a, x1 plus x2 plus x3 plus and we'd have to continue this process until x 100. Someone must have come up with a better way to notate this.


@@@
1
00:00:00,000 --> 00:00:11,664
There are common ways to notate most aggregations. An aggregation is just a way to turn multiple numbers into fewer numbers, usually just one number.

2
00:00:11,664 --> 00:00:19,429
Common aggregations include the measures of center we introduced earlier, like the mean, the median, and the mode.

3
00:00:19,429 --> 00:00:26,070
Each of these takes many numbers and provides a single value to give information about the data set.

4
00:00:26,070 --> 00:00:34,054
In this example, we want to aggregate all the values into a single sum. In other words, add them up.

5
00:00:34,054 --> 00:00:44,144
The Greek alphabet is a popular place to pull notation. Similar to English, there are both lower and upper case letters in the Greek alphabet.

6
00:00:44,145 --> 00:00:52,304
For summing many values together, we use a Greek symbol known as Sigma. Specifically, we use the uppercase Sigma.

7
00:00:52,304 --> 00:01:05,700
We generally use the symbol in the following way: you will notice that instead of writing multiple x values, each with a different subscript, we write a single x with a subscript of i.

8
00:01:05,700 --> 00:01:21,070
Here, the I is a placeholder that tells us which x values we'll be summing up. In our first case, summing only the first two values, we want this i to be the value of either one or two.

9
00:01:21,069 --> 00:01:29,854
Now you might be asking, how does this notation show which values we're summing together. This is a great question.

10
00:01:29,855 --> 00:01:38,320
If we would like to sum just the first two values, we would write something like this. Notice at the bottom, we are giving our starting point.

11
00:01:38,319 --> 00:01:47,484
That is, we would like the first x value to be where i equals one. You can imagine replacing the one into this subscript.

12
00:01:47,484 --> 00:02:04,725
Then the value at the top gives us an ending point for where we stop. Here, our ending is two. The Sigma tells us we would like to sum from this bottom value up through all the values until we hit this top value.


@@@
1
00:00:00,000 --> 00:00:08,044
But now, if we want to sum all the values in our original example, we no longer need to write out all of the xs.

2
00:00:08,044 --> 00:00:22,175
Instead, we can write our summation starting at i=1 and ending at the fifth value. This is way better when wanting to extend to 10 or 20 or even 100 values.

3
00:00:22,175 --> 00:00:37,192
We no longer need to write out all of those xs. However, we can be even more efficient than this. Each time our data set changes, I have to change this number at the top to indicate how many values I'm summing.

4
00:00:37,192 --> 00:00:47,164
If we really just want to sum all of our values, we can replace this top value with n. Now our notation will work for any data set.

5
00:00:47,164 --> 00:01:01,215
So far, we have a way to sum all of our values regardless of how many are in our data set. In order to finish this calculation of the mean, we need to divide our sum by how many values are in our data set.

6
00:01:01,215 --> 00:01:20,150
But this is just n. You would commonly see the mean of the data set notated like this, which we pronounce as x-bar, and it is calculated with this notation which says you sum all of the values in the data set and then divide by the number of values in the data set.

7
00:01:20,150 --> 00:01:34,000
Learning notation can be tough. Don't be afraid to watch this video more than once. Before we move on to the next sections, there are quizzes available in the next module to assure you've mastered the concepts we've introduced here.


@@@
1
00:00:00,000 --> 00:00:12,459
That last section was intense. I hope the quizzes reinforce what was shown in the videos. I know the first time I saw a notation, it totally went over my head.

2
00:00:12,460 --> 00:00:20,815
We will begin this lesson looking at the second aspect with regard to analyzing quantitative variables, the spread.

3
00:00:20,815 --> 00:00:30,015
When we discussed measures of spread, we are considering numeric values that are associated with how far points are from one another.

4
00:00:30,015 --> 00:00:38,009
Common values of spread include the range, the inter-quartile range, the standard deviation, and the variance.


@@@
1
00:00:00,000 --> 00:00:09,779
It is easy as to understand the spread of our data visually. The most common visual for quantitative data is known as a histogram.

2
00:00:09,779 --> 00:00:21,639
In this video, we will take a look at exactly how histograms are constructed. In order to understand how histograms are constructed, consider we have the following data-set.

3
00:00:21,640 --> 00:00:29,609
First, we need to bin our data. You as the histogram creator ultimately choose how the binning occurs.

4
00:00:29,609 --> 00:00:43,494
Here, I have chosen our bins as one to four, five to eight, 9-12 and 13-16. Because these first four values are between one and four, they go into the first bin.

5
00:00:43,494 --> 00:00:54,039
These next three values are between five and eight, so they fall in the next bin, then these two values fall in this bin and 15 falls into our last bin.

6
00:00:54,039 --> 00:01:04,859
The number of values in each bin determine the height of each histogram bar. Changing the bins will result in a slightly different visual.

7
00:01:04,859 --> 00:01:12,875
There really isn't a right answer to choosing our bins, and in most cases software will choose the appropriate bins for us.


@@@
1
00:00:00,000 --> 00:00:08,835
Here, are two histograms comparing the number of dogs that I saw on weekdays to the number of dogs I saw on weekends from last year.

2
00:00:08,835 --> 00:00:22,755
You will notice that the tallest bins for both weekdays and weekends are associated with 13 dogs. So the number of dogs I expect to see are essentially the same for weekdays as on weekends.

3
00:00:22,754 --> 00:00:31,575
And the measures of center would basically be the same here. Both have a mean, median, and mode that are about 13 dogs.

4
00:00:31,574 --> 00:00:42,790
But something is different about these two distributions. So what's the difference? Well, the difference is how spread out the data are for each group.

5
00:00:42,789 --> 00:00:52,830
You can see that the number of dogs I see on weekdays, ranges from 10-16. While on weekends, it ranges from 6-18.


@@@
1
00:00:00,000 --> 00:00:12,564
One of the most common ways to measure the spread of our data is by looking at the Five Number Summary, which gives us values for calculating the range and interquartile range.

2
00:00:12,564 --> 00:00:24,176
The Five Number Summary consists of five values, the maximum, the minimum, the first quartile, the second quartile which is also the median, and the third quartile.

3
00:00:24,176 --> 00:00:34,105
Consider we have the following dataset. The first thing we need to do to calculate the Five Number Summary is to order our values.

4
00:00:34,104 --> 00:00:43,270
Once ordered, the minimum and the maximum values are easy to identify as the smallest and largest values.

5
00:00:43,270 --> 00:00:51,280
As we calculate it in the section on measures of center, the median is the middle value in our dataset.

6
00:00:51,280 --> 00:01:01,460
We also call this Q2 or the second quartile because 50% of our data or two quarters fall below this value.

7
00:01:01,460 --> 00:01:13,455
The remaining two values to complete the Five Number Summary are Q1 and Q3. These values can be thought of as the medians of the data on either side of Q2.

8
00:01:13,456 --> 00:01:28,254
That is the median of these data points is Q1, this value is such that 25% of our data fall below it, and the median of these data points is Q3 or the third quartile.

9
00:01:28,254 --> 00:01:41,680
This value is such that 75% of our data fall below this mark. Notice, Q2 was not an either a set of these points used to calculate Q1 or Q3.

10
00:01:41,680 --> 00:01:51,409
This provides our Five Number Summary as the following. Let's consider another example for in our dataset has an even set of values.

11
00:01:51,409 --> 00:02:07,340
Again, we first need to order the values. We can quickly identify the maximum and the minimum. Remember, with an even number of values, the median or Q2 is given as the mean of these two values here.

12
00:02:07,340 --> 00:02:18,304
In order to find Q1 and Q3, we divide our dataset between the two values we use to find the median. This provides these two datasets.

13
00:02:18,305 --> 00:02:33,155
Finding the median of each of these will provide Q1 and Q3. For this dataset, Q1 will be the mean of these two values, and Q3, will be the mean of these two values.

14
00:02:33,155 --> 00:02:44,905
This provides our Five Number Summary as the following. Once we've calculated all the values for the Five Number Summary, finding the range and interquartile range is no problem.

15
00:02:44,905 --> 00:02:54,865
For the first dataset, the range is calculated as the maximum minus the minimum. For the first dataset, this was 10 minus 1 or 9.


@@@
1
00:00:00,000 --> 00:00:09,390
Looking back at the distributions we found for the number of dogs I see, we can mark the values of our five number summary like this.

2
00:00:09,390 --> 00:00:29,684
If we take just these marks, this makes a common plot for data known as a box plot. Though I prefer a histogram in most cases, a box plot can be useful for quickly comparing the spread of two data sets across some key metrics, like our quartiles, and the maximum and minimum.

3
00:00:29,684 --> 00:00:42,085
From both the histogram and the five-number summary, we can quickly see that the number of dogs I see on weekends varies much more than the number of dogs I see on weekdays.

4
00:00:42,085 --> 00:00:52,099
We can also visualize the distance from here to here as the range, while the distance from here to here is the interquartile range.

5
00:00:52,100 --> 00:01:05,159
There are lots of useful metrics we can get from these box plots. But what if we want to compare the spreads of these distributions without having to carry around all five of these values for each distribution?


@@@
1
00:00:00,000 --> 00:00:09,779
The most common way that professionals measure the spread of a data set with a single value is with the standard deviation or variance.

2
00:00:09,779 --> 00:00:17,994
Here, we will focus on the standard deviation, but we will actually learn how to calculate the variance in the process.

3
00:00:17,995 --> 00:00:23,940
If you have never heard of these measures before, this calculation will probably look pretty complex.

4
00:00:23,940 --> 00:00:35,155
When all's said and done with this calculation, the standard deviation will tell us on average how far every point is from the mean of the points.

5
00:00:35,155 --> 00:00:42,019
As a quick mental picture, imagine we wanted to know how far employees were located from their place of work.

6
00:00:42,020 --> 00:00:52,655
One person might be 15 miles, another 35, another only one mile, and another might be remote and is 103 miles.

7
00:00:52,655 --> 00:01:02,359
We could aggregate all of these distances together to show that the average distance employees are located from their work is 18 miles.

8
00:01:02,359 --> 00:01:12,519
But now, we want to know how the distance to work varies from one employee to the next. We could use the five number summary as a description.

9
00:01:12,519 --> 00:01:19,570
But if we wanted just one number to talk about the spread, we'd probably choose the standard deviation.

10
00:01:19,569 --> 00:01:37,915
The standard deviation is on average how much each observation varies from the mean. For this example this is, how much on average the distance each person is from work differs from the average distance all of them are from work.

11
00:01:37,915 --> 00:01:45,534
So, this one is three miles farther from work than the average while this individual is four miles closer to work than the average.

12
00:01:45,534 --> 00:01:58,500
The standard deviation is how far on average are individuals located from this mean distance. So, it is like the average of all of these distances.

13
00:01:58,500 --> 00:02:07,000
We will take a closer look at this but hopefully this gives you a strong conceptual understanding of what we'll be calculating in the next sections.


@@@
1
00:00:00,000 --> 00:00:11,849
In the last video, we got an idea of what the standard deviation is measuring. In this video, we will look at the math that actually occurs when calculating this measure.

2
00:00:11,849 --> 00:00:27,765
We will work with data to calculate this measure as well as associate notation with it. It is worth noting that after this lesson, you probably won't calculate this measure by hand ever again, because you'll learn software to do it for you.

3
00:00:27,765 --> 00:00:42,565
The calculating it yourself will give you intuition behind what it's actually doing. And this intuition is necessary to become good at understanding data and choosing the right analysis for your situation.

4
00:00:42,564 --> 00:00:55,219
Imagine we have a data set with four values, 10, 14, 10, and 6. The first thing we need to do to calculate the standard deviation is to find the mean.

5
00:00:55,219 --> 00:01:09,115
In notation, we have this as X bar. For our values, the sum is 40, and we have four numbers. So the mean is 40 over 4 or 10.

6
00:01:09,114 --> 00:01:18,515
Then we want to look at the distance of each observation from this mean. Two of these observations are exactly equal to the mean.

7
00:01:18,515 --> 00:01:34,539
So the distance here is zero. One is 4 larger the 14, while the other is 4 smaller the 6. In notation, each of these is XI minus X bar.

8
00:01:34,540 --> 00:01:45,525
Then, if we were to average these distances, the positive would cancel with the negative value. And the value of zero isn't a great measure of the spread here.

9
00:01:45,525 --> 00:01:56,604
Zero would suggest that all the values are the same or that there's no spread. So instead, we need to make all of these values positive.

10
00:01:56,605 --> 00:02:09,420
The way we do this when calculating the standard deviation is by squaring them all. If we do that here, our negative and positive 4 values will become 16s.

11
00:02:09,419 --> 00:02:20,810
Now, we could average these to find the average squared distance of each observation from the mean. This is called the variance.

12
00:02:20,810 --> 00:02:29,064
Finding the average, just as we did before, means adding all of these values and dividing by how many there are.

13
00:02:29,064 --> 00:02:48,040
In our case, we had 0, 16, 0, 16 and we divide by 4 because we have four observations. However, this is an average of squared values which we only did to get positive values in the first place.

14
00:02:48,039 --> 00:02:57,930
So to get our standard deviation, we take the square root of this ending value. Here, our standard deviation is 2.83.

15
00:02:57,930 --> 00:03:07,000
So this is on average how far each point in our data set is from the mean, which is the definition of the standard deviation.


@@@
1
00:00:00,360 --> 00:00:11,984
So it might seem absurd to do this calculation of the standard deviation. I mean, it's such a complicated way to measure the spread of our data compared to the five number summary we saw earlier.

2
00:00:11,984 --> 00:00:21,140
But it turns out that the standard deviation is used all the time to get a single number to compare the spread of two data sets.

3
00:00:21,140 --> 00:00:30,000
It is kind of nice to be able to talk about how spread out our data are from one another without having to report an entire table of values.

4
00:00:30,000 --> 00:00:35,315
We can just compare the standard deviation for one group to the standard deviation of another group.

5
00:00:35,314 --> 00:00:46,230
And we have a way to tell which dataset is more spread out. Having one number simplifies the amount of information that the person you're reporting to needs to consume.

6
00:00:46,229 --> 00:00:55,054
Having the single value also has other advantages with regard to what is known as inferential statistics, but that's beyond what we need to know now.

7
00:00:55,054 --> 00:01:04,280
For now, we just need to know that we have a way to take all of our values and get a single number that tells us how spread out they are from one another.


@@@
1
00:00:00,000 --> 00:00:13,589
A few quick final points to keep in mind about the standard deviation. First, the standard deviation is frequently used to compare the spread of different groups to determine which is more spread out.

2
00:00:13,589 --> 00:00:22,625
Second, when data pertains to money or the economy, having higher standard deviation is associated with having higher risk.

3
00:00:22,625 --> 00:00:35,219
In comparing stock prices, a stock price that changes with higher standard deviation over time is considered more risky than a stock price that fluctuates with lower standard deviation.

4
00:00:35,219 --> 00:00:49,519
Third, for a comparison to be fair, all data must be in the same units. If you're measuring in dollars for one data set, but euros for another, it's not fair to compare these data sets to determine which has the greatest spread.

5
00:00:49,520 --> 00:01:04,469
Finally, the variance has squared units of your original measurements. For example, if you were measuring revenue in dollars, the variance has units of dollars squared, which isn't particularly useful.

6
00:01:04,469 --> 00:01:16,655
For this reason, the standard deviation, which is the square root of the variance, is often deemed a more useful measurement of spread as it shares the units of the original data set.


@@@
1
00:00:00,000 --> 00:00:07,550
Now that we've discussed how to build a histogram, we can use this to determine the shape associated with our data.

2
00:00:07,549 --> 00:00:20,420
Here we have three histograms, showing the shape for three different data sets. The histogram that has shorter bins on the left and taller bins on the right, is considered a left skewed shape.

3
00:00:20,420 --> 00:00:26,859
This histogram that has shorter bins on the right and taller bins on the left, is considered a right skewed shape.

4
00:00:26,859 --> 00:00:35,289
Any distribution where you can draw a line down the middle and the right side mirrors the left side is considered symmetric.

5
00:00:35,289 --> 00:00:42,765
One of the most common symmetric distributions, is known as a normal distribution and it's also called the Bell Curve.

6
00:00:42,765 --> 00:00:57,359
The shape of the distribution can actually tell us a lot about the measures of center and spread. Symmetric distributions, like this one have a mean that's equal to the median, which also equals the mode.

7
00:00:57,359 --> 00:01:05,194
Each of these measures sits here in the center. The mode is essentially the tallest bar in our histogram.

8
00:01:05,194 --> 00:01:13,579
When we have skewed distributions, it's the case of the mean is pulled by the tail of the distribution, while the median stays closer to the mode.

9
00:01:13,579 --> 00:01:21,575
For example, in this right skewed distribution, the mean would be pulled higher, resulting in a mean that's greater than our median.

10
00:01:21,575 --> 00:01:30,454
Alternatively, in a left skewed distribution, our mean is pulled down here, resulting in a mean that's less than our median.

11
00:01:30,454 --> 00:01:42,060
In order to relate this to the visual of a histogram, back to the five number summary we saw on the earlier lessons, here are the corresponding box plots below each histogram.

12
00:01:42,060 --> 00:01:55,099
Notice how the whiskers stretch in the direction of the skew, for each of the skewed distributions. That is the longer whisker is on the left, for a left skewed distribution and it's on the right, for the right skewed distribution.


@@@
1
00:00:00,290 --> 00:00:23,669
If you're working with data, you can always build a Quick Plot to see the shape. Just to apply some context, some examples of approximately Bell-Shaped data include heights and weights, standardized test scores, precipitation amounts, the mean of a distribution, or errors in manufacturing processes.

2
00:00:23,670 --> 00:00:32,329
Common data that follow Left Skewed Distributions include GPAs, the age of death, and asset price changes.

3
00:00:32,329 --> 00:00:43,054
Common data that follow approximately Right Skewed Distributions include the amount of drug left in your bloodstream over time, the distribution of wealth, and human athletic abilities.

4
00:00:43,054 --> 00:00:48,390
There are links below in the instructor notes in case you want to learn more about each of these cases.

5
00:00:48,390 --> 00:01:00,480
Though these three, Right Skewed, Left Skewed and Symmetric, are the most common distributions, data in the real world can be messy and it might not follow any of these distributions.


@@@
1
00:00:00,020 --> 00:00:08,344
In this video, we want to look at the final aspect used to describe quantitative variables, Outliers.

2
00:00:08,345 --> 00:00:19,714
Outliers are data points that fall very far from the rest of the values in our data set. In order to determine what is very far, there are a number of different methods.

3
00:00:19,714 --> 00:00:31,665
My usual method for detecting outliers isn't very scientific. Usually, I just look at a histogram and see if the point is really far from any of the other data points.

4
00:00:31,664 --> 00:00:48,085
Again, a quick plot of your data can often help you understand a lot in a short amount of time. In order to illustrate the impact that outliers can have on the way we report summary statistics, let's consider the salaries of entrepreneurs.

5
00:00:48,085 --> 00:00:58,950
Imagine I select ten entrepreneur earnings and I pull these nine values here as earnings in thousands of dollars, and the tenth is the CEO of Facebook.

6
00:00:58,950 --> 00:01:19,434
According to a post by CNN in 2016, he earned 4.4 million dollars per day. Here, we can calculate the mean of these salaries for entrepreneurs based on this data to be approximately 160 million dollars.

7
00:01:19,435 --> 00:01:29,719
This is incredibly misleading. Literally zero of the entrepreneurs earned this salary. None of the ten salaries are even close to this amount.

8
00:01:29,719 --> 00:01:43,070
A better measure of center would certainly be the median. The median here is calculated at 62,000 dollars a year and is a better indication of what an entrepreneur is likely to earn based on our data.

9
00:01:43,069 --> 00:01:57,914
Our standard deviation is also not a great measure in this case. At approximately 482 million dollars, all this suggests is that our earnings for entrepreneurs are really spread out, but that really isn't fair either.


@@@
1
00:00:00,050 --> 00:00:07,385
How should we work with these outliers in practice? At the very least, we should note that they exist.

2
00:00:07,384 --> 00:00:16,835
We need to realize the impact they have on our summary statistics. In this case, greatly increasing our mean and our standard deviation.

3
00:00:16,835 --> 00:00:28,369
If the outliers are typos or data entry errors, this is a reason to remove these points. Or if we know what they should be, we could change them with the correct values.

4
00:00:28,370 --> 00:00:36,530
In cases like the example above, we might try to understand, what was so different about the outlier when compared to the other individuals?

5
00:00:36,530 --> 00:00:48,729
How did this entrepreneur become so successful? And why are the earnings so large in comparison? There is an entire field aimed at this idea called the anomaly detection.

6
00:00:48,729 --> 00:00:59,770
You will notice measures associated with the five number summary, like the minimum, the maximum and the quartiles one, two, three, are more informative when outliers are present.

7
00:00:59,770 --> 00:01:09,979
This example shows that you need to be careful about how we share our results and state our conclusions using summary statistics when we have outliers.

8
00:01:09,980 --> 00:01:19,420
A single number can be very misleading about what is actually happening in our data. Some statistics are more misleading than others.

9
00:01:19,420 --> 00:01:30,000
If you are the consumer of information based on data, which we all are, it's important to know how to ask the right questions regarding the statistics around you.


@@@
1
00:00:00,070 --> 00:00:09,625
If you're the one doing the reporting, here are some of my personal guidelines when analyzing data. First, plot your data.

2
00:00:09,625 --> 00:00:17,650
Second, if you have outliers, determine how you should handle them. This might require a domain expert of the field.

3
00:00:17,649 --> 00:00:35,314
Should you remove them? Should you fix them? Should you keep them? Third, if you're working with data that are normally distributed, the bell shape that we saw before, you can find out every little detail about the data with only the mean and the standard deviation.

4
00:00:35,314 --> 00:00:49,345
This may seem surprising but it's true. However, if our data are skewed, the five-number summary provides much more information for these data sets than the mean and the standard deviation can provide.

5
00:00:49,344 --> 00:01:03,000
Again, the most useful and informative summary you can get is frequently a visual. In upcoming lessons, we will focus specifically on the visuals that will best convey our message.


@@@
1
00:00:00,000 --> 00:00:09,949
The topics covered this far have all been aimed at descriptive statistics. That is, describing the data we've collected.

2
00:00:09,949 --> 00:00:23,375
There's an entire other field of statistics known as inferential statistics that's aimed at drawing conclusions about a population of individuals based only on a sample of individuals from that population.

3
00:00:23,375 --> 00:00:37,289
Imagine I want to understand what proportion of all Udacity students drink coffee. We know you're busy, and in order to get projects in on time, we assume you almost drink a ton of coffee.

4
00:00:37,289 --> 00:00:44,295
I send out an email to all Udacity alumni and current students asking the question, do you drink coffee?

5
00:00:44,295 --> 00:00:54,445
For purposes of this exercise, let's say the list contained 100,000 emails. Unfortunately, not everyone responds to my email blast.

6
00:00:54,445 --> 00:01:08,424
Some of the emails don't even go through. Therefore, I only receive 5,000 responses. I find that 73% of the individuals that responded to my email blast, say they do drink coffee.

7
00:01:08,424 --> 00:01:20,765
Descriptive statistics is about describing the data we have. That is, any information we have and share regarding the 5,000 responses is descriptive.

8
00:01:20,765 --> 00:01:32,734
Inferential statistics is about drawing conclusions regarding the coffee drinking habits of all Udacity students, only using the data from the 5,000 responses.

9
00:01:32,734 --> 00:01:45,620
Therefore, inferential statistics in our example is all about drawing conclusions regarding all 100,000 Udacity students using only the 5,000 responses from our sample.

10
00:01:45,620 --> 00:01:54,394
The general language associated with this scenario is as shown here. We have a population which is our entire group of interest.

11
00:01:54,394 --> 00:02:05,109
In our case, the 100,000 students. We collect a subset from this population which we call a sample. In our case, the 5,000 students.

12
00:02:05,109 --> 00:02:15,485
Any numeric summary calculated from the sample is called a statistic. In our case, the 73% of the 5,000 that drink coffee.

13
00:02:15,485 --> 00:02:31,495
This 73% is the statistic. A numeric summary of the population is known as a parameter. In our case, we don't know this value as it's a number that requires information from all Udacity students.


@@@
1
00:00:00,000 --> 00:00:13,010
Congratulations on making it through the statistics portion of this program. This foundation in working with data will make later sections using Spreadsheets, SQL and Tableau more intuitive.

2
00:00:13,009 --> 00:00:20,689
I hope this section reinforce some ideas you're already familiar with, while also introducing you to some new ones that you've now mastered.


@@@
1
00:00:00,000 --> 00:00:08,910
Welcome to the first lesson in the practical statistics course. In this case study, you're going to witness an instance of Simpson's paradox.

2
00:00:08,910 --> 00:00:20,955
A phenomenon that shows how powerful and dangerous statistics can be. Sometimes just grouping your data differently for your analysis can make your conclusions disappear or even be reversed.


@@@
1
00:00:00,000 --> 00:00:15,000
The problem I'd like to tell you about is motivated by an actual study the University of California Berkeley, which many years back wanted to know whether it's admissions procedure is gender biased.

2
00:00:15,000 --> 00:00:23,000
I looked at various admission statistics to understand whether than admissions policies had a preference for a certain gender.

3
00:00:23,000 --> 00:00:38,000
And while the numbers I'll be giving you are not the exact same that UC Berkeley found, the paradox is indeed the same and is often called, "Simpson's Paradox." I'm just giving you a simplified version of the problem.

4
00:00:38,000 --> 00:00:49,000
Here is the data. Among male students, we find that from 900 applicants in major A 450 are admitted.


@@@

@@@

@@@

@@@
1
00:00:00,000 --> 00:00:15,000
The same statistic was run for female students. Again, I made up the data to illustrate the effect. Females tended to apply predominantly for major B with 900 applications for major B and just 100 for major A.

2
00:00:15,000 --> 00:00:15,000
The university accepted 80 out of 100 applications in major A and 180 out of 900 in major B. Please tell me the rate of acceptance in percent for major A for the females student population.


@@@

@@@

@@@

@@@
1
00:00:00,000 --> 00:00:00,000
So, just looking at these numbers for the two different majors, would we believe--in terms of the acceptance rate-- is there a gender bias?  Yes or no?


@@@
1
00:00:00,000 --> 00:00:10,000
And I would say yes, in part because the acceptance rate is so different for the different student populations, even though  the numbers are relatively large.

2
00:00:10,000 --> 00:00:10,000
So, it doesn't seem just like random deviations. But the thing that will blow your mind away is a different question.


@@@
1
00:00:00,000 --> 00:00:17,000
Who is being favored--the male students or the female students? And looking at the data alone, it makes sense to say the female students are favored because for both majors, they have a better admission rate than the corresponding male students.

2
00:00:17,000 --> 00:00:28,000
But now, let's do the trick. Let's look at the admission statistics independent of the major. So, let's talk about both majors, and I would wonder how many male students applied.


@@@

@@@

@@@

@@@
1
00:00:00,000 --> 00:00:12,000
Now, do the same for the female student population, and we had a 1000 applicants, same number as in the male case, and 260 students admitted.


@@@

@@@
1
00:00:00,000 --> 00:00:00,000
So, across both majors, I'm asking you the same question again now. Who is actually being favored? Males or females?


@@@
1
00:00:00,000 --> 00:00:08,000
And surprisingly, when  you look at both majors together, you find that males have a much higher admissions rate than females.

2
00:00:08,000 --> 00:00:17,000
I'm not making this up. These numbers might be fake, but that specific affect was observed at the University of California at Berkley many years ago.

3
00:00:17,000 --> 00:00:31,000
But when you look at majors individually, then you find in each major individually the acceptance rate for females trumps that of males, both in the first major and the second major.

4
00:00:31,000 --> 00:00:39,000
Going from the individual major statistics to the total statistics, we haven't added anything. We just regrouped the data.

5
00:00:39,000 --> 00:00:39,000
So how come, when you do this, what looks like an admissions bias in favor of females switches into admissions bias in favor of males?


@@@
1
00:00:00,230 --> 00:00:08,580
As you've seen in this example, on Simpson's paradox, the way you choose to look at your data can lead to completely different results.

2
00:00:08,580 --> 00:00:15,869
And often, you can majorly impact what people believe to be true with how you choose to communicate your findings.

3
00:00:15,869 --> 00:00:22,260
You can guess how people intentionally or unintentionally come to false conclusions with these choices.

4
00:00:22,260 --> 00:00:28,410
Next, you're going to walk through a similar example of Simpson's paradox on a full dataset in a Jupyter notebook.


@@@
1
00:00:00,310 --> 00:00:07,625
I hope this example made you think and learn to be skeptical, of your own results and the results from others.

2
00:00:07,625 --> 00:00:18,565
Moving forward even when you feel very confident about the statistics you use for your analysis, take a moment to reconsider other ways of looking at your data and whether you chose wisely.


@@@
1
00:00:01,399 --> 00:00:21,799
Statistics and probability are different but strongly related fields of mathematics. In probability, we make predictions about future events based on models or causes that we assume whereas in statistics we analyze the data from the past events to infer what those models or causes could be.

2
00:00:21,800 --> 00:00:30,734
There's almost an opposite relation between these two. In one, you're predicting data and in the other, you're using data to predict.

3
00:00:30,734 --> 00:00:40,260
Although not all topics and both fields require an understanding of the other, you'll need a good understanding in probability for the foundation you'll be building in statistics.


@@@
1
00:00:00,000 --> 00:00:10,000
I have here a U.S. dollar coin. It has two sides, one showing a head and one showing what's called tails.

2
00:00:10,000 --> 00:00:27,000
In probability, I'm giving a description of this coin, and I'm making data. We just make data. [sound of coin spinning] So if we look at the coin, it came up heads.

3
00:00:27,000 --> 00:00:44,000
So I just made a data point of flipping the coin once, and it came up heads. Let me do it again. [sound of coin spinning] And--wow! It came up heads again.

4
00:00:44,000 --> 00:00:54,000
So my new data is {heads, heads}. And you can see how it relates to the data we studied before when we talked about histograms and pie charts and so on.

5
00:00:54,000 --> 00:01:08,000
Let me give it a third try. [sound of coin spinning] And, unbelievably, it comes up once again heads.

6
00:01:08,000 --> 00:01:17,000
So let me ask a statistical question to test your intuition. Do you think if I twist this coin more frequently will it always come up heads?


@@@
1
00:00:00,000 --> 00:00:11,000
And you can debate it, but I think the best answer is no. This is what's called a fair coin, and that means it really has a 50% chance of coming up tails.

2
00:00:11,000 --> 00:00:26,000
So let me spin it again. [sound of coin spinning] And, not surprisingly, it actually came up tails this time.


@@@
1
00:00:00,000 --> 00:00:08,000
Let's talk about a fair coin. The probability of the coin coming up heads is written in this P notation.

2
00:00:08,000 --> 00:00:22,000
This reads probability of the coin coming up heads. And in a fair coin, the chances are 50%. That is, in half the coin flips, the coin should come up heads.

3
00:00:22,000 --> 00:00:29,000
In probability we often write 0.5, which is half of 1. So a probability of 1 means it always occurs.

4
00:00:29,000 --> 00:00:29,000
A probability of 0.5 means it occurs half the time. And let me just ask you what do you think, for this coin, is the probability of tails?


@@@

@@@
1
00:00:00,000 --> 00:00:12,000
A loaded coin is one that comes up with one of the two much more frequently than the other. So, for example, suppose I have a coin that always comes up heads.

2
00:00:12,000 --> 00:00:12,000
What probability would I assess for this coin to come up heads? What would be the right number over here?


@@@

@@@

@@@
1
00:00:00,000 --> 00:00:11,000
And, yes, the answer is zero. And we find a little law here we just want to point out, which is the probability of heads plus the probability of tails equals 1.

2
00:00:11,000 --> 00:00:18,000
And the reason why that's the case is the coin either comes up heads or tails. There is no other a choice.

3
00:00:18,000 --> 00:00:28,000
So no matter what happens, if I look at heads and tails combined the chances of either of those occurring is 1, because we know it's going to happen.


@@@
1
00:00:00,000 --> 00:00:00,000
So suppose the probability of heads is 0.75, that is, 3 out of 4 times we're going to get heads. What is the probability of tails?


@@@
1
00:00:00,000 --> 00:00:00,000
And the answer is 0.25, which is 1 - 0.75 using the law down here. As you can verify, 0.75 + 0.25 =1.


@@@
1
00:00:00,000 --> 00:00:07,000
So we just learned something important. There's a probability for an outcome; I'm going to call it A, for now.

2
00:00:07,000 --> 00:00:20,000
And we learned that the probability of the opposite outcome, which we're going to call ¬A (this over here just means "not") is 1 minus the probability as expressed right over here.

3
00:00:20,000 --> 00:00:20,000
That's a very basic law of probability, which will become handy as we go forward, so please remember it.


@@@
1
00:00:00,000 --> 00:00:15,000
In our example, we observed heads twice. So now I want to ask you a really tricky question: What's the probability of observing heads and heads if you flip the same unbiased coin twice?


@@@
1
00:00:00,000 --> 00:00:08,000
That was a tricky question, and you couldn't really know the answer if you've never seen probability before, but the answer is 0.25.

2
00:00:08,000 --> 00:00:18,000
And I will derive it for you using something called a truth table. In a truth table, you draw out every possible outcome of the experiment that you conducted.

3
00:00:18,000 --> 00:00:31,000
There were two coin flips--flip 1 and flip 2--and each had a possible outcome of heads, heads; heads, tails; tail, heads; and tail, tail.

4
00:00:31,000 --> 00:00:39,000
So when we look at this table, you can see every possible outcome of these two coin flips. There happens to be four of them.

5
00:00:39,000 --> 00:00:45,000
And I would argue because heads and tails are equally likely, each of these outcomes is equally likely.

6
00:00:45,000 --> 00:00:54,000
Because we know that the probability of all outcomes has to add up to 1, we find that each outcome has a chance of a quarter, or, 0.25.

7
00:00:54,000 --> 00:01:09,000
Another way to look at this is the probability of heads followed by heads is the product . What are the chances of the first outcome to be heads multiplied by the probability of the second outcome to be heads?

8
00:01:09,000 --> 00:01:09,000
The first is 0.5, as is the second. And if you multiply these two numbers, it's 0.25, or, a quarter.


@@@
1
00:00:00,000 --> 00:00:10,000
Let me now challenge you and give you a loaded coin I flipped twice. And for this loaded coin, I assumed the probability of heads is 0.6.

2
00:00:10,000 --> 00:00:26,000
That really changes all of the numbers in the table so far, but you can apply the same method of truth tables to arrive at an answer for what is the probability of seeing heads twice under the assumption that the probability of heads equals 0.6?

3
00:00:26,000 --> 00:00:35,000
And I want to do this in steps, so rather than asking the question directly, let me help you derive it by first asking: What's the probability of tails?


@@@

@@@
1
00:00:00,000 --> 00:00:06,748
And now please fill out the entire truth table. There are four values over here, so please compute them for me.


@@@
1
00:00:00,000 --> 00:00:14,000
And the answer using our product rule is heads, heads comes out to 0.6  0.6, which is 0.36. Heads followed by tails is 0.6  0.4, which is 0.24.


@@@
1
00:00:00,000 --> 09:59:59,000
If you add up these numbers over here-- please go ahead and add them up and tell me what the sum of those numbers is.


@@@
1
00:00:00,000 --> 00:00:11,000
And, not surprisingly, it's 1. That is, the truth table always has a probability that adds up to 1 because it considers all possible cases, and all possible cases together have a probability of 1.

2
00:00:11,000 --> 00:00:20,000
So we just check this and make sure it's correct. Reading from this table, we find that the probability of (H,H) is 0.36.


@@@
1
00:00:00,000 --> 00:00:11,000
Let's now go to the extreme, and this is a challenging probability question. Suppose the probability of heads is 1, so my coin always comes up with heads.


@@@
1
00:00:00,000 --> 00:00:07,000
And the answer is 1. To see this, we know that the probability of tails is 0. All the probability goes to heads.

2
00:00:07,000 --> 00:00:07,000
1  1 = 1 1  0 = 0 0  1 = 0 And 0  0 = 0. And it's easy to verify that all these things add up to 1. Our (H,H) is just 1.


@@@
1
00:00:00,000 --> 00:00:08,000
The truth table gets more interesting when we ask different questions. Suppose we flip our coin twice.

2
00:00:08,000 --> 00:00:14,000
What we care about is that exactly one of the two things is heads, and thereby exactly the other one is tails.

3
00:00:14,000 --> 00:00:14,000
For a fair coin, what do you think the probability would be that if I flip it twice we would see heads exactly once?


@@@
1
00:00:00,000 --> 00:00:14,000
And the answer shall be 0.5. And this is a nontrivial question. Let's do the truth table. So, for flip-1, we have the outcomes of heads, heads, tails, tails.

2
00:00:14,000 --> 00:00:25,000
For flip-2, heads and tails and heads and tails. These are all possible outcomes. And we know for the fair coin each outcome was equally likely.


@@@
1
00:00:00,000 --> 00:00:12,000
Given that, we now have to associate a truth table with the question we're asking. So where exactly is, in the outcome, heads represented once?


@@@
1
00:00:00,000 --> 00:00:07,000
And, yes, it's in the second case and in the third case. The extreme cases of heads, heads and tails, tails don't satisfy this condition.

2
00:00:07,000 --> 00:00:21,000
So the trick now has been to take the 0.25 probability of these two cases and add them up, which gives us 0.25 + 0.25 = 0.5.


@@@
1
00:00:00,000 --> 00:00:00,000
Let me now make it really, really challenging for you. I take a fair coin and flip it 3 times, and I want to know the probability that exactly 1 of those 3 flips comes up heads.


@@@
1
00:00:00,000 --> 00:00:08,000
And this answer is tricky. We will derive it through the truth table. Now there's eight possible cases.

2
00:00:08,000 --> 00:00:19,000
Flip one can come of heads or tail; same for flip two, heads, tail, heads, tail; and the same for flip three and if you look at this every possible combination is represented.

3
00:00:19,000 --> 00:00:27,000
For example, these are heads, tail, tail. Now each of those outcomes has the same probability of an eighth, because it's eight cases.

4
00:00:27,000 --> 00:00:37,000
So 8 x 1/8 sums up to 1. In how many cases do we have exactly one H? It turns out that it's true for only three cases.

5
00:00:37,000 --> 00:00:49,000
The H could be in the first position, in the second position, or in the third position. So three out of eight cases have a single H.

6
00:00:49,000 --> 00:00:58,000
Each of those carries a probability so we sum those cases up to carry a total of 3/8 of a probability.


@@@
1
00:00:00,000 --> 00:00:10,000
Now that was a challenging question. I'm going to make it even more challenging for you now. I'll give you a loaded coin--the probability for H is 0.6.

2
00:00:10,000 --> 00:00:16,000
I expect this will take you awhile on a piece of paper to really calculate this probability over here.

3
00:00:16,000 --> 00:00:27,000
But you can do exactly the same thing. You go through the truth table. You apply the multiplication I showed you before to calculate the probability of each outcome; they're not the same anymore.

4
00:00:27,000 --> 00:00:27,000
H, H, H is clearly more likely than T, T, T. And when you've done this, add the corresponding figures up, and tell me what the answer is.


@@@
1
00:00:00,000 --> 00:00:23,000
And my answer is 0.288. How do I get that? Let's look at the three critical cases. H T T is 0.6 for H times 0.4 for tails times another 0.4 for tails again and it gives me 0.096.

2
00:00:23,000 --> 00:00:50,000
Now it turns out this case over here has the same probability because all we do is we order it 0.4 x 0.6 x 0.4 and we know that in multiplication the order doesn't matter, so you get the same 0.096, and by the same logic, if third one also gets me 0.096.

3
00:00:50,000 --> 00:01:03,000
So adding this 0.096s together, if we get them, gives me 0.288. So I did not have to fill the entire truth table, which you might have done in the derivation.


@@@
1
00:00:00,000 --> 00:00:09,000
So let's do one final exercise. Now I am throwing dice. The difference between dice and coins is that there are now 6 possible outcomes.

2
00:00:09,000 --> 00:00:23,000
Let me just draw them, and say it's a fair die, which means each of the different sides comes up with a probability over 6 for any of the numbers you can plug in over here.

3
00:00:23,000 --> 00:00:34,000
What do you think the probability is the die comes up with an even number? I'm going to write this as the outcome of the die is even.


@@@
1
00:00:00,000 --> 00:00:19,000
In truth table-speak, there are 6 outcomes, 1 to 6. Each has the same probability over six. Half of those numbers are even--2, 4,  and 6, so if we add those up, we get 3  1/6--the same as a half.

2
00:00:19,000 --> 00:00:19,000
The outcomes is 0.5. Now I'm finally going to make, as my final quiz, a really challenging question for you.


@@@
1
00:00:00,000 --> 00:00:12,000
Suppose we throw a fair die twice. What do you think the probability of a double is? Double means both outcomes are identical with the same number regardless of what that number is.

2
00:00:12,000 --> 00:00:21,000
The actually an important number because in many games involving two dice, have different rules when these come up with the same number.


@@@
1
00:00:00,000 --> 00:00:16,000
And once again, we can answer this using a truth table. Now the truth table will have 36 different entries, six for the first throw times six for the second throw, and there isn't enough space on this tablet to draw all the 36 entries.

2
00:00:16,000 --> 00:00:24,000
So, let me just draw the ones that really matter, one-one, two-two, and so on all the way to a six-six.

3
00:00:24,000 --> 00:00:36,000
So, each one of those is a probability of 1/6 for the first outcome times 1/6 for the second, which gives me 1/36, and the same logic applies everywhere.

4
00:00:36,000 --> 00:00:47,000
So, for all of these six outcomes, I have 1/36 of a chance this outcome would materialize. Adding them all up gives me 1/6, why?

5
00:00:47,000 --> 00:01:18,000
Because, I get 6 times 36 and I can simply this back to 1/6 that's just the same as 0.16667. So, 1/6 times, you will get a double Now, when you're play a game like backgammon, which is played with two dice, it might not feel like this, I can swear I don't get a double of 1/6 moves, but it's actually true that that's the right--that's the correct probability.


@@@
1
00:00:02,359 --> 00:00:09,824
You now have a basic understanding of probability. Great job! Let's quickly summarize what was covered in this lesson.

2
00:00:09,824 --> 00:00:19,769
You learned about the probability of an event. Such as the outcome of a coin flip. You learned that the probability of the opposite event is one minus the probability of this event.

3
00:00:19,769 --> 00:00:27,375
And you learned about the probability of a composite event, which was in the form of P times P times P and so on.

4
00:00:27,375 --> 00:00:40,649
This rule is true because the events we've observed so far are independent of one another, which means the outcome of one event doesn't affect the outcome of another, which is also the case in the next lesson on binomial distributions.

5
00:00:40,649 --> 00:00:49,170
However, if we're looking at the probability of dependent events, say the probability of rain tomorrow given that it rained today, this rule doesn't hold.

6
00:00:49,170 --> 00:00:58,439
Dependent events where the outcomes of later events depend on what has already happened are at the heart of the following topics, unconditional probability and Bayes Rule.


@@@
1
00:00:00,000 --> 00:00:10,000
So we talked about coin flips, and we flipped some coins. Now, I want to flip many coins including this 2-dollar coin whose country of origin I just don't remember.

2
00:00:10,000 --> 00:00:19,000
Perhaps you can post on the forum where this 2-dollar coin might be from. So let's ask an easy quiz. Suppose we do 2 coin flips.

3
00:00:19,000 --> 00:00:31,000
I would like to know how many outcomes of these 2 coin flips are there and which number of heads equals the number of tails which in this example means you would have head exactly once and tails exactly once.


@@@
1
00:00:00,000 --> 00:00:07,000
And the answer is 2. If you look at the truth table head-head, head-tail, tail-head, and tail-tail—these are the four possible outcomes.


@@@

@@@
1
00:00:00,000 --> 00:00:07,000
In going through the truth table, there's a bit more info. So let's find the one where the number of heads and tails are the same.


@@@
1
00:00:00,000 --> 00:00:00,000
Now. Let's go to 5 coin flips. How many outcomes have the same number of heads and tails? This is a trick question.


@@@
1
00:00:00,000 --> 09:59:59,000
And the answer is 0. With an odd number of coin flips, one has to be more than the other. There's no other way. Okay. I tricked you a little bit.


@@@

@@@
1
00:00:00,000 --> 00:00:00,000
The answer is 5. There's 5 different ways in these 5 outcomes. To place heads--could be first, second, third, fourth or fifth. So these are 5 different ways.


@@@
1
00:00:00,000 --> 00:00:00,000
Let's now make you think really hard. In 5 coin flips, how many outcomes will you have 2 heads. This is a serious and non-trivial question.


@@@
1
00:00:00,000 --> 00:00:13,000
And the answer is 10 and this is a non-trivial answer. So you could go and place the first heads anywhere in these five elements--say here-- and there's five different ways to place heads.

2
00:00:13,000 --> 00:00:25,000
You can now place the second heads among the remaining four--for example, you could place it over here and it gives you a factor of four different ways of placing the second heads.


@@@

@@@
1
00:00:00,000 --> 00:00:06,000
And that log is 10 again. There's two proofs. One is I can just flip heads and tails. So three heads means two tails.

2
00:00:06,000 --> 00:00:16,000
I can do the exact same game as before where I placed tails as opposed to heads and it gives me the same equation as before, but let's do it the new way, three heads.

3
00:00:16,000 --> 00:00:28,000
I can place 5*4*3--the first heads, the second and the third.** For the first, I have five positions, for the second--four, and for the third--three are left.

4
00:00:28,000 --> 00:00:34,000
This gives me the combinatorics for those heads, but now I'm over counting. How much am I over counting?

5
00:00:34,000 --> 00:00:41,000
Well, suppose I'm committed to put the three heads into the three slots over here and that's not given.

6
00:00:41,000 --> 00:00:47,000
And I just wonder in which order I've put them in, so I might put the first one here, the first one here, the first one here.

7
00:00:47,000 --> 00:00:59,000
Then for the first one placed in here, there's now three different ways of placing it. For the second one, there's two different ways of placing it.

8
00:00:59,000 --> 00:01:17,000
For the third one, it's not deterministic--there's just one slot left. So I over count this by a factor of 6--there are 6 different ways of placing these three heads into these three slots, so the result is 543/321 producing the 5*2=10.


@@@
1
00:00:00,000 --> 00:00:17,000
Let's say I have 10 coins. You just get 10 of these shiny silver coins here. So we got 10 coins. And I want about 4 heads in these 10 coins, and just apply the same logic to find an answer.

2
00:00:17,000 --> 00:00:28,000
The first time I'll do it for you. I can place those heads in 1098*7 different slots if they were counted.

3
00:00:28,000 --> 00:00:56,000
Now that I'm committed to having chosen these slots, the implementations of those are 432*1. That gives me 5,040/24, also known as 210, so that's 210 outcomes out of 2 to 10th outcomes which is 1024 in which exactly 4 heads are observed and 5 tails.


@@@
1
00:00:00,000 --> 00:00:00,000
I was given 10*9*8*7*6 which is 30,240 with a pick of those five heads and we're over counting by 5*4*3*2*1 which is 120 and this gives us as a quotient 252.


@@@
1
00:00:00,000 --> 00:00:23,000
So let me give you the mathematical formula that you might be familiar with. n! for any number n is the same as n(n-1)(n-2) all the way to times 1.

2
00:00:23,000 --> 00:00:36,000
Let's call it factorial. So 10 factorial for example will be 109876 and so on. If you look at this equation over here, I'll give you a couple of choices how to write it.

3
00:00:36,000 --> 00:01:12,000
It could be n!; it could be n!/k! where this over here is k which is 5 and this over here is n. It could be n!/k!*k! or it could be n!/k!*(n-k)! These are four choices. One of this is actually correct for the formula that we've computed before.


@@@
1
00:00:00,000 --> 00:00:00,000
And this one is the correct one and I have to admit I made a mistake a little bit by taking a symmetric example where k is exactly half of n.


@@@
1
00:00:00,000 --> 00:00:12,000
The key observation is that this thing over here is n!/(n-k)! and to see this let's go back to the case where we had 4 heads.

2
00:00:12,000 --> 00:00:22,000
In the 4 heads case, we multiplied all the way to times 5 over here. We only multiplied all the way to 7. These are the 4 heads that we placed.

3
00:00:22,000 --> 00:00:47,000
So n! goes from 10 all the way to 1. (n-k)! goes from 10-4 and that's 6 all the way to 1. So this blue expression over here will go from 10 x 9 x 8 and so on all the way to 1 over now n-k, 10-4 is 6. 6*5*4 and so on.

4
00:00:47,000 --> 00:00:59,000
When you look at this, the 6*5*4 occurs on top as well. So we can cut those out and what remains is 10*9*8*7. This one over here.

5
00:00:59,000 --> 00:01:09,000
Now once we place these 4 heads, we have to divide by a 4 factorial which is different ways of placing those 4 points into the predefined bins.

6
00:01:09,000 --> 00:01:29,000
So put differently, this is k! so if we put all this together you get n!/(n-k)!*k! This is the expression over here. Let's practice this one last time.

7
00:01:29,000 --> 00:01:42,000
Say you have 125 coins and you ask how many ways exists in which 3 coins come up heads. What is the resulting outcome? Be careful when you use your calculator.


@@@
1
00:00:00,000 --> 00:00:25,000
So, I get 317,750, and the logic is I just plug in these numbers, 125!/122! 3!. When I evaluate this, I find that these guys can be easily reduced to 125*124*123.

2
00:00:25,000 --> 00:00:25,000
From 122 on, these factors exactly cancel. 3 expands to 3*2*1 also known as 6. When you divide these two things, you get 317,750.


@@@
1
00:00:00,000 --> 00:00:06,000
And let's really ask about probabilities. Let's say for now, we have a fair coin with a probability of heads is 0.5.

2
00:00:06,000 --> 09:59:59,000
If I flip a coin five times, what's the probability the number of heads is exactly 1. You should be able to compute this.


@@@
1
00:00:00,000 --> 00:00:07,000
Now, we know from our previous consideration that there are five ways in which the number of heads could be one.

2
00:00:07,000 --> 00:00:24,000
5!/4! 1! happens to be 5. We also know that there are 32 possible outcomes. It is 2⁵=32 outcomes. This is the size of a truth table. So, 5 out of 32 outcomes has exactly 1 head.

3
00:00:24,000 --> 00:00:34,000
So, I would suggest that the answer is 5/32, and my calculator tells me this is 0.15625. So, that is actually interesting.

4
00:00:34,000 --> 00:00:34,000
If you flip a coin five times, there's a chance that it only comes at head exactly once and that is the probability 0.15625.


@@@
1
00:00:00,000 --> 00:00:00,000
Let's now modify it and ask, what are the chances it becomes a head three times? What's the probability for that to happen?


@@@

@@@
1
00:00:00,000 --> 00:00:14,000
Now, I'm going to make it really difficult. I'm going to give you a coin--let's call it loaded. So, the probability for heads will now be 0.8 and therefore the probability for tails is 0.2.

2
00:00:14,000 --> 00:00:21,000
To make it easier, assume only a 3 coin flips and ask the probability of heads coming up exactly once.

3
00:00:21,000 --> 00:00:21,000
What is that probability for the loaded coin that I gave you here. I recommend answering it using a truth table.


@@@
1
00:00:00,000 --> 00:00:13,000
So here is my truth table of these eight different outcomes. The ones that has head exactly once are this one, this one, and this one, but they're not all equally likely.

2
00:00:13,000 --> 00:00:19,000
Heads, heads, heads is much more likely than say tails, tails, tails because heads has a probability of 0.8^3.

3
00:00:19,000 --> 00:00:26,000
This one here has an outcome probability of 0.8³ while this one has an outcome probability of 0.2^3.

4
00:00:26,000 --> 00:00:36,000
All of the green ones have the same outcome probability. They all have exactly one head 0.8 and two tails.

5
00:00:36,000 --> 00:00:50,000
So as before, we took the probability to be one of these heads in the truth table. This time each of those has a probability of 0.032. That's each one of the three.

6
00:00:50,000 --> 00:01:02,000
Now, we have to consider all of these three outcomes, which means you're going to add 0.032 for each one of those three guys over here and this gives me as an answer 0.096.


@@@
1
00:00:00,000 --> 00:00:10,000
Let's now say for the same loaded coin, we flip that coin five times with that same load of probability over here and we care about the out of five times heads comes exactly four times.

2
00:00:10,000 --> 00:00:10,000
Now, I want you to compute this probability over here, It's a completely nontrivial question. If it's too hard, hit the next button.


@@@
1
00:00:00,000 --> 00:00:12,000
So interestingly enough, we can do the trick as before 5!/4!*1!, which is the number of outcomes at exactly 4 heads and we know that's 5.

2
00:00:12,000 --> 00:00:21,000
Four heads means one tails. There's five ways to place one tails and this one over here. The question now what's the probability of those?

3
00:00:21,000 --> 00:00:52,000
Well, they have heads four times, (0.8)⁴ and tail once to (0.2)¹. So we do is we multiply the total number of outcomes that have this property with each one probability, which happens to be the same because we get exactly four times heads and 1 times tail and multiplying these things together gives us 0.4096.

4
00:00:52,000 --> 00:01:04,000
This over here is indeed 0.4096 and the 5*0.2 cancels the others out, so that's the result in this specific case.


@@@
1
00:00:00,000 --> 09:59:59,000
So, this does not go and get the same for 3 heads. I leave this here, but obviously, the numbers aren't correct anymore.


@@@
1
00:00:00,000 --> 00:00:10,000
And here is my answer 5!/3!*2! is 10, and we have 3 heads, so we put 3 in here and 2 tails, so we put 2 in here.


@@@
1
00:00:00,000 --> 00:00:08,000
Now, you're ready for the real challenge. You flip the coin 12 times in the care about how likely it is to get heads 9 times out of the 12.


@@@
1
00:00:00,000 --> 00:00:12,000
And the approximate answer is 0.236. That's the probability of exactly 9 heads out of 12 coin flips for this heavily loaded coin that mostly gives you heads.

2
00:00:12,000 --> 00:00:12,000
And again, the answer is 12!/12-9. This is 3! 9!. Then we have to compute the probabilities being (0.8)⁹ and 1-0.8 is 0.2³ and that is the number over here.


@@@
1
00:00:03,240 --> 00:00:18,405
In this lesson, we found the probability that a coin would land on heads k times out of n flips. If the probability of heads for the coin is p, we get the following formula for the probability that the number of heads will be k.

2
00:00:18,405 --> 00:00:32,519
The first half is n factorial over n minus k factorial times k factorial. This part keeps track of the total number of ways we can get k heads and n total flips of the coin.

3
00:00:32,520 --> 00:00:43,795
For example, if we were looking at three heads and four flips, we could get this set of flips, or this set, or this set, or this set.

4
00:00:43,795 --> 00:00:55,380
There are four possible ways we can get three heads out of four flips. This part of the formula keeps track of how many possible ways we can get these k heads and n flips.

5
00:00:55,380 --> 00:01:10,575
In this case four. Each one of these ways have the following probability of event occurring. P to the k times 1 minus p to the n minus k. The p to the k keeps track of the probability associated with the number of heads.

6
00:01:10,575 --> 00:01:20,534
And this part keeps track of the probability associated with the number of tails. Altogether, this formula gives the probability of what's called the binomial distribution.

7
00:01:20,534 --> 00:01:31,325
You've now seen how you can take very large experiments with many coin flips and compute the probability that the coin will land heads a certain number of times with this formula.

8
00:01:31,325 --> 00:01:39,740
Although the examples in this lesson use coin flips, you can really perform these calculations with any events that have two outcomes.

9
00:01:39,739 --> 00:01:50,759
Does a customer buy or not? Is a transaction fraud or not? Or if a coin flip is heads or tails. Binomial distributions are used to give us insight about all of these.

10
00:01:50,760 --> 00:02:01,570
Later in the course, you will explore large experiments again using the normal distribution. But first, let's move on to conditional probability where events are no longer independent.


@@@
1
00:00:02,950 --> 00:00:15,379
In real life, events often depend on each other. Say you can be an early bird or night owl. And for the sake of simplicity, let's assume there's a 50 percent chance for each one.

2
00:00:15,380 --> 00:00:24,939
Now whether you decide to go for a run at 5:00 a.m. tomorrow is not entirely independent. I would argue that going for a run at 5:00 a.m. is not likely for most people.

3
00:00:24,940 --> 00:00:37,199
But it is certainly more likely for individuals who are actually awake at that hour. If you're an early bird, the probability might be a two percent chance, while if you're a night owl, the probability might be zero.

4
00:00:37,200 --> 00:00:52,585
You can think of these events as two consecutive coin flips. The first coin determines whether you're an early bird, and the second coin is only flipped given that you are an early bird, and it determines if you go for the run.

5
00:00:52,585 --> 00:01:02,105
We are no longer observing independent events, like we did in the last lesson, where the outcome of the first coin flip didn't affect the outcome of the second.

6
00:01:02,104 --> 00:01:09,139
Starting with this lesson, we're going to look at case studies where the outcome of the first does impact the outcome of the second.


@@@
1
00:00:00,000 --> 00:00:10,000
To do so, let's study a medical example--supposed there's a patient in the hospital who might suffer from a medical condition like cancer.

2
00:00:10,000 --> 00:00:10,000
Let's say the probability of having this cancer is 0.1. That means you can tell me what's the probability of being cancer free.


@@@

@@@
1
00:00:00,000 --> 00:00:07,000
Of course, in reality, we don't know whether a person suffers cancer, but we can run a test like a blood test.

2
00:00:07,000 --> 00:00:19,000
The outcome of it blood test may be positive or negative, but like any good test, it tells me something about the thing I really care about--whether the person has cancer or not.

3
00:00:19,000 --> 00:00:37,000
Let's say, if the person has the cancer, the test comes up positive with the probability of 0.9, and that implies if the person has cancer, the negative outcome will have 0.1 probability and that's because these two things have to add to 1.

4
00:00:37,000 --> 00:00:49,000
I've just given you a fairly complicated notation that says the outcome of the test depends on whether the person has cancer or not.

5
00:00:49,000 --> 00:01:02,000
That is more complicated than everything else we've talked about so far. We call this thing over here a conditional probability, and the way to understand this is a very funny notation.

6
00:01:02,000 --> 00:01:12,000
There's a bar in the middle, and the bar says what's the probability of the stuff on the left given that we assume the stuff on the right is actually the case.

7
00:01:12,000 --> 00:01:27,000
Now, in reality, we don't know whether the person has cancer or not, and in a later unit, we're going to reason about whether the person has cancer given a certain data set, but for now, we assume we have god-like capabilities.

8
00:01:27,000 --> 00:01:35,000
We can tell with absolute certainty that the person has cancer, and we can determine what the outcome of the test is.

9
00:01:35,000 --> 00:01:46,000
This is a test that isn't exactly deterministic--it makes mistakes, but it only makes a mistake in 10% of the cases, as illustrated by the 0.1 down here.

10
00:01:46,000 --> 00:01:57,000
Now, it turns out, I haven't fully specified the test. The same  test might also be applied to a situation where the person does not have cancer.

11
00:01:57,000 --> 00:02:13,000
So this little thing over here is my shortcut of not having cancer. And now, let me say the probability of the test giving me a positive results--a false positive result when there's no cancer is 0.2.

12
00:02:13,000 --> 00:02:13,000
You can now tell me what's the probability of a negative outcome in case we know for a fact the person doesn't have cancer, so please tell me.


@@@
1
00:00:00,000 --> 00:00:09,000
And the answer is 0.8. As I'm sure you noticed in the case where there is cancer, the possible test outcomes add up to 1.


@@@
1
00:00:00,000 --> 00:00:16,000
Look at this, this is very nontrivial but armed with this, we can now build up the truth table for all the cases of the two different variables, cancer and non-cancer and positive and negative tests outcome.

2
00:00:16,000 --> 00:00:29,000
So, let me write down cancer and test and let me go through different possibilities. We could have cancer or not, and the test may come up positive or negative.

3
00:00:29,000 --> 00:00:45,000
So, please give me the probability of the combination of those for the very first one, and as a hint, it's kind of the same as before where we multiply two things, but you have to find the right things to multiple in this table over here.


@@@
1
00:00:00,000 --> 00:00:00,000
And the answer is probability of cancer is 0.1, probability of test being positive given that he has cancer is the one over here--0.9, multiplying those two together gives us 0.09.


@@@
1
00:00:00,000 --> 00:00:08,000
Moving to the next case--what do you think the probability is that the person does have cancer but the test comes back negative?


@@@
1
00:00:00,000 --> 00:00:00,000
And once again, we'd like to refer the corresponding numbers over here on the right side 0.1 for the cancer times the probability of getting a negative result conditioned on having cancer and that is 0.1  0.1, which is 0.01.


@@@

@@@
1
00:00:00,000 --> 00:00:12,000
And here the answer is 0.18 by multiplying the probability of not having cancer, which is 0.9, with the probability of getting a positive test result for a non-cancer patient 0.2.


@@@

@@@
1
00:00:00,000 --> 00:00:00,000
Here you get 0.72, which is the product of not having cancer in the first place 0.9 and the probability of getting a negative test result under the condition of not having cancer.


@@@

@@@
1
00:00:00,000 --> 00:00:10,000
And as usual, the answer is 1. That is, we study in the truth table all possible cases. and when we add up the probabilities, you should always get the answer of 1.


@@@
1
00:00:00,000 --> 00:00:00,000
Now let me ask you a really tricky question. What is the probability of a positive test result? Can you sum or determine, irrespective of whether there's cancer or not, what is the probability you get a positive test result?


@@@
1
00:00:00,000 --> 00:00:08,000
And the result, once again, is found in the truth table, which is why this table is so powerful. Let's look at where in the truth table we get a positive test result.

2
00:00:08,000 --> 00:00:26,693
I would say it is right here, right here. If you take corresponding probabilities of 0.09 and 0.18, and add them up, we get 0.27, and that's the correct answer for getting a positive result.


@@@
1
00:00:00,000 --> 00:00:10,000
Putting all of this into mathematical notation we've given the probability of having cancer and from there, it follows the probability of not having cancer.

2
00:00:10,000 --> 00:00:21,000
And they give me 2 conditional probability that are the test being positive. If we have have cancer, from which we can now predict the probability of the test being negative of having cancer.

3
00:00:21,000 --> 00:00:31,000
And the probability of the test being positive can be cancer free which can complete the probability of a negative test result in the cancer-free case.

4
00:00:31,000 --> 00:00:59,000
So these things are just easily inferred by the 1 minus rule. Then when we read this, you complete the probability of a positive test result as the sum of a positive test result given cancer times the probability of cancer, which is our truth table entry for the combination of P and C plus the same given we don't have of cancer.

5
00:00:59,000 --> 00:01:18,545
Now this notation is confusing and complicated if we ever dive deep into probability, that's called total probability, but it's useful to know that this is very, very intuitive and to further develop intuition let me just give you another exercise of exactly the same type.


@@@
1
00:00:00,184 --> 00:00:10,358
This time around, we have a bag, and in the bag are 2 coins,coin 1 and coin 2. And in advance, we know that coin 1 is fair.

2
00:00:10,358 --> 00:00:22,993
So P of coin 1 of coming up heads is 0.5 whereas coin 2 is loaded, that is, P of coin 2 coming up heads is 0.9.

3
00:00:22,993 --> 00:00:31,296
Quickly, give me the following numbers of the probability of coming up tails for coin 1 and for coin 2.


@@@
1
00:00:00,153 --> 00:00:11,000
And the answer is 0.5 for coin 1and 0.1 for coin 2, because these things have to add up to 1 for each of the coins.


@@@
1
00:00:00,000 --> 00:00:10,856
So now what happens is, I'm going to remove one of the coins from this bag, and each coin, coin 1 or coin 2, is being picked with equal probability.

2
00:00:10,856 --> 00:00:24,110
Let me now flip that coin once, and I want you to tell me, what's the probability that this coin which could be 50% chance fair coin and 50% chance a loaded coin.

3
00:00:24,110 --> 00:00:30,814
What's the probability that this coin comes up heads? Again, this is an exercise in conditional probability.


@@@
1
00:00:00,056 --> 00:00:07,689
And let’s do the truth table. You have a pick event followed by a flip event We can pick coin 1 or coin 2.

2
00:00:07,689 --> 00:00:15,057
There is a 0.5 chance for each of the coins. Then we can flip and get heads or tails for the coin we've chosen.

3
00:00:15,057 --> 00:01:07,094
Now what are the probabilities? I'd argue picking 1 at 0.5 and once I pick the fair coin, I know that the probability of heads is, once again, 0.5 which makes it 0.25 The same is true for picking the fair coin and expecting tails but as we pick the unfair coin with a 0.5 chance we get a 0.9 chance of heads So 0.5 times 0.95 gives you 0.45 whereas the unfair coin, the probability of tails is 0.1 multiply by the probability of picking it at 0.5 gives us 0.05 Now when they ask you, what's the probability of heads we'll find that 2 of those cases indeed come up with heads so if you add 0.25 and 0.45 and we get 0.7.


@@@
1
00:00:00,000 --> 00:00:10,000
Now let me up the ante by flipping this coin twice. Once again, I'm drawing a coin from this bag, and I pick one at 50% chance.

2
00:00:10,000 --> 00:00:19,000
I don't know which one I have picked. It might be fair or loaded. And in flipping it twice, I get first heads, and then tails.

3
00:00:19,000 --> 00:00:28,000
What's the probability that if I do the following, I draw a coin at random with the probabilities shown, and then I flip it twice, that same coin.

4
00:00:28,000 --> 00:00:34,000
I just draw it once and then flip it twice. What's the probability of seeing heads first and then tails?


@@@
1
00:00:00,000 --> 00:00:06,000
This is a non-trivial question, and the right way to do this is to go through the truth table, which I've drawn over here.

2
00:00:06,000 --> 00:00:20,000
There's 3 different things happening. We've taken initial pick of the coin, which can take coin 1 or coin 2 with equal probability, and then you go flip it for the first time, and there's heads or tails outcomes, and we flip it for the second time with the second outcome.

3
00:00:20,000 --> 00:00:29,000
So these different cases summarize my truth table. I now need to observe just the cases where head is followed by tail.

4
00:00:29,000 --> 00:00:37,000
This one right here and over here. Then we compute the probability for those 2 cases. The probability of picking coin 1 is 0.5.

5
00:00:37,000 --> 00:00:49,000
For the fair coin, we get 0.5 for heads, followed by 0.5 for tails. They're together is 0.125. Let's do it with the second case.

6
00:00:49,000 --> 00:01:00,000
There's a 0.5 chance of taking coin 2. Now that one comes up with heads at 0.9. It comes up with tails at 0.1.

7
00:01:00,000 --> 00:01:18,000
So multiply these together, gives us 0.045, a smaller number than up here. Adding these 2 things together results in 0.17, which is the right answer to the question over here.


@@@
1
00:00:00,000 --> 00:00:09,000
Let me do this once again. There are 2 coins in the bag, coin 1 and coin 2. And as before, taking coin 1 at 0.5 probability.

2
00:00:09,000 --> 00:00:20,000
But now I'm telling you that coin 1 is loaded, so give you heads with probability of 1. Think of it as a coin that only has heads.

3
00:00:20,000 --> 00:00:20,000
And coin 2 is also loaded. It gives you heads with 0.6 probability. Now work out for me into this experiment, what's the probability of seeing tails twice?


@@@
1
00:00:00,000 --> 00:00:12,000
And the answer is depressing. If you, once again, draw the truth table, you find, for the different combinations, that if you've drawn coin 1, you'd never see tails.

2
00:00:12,000 --> 00:00:32,000
So this case over here, which indeed has tails, tails. We have 0 probability. We can work this out probability of drawing the first coin at 0.5, but the probability of tails given the first coin must be 0, because the probability of heads is 1, so 0.5 times 0 times 0, that is 0.

3
00:00:32,000 --> 00:00:56,000
So the only case where you might see tails/tails is when you actually drew coin 2, and this has a probability of 0.5 times the probability of tails given that we drew the second coin, which is 0.4 times 0.4 again, and that's the same as 0.08 would have been the correct answer.


@@@
1
00:00:00,000 --> 00:00:07,000
So there're important lessons in what we just learned, the key thing is we talked about conditional probabilities.

2
00:00:07,000 --> 00:00:17,000
We said that the outcome in a variable, like a test is actually not like the random coin flip but it depends on something else, like a disease.

3
00:00:17,000 --> 00:00:26,000
When we looked at this, we were able to predict what's the probability of a test outcome even if we don't know whether the person has a disease or not.

4
00:00:26,000 --> 00:00:44,000
And we did this using the truth table, and in the truth table, we summarized multiple lines. For example, we multiplied the probability of a test outcome condition on this unknown variable, whether the person is diseased multiplied by the probability of the disease being present.

5
00:00:44,000 --> 00:00:52,000
Then we added a second row of the truth table, where our unobserved disease variable took the opposite value of not diseased.

6
00:00:52,000 --> 00:00:58,000
Written this way, it looks really clumsy, but that's effectively what we did when we went to the truth table.

7
00:00:58,000 --> 00:01:20,000
So we now understand that certain coin flips are dependent on other coin flips, so if god, for example, flips the coin of us having a disease or not, then the medical test again has a random outcome, but its probability really depends on whether we have the disease or not.

8
00:01:20,000 --> 00:01:29,000
We have to consider this when we do probabilistic inference. In the next unit, we're going to ask the real question.

9
00:01:29,000 --> 00:01:41,000
Say we really care about whether we have a disease like cancer or not. What do you think the probability is, given that our doctor just gave us a positive test result?


@@@
1
00:00:00,000 --> 00:00:07,000
So this unit is a tough one. We're going to talk about perhaps the holy grail of probabilistic inference.

2
00:00:07,000 --> 00:00:25,000
It's called Bayes Rule. Bayes Rule is based on Reverend Thomas Bayes, who used this principle to infer the existence of God, but in doing so, he created a new family of methods that has vastly influenced artificial intelligence and statistics.


@@@
1
00:00:00,000 --> 00:00:14,000
Let's use the cancer example from my last unit. There's a specific cancer that occurs in 1% of the population, and a test for this cancer and with 90% chance it is positive if they have this cancer, C.

2
00:00:14,000 --> 00:00:24,000
That's usually called the sensitivity. But the test sometimes is positive, even if you don't have C.

3
00:00:24,000 --> 00:00:39,000
Let's say with another 90% chance it's negative if we don't have C. That's usually called the specificity.

4
00:00:39,000 --> 00:00:47,000
So here's my question. Without further symptoms, you take the test, and the test comes back positive.

5
00:00:47,000 --> 00:00:55,000
What do you think is the probability of having that specific type of cancer? To answer this, let's draw a diagram.

6
00:00:55,000 --> 00:01:12,000
Suppose these are all of the people, and some of them, exactly 1%, have cancer. 99% is cancer free. We know there's a test that if you have cancer, correctly diagnose it with 90% chance.

7
00:01:12,000 --> 00:01:26,000
So if we draw the area where the test is positive, cancer and test positive, then this area over here is 90% of the cancer circle.

8
00:01:26,000 --> 00:01:34,000
However, this isn't the full truth. The test sent out as positive even if the person doesn't have cancer.

9
00:01:34,000 --> 00:01:54,000
In fact, in our case, it happened to be in 10% of all cases. So we have to add more area, because as big as 10% of this large area is as big as 10% of this large area where the test might go positive, but the person doesn't have cancer.

10
00:01:54,000 --> 00:02:12,000
So this blue area is 10% of all the area over here minus the little small cancer circle. And clearly, all the area outside these circles corresponds a situation of no cancer, and the test is negative.

11
00:02:12,000 --> 00:02:12,000
So let me ask you again. Suppose we have a positive test, what do you think? Would a prior probability of cancer of 1%, a sensitivity and specificity of 90%, Do you think your new chances are now 90% or 8% or still just 1%?


@@@
1
00:00:00,000 --> 00:00:13,000
And I would argue it's about 8%. In fact, as we see, it will come out at 8 1/3% mathematically. And the way to see this in this diagram is this is the region that should test as positive.

2
00:00:13,000 --> 00:00:20,000
By having a positive test, you know you're in this region, and nothing else matters. You know you're in this circle.

3
00:00:20,000 --> 00:00:29,000
But within this circle, the ratio of the cancerous region relative to the entire region is still pretty small.

4
00:00:29,000 --> 00:00:29,000
It increase, obviously, having a positive test changes your cancer probability, but it only increases by a factor of about 8, as we will see in a second.


@@@
1
00:00:00,000 --> 00:00:12,000
So this is the essence of Bayes Rule, which I'll give to you to you in a second. There's some sort of a prior, of which we mean the probability before you run a test, and then you get some evidence from the test itself.

2
00:00:12,000 --> 00:00:19,000
and that all leads you to what's called a posterior probability. Now this is not really a plus operation.

3
00:00:19,000 --> 00:00:35,000
In fact, in reality, it's more like a multiplication, but semantically, what Bayes Rule does is it incorporates some evidence from the test into your prior probability to arrive at a posterior probability.

4
00:00:35,000 --> 00:00:45,000
So let's make this specific. In our cancer example, we know that the prior probability of cancer is 0.01, which is the same as 1%.

5
00:00:45,000 --> 00:01:03,000
The posterior of the probability of cancer given that our test is positive, abbreviate here as positive, is the product of the prior times our test sensitivity, which is what is the chance of a positive result given that I have cancer?

6
00:01:03,000 --> 00:01:22,000
And you might remember, this was 0.9, or 90%. Now just to warn you, this isn't quite correct. To make this correct, we also have to compute the posterior for the non cancer option, which there is no cancer given a positive test.

7
00:01:22,000 --> 00:01:36,000
And using the prior, we know that P of not C is 0.99. It's minus P of C Times the probability of getting a positive test result given not C.

8
00:01:36,000 --> 00:01:42,000
Realize these 2 equations are the same, but I exchanged C for not C. And this one over here takes a moment to computer.

9
00:01:42,000 --> 00:01:55,000
We know that our test gives us a negative result if it's cancer free, 0.9 chance As a result, it gives us a positive result in the cancer free case, with 10% chance.

10
00:01:55,000 --> 00:02:04,000
Now what's interesting is this is about the correct equation except the probabilities don't add up to 1.

11
00:02:04,000 --> 00:02:20,000
To see I'm going to ask you to compute those, so please give me the exact numbers for the first expression and the second expression written over here using our example up there.


@@@
1
00:00:00,000 --> 00:00:16,000
Obviously, P(C) is 0.01  (times) 0.9 is 0.009, whereas 0.99  (times) 0.1, this guy over here, is 0.099.

2
00:00:16,000 --> 00:00:16,000
What we've computed is here is the absolute area in here which is 0.009 and the absolute area in here which is 0.099.


@@@
1
00:00:00,000 --> 00:00:09,000
The normalization proceeds in two steps. We just normalized these guys to keep ratio the same but make sure they add up to 1.


@@@
1
00:00:00,000 --> 00:00:13,000
And, yes, the answer is 0.108. Technically, what this really means is the probability of a positive test result-- that's the area in the circle that I just marked.


@@@
1
00:00:00,000 --> 00:00:09,000
And now finally, we come up with the actual posterior, whereas this one over here is often called the joint probability of two events.

2
00:00:09,000 --> 00:00:27,000
And the posterior is obtained by dividing this guy over here with this normalizer. So let's do this over here--let's divide this guy over here by this normalizer to get my percent distribution of having cancer given that I received the positive test result.


@@@

@@@
1
00:00:00,000 --> 09:59:59,000
Let's do the same for the non-cancer version, pick the number over here to divide and divide it by this same normalizer.


@@@

@@@

@@@
1
00:00:00,000 --> 00:00:09,000
And the answer is 1 as you will expect. Now this was really challenging. You can see a lot of math in this slide.


@@@
1
00:00:00,000 --> 00:00:09,000
Well, we really said that we had a situation where the prior P(C), a test with a certain sensitivity (Pos/C), and a certain specificity (Neg/₇C).

2
00:00:09,000 --> 00:00:24,000
When you receive, say, a positive test result, what you do is, you take your prior P(C) you multiply in the probability of this test result, given C, and you multiply in the probability of the test result given (Neg/₇C).

3
00:00:24,000 --> 00:00:31,000
So, this is your branch for the consideration that you have cancer. This is your branch for the consideration of no cancer.

4
00:00:31,000 --> 00:00:37,000
When you're done with this, you arrive at a number that now combines the cancer hypothesis with the test result.

5
00:00:37,000 --> 00:00:49,000
Look for the cancer hypothesis and the no cancer hypothesis. Now, what you do, you add those up and then normally don't add up to one.

6
00:00:49,000 --> 00:00:56,000
You get a certain quantity which happens to be the total probability that the test is what it was in this case positive.

7
00:00:56,000 --> 00:01:06,000
And all you do next is divide or normalize this thing over here by the sum over here and the same on the right side.

8
00:01:06,000 --> 00:01:16,000
The divider is the same for both cases because this is your cancer branch, your non-cancer branch, but this score does not depend on the cancer variable anymore.

9
00:01:16,000 --> 00:01:24,000
What you now get out is the desired posterior probability, and those add up to 1 if you did everything correct, as shown over here.


@@@
1
00:00:00,000 --> 00:00:09,000
Now, the same algorithm works if your test says negative. We'll practice this in just 1 second. Suppose your test result says negative.

2
00:00:09,000 --> 00:00:17,000
You could still ask the same question: Now, what's my probability having cancer or not? But now all the positives in here become negatives.

3
00:00:17,000 --> 00:00:37,000
The sum is the total probability of negative test results,  and we may now divide by this score, you now get the posterior probability for cancer and non-cancer assuming you had a negative test result, which of course to be much, much more favorable for you because none of us wants to have cancer.

4
00:00:37,000 --> 00:00:37,000
So, look at this for a while and let's now do the calculation for the negative case using the same numbers I gave you before, and with the step by step this time around so it can really guide you through the process.


@@@
1
00:00:00,000 --> 00:00:10,000
We begin with our prior probability, our sensitivity and our specifitivity, and I want you to begin by filling in all the missing values.

2
00:00:10,000 --> 00:00:10,000
So, there's the probability of no cancer, probability of negative, which is negation of positive, given C, and probability of negative-positive given not C.


@@@

@@@
1
00:00:00,000 --> 00:00:00,000
Now assume the test comes back negative, the same logic applies as before. So please give me the combined probability of cancer given the negative test result and the combined probability of being cancer-free given the negative test result.


@@@
1
00:00:00,000 --> 00:00:17,000
The number here is 0.001 and it's the product of my prior for cancer which is 0.01, and the probability of getting a negative result in the case of cancer which is right over here, 0.1.

2
00:00:17,000 --> 00:00:39,000
If I multiply these two things together, I get 0.001. The probability here is 0.891. And when I'm multiplying is the prior probability of not having cancer which is 0.99 with the probability of seeing a negative result in the case of not having cancer, and that is the one right over here, 0.9.


@@@

@@@

@@@
1
00:00:00,000 --> 00:00:11,000
And now finally tell me what is posterior probability of cancer given that we know we had a negative test result and the probability of negative cancer given there is a negative test result.


@@@
1
00:00:00,000 --> 00:00:29,000
This is approximately 0.0011, which we get by dividing 0.001 by the normalizer 0.892, and the posterior probability of being cancer-free after the test is approximately 0.9989, and that's obtained by dividing this probability over here by the normalizer and not surprisingly, these two values indeed add up to 1.

2
00:00:29,000 --> 00:00:41,000
Now, what's remarkable about this outcome is really what it means. Before the test, we had a 1% chance of having cancer, now, we have about a 0.9% chance of having cancer.

3
00:00:41,000 --> 00:00:51,000
So, a cancer probability went down by about a factor of 9. So, the test really helped us gaining confidence that we are cancer-free.

4
00:00:51,000 --> 00:00:51,000
Conversely, before we had a 99% chance of being cancer free, now it's 99.89%. So, all the numbers are working exactly how we expect them to work.


@@@
1
00:00:00,000 --> 00:00:10,000
Let me now make your life harder. Suppose our probability of a certain other kind of disease is 0.1, so 10% of the population has it.

2
00:00:10,000 --> 00:00:20,000
Our test in the positive case is really informative, but there's a 0.5 chance that if I'm cancer-free the test, indeed, says the same thing.

3
00:00:20,000 --> 00:00:20,000
So the sensitivity is high, the specificity is lower. And let's start by filling in the first 3 of them.


@@@
1
00:00:00,000 --> 00:00:10,000
Obviously, these are just 1 minus those: 0.9, 0.1, and 0.5. Notice that these two numbers may very well be different.

2
00:00:10,000 --> 00:00:21,000
There is no contradiction here. These guys have to add up to 1, so given ¬C, the probability of positive and negative have to add up to 1, but these guys don't.

3
00:00:21,000 --> 00:00:21,000
It takes a lot of practice to understand which numbers have to add up to 1. But I set it up in a way that you should have gotten it right.


@@@

@@@
1
00:00:00,000 --> 09:59:59,000
And the answer is 0.01. P(C) = 0.1, and P(Neg│C) is also 0.1, so if you multiply those two they are 0.01.


@@@

@@@

@@@

@@@

@@@

@@@
1
00:00:00,000 --> 00:00:27,000
The first one is 0.01 divided by normalized 0.46 and that gives us 0.0217, and the second one is called over here 0.45 divided by 0.46 and that gives us 0.9783 and uses the correct posteriors, restarted our chance of 10% of having cancer.


@@@
1
00:00:00,000 --> 09:59:59,000
Let's now consider the case that the test result is positive, and I want you to just give me the two numbers over here and not the other ones.


@@@
1
00:00:00,000 --> 00:00:18,000
So once again, we have 0.9, 0.1, and 0.5 over here. Very quickly multiplying this guy with this girl over here 0.09. This guy with this girl over here 0.45.

2
00:00:18,000 --> 00:00:38,000
Adding them up gives us 0.54, and dividing those correspondingly 0.9 divided by 0.54 gives us 0.166 and so on and 0.833 and so on for dividing 0.45 by 0.54.

3
00:00:38,000 --> 00:00:46,000
And with this means, with the positive test result, our chance of cancer increased from 0.1 to 0.16.

4
00:00:46,000 --> 00:00:46,000
Obviously, our chance of having no cancer decreased accordingly. You got this, so let's just summarize.


@@@
1
00:00:00,000 --> 00:00:08,000
In Bayes rule, we have a hidden variable we care about--whether they have cancer or not. But we can't measure it directly and instead we have a test.

2
00:00:08,000 --> 00:00:22,000
We have a prior of how frequent this variable is true and the test is generally characterized by how often it says positive when the variable is true and how often it is negative and the variable is false.

3
00:00:22,000 --> 00:00:41,000
Bayes rule takes a prior, multiplies in the measurement, which in this case we assume to be the positive measurement to give us a new variable and does the same for all actual measurement, given the opposite assumption about our hidden variable of cancer and that multiplication gives us this guy over here.

4
00:00:41,000 --> 00:00:55,000
We add those two things up and then it gives us a new variable and then we divide these guys to arrive the best estimate of the hidden variable c given our test result.

5
00:00:55,000 --> 00:01:01,000
And this example, I used the positive example is a test result but it might do the same with a negative example.

6
00:01:01,000 --> 00:01:09,000
This was exactly the same as in our diagram in the beginning. There was a prior of our case, we have this specific variable to be true.

7
00:01:09,000 --> 00:01:22,000
We noticed inside this prior, it can cover the region for which our test result applies. We noticed that test result also apply when the condition is not fulfilled.

8
00:01:22,000 --> 00:01:31,000
So, this expression over here and this expression over here corresponds exactly to the red area over here and the green area over here.

9
00:01:31,000 --> 00:01:44,000
But then we noticed that these two areas don't add up to 1. The reason is that's lots of stuff outside, so we calculated the total area which was this expression over here, pPos.

10
00:01:44,000 --> 00:01:44,000
And then we normalized these two things over here by the total area to get the relative area that is assigned the red thing versus the green thing and at this time by just dividing by the total area in this region over here; thereby, getting rid of any of the other cases.


@@@
1
00:00:00,000 --> 00:00:05,000
Now, I should say if we got this, you don't find any immediate significant about statistics and probability.

2
00:00:05,000 --> 00:00:16,000
This is totally nontrivial, but it comes in very handy. So, I'm going to practice this with you using a second example. In this case, you are a robot.

3
00:00:16,000 --> 00:00:36,000
This robot lives in a world of exactly two places. There is a red place and a green place, R and G. Now, I say initially, this robot has no clue where it is, so the prior probability for either place, red or green, is 0.5.

4
00:00:36,000 --> 00:00:41,000
It also has a sensor as it can see through its eyes, but his sensor seems to be somewhat unreliable.

5
00:00:41,000 --> 00:00:55,000
So, the probability of seeing red at the red grid cell is 0.8, and the probability of seeing green at the green cell is also 0.8.

6
00:00:55,000 --> 00:01:14,000
Now, I suppose the robot sees red. What are now the posterior probabilities that the robot is at the red cell given that it just saw red and conversely what's the probability that it's at the green cell even though it saw red.


@@@
1
00:00:00,000 --> 00:00:12,000
In this example, it gives us funny numbers. It was 3 for red as 0.8 and one for the green as 0.2. And it's all to do with the fact that in the beginning where there had no clue where it is.

2
00:00:12,000 --> 00:00:12,000
The joint for red after seeing red is 0.4. The same for green is 0.1. 0.4+0.1, S to 0.5. If you normalized 0.4 divided by 0.5, you get 0.8, and if you normalized 0.1 by 0.5, you get 0.2.


@@@
1
00:00:00,000 --> 00:00:10,000
If I now change some parameters--say the robot knows the probability that it's red, and therefore, the probability 1 is under the green cell as a prior.

2
00:00:10,000 --> 00:00:10,000
Please calculate once again using Bayes rule these posteriors. I have to warn you--this is a bit of a tricky case.


@@@
1
00:00:00,000 --> 00:00:12,000
And the answer is, the prior isn't affected by the measurement, so the probability of 0 is at red, and the probability of 1 at green, despite the fact that it's all red.

2
00:00:12,000 --> 00:00:25,000
To see this, you find the joint of seeing it red and seeing red is 0 times 0.8, that's 0. That's the same join for green is 1 times 0.2.

3
00:00:25,000 --> 00:00:37,000
So you have to normalize 0 and 0.2. The sum of those is 0.2. So let's divide 0 by 0.2, gives us 0, and 0.2 divided by 0.2 gives us 1.


@@@
1
00:00:00,000 --> 00:00:07,000
To change this example even further. Let's make this over here a 0.5 and revert back to a uniform prior.


@@@
1
00:00:00,000 --> 00:00:21,000
Now the answer is about 0.615 or 0.385. These are approximate. Once again, 0.5 times 0.8 is 0.4. 0.5 minus this guy is again 0.5.

2
00:00:21,000 --> 00:00:21,000
0.25, add those up, 0.65, normalizing 0.4 divided by 0.65 gives approximately 0.615. 0.25 divided by 0.65 is approximately 0.385, so now you you've got it.


@@@
1
00:00:00,000 --> 00:00:08,000
I will now make your life really hard. Suppose there are 3 places in the world, not just 2. There are a red one and 2 green ones.

2
00:00:08,000 --> 00:00:20,000
And for simplicity, we'll call them A, B, and C. Let's assume that all of them have the same prior probability of 1/3 or 0.333, so on.

3
00:00:20,000 --> 00:00:35,000
Let's say the robot sees red, and as before, the probability of seeing red in Cell A is 0.9. The probability of seeing green in Cell B 0.9.

4
00:00:35,000 --> 00:00:46,000
Probability of seeing green in Cell C is also 0.9. So what I've changed is, I've given the hidden variable, kind of like the cancer/non cancer variable, 3 states.

5
00:00:46,000 --> 00:00:59,000
There's not just 2 as before, A or B. It's now A, B, or C. Let's solve this problem together, because it follows exactly the same recipe as before, even though it might not be obvious.

6
00:00:59,000 --> 00:01:08,425
So let me ask you, what is the joint of being in Cell A after having seen the red color? This is the joint as before.


@@@

@@@

@@@
1
00:00:00,000 --> 00:00:12,000
Well, the answer is you multiply our prior of 1/3 with the probability of seeing red in Cell B, as seeing green at 0.9 probability, so red is 0.1.


@@@

@@@
1
00:00:00,000 --> 00:00:00,000
And the answer is exactly the same as this over here, because the prior is the same for B and C, and those probabilities are the same for B and C, so they should be exactly the same.


@@@

@@@

@@@
1
00:00:00,000 --> 00:00:08,615
And now we calculate the desired posterior probability for all 3 possible outcomes. So please plug them in over here.


@@@
1
00:00:00,000 --> 00:00:10,850
As usual, we divide this guy over here by the normalizer, which gives us 0.818. Realize all these numbers are a little bit approximate here.

2
00:00:10,850 --> 00:00:23,646
Same for this guy, it's approximately 0.091. And this is completely symmetrical, 0.091. And surprise, these guys all add up to 1.


@@@
1
00:00:00,000 --> 00:00:07,000
So what have you learned? In Bayes Rule, there will be more than just 2 underlying causes of cancer/non cancer.

2
00:00:07,000 --> 00:00:14,000
There might be 3, 4, or 5, any number. We can apply exactly the same math, but we have to keep track of more values.

3
00:00:14,000 --> 00:00:23,000
In fact, the robot might also have more than just 2 test outcomes. Here was red or green, but it could be red, green, or blue.

4
00:00:23,000 --> 00:00:31,000
And this means that our measurement probability will be more elaborate. I have to give you more information, but the math remains exactly the same.

5
00:00:31,000 --> 00:00:42,000
We can now deal with very large problems that have many possible hidden causes of where the world might be, and we can still apply Bayes Rule to find all of these numbers.


@@@
1
00:00:00,000 --> 00:00:10,000
This test is actually directly taken from my life and you'll smile when you see my problem. I used to travel a lot. It was so bad for a while.

2
00:00:10,000 --> 00:00:24,000
I would find myself in a bed not knowing what country I'm in. I kid you not. So let's say, I'm gone 60% of my time and I'm at home only 40% of my time.

3
00:00:24,000 --> 00:00:34,000
Now at summer, I live in California and it truly doesn't rain in the summer. Whereas in many of the countries I have traveled to, there's a much higher chance of rain.

4
00:00:34,000 --> 00:00:45,000
So let's now say, I lie in my bed, here I am lying in bed, and I wake up and I open the window and I see it's raining.

5
00:00:45,000 --> 00:00:54,897
Let's now apply Bayes rule--What do you think is the probability I'm home now that I see it's raining--just give me this one number.


@@@
1
00:00:00,000 --> 00:00:32,000
And I get 0.0217, which is a really small thing. And the way I get there is what taking home times the probability of rain at home normalizing it using the same number of a year plus the calculation for the same probability of being gone is 0.6 times the rain I've been gone has a probability of 0.3 and that results is 0.0217 or the better of 2%--did you get this?

2
00:00:32,000 --> 00:01:07,000
If so, you now understand something that's really interesting. You're able to look at a hidden variable, understand how a test can give you information back about this hidden variable and that's really cool because it allows you to apply the same scheme to great many practical problems in the world--congratulations! In our next unit, which is optional, I like you to program all of this so you can try the same thing in an actual program interface and writes software that implements things such as Bayes rule.


@@@
1
00:00:00,000 --> 00:00:06,009
Let's talk a bit more about why uncertainty is so important in the field of robotics and self-driving cars.

2
00:00:06,009 --> 00:00:13,324
We know that measurements like the speed, the direction, and the location of a car are challenging to measure and we can't measure them perfectly.

3
00:00:13,324 --> 00:00:19,480
There's some uncertainty in each of these measurements. We also know that many of these measurements affect one another.

4
00:00:19,480 --> 00:00:28,054
For example, if we are uncertain about the location of a car, we can reduce that uncertainty by collecting data about the car's surroundings and its movement.

5
00:00:28,053 --> 00:00:35,129
Self-driving cars measure all of these things from a car's speed, to the scenery and objects that surround it, with sensors.

6
00:00:35,130 --> 00:00:48,590
And though these sensor measurements aren't perfect, when the information they provide is combined using conditional probability and something called Bayes rule, they can form a reliable representation of a car's position, its movement and its environment.


@@@
1
00:00:00,000 --> 00:00:08,285
Once we gather sensor data about the car's surroundings and its movement, we can then use this information to improve our initial location prediction.

2
00:00:08,285 --> 00:00:22,225
For example, say we sense lane markers and specific terrain, and we say, hmm. Actually, we know from previously collected data that if we sense landlines close to the sides of the car, the car is probably located in the center of the lane.

3
00:00:22,225 --> 00:00:28,774
We also know that if we sense that our tires are pointing to the right, we're probably on a curved section of the road.

4
00:00:28,774 --> 00:00:37,674
So this sensor data, combined with what we already know about the road and the car, gives us more information about where our location is most likely to be.

5
00:00:37,674 --> 00:00:43,814
So using the sensor information, we can improve our initial prediction and better estimate our car's location.

6
00:00:43,814 --> 00:00:52,640
Bayes rule gives us a mathematical way to correct our measurements, and let's us move from an uncertain prior belief to something more and more probable.

7
00:00:52,640 --> 00:00:59,349
You'll see Bayes rule come up again and again in robotics. And in this lesson, you'll gain a greater understanding of Bayes rule.


@@@
1
00:00:00,080 --> 00:00:11,279
Great job finishing this lesson on Bayes rule. At this point, you've now gained a ton of valuable knowledge about probability, conditional probability and Bayes rule.

2
00:00:11,279 --> 00:00:16,949
To reinforce your understanding of these topics, let's go through some probability practice using Python.


@@@
1
00:00:00,000 --> 00:00:13,710
Bayes rule is extremely important in robotics and it can be described in one sentence. Given an initial prediction, if we gather additional related data, data that our initial prediction depends on, we can improve that prediction.

2
00:00:13,710 --> 00:00:20,655
For example, let's say our initial prediction also known as a prior belief, is an estimate of a car's location on a road.

3
00:00:20,655 --> 00:00:29,490
This might be the location given by a slightly inaccurate GPS signal. Then, we use sensors to gather data about the car's surroundings and how the car is moving.


@@@
1
00:00:00,000 --> 00:00:07,025
Now that you've seen some examples and the math involved with them, you're going to apply this knowledge to problems using Python.

2
00:00:07,025 --> 00:00:16,390
This lesson includes screencast and Jupyter notebooks to help you practice using Python to explore the topics of probability you just learned. Let's get started.


@@@
1
00:00:00,000 --> 00:00:08,250
NumPy has a module for random sampling, that makes it really easy for us to simulate random events like coin flips and Python.

2
00:00:08,250 --> 00:00:15,424
We're going to start with a simple example, simulating a single coin flip. For this, we'll use a function called randint.

3
00:00:15,425 --> 00:00:24,640
This generates however many random integers, we specify between a lower bound inclusive and upper bound exclusive.

4
00:00:24,640 --> 00:00:31,019
We can use these integers to represent the outcomes of our events, like coin flips. Let's try this out.

5
00:00:31,019 --> 00:00:39,895
We call randint from NumPy's random sampling module like this. Let's use zero to represent heads and one to represent tails.

6
00:00:39,895 --> 00:00:49,600
Let's make this function randomly produce zero or one. The lower bound would be zero and the highest would be two, because it's exclusive.

7
00:00:49,600 --> 00:01:05,644
Since the lower bound is zero, this is actually the default. We don't need to include it. If we run the cell again and again, we'll keep getting a random outcome of zero or one or, we can specify a size to just give us more events.

8
00:01:05,644 --> 00:01:18,545
Cool. Now, we have the results of 10,000 random coin flips. The average of these outcomes produced here should be very close to 0.5, since right now there's an equal probability of getting a zero or a one.

9
00:01:18,545 --> 00:01:27,980
But what if we want to flip a biased coin, that had a higher probability of landing on heads. There's actually another function for this called random.choice.

10
00:01:27,980 --> 00:01:40,179
This function works a little differently. It randomly chooses a number of values from an array that you provide and you can also give it a set of probabilities for each value in that array.

11
00:01:40,180 --> 00:01:53,704
Let's call random.choice and give it the array of the possible outcomes zero and one. If we don't specify probabilities, it gives us each value and an equal probability by default.

12
00:01:53,704 --> 00:02:00,355
We can run this similar to the above example in this way. So the average is very close to 0.5 Again.

13
00:02:00,355 --> 00:02:09,784
To make this a biased coin, we can specify the parameter p with an array of probabilities. Say 0.8 for heads and 0-2 for tails.

14
00:02:09,784 --> 00:02:18,729
Now, you can see that the mean is close to 0.2 which makes sense because the zero or heads should be chosen 80% of the time.

15
00:02:18,729 --> 00:02:25,134
You probably noticed that the mean values we get from these outcomes don't always reflect the true probabilities perfectly.

16
00:02:25,134 --> 00:02:31,900
However, they do tend to reflect the true probability more closely as we increase the number of flips.


@@@
1
00:00:00,000 --> 00:00:08,759
So far, we've been stimulating event outcomes by generating random numbers with NumPy's random.randint and random.choice.

2
00:00:08,759 --> 00:00:24,585
However, there is a better function for simulating large binomial experiments like coin flips. Here, you see NumPy's random.binomial function, which simulates a number of events n, which each have probability of success p.

3
00:00:24,585 --> 00:00:32,280
Success just represents one of the two outcomes of the event. Really, either outcome could be the success.

4
00:00:32,280 --> 00:00:38,792
For example, if our event is flipping a fair coin 10 times, we could define success as the number of heads.

5
00:00:38,792 --> 00:00:48,164
n would be the number of flips. In this case, 10, and p would be the probability of heads for each flip which is 0.5.

6
00:00:48,164 --> 00:00:58,979
To try this out, let's set n equal to 10 and p equal to 0.5. Notice, this returns 1 integer instead of an array of 10 outcomes.

7
00:00:58,979 --> 00:01:07,125
Since this function is only for binomial outcomes, it can simplify the output by just returning the number of successes.

8
00:01:07,125 --> 00:01:15,899
In this case, 4. This is the number of heads. Again, we can run the simulation many times. Let's run this 20 times.

9
00:01:15,900 --> 00:01:23,340
Each number in this array represents the number of heads that resulted from each test of 10 coin flips.

10
00:01:23,340 --> 00:01:33,969
This is the number of heads on the first 10 coin flips, while this one is the number on the second 10 coin flips and so on, for 20 different flippings of the coin.

11
00:01:33,969 --> 00:01:41,269
Let's find the mean number of heads for these tests. Since this is a fair coin, we would expect this mean to be close to 5.

12
00:01:41,269 --> 00:01:49,724
You can see that this is a little bit of a ways away, probably because we only ran 20 tests. Let's see what happens if we increase the number of tests.

13
00:01:49,724 --> 00:01:56,549
Since these events are random, it's not guaranteed that each simulation will perfectly average five heads.

14
00:01:56,549 --> 00:02:03,045
However, as this number of tests increase, the simulation more closely reflects the fairness of the coin.

15
00:02:03,045 --> 00:02:12,439
With 10,000 tests, the mean is much closer to five heads. Let's use matplotlib to plot a histogram of the outcomes in this simulation.


@@@
1
00:00:00,000 --> 00:00:12,485
In this lesson, you put your new probability skills to practice using Python. In order to gain an understanding of some of the complex topics in the next sections, we'll be working with Python more and more.

2
00:00:12,484 --> 00:00:19,675
It's often easier to understand these complex ideas through simulations and Python than it is to prove them mathematically.

3
00:00:19,675 --> 00:00:25,230
You will see a bit of both in the upcoming lessons. So your practice here will definitely come in handy.


@@@
1
00:00:00,000 --> 00:00:09,000
What you are about to see is one of the most transformative parts of modern statistics and it uses things if like we've never seen before.

2
00:00:09,000 --> 00:00:24,000
We start out with the binomial distribution that you're familiar with from our last unit and then we move into the central limit theorem which basically means we take a number of coin flips to infinity.

3
00:00:24,000 --> 00:00:35,000
From that, we arrive at the normal distribution which is basis to so much in statistics-- all of testing and confidence intervals are defined though the normal distribution.

4
00:00:35,000 --> 00:00:51,000
And the reason why this matters is much of what we've done in coin flips had one or two coin flips but in statistics experiments, you often have 1000 of patients or 1000 of data points and then starting the normal distribution as an approximation to the binomial distribution is much more practical.

5
00:00:51,000 --> 00:01:10,000
So let's start--so when we start with our now well established formula for binomial distributions where N is the number of coin flips, k is how often it comes up with heads and p is the probability that the coin comes up with heads, usually 0.5.

6
00:01:10,000 --> 00:01:32,000
And I want to graph for fix n this function here. K can go from 0 all the way to 20. Instead of letting you compute this thing over here for all those different values of k, I'm going to ask a different question--suppose you have a fair coin, what do you think this value takes on its maximum value, which value of k maximizes our expression.

7
00:01:32,000 --> 00:01:32,000
I know you can't really know this but with some thought, I believe you'll arrive at the correct answer.


@@@
1
00:00:00,000 --> 00:00:16,000
I just programmed and ran the experiment, and the answer is 10. The reason why the answer is 10 is because the number of combinations to place 10 positives and 10 negatives into our list of 20 is larger than any other number.


@@@
1
00:00:00,000 --> 00:00:09,000
The other interesting thing is things fall down in the interesting fashion as you deviate from 10, 11, 12, 13, 14 all the way to 0 or 20.

2
00:00:09,000 --> 00:00:21,000
Obviously we got a curve that looks a bit like this. This curves is often called a bell curve because it's quite feasible to think of it as a church bell-- that's move left and right and rings the bells.

3
00:00:21,000 --> 00:00:42,000
I did a related experiment--actually I bought a piece of software to flip a coin 1000 times and if you did the last optional units on programming, you wrote a piece of software to flip a coin a 1000 times and from that, I looked at the empirical frequency which is the same as that count of heads divided by a 1000, but this one scales between 0 and 1.

4
00:00:42,000 --> 00:00:52,000
I called this thing an experiment--I flip the coin a 1000 times, out comes a one singular number which is the ratio of heads to the total number of experiments.

5
00:00:52,000 --> 00:01:08,000
It should be 0.5 in the ideal case but often it's a little bit off 0.5. I repeated this experiment 1000 times and that means I've got 1000 samples of this ratio over here. When I do this, I got a whole bunch of means.

6
00:01:08,000 --> 00:01:19,000
When I run a histogram over those, I might see a curve. What shape do you think the curve has--is it going to be like this, is it going to be like this, like this or like this, all are focused on 0.5.


@@@
1
00:00:00,000 --> 00:00:14,000
And it's this one. Let me show you. Here's a typical one, and I apologize the axis over here can't really be read, but you can take with faith that the center is 0.5, and you can see the characteristic bell curve for this simple coin-flipping experiment.

2
00:00:14,000 --> 00:00:14,000
For this run the mean was 0.50006. If I run it again, I get a different sample. And there is some randomness involved as this bar over here illustrates, but over the randomness you can clearly see the bell-shaped curve that flattens off on the sides.


@@@
1
00:00:00,000 --> 00:00:00,000
The question really is can we find a better formula for this bell-shaped curve. The answer is, well, take your guess.


@@@
1
00:00:00,000 --> 00:00:09,000
The answer is a resounding yes. Even more so, what I'm going to show you doesn't just apply to binomial distributions with fair coins.

2
00:00:09,000 --> 00:00:16,000
It applies to almost any distribution that is sampled many, many times, which is a  very deep statistical result.


@@@
1
00:00:00,000 --> 00:00:09,000
I will define for you a normal distribution with a specific mean that's often called µ, Greek letter µ, and a variance that's often called σ².

2
00:00:09,000 --> 00:00:17,000
We already know that variance is a quadratic expression. In normal land we often use µ and σ². Let's do this.

3
00:00:17,000 --> 00:00:26,000
The very first element is that for any outcome x we write the quadratic difference between this outcome x and µ.

4
00:00:26,000 --> 00:00:33,000
This is indeed a function in x. So, look at this. Here are four possible hypotheses of what this function might look like.

5
00:00:33,000 --> 00:00:42,000
Each case is µ is on the right side some where. The horizontal axis is x, and we're graphing f(x). The first I'll give you is a triangular function.

6
00:00:42,000 --> 00:00:49,000
The second is a quadratic function. The third one is a negative quadratic function. And the fourth one is a quadratic function that doesn't quite touch µ.


@@@
1
00:00:00,000 --> 00:00:09,000
I would submit that it's this one over here. The reason is this expression is 0 when x = µ. As a result, it can't be the fourth of these choices.

2
00:00:09,000 --> 00:00:17,000
It's strictly non-negative, so it can't go down into the negative area, so this one is being out ruled. It's quadratic.


@@@
1
00:00:00,000 --> 00:00:07,000
The next thing I'll do is I'll divide it by σ². Without telling you why I'm doing this, I want to see what the effect is.

2
00:00:07,000 --> 00:00:18,000
Suppose σ² = 4. That means we have a variance of 4 and a standard deviation of 2. I've given you already the quadratic function when it isn't divided by σ².

3
00:00:18,000 --> 00:00:31,000
It's the same as saying σ² = 1. What I'd like to know is whether our new version where σ² = 4 makes this quadratic wider or whether it makes it narrower, assuming that this is our new function f(x).


@@@
1
00:00:00,000 --> 00:00:09,000
The answer is it makes it wider. To see why is this affects the vertical dimension--the output and scales it down by a factor of 4.

2
00:00:09,000 --> 00:00:16,000
So that we said before gets dragged down by a factor of 4. That means this point over here finds itself here.

3
00:00:16,000 --> 00:00:26,000
This one over here finds itself here. That means we'll widen out the quadratic. Observe that large variances yield wide quadratics.


@@@
1
00:00:00,000 --> 00:00:00,000
In particular, if we now look at the quadratic over here, which is much tighter, which of the following potential σ² would you think is best representative of this narrow function over here, provided that this is the quadratic that corresponds to σ² = 1. Check one of those four--4, 1, ¼, and 0.


@@@
1
00:00:00,000 --> 00:00:06,000
It follows it's got to be something like a quarter. We already learned that 4 widens the quadratic, so that can't be it.

2
00:00:06,000 --> 00:00:18,000
One is already shown over here. Zero makes no sense because we have division by 0 which you can think of as a quadratic that shoots up almost like a straight line, but honestly it doesn't make any sense.

3
00:00:18,000 --> 00:00:18,000
A quarter is the one that really describes this particular quadratic the best. Now, that's great. Now we understand this expression over here.


@@@
1
00:00:00,000 --> 00:00:07,000
Let's go further, and let's now take this function and multiply it by -½. Again, I ask you what the affect it.

2
00:00:07,000 --> 00:00:17,000
If this is your original quadratic, then what do we get? We already know that it's going to flatten it, because you are dividing the f value by half, but are we going to get something like this or perhaps something like this?


@@@
1
00:00:00,000 --> 00:00:07,000
And quite interestingly, we inverted the sign, so all of a sudden the function is negative. Quite obviously, green is the correct answer.

2
00:00:07,000 --> 00:00:07,000
So, now we have a quadratic that points down into the negative space whose maximum value is 0 and otherwise, it's strictly negative. That's this function f over here.


@@@
1
00:00:00,000 --> 00:00:06,000
And now I'm going to take the most extreme of all steps. I'm going to make this the exponent of the e function.

2
00:00:06,000 --> 00:00:17,000
Remember, the inner argument is a quadratic that points down. This a bit does depend on σ. This mean is µ so I call this f(x) where f(x) maximize.

3
00:00:17,000 --> 00:00:17,000
And I'll give you several choices for x=µ, x=0, x=-infinity, or x=+infinity. Where will this thing be the largest?


@@@
1
00:00:00,000 --> 00:00:10,000
to understand the solution, it's useful to draw the exponential function. e⁰ is 1 and then it goes up exponentially to really large numbers.

2
00:00:10,000 --> 00:00:18,000
If you've ever heard Ray Kurzweil  talk about the future of society, you've seen these curves--everything goes up exponentially.

3
00:00:18,000 --> 00:00:24,000
Everything is just exponential. And further, if you go back in time to negative values, this thing slowly drifts down to 0.

4
00:00:24,000 --> 00:00:40,000
Of course, that's not very exciting, so we never talk about exponential in the negative space. However, it turns out that all the arguments of the exponential are at best 0 and otherwise are negative, because the exponential is monotonic-- that is the larger its argument, the larger its exponential value.

5
00:00:40,000 --> 00:00:40,000
It ought to be optimized where this thing over here is the largest, and where is that the case? Well, it's exactly where µ hits 0.


@@@
1
00:00:00,000 --> 00:00:09,000
Let me ask you another question. What is the value of this function if we go to the point where it's maximum, which is x = µ? That's the way to write this.


@@@
1
00:00:00,000 --> 00:00:07,000
Quite interestingly, even though this formula looks complex, it a really easy answer, which is when x = µ this thing here is 0.


@@@
1
00:00:00,000 --> 00:00:10,000
Next I'd like to know where is f(x) minimized? For what value of x would we get the possible smallest value of this entire expression over here?


@@@
1
00:00:00,000 --> 00:00:09,000
The answer is now ± ∞. If you look at this, if you put a really large positive or negative value in, the difference to any µ will be enormous.

2
00:00:09,000 --> 00:00:15,000
The square will be even more enormous. Therefore, this entire expression on the right side will be huge.

3
00:00:15,000 --> 00:00:15,000
Put a minus sign in front of it, and you have a hugely negative number. You have e^-∞. e^-∞ drive the e curve all the way to the left where this just ends up to be minimized.


@@@

@@@
1
00:00:00,000 --> 00:00:08,000
The answer is 0. As x goes to infinity, this expression goes to negative infinity. The exponential of negative infinity converges to 0.


@@@
1
00:00:00,000 --> 00:00:11,000
We have a function f that assumes the value 1 when x = µ that goes to 0 when x goes to ±∞. It so happens that it looks like a bell curve.

2
00:00:11,000 --> 00:00:16,000
The fact that it looks like a bell curve is not entirely obvious, but you have to take my word for it.

3
00:00:16,000 --> 00:00:24,000
This--what I would consider a relatively simple formula-- describes the limit of making infinitely many coin flips.

4
00:00:24,000 --> 00:00:32,000
In fact, it describes the limit of computing a mean over any set of experiments. This is a very powerful result.

5
00:00:32,000 --> 00:00:42,000
No matter what you do when you drive n to very large numbers you get a bell curve like this. There is one flaw here, and I'll tell you about the flaw without going into detail.

6
00:00:42,000 --> 00:00:51,000
That is the area underneath this curve doesn't always add up to 1. In fact, without proof, it adds up to √2πσ².

7
00:00:51,000 --> 00:01:04,000
The reason why this matters is deeply buried in probability theory. But it turns out we want all these areas to add up to 1 just as much as we wanted a coin flip and its complement to add up to 1.

8
00:01:04,000 --> 00:01:21,000
The true normal distribution is normalized by just the inverse of this thing over here--1/√2πσ². So, that is the normal distribution of any value x indexed by the parameter µ and σ².

9
00:01:21,000 --> 00:01:21,000
So, this is a very deep piece of mathematics. Now, we will apply it a little bit for you to practice how the normal distribution looks in the field.


@@@
1
00:00:00,000 --> 00:00:09,000
So, here is our normal distribution again. I'm going to write it as "exp" for exponential         {-½ (x - µ)²/σ²}.

2
00:00:09,000 --> 00:00:19,000
The truth is when you're new to this this looks really cryptic. When you're with statistics for many years as I have been, you wake up in the middle of the night and you can recite this formula.

3
00:00:19,000 --> 00:00:28,000
It's as normal as getting breakfast in the morning or having a beer after dinner. What I want to get into your brains is not the complexity of the formula.

4
00:00:28,000 --> 00:00:38,000
I want you to really understand how this formula is constructed. I you to understand the quadratic penalty term of deviations from the expectation of the mean of this expression.

5
00:00:38,000 --> 00:00:50,000
Then the exponential that squeezes it back into the curves. That's basically what it is. We can draw values from this normal distribution just the same way as we flipped coins before.

6
00:00:50,000 --> 00:01:02,000
The way to look at this is any value x has this probability up here. This is nothing else but a notation of the probability of x for a normal distribution with µ and σ².

7
00:01:02,000 --> 00:01:13,000
So, a value x that has twice the high bar than some other value x' will have twice as much of a probability of being drawn.

8
00:01:13,000 --> 00:01:22,000
Now, obviously, the normal has an entire continuous space of outcomes. And obviously that renders each individual outcome of probability 0.

9
00:01:22,000 --> 00:01:22,000
But, in essence, you can think of the height of this thing over here as being proportional to the probability that this value is being drawn.


@@@
1
00:00:00,670 --> 00:00:09,890
So let's look for a second at different ways to compute probabilities for coin flips. You have a coin that has probability P of coming up heads.


@@@
1
00:00:00,470 --> 00:00:14,330
As I'm sure you've guessed this is the probability for a single coin flip. This is the formula we can use for multiple coin flips and as we go to very large numbers, the [UNKNOWN] is often a good approximation for the outcome of many coin flips.


@@@
1
00:00:00,000 --> 00:00:11,000
What I've shown you in the beginning of class have from a coin flip to a binomial distribution all the way to a normal distribution, and you might think that this was challenging and indeed it was.

2
00:00:11,000 --> 00:00:20,000
As it turns out, you can treat all this things about the same. In fact, if you're a medical doctor and you have one patient, you might think of it as a coin flip.

3
00:00:20,000 --> 00:00:36,000
If you have 10 patients, you might think of it as binomial distribution. If you do what's normally done, when your test say a new drug and you have, say 10,000 patients then this thing over here is a beautiful and very compact representation of it.

4
00:00:36,000 --> 00:00:43,000
Otherwise, it would be almost impossible to compute. That's the purpose of normal distribution for the sake of this class.

5
00:00:43,000 --> 00:00:52,000
As you go forward and look into hypotheses testing and confidence then develops. You don't do this for this relatively complicated expressions over here.

6
00:00:52,000 --> 00:00:52,000
We just do it for the normal distribution that I think relatively easy to compute. Welcome to the world of normal distributions.


@@@
1
00:00:00,000 --> 00:00:12,425
In this lesson, you'll be learning about sampling distributions. In order to gain a firm grasp of how sampling distributions work, it's important to first have a strong grasp of inferential statistics.

2
00:00:12,425 --> 00:00:19,230
We will do a recap in the next concepts to make sure you're comfortable with the ideas surrounding inferential statistics.

3
00:00:19,230 --> 00:00:27,965
This is also a shift from earlier content, where you were thinking of probability functions. From this point, you'll be thinking more about statistics.

4
00:00:27,964 --> 00:00:34,390
That is, we'll be learning from data to draw our conclusions, rather than using probability to draw our conclusions.


@@@
1
00:00:00,000 --> 00:00:09,949
The topics covered this far have all been aimed at descriptive statistics. That is, describing the data we've collected.

2
00:00:09,949 --> 00:00:23,375
There's an entire other field of statistics known as inferential statistics that's aimed at drawing conclusions about a population of individuals based only on a sample of individuals from that population.

3
00:00:23,375 --> 00:00:37,289
Imagine I want to understand what proportion of all Udacity students drink coffee. We know you're busy, and in order to get projects in on time, we assume you almost drink a ton of coffee.

4
00:00:37,289 --> 00:00:44,295
I send out an email to all Udacity alumni and current students asking the question, do you drink coffee?

5
00:00:44,295 --> 00:00:54,445
For purposes of this exercise, let's say the list contained 100,000 emails. Unfortunately, not everyone responds to my email blast.

6
00:00:54,445 --> 00:01:08,424
Some of the emails don't even go through. Therefore, I only receive 5,000 responses. I find that 73% of the individuals that responded to my email blast, say they do drink coffee.

7
00:01:08,424 --> 00:01:20,765
Descriptive statistics is about describing the data we have. That is, any information we have and share regarding the 5,000 responses is descriptive.

8
00:01:20,765 --> 00:01:32,734
Inferential statistics is about drawing conclusions regarding the coffee drinking habits of all Udacity students, only using the data from the 5,000 responses.

9
00:01:32,734 --> 00:01:45,620
Therefore, inferential statistics in our example is all about drawing conclusions regarding all 100,000 Udacity students using only the 5,000 responses from our sample.

10
00:01:45,620 --> 00:01:54,394
The general language associated with this scenario is as shown here. We have a population which is our entire group of interest.

11
00:01:54,394 --> 00:02:05,109
In our case, the 100,000 students. We collect a subset from this population which we call a sample. In our case, the 5,000 students.

12
00:02:05,109 --> 00:02:15,485
Any numeric summary calculated from the sample is called a statistic. In our case, the 73% of the 5,000 that drink coffee.

13
00:02:15,485 --> 00:02:31,495
This 73% is the statistic. A numeric summary of the population is known as a parameter. In our case, we don't know this value as it's a number that requires information from all Udacity students.


@@@
1
00:00:02,370 --> 00:00:11,440
In this video, we'll be defining the term Sampling Distribution, and looking at an example of one specific sampling distribution.

2
00:00:11,439 --> 00:00:22,225
A sampling distribution is the distribution of a statistic. This could be any statistic, but what does it really mean, to look at the distribution of a statistic?

3
00:00:22,225 --> 00:00:37,905
Consider again the coffee drinking habits of all Udacity students. Let's say, each of these cups represents a student, and a green cup represents a student that drinks coffee, while a red cup represents a student that doesn't drink coffee.

4
00:00:37,905 --> 00:00:45,449
And even though there are more students than we represent here, pretend that this represents all Udacity students.

5
00:00:45,450 --> 00:00:55,479
If we were to select this group of students to ask about their coffee drinking habits, what would our population, parameter, sample and statistic be?

6
00:00:55,479 --> 00:01:02,000
Use the quiz below to answer. You may want to pause the screen here to return and check your answers.


@@@
1
00:00:02,370 --> 00:00:14,734
If we wanted to identify the sample and statistic, from this visual, we would only use these cups, which give a sample of five students where four of them don't drink coffee.

2
00:00:14,734 --> 00:00:23,125
This gives us a statistic that 20% of students drink coffee. Remember, a population is our entire group of interest.

3
00:00:23,125 --> 00:00:38,755
Therefore, we have a population of 21 students, and a portion that drink coffee of 71%. Moving back to our sample and statistic, what if we didn't select these five individuals, but instead we selected these five here?


@@@
1
00:00:02,419 --> 00:00:13,380
So you probably notice that though our sample is still five students, our statistic changed, because we chose five different students than were chosen in the first sample.

2
00:00:13,380 --> 00:00:22,774
We could select all possible combinations of five cups, and we could recompute the proportion of coffee drinkers for each of these samples.

3
00:00:22,774 --> 00:00:36,539
If we were to look at how these statistics change from one sample to the next, that is, if we looked at the distribution of the proportions across all samples of size five, this is what is known as the sampling distribution.


@@@
1
00:00:00,000 --> 00:00:07,439
It's important to understand notation. You might not even know it, but you use notation all the time.

2
00:00:07,440 --> 00:00:17,219
Consider this example of five plus three. Plus is an English word. This symbol is notation, and it's universal.

3
00:00:17,219 --> 00:00:32,910
Notation is a common math language used to communicate. Regardless of whether you speak English, Spanish, Greek, or any other language, you can work together using notation as a common language to solve problems.

4
00:00:32,909 --> 00:00:42,039
Like learning any new language, notation can be frightening at first. But it's an essential tool for communicating ideas associated with data.


@@@
1
00:00:01,990 --> 00:00:19,309
There are common ways to notate parameters that are different than the way we notate statistics. In general, parameters are notated with Greek symbols, where statistics are either notated by lower case letters or the same Greek symbol with a hat on it.

2
00:00:19,309 --> 00:00:29,039
Here, this symbol called mu represents the mean of a population, while these represent the mean of a sample.

3
00:00:29,039 --> 00:00:40,179
Similarly, this Greek symbol called sigma represents the standard deviation of a population, while either of these can be used to represent the standard deviation of a sample.

4
00:00:40,179 --> 00:00:57,274
This same pattern continues. Here, you can see common parameters and statistics. You can see the parameter and statistic values for not only the mean and standard deviation but also for the variance, proportions, and the values of coefficients and regression.

5
00:00:57,274 --> 00:01:11,000
You'll see all of these in later lessons in the statistics class. This notation will be consistently used throughout these lessons, as well as beyond this course, in books, blogs, and other use cases.


@@@
1
00:00:02,490 --> 00:00:12,309
You now have seen how a sampling distribution provides how a statistic varies. As we saw, the proportions change with different samples of students.

2
00:00:12,310 --> 00:00:25,570
However, we might also look at the distribution of other statistics, like how the sample standard deviation, the variance, difference in mean or any other statistic varies from one sample to the next.

3
00:00:25,570 --> 00:00:38,570
Notice, we are not looking at the distribution parameters. As you saw in the first example, the parameter that is a numeric summary of a population is a fixed value, so these values do not change.

4
00:00:38,570 --> 00:00:46,750
However, statistics will change based on a sample you select from the population. So what are the common traits of sampling distributions??


@@@
1
00:00:02,770 --> 00:00:12,169
There are two mathematical theorems that are commonly discussed when looking at sampling distributions, the Law of Large Numbers and the Central Limit Theorem.

2
00:00:12,169 --> 00:00:26,669
First, let's go through the Law of Large Numbers. This theorem makes a lot of sense and it tells us that if we choose the right statistics to estimate a parameter, the larger our sample size, the closer the statistic gets to the parameter.

3
00:00:26,670 --> 00:00:39,599
Makes a lot of sense, right? If you want to know the mean of a population and you estimate it with the sample mean, the larger the sample size, the better your sample mean will be, estimating the population mean.

4
00:00:39,600 --> 00:00:47,524
So you might be wondering, what makes a statistic the right estimate of a parameter? There are a number of ways to estimate parameters.


@@@
1
00:00:02,790 --> 00:00:13,605
The second theorem is one of the most popular theorems in all of statistics. And it pertains specifically to the sample mean and sample proportions statistics.

2
00:00:13,605 --> 00:00:28,780
The central limit theorem states, that with a large enough sample size, the sampling distribution of the mean will be normally distributed, since a proportion is like a mean of zero and one data values, it also abides by the central limit theorem.


@@@
1
00:00:00,570 --> 00:00:09,214
You now have gained some intuition for how the Central Limit Theorem works. But it doesn't work for all sampling distributions.

2
00:00:09,214 --> 00:00:17,699
Sure, a mean and proportion are normally distributed with large enough sample sizes. But what does it mean for a sample size to be large enough?

3
00:00:17,699 --> 00:00:28,759
Is a sample size of 10 large enough? Or 30? Or 100? You might memorize a list of statistics for which the Central Limit Theorem applies.

4
00:00:28,760 --> 00:00:35,925
It applies for the mean, and for proportions, and applies for the difference in means, and difference in proportions.

5
00:00:35,924 --> 00:00:42,539
However, the Central Limit Theorem doesn't apply to the sampling distribution for the variance, or for the correlation coefficient.

6
00:00:42,539 --> 00:00:56,524
It doesn't apply for the sampling distribution of the maximum value in a data set. With all the data being collected in the world, and tons of compute power available at our fingertips, the Central Limit Theorem is still useful for some sampling distributions.

7
00:00:56,524 --> 00:01:03,210
But instead of relying on this theorem, what if we just simulated whatever sampling distribution we were interested in instead?


@@@
1
00:00:00,030 --> 00:00:19,359
So in the last video, we talked about how relying on mathematical theorems, like the central limit theorem leads to gaps in whether we've achieved a large enough sample size or, which statistics the theorem applies to, and that instead of relying on theorems we could simulate the sampling distribution.

2
00:00:19,359 --> 00:00:26,800
This introduces a technique known as bootstrapping. Bootstrapping in statistics, means sampling with replacement.

3
00:00:26,800 --> 00:00:33,329
If we want to bootstrap sample five individuals from this group, we could randomly sample these five here.

4
00:00:33,329 --> 00:00:43,990
However, in bootstrap sampling, we are sampling with replacement. So we could actually end up sampling these four, and this one might end up being sampled again.

5
00:00:43,990 --> 00:00:55,589
We could look at this a bit slower. In sampling, we might end up choosing this individual. And immediately after choosing it as a part of the sample, it goes back to potentially be chosen again.

6
00:00:55,590 --> 00:01:03,745
It is possible, though unlikely, in bootstrap sampling to choose the same individual for every random draw.

7
00:01:03,744 --> 00:01:15,500
We could end up choosing this individual or any of these five times in a row. After a little more practice with this, you'll be ready to apply bootstrapping to simulate sampling distributions.


@@@
1
00:00:00,180 --> 00:00:11,324
Here's the idea of using bootstrapping to simulate the sampling distribution for any statistic. This might take more than one time through to fully grasp. So bear with me.

2
00:00:11,324 --> 00:00:19,379
We know that in inferential statistics, we want to use a statistic to try and say something about the corresponding population parameter.

3
00:00:19,379 --> 00:00:33,125
Imagine we treat our sample as if it were the entire population. So although these 21 cups represent only a sample of Udacity students, imagine we treat them as though they were in our entire population.

4
00:00:33,125 --> 00:00:45,274
If these 21 individuals are truly representative of our population, we can bootstrap sample from them to understand how the proportion of coffee drinkers might change from one sample to the next.

5
00:00:45,274 --> 00:00:56,800
Here, I set up an array of 21 ones and zeroes to represent the cups and the previous image. I also set the seed, in case you want to follow along and get the same results.

6
00:00:56,799 --> 00:01:10,129
We can calculate the proportion of coffee drinkers in our original sample. Now, let's take our first bootstrap sample of the same size as the original sample and calculate the proportion of coffee drinkers.

7
00:01:10,129 --> 00:01:20,319
Notice, this proportion doesn't match the original because we're not just sampling the original observations again but rather, we're sampling them with replacement.

8
00:01:20,319 --> 00:01:27,564
We could write a loop to reform the same sampling 10,000 times to see how the proportion will change.

9
00:01:27,564 --> 00:01:36,129
Finally, let's plot the proportions to look at the sampling distribution. Use your sampling distribution results to answer the questions in the next concept.


@@@
1
00:00:02,310 --> 00:00:12,664
If bootstrap sampling seems pretty amazing, that's because it kind of is. But the application of bootstrap sampling actually goes beyond even the use cases here.

2
00:00:12,664 --> 00:00:20,224
Bootstrapping techniques have been used for leading machine learning algorithms. More on this as provided in the instructor notes below.

3
00:00:20,225 --> 00:00:41,644
This technique is credited to Bradley Efron in 1979, a Minnesota born statistician. The name bootstrapping was given to the technique because of the amazement and how well it works is similar to the idea of being able to walk into quicksand and pull yourself out of the quicksand to safety by your own bootstraps.

4
00:00:41,645 --> 00:00:50,364
The way we can draw inference about a population parameter by only performing repeated sampling within our existing sample is just as amazing.

5
00:00:50,365 --> 00:00:58,189
We actually gain confidence about where parameter is likely to exist without having to collect any additional data.


@@@
1
00:00:03,379 --> 00:00:11,339
In this lesson, you've looked a lot at sampling distributions. We might still not understand the use of this idea in practice.

2
00:00:11,339 --> 00:00:19,234
In the next lessons, you're going to learn more about inference. Specifically, you'll be learning about Confidence Intervals and Hypothesis Testing.

3
00:00:19,234 --> 00:00:26,959
When looking at these techniques online, you might find a lot of formulas and built-in calculators for computing the final results for these techniques.

4
00:00:26,960 --> 00:00:43,320
However, these built-in calculators hide the assumptions and potential biases. With your new understanding of sampling distributions and bootstrapping, you're ready to tackle not only using the built-in techniques, but to extend these techniques to a multitude of other situations.


@@@
1
00:00:01,860 --> 00:00:12,615
In this lesson, you'll be learning about confidence intervals. When we try to estimate a population parameter based on a sample statistic, this is like fishing with a fishing pole.

2
00:00:12,615 --> 00:00:22,644
And there's nothing wrong with fishing this way. It's a perfectly good way to fish. However, you'd be much more likely to catch a fish if you cast an entire net.

3
00:00:22,644 --> 00:00:33,469
How much more likely are you that you'll catch a fish? Well, that depends on the size of the net. The larger the net, the more confident you can be that you'll actually capture a fish.

4
00:00:33,469 --> 00:00:47,155
This net is much more likely to catch a fish than the smaller net. Though this example isn't exactly the same as building a confidence interval, because we consider a parameter to be a non-moving number and not a moving fish amongst water.

5
00:00:47,155 --> 00:00:57,750
This idea that providing an interval and not just a single estimate will help us gain confidence in our ability to capture a population parameter is at the core of confidence intervals.

6
00:00:57,750 --> 00:01:05,010
And the wider our interval, the more confident we can be that we capture our parameter of interest. Let's take a closer look.


@@@
1
00:00:02,430 --> 00:00:11,669
In the previous lesson, you saw how we can do sampling distributions and bootstrapping to understand the values of a statistic that are possible.

2
00:00:11,669 --> 00:00:18,381
It turns out that we can use these sampling distributions to understand the most likely values for a parameter as well.

3
00:00:18,381 --> 00:00:31,324
In the real world, we don't usually know the value of a parameter as our populations of interest tend to be things like everyone in the world or all the past, present, and future transactions of a company.

4
00:00:31,324 --> 00:00:47,185
We just don't have all the information we might like to about these populations. So now, the question is, how do we use what we do know about sampling distributions to infer something about these parameters for these large populations?

5
00:00:47,185 --> 00:00:56,909
And here is where the interesting part starts. Imagine this distribution here is the sampling distribution for some statistic, any statistic of interest.

6
00:00:56,909 --> 00:01:03,460
We can actually use this sampling distribution to build a confidence interval for our parameter of interest.

7
00:01:03,460 --> 00:01:11,174
If we want a 95 percent confidence interval, we could cut off two and a half percent from here and another two and a half percent from up here.

8
00:01:11,174 --> 00:01:19,378
These values would then give the range where we believe the parameter would be, with 95 percent confidence.

9
00:01:19,378 --> 00:01:35,000
If alternatively, we wanted a 99 percent confidence interval, we would cut off a half a percent from each side, and these values would then give the range where we believe the parameter to be with 99 percent confidence.


@@@
1
00:00:00,000 --> 00:00:10,224
Let's work together through an example of building a confidence interval. Specifically, we'll be looking at how heights range based on coffee drinking habits and based on age.

2
00:00:10,224 --> 00:00:19,919
Here, I have two datasets, which I have labeled coffee full, and coffee red for coffee reduced. Notice that the second is just a subset of the first.

3
00:00:19,920 --> 00:00:26,980
The csv is linked in the resources so you can follow along, as well as a workspace on the next concept.

4
00:00:26,980 --> 00:00:36,450
I set the seed, which you can do to follow exactly my steps. Consider that the larger dataset is all of the individuals in our population.

5
00:00:36,450 --> 00:00:48,570
While the smaller dataset is just a random sample that we selected. In the real world, we wouldn't necessarily have all the information in the full dataset, but it will be useful for some of the examples we'll be working through.

6
00:00:48,570 --> 00:01:03,449
Let's calculate some statistics about the individuals who drink coffee in our sample. We can see that we have approximately 57 percent of individuals that drink coffee in our sample, and we can see the average height of these individuals is approximately 68.52 inches.

7
00:01:03,450 --> 00:01:10,903
Now, let's boot strap from the sample to build a confidence interval. Remember, we don't have the population data to work with.

8
00:01:10,903 --> 00:01:22,454
So, we cannot use that to build the interval. By cutting off the bottom two and a half and the top two and a half percent, we built 95 percent in the middle portion.

9
00:01:22,454 --> 00:01:39,894
We can interpret these values as the bounds where we believe the mean height of all coffee drinkers in the population to be, with 95 percent confidence, between 68.06 and 68.97 inches tall.

10
00:01:39,894 --> 00:01:48,704
Let's go back and see what the population mean actually was. Looks like in this case, we were successful using our confidence interval.

11
00:01:48,704 --> 00:01:55,509
But that's not always the case. Your turn. Use the notebook and prompts in the next concept to answer the quiz question that follow.


@@@
1
00:00:00,000 --> 00:00:10,925
Now that you've seen how you can use sampling distributions to build confidence intervals for a single parameter, let's take a look at how we might do something similar to estimate the difference in two parameters.

2
00:00:10,925 --> 00:00:19,050
Here, we might have a question. What is the difference in average heights for those who drink coffee versus those who do not?

3
00:00:19,050 --> 00:00:28,960
Do we have evidence of a difference in the average height? In order to build a confidence interval for the difference in the average heights for these two groups, we can do something similar to what you've already done.

4
00:00:28,960 --> 00:00:45,540
But for each iteration of taking the mean for each group, we're also going to take the difference. Now, we can iterate this process some large number of times and use the resulting differences to build the confidence interval for the difference in the means.

5
00:00:45,539 --> 00:01:03,664
Here, I've set up a difference list and we'll append the differences into it. We could again plot the difference in the means of the two groups and we could cut off the bottom two and a half and top two and a half percent, to build the 95 percent confidence interval for where we believed the difference in the means of the two groups to exist.

6
00:01:03,664 --> 00:01:12,410
In this case, you saw that our confidence interval doesn't contain zero. And therefore, this would suggest that there is a difference in the population means.

7
00:01:12,409 --> 00:01:18,310
Further, we would suggest that on average, coffee drinkers are actually taller than non-coffee drinkers.


@@@
1
00:00:01,889 --> 00:00:10,259
In the previous concepts, you saw how we could build confidence intervals for different parameters, like the mean or difference in means.

2
00:00:10,259 --> 00:00:15,205
So what are some of the scenarios for which we would want to build a confidence interval for the difference in means?

3
00:00:15,205 --> 00:00:27,454
Well, we could look at the effectiveness of different drugs by comparing two groups who take two different drugs, or comparing a group that takes a drug to a group that did not take a drug at all.

4
00:00:27,454 --> 00:00:41,649
This type of testing is common for any of the health conditions around the world. Another use case specific to learning could be to implement two different ways of teaching the same topic and see which way improves retention.

5
00:00:41,649 --> 00:00:54,559
One of the most common use cases for comparing two groups in this way is known as A/B testing, where we compared different webpages to one another to determine which web designs drive the largest amount of traffic.

6
00:00:54,560 --> 00:01:01,679
The last topic will be covered in more detail in a later lesson, as it's the key to survival for many online companies.


@@@
1
00:00:02,589 --> 00:00:11,855
You're now been introduced to a few different applications for confidence intervals, as well as a bootstrapping approach to creating these confidence intervals.

2
00:00:11,855 --> 00:00:26,184
We found that creating a confidence interval for the differences in means of two groups, rather than just comparing two point value estimates, is important to assure that the differences are not just occurring due to the randomness associated with the sample that was chosen.

3
00:00:26,184 --> 00:00:35,390
Confidence intervals are certainly useful. But there are issues that can arise when exclusively using confidence intervals to make decisions.

4
00:00:35,390 --> 00:00:45,729
This brings up the ideas of Practical and Statistical Significance. Let's consider an example to illustrate the difference between these two terms.

5
00:00:45,729 --> 00:00:55,629
Imagine that I own a dog walking business and I'm advertising online. I would like to know which of these two ads would help me generate the most interest in my business.

6
00:00:55,630 --> 00:01:04,664
I send out each ad to the same number of randomly selected users and I build the confidence interval, that suggests that more people click this ad.

7
00:01:04,665 --> 00:01:11,700
Based on my confidence interval, I have statistical evidence to suggest that the second ad is better.

8
00:01:11,700 --> 00:01:23,804
This evidence we get from the confidence interval is just this, Statistical Significance. Let's say that both ads generate enough interest to achieve more dogs than I can even handle watching.

9
00:01:23,805 --> 00:01:38,549
And that the second ad is much more expensive and time-consuming to create. If a friend of yours decides that they would like to build a similar ad campaign to start their own dog walking business, which type of ad would you recommend to them?

10
00:01:38,549 --> 00:01:45,695
In this case, you might suggest something like the first ad, even though you statistically prove that the second ad was better.

11
00:01:45,694 --> 00:01:52,705
The first ad will generate enough interest in their business and be less expensive and time intensive.

12
00:01:52,704 --> 00:02:05,819
Your suggestion for them to use the first ad is an example of Practical Significance. Practical significance takes into consideration additional aspects and the world around us, rather than just the numbers.


@@@
1
00:00:02,259 --> 00:00:10,070
The way we've been building confidence intervals thus far is based on bootstrapping and our knowledge of sampling distributions.

2
00:00:10,070 --> 00:00:17,230
This is an extremely effective method for building confidence intervals for essentially any parameters we might be interested in.

3
00:00:17,230 --> 00:00:26,445
However, if you've taken a statistics course elsewhere, you might be confused as to why this notation is different than what you've done in those other courses.

4
00:00:26,445 --> 00:00:39,159
You might have seen equations like these for capturing a population mean or proportion. Similarly, you might have seen equations like these for capturing the difference in means or the difference in proportions.

5
00:00:39,159 --> 00:00:56,899
All of these formulas have underlying assumptions that may or may not be true. If you truly believe that your data are representative of your population of interest, the bootstrapping method should provide a better representation for where the parameter is likely to be.

6
00:00:56,899 --> 00:01:05,555
However, with large enough sample sizes, these formulas should provide very similar results to those that we've seen in bootstrapping methods.


@@@
1
00:00:00,000 --> 00:00:11,429
In this video, we're going to do a quick comparison of the traditional approaches for building confidence intervals, which are actually built in the Python, and the approaches you've already seen using bootstrapping in this lesson.

2
00:00:11,429 --> 00:00:16,125
There are lots of different names for hypothesis tests and the way that we build confidence intervals.

3
00:00:16,125 --> 00:00:23,760
Like a one sample T- test, which is used for the population mean, or two sample T- test, which is used for comparing two means.

4
00:00:23,760 --> 00:00:32,804
There is also a paired T- test often used for comparing an individual to themselves, or a Z- test, or a chi-square test, or an F-test.

5
00:00:32,804 --> 00:00:37,710
There are so many hypothesis tests which are linked to the way that we create the confidence intervals.

6
00:00:37,710 --> 00:00:45,770
And the bootstrapping approach can actually be used in place of any of these. In this video, we will illustrate this by example.

7
00:00:45,770 --> 00:00:54,905
First, let's read in our data and the libraries. I've also set the seed so you can follow along. Next, let's look at a confidence interval for the difference in means.

8
00:00:54,905 --> 00:01:04,629
Here's the bootstrap approach that you did in an earlier quiz. While that runs, I'm going to go look at StackOverflow for a post that could be helpful for finding the difference in means.

9
00:01:04,629 --> 00:01:11,349
By finding confidence interval for a T-test, difference in comparison in means, I came across the stack overflow post.

10
00:01:11,349 --> 00:01:17,480
If we scroll down, you can see that this is the documentation that they used to get their confidence interval.

11
00:01:17,480 --> 00:01:25,517
So we already read in Numpy, I'm going to pull this part out. And here, you can see they're just setting up some random data to work with.

12
00:01:25,516 --> 00:01:36,670
So let's actually pull that. We don't need it. We want our data to be a comparison of those who drink coffee and the heights who don't drink coffee.

13
00:01:36,670 --> 00:01:43,688
So we just want the heights of each of those groups, so actually, it's going to look a lot like this.

14
00:01:43,688 --> 00:01:53,079
But instead of a bootstrap sample, we just want it from our original sample set. And this print statement will only work in 2.7.

15
00:01:53,079 --> 00:02:00,230
Notice that intervals for the bootstrapping method and the built in using the traditional method are nearly identical.


@@@
1
00:00:01,940 --> 00:00:16,193
In this video, you'll learn a few more terms associated with confidence intervals, as well as some of the relationships you can expect between what happens to the inputs of a confidence interval and how this affects the interval you get as a result.

2
00:00:16,193 --> 00:00:27,155
There are a few terms that we should discuss that are true for any confidence interval. In the literature, you will frequently see terms like the margin of error and the confidence interval width.

3
00:00:27,155 --> 00:00:36,799
A common way that we see political results is in the following way. Candidate A has 34 percent of the vote plus or minus 3 percent.

4
00:00:36,799 --> 00:00:56,935
And Candidate B has 22 percent of the vote plus or minus 3 percent. Then in small print, you might see something like, "These figures are based on the 95 percent confidence interval." In the sample, each candidate has this respective 34 and 22 percent, and this 3 percent is called the margin of error.

5
00:00:56,935 --> 00:01:14,769
In order to build a confidence interval, we actually add and subtract this amount. So our confidence interval for the true proportion that Candidate A controls for the population is at 31 to 37 percent, and for Candidate B, it would be 19 to 25 percent.

6
00:01:14,769 --> 00:01:21,854
If this confidence interval for each is larger than we want it, we could actually collect a larger sample size.

7
00:01:21,855 --> 00:01:33,349
Because of the Law of Large Numbers, we know that the larger the sample size, the better our estimates will be at approximating our parameter and therefore, this will narrow our interval.


@@@
1
00:00:02,740 --> 00:00:09,500
You've already learned a ton about how to build, interpret, and use confidence intervals in practice.

2
00:00:09,500 --> 00:00:19,085
This video is just a reminder on the types of conclusions we can make with confidence intervals and the types of conclusions we cannot make, which are commonly confused.

3
00:00:19,085 --> 00:00:26,995
When we build confidence intervals, they're aimed at parameters. That is, they're aimed at a single numeric value in our population.

4
00:00:26,995 --> 00:00:37,849
These values include the population mean, or the population standard deviation, potentially the difference in two population means, or any other numeric summary in the population.

5
00:00:37,850 --> 00:00:45,380
Notice that confidence intervals do not allow us to say something specific about any individual in our population.

6
00:00:45,380 --> 00:00:57,070
More advanced techniques in Machine Learning do aim at giving us information about every individual in a population, but commonly, confidence intervals are not aimed at solving these types of problems.


@@@
1
00:00:02,049 --> 00:00:09,250
This concludes this lesson on confidence intervals. You've learned a ton about how you can build and interpret confidence intervals.

2
00:00:09,250 --> 00:00:16,894
In the next lesson, you will learn another technique that's very similar to confidence intervals called hypothesis testing.

3
00:00:16,894 --> 00:00:28,199
The topics of confidence intervals and hypothesis testing essentially do the same thing. But depending on who you talk to or what source you're reading from, it's important to understand both.

4
00:00:28,199 --> 00:00:34,090
With that, check out the recap on this lesson on the next concept and let's get ready for hypothesis testing.


@@@
1
00:00:00,390 --> 00:00:11,390
Welcome to this lesson on hypothesis testing. Hypothesis testing is one of my favorite topics, because it really does bring an art to the way that we think about statistics.

2
00:00:11,390 --> 00:00:20,494
This can bring a lot of complexity. And you may need to watch videos in the section multiple times, or practice the quizzes multiple times.


@@@
1
00:00:02,890 --> 00:00:13,164
Academic and industry professionals have questions about, well, just about everything. As data analysts, we try to help them answer these questions.

2
00:00:13,164 --> 00:00:24,309
But first, we need to translate the questions into what are known as hypothesis. Then, we need to be able to collect data to justify which hypothesis are likely to be true.

3
00:00:24,309 --> 00:00:31,339
As an example, just the other day, I was in a debate with a friend, about what the most popular ice cream flavor in the world is?

4
00:00:31,339 --> 00:00:46,960
Where I assumed, the most popular was chocolate, and they were sure it was vanilla. In this case, we could generate hypothesis where the most favorite ice cream is chocolate, and collect data to see if this hypothesis is actually supported by the data.

5
00:00:46,960 --> 00:00:53,424
But how can we truly know this unless we talk to everyone? How do we know if our conclusions are reliable?

6
00:00:53,424 --> 00:01:04,879
Well, it turns out, you can use hypothesis testing, or confidence intervals which you just saw in the last lesson, to draw conclusions about a population, only using sample data.

7
00:01:04,879 --> 00:01:14,120
Not all hypothesis testing is this straightforward. Within medical studies, say I wanted to test if a cancer drug is effective at helping patients.

8
00:01:14,120 --> 00:01:19,670
Well now, the hypothesis could go lots of different ways. Is the drug helpful if it makes patients feel better?

9
00:01:19,670 --> 00:01:30,320
Is it helpful if they live longer? Is it helpful if it reduces the tumor sizes? In this lesson, you will learn about how to set up and evaluate the results of hypothesis testing.

10
00:01:30,319 --> 00:01:39,000
Hypothesis testing is all about helping businesses make better and more informed database decisions. So let's get started.


@@@
1
00:00:01,929 --> 00:00:10,899
When performing hypothesis testing, the first thing we need to do is translate a question into two competing hypotheses.

2
00:00:10,898 --> 00:00:20,210
One of these hypotheses is called the Null, and it's associated with the symbol, and the other is called the Alternative and it's commonly notated in this way.

3
00:00:20,210 --> 00:00:26,404
Setting up these hypotheses can be a bit subjective, but here are a few general rules that will go through.

4
00:00:26,405 --> 00:00:40,790
The null hypothesis is the condition we believe to be true before we collect any data. Mathematically, the null hypothesis is commonly a statement of two groups being equal or of an effect being zero.

5
00:00:40,789 --> 00:00:53,185
The null and the alternative hypotheses should be competing and non overlapping hypotheses. The alternative hypothesis is often associated with what you want or what you want to prove to be true.

6
00:00:53,185 --> 00:01:02,435
Mathematically, the null hypothesis tends to hold an equal sign while the alternative holds a greater than, less than, or not equal to sign.

7
00:01:02,435 --> 00:01:14,675
We will go through a couple of examples to connect these ideas to. In the US judicial system we say "Innocent until proven guilty." This is actually a setup for a hypothesis test.

8
00:01:14,674 --> 00:01:36,415
In a judicial case, every individual is either innocent or guilty of an act. This statement of "Innocent until proven guilty" is a statement that says "We believe everyone to be innocent initially" that is the null hypothesis for every individual is innocent.

9
00:01:36,415 --> 00:01:46,600
It is this statement we believe to be true before we collect any data. Therefore, the competing alternative hypothesis is that an individual is guilty.


@@@
1
00:00:02,140 --> 00:00:12,769
In the previous video, you were introduced to the terms of the null and alternative hypotheses and came up with a few guidelines for setting these up as shown here.

2
00:00:12,769 --> 00:00:18,989
You also saw how the claim of innocent until proven guilty relates to the null and alternative hypotheses.

3
00:00:18,989 --> 00:00:31,169
In this video, I want to provide a second example. Imagine you create a new web page layout and we'd like to know if this new page drives more traffic than the existing page.

4
00:00:31,170 --> 00:00:44,555
We could set this up to specifically ask, "Does the average web traffic increase with the new web page as compared to the existing page?" Before we even implement this test, we might hope that the new page is better.

5
00:00:44,554 --> 00:00:55,700
That's why we built it after all. But we need to prove this. This is an indication that the new page being better than the existing page belongs in the alternative hypothesis.

6
00:00:55,700 --> 00:01:03,965
Therefore, the null would be that the average web traffic is the same for each of these groups or that the old page is actually better.

7
00:01:03,965 --> 00:01:12,815
Mathematically, we could set this up in this way, where the average traffic for the existing page is equal to the average traffic for the new page.

8
00:01:12,814 --> 00:01:21,140
Then, the alternative would look like this, where the average traffic for the new page is greater than the average traffic for the existing page.

9
00:01:21,140 --> 00:01:31,070
And again, we can collect data to see which hypothesis is supported. Here, our guidelines served very useful in defining the null and alternative hypotheses.

10
00:01:31,069 --> 00:01:41,840
Setting up hypothesis tests can be tricky because there isn't really one right answer. In the following concepts, you will see why this really matters for our decision-making process.

11
00:01:41,840 --> 00:01:48,000
For now, use this example along with these four guidelines to get some practice setting up hypotheses.


@@@
1
00:00:02,319 --> 00:00:10,515
Now that you've had some practice with setting up hypotheses, you might be asking yourself, why does all this null and alternative stuff really matter?

2
00:00:10,515 --> 00:00:20,550
Well, it actually matters a lot. Let's consider again the judicial example from before. There are four potential reality decision combos that could be made.

3
00:00:20,550 --> 00:00:38,429
In order to look at each one of these outcomes, consider this grid where we have the truth of whether or not someone is innocent or guilty represented by this axis, and then the decision made by the jury represented as either innocent or guilty on this axis here.

4
00:00:38,429 --> 00:00:48,046
This creates a grid of four potential outcomes. In this corner, the truth is that an individual is innocent, and the jury also believes that individual to be innocent.

5
00:00:48,046 --> 00:00:53,969
While in this corner, the truth is that the individual is guilty, and the jury believes them to be guilty.

6
00:00:53,969 --> 00:01:01,215
In either of these other two, the jury has made a mistake as the truth doesn't match the jury decision.

7
00:01:01,215 --> 00:01:12,310
This provides us with two potential errors that are possible in hypothesis testing. The first error that is possible is that the jury might consider an individual innocent when they are truly guilty.

8
00:01:12,310 --> 00:01:20,254
This error sets guilty people free. The second type of error is that the jury considers an individual guilty when the truth is that they're innocent.


@@@
1
00:00:01,960 --> 00:00:15,320
From the quizzes, you've now been introduced to two types of errors. Type one and type two errors. Correctly setting up the null and alternative hypotheses is important for exactly this reason.

2
00:00:15,320 --> 00:00:26,270
They define the importance of the errors that we're making. In the previous example, a type one error is one where we decided that an individual is guilty, but they're actually innocent.

3
00:00:26,269 --> 00:00:34,344
The definition of a type one error is an error where the alternative hypothesis is chosen, but the null hypothesis is actually true.

4
00:00:34,344 --> 00:00:40,655
You might also hear this called, a false-positive, and it is frequently denoted with a symbol alpha.

5
00:00:40,655 --> 00:00:50,545
Type one errors are considered the worse of the two possible errors. The other type of error that might occur, is that we might set a guilty individual free.

6
00:00:50,545 --> 00:00:59,449
This is a type two error. By definition, a type two error is when the null hypothesis is chosen, and the alternative is actually true.

7
00:00:59,450 --> 00:01:16,484
What we can see in this example is that there are two potential extremes. The jury might decide that they never want to commit a type one error, in which case, regardless of the evidence, they just always set everyone as innocent, and then they're going to commit many more type two errors.

8
00:01:16,484 --> 00:01:25,070
Alternatively, if the jury just decided that everyone was guilty, then they would never commit a type two error, but they would commit many more type one errors.

9
00:01:25,069 --> 00:01:34,954
Because of this relationship between type one and type two errors, professionals frequently just set a threshold for how many type one errors they're willing to commit.

10
00:01:34,954 --> 00:01:41,465
And then, they try to keep the type two errors as low as possible while still meeting this threshold.

11
00:01:41,465 --> 00:01:49,739
Common type one error rates are one percent from medical field, while they're five percent for research journals and other business applications.

12
00:01:49,739 --> 00:02:01,069
But really, this rate should be contingent on your application. In the next concept, you will see one application where neither five percent nor a one percent error rate makes very much sense.


@@@
1
00:00:01,800 --> 00:00:08,830
Hopefully, you're starting to feel more comfortable identifying hypotheses and the types of errors that you can make.

2
00:00:08,830 --> 00:00:20,199
In this video, we'll go through one more example to iterate on these ideas. This example is one that really helped me connect all the dots of hypotheses testing and type I and type II errors.

3
00:00:20,199 --> 00:00:28,954
I hope it does the same for you. Imagine you own a skydiving shop. And as a part of your job, you must check parachutes to assure they worked correctly.

4
00:00:28,954 --> 00:00:37,545
There are two potential outcomes, either a parachute works or it doesn't. You can create these as our two potential hypotheses.

5
00:00:37,545 --> 00:00:47,659
You know that there are four potential outcomes for each skydiver-parachute combination. First, you check each parachute to make a decision.

6
00:00:47,659 --> 00:00:55,690
Either the parachute works or it doesn't. If you determine that it works, you put it on the shelf for skydivers to use.

7
00:00:55,689 --> 00:01:04,569
If it doesn't, you throw it out. Now, there are two possible truths, either the parachute actually works or it doesn't.

8
00:01:04,569 --> 00:01:12,155
For the parachute that we threw out, if it doesn't work, that's great, but if it did, then we probably lost like 30 bucks.

9
00:01:12,155 --> 00:01:20,795
Now, for the parachute we put on the shelf, if it works, then the skydiver uses it to jump out of a plane and land safely on the ground.

10
00:01:20,795 --> 00:01:28,854
However, if it doesn't, we clearly just committed the worst type of error possible. This should clearly be the type I error in this example.

11
00:01:28,855 --> 00:01:36,359
So, the other type of error is the type II error. This helps us align our null and alternative hypotheses to look like this.

12
00:01:36,359 --> 00:01:48,685
As we know that type I error rate should be choosing the alternative when the null is true. And it helps us align an appropriate type I error rate as one or five percent as entirely too high.

13
00:01:48,685 --> 00:01:56,000
Committing five or even one of these errors out of every 100 individuals would just be unacceptable.


@@@
1
00:00:00,000 --> 00:00:08,417
Now that you've had a chance to gain some familiarity with setting up hypothesis test, let's look at some of the most common tests that are done in practice.

2
00:00:08,417 --> 00:00:22,280
One common test is to test the mean or proportion of a population being equal to some value. For example, in finance, we could have the question of, if you can expect a return greater than six percent on an investment.

3
00:00:22,280 --> 00:00:33,079
So we could set up the hypothesis test in the following way, where the null is that you earn less than or equal to six percent, and the alternative is that you earn greater than six percent.

4
00:00:33,079 --> 00:00:39,804
Another common hypothesis test is to ask which of two marketing campaigns might drive more traffic to our website?

5
00:00:39,804 --> 00:00:53,694
In which case, we could set up a null and alternative that looks like this, where the null suggests that the proportion of individuals that click through to our page is the same for each campaign, and the alternative is that one page drives more traffic.

6
00:00:53,695 --> 00:01:13,674
In which case, the proportion of traffic is different from one page to the next. If we really wanted to test if a new campaign was better than an old campaign, we might use a one sided hypothesis test like this one, where the greater than here, suggests that the proportion of individuals that travel to our site will be higher for the new campaign.

7
00:01:13,674 --> 00:01:22,660
You can do some algebra to change the exact same logic to look like this. Notice that all of these hypothesis tests are regarding parameters.

8
00:01:22,659 --> 00:01:33,034
They are not about statistics. This is always the case. There is no need to do hypothesis testing on statistics as they are exact values in our data set.


@@@
1
00:00:00,000 --> 00:00:08,765
Once we set up our null and alternative hypotheses, we need used data to figure out which hypothesis we actually think is more likely to be true.

2
00:00:08,765 --> 00:00:24,419
And there are two ways we might approach choosing one of these hypotheses. One is the approach that we saw using confidence intervals, where we simulate the sampling distribution of our statistic, and then we could see if our hypothesis is consistent with what we observe in the sampling distribution.

3
00:00:24,420 --> 00:00:35,100
The second way we could approach choosing a hypothesis is simulating what we believe to be possible under the null and then seeing if our data is actually consistent with that.


@@@
1
00:00:00,000 --> 00:00:07,375
I have attached the data for this example in the Resources tab. And you also can find it in the works pace on the next concept of our work through.

2
00:00:07,375 --> 00:00:14,574
This is the same coffee data you saw in the previous lesson on confidence intervals. So you might already have it saved on your local as well.

3
00:00:14,574 --> 00:00:20,759
What if we wanted to ask the question of if the average height for all coffee drinkers is greater than 70 inches.

4
00:00:20,760 --> 00:00:34,829
We could set up our known alternative hypothesis in the following way. Here, we have that the average height of all coffee drinkers is less than or equal to 70 in the null while in this statement we have that the average height is greater than 70 in the alternative.

5
00:00:34,829 --> 00:00:41,969
Notice, we are always testing a parameter. So I'm using new here to represent the mean of all coffee drinkers.

6
00:00:41,969 --> 00:00:54,674
Based on what we just did with confidence intervals, you can imagine a very intuitive approach for determining if the null is possible, is just to bootstrap a sample set of data and compute the sample mean again and again.

7
00:00:54,674 --> 00:01:04,075
And build the sampling distribution and corresponding confidence interval to determine what are the reasonable values for the population mean with some level of confidence.

8
00:01:04,075 --> 00:01:12,359
Let's put this to practice, imagine from our dataset we achieve this sample. Then we can bootstrap this in the following way.

9
00:01:12,359 --> 00:01:28,775
Now, let's bootstrap a number of times and compute the mean for each bootstrap sample. Here, we have our bootstrap sample, and here, I've created an empty vector of means that we're going to append each of our bootstrap means into.

10
00:01:28,775 --> 00:01:37,935
Now, we have all of our means and we can create our confidence interval. Here, I have the lower bound, and here is the upper bound.


@@@
1
00:00:00,000 --> 00:00:07,214
Consider the same example as earlier, where we asked if the mean height for all coffee drinkers was greater than 70 inches.

2
00:00:07,214 --> 00:00:28,559
We could again set up a null and alternative hypotheses like these. A second approach that is commonly done for making decisions in hypothesis testing is the following, we assume that the null is true and we know what the sampling distribution would look like if we were to simulate from the closest value under the null to the alternative.

3
00:00:28,559 --> 00:00:36,659
That is, this value of 70. That's the closest value under this hypothesis to our alternative hypothesis.

4
00:00:36,659 --> 00:00:47,199
We could use the standard deviation of the sampling distribution to determine what the sampling distribution would look like if it came from the null hypothesis.

5
00:00:47,200 --> 00:00:56,835
We'll simulate from a normal distribution in this case. I'm going to pull over the code that we used before to get the standard deviation of our sampling distribution.

6
00:00:56,835 --> 00:01:10,534
So the standard deviation of our sampling distribution is equal to 0.2658. And we know that if it came from this null hypothesized value of 70 what it would look like.

7
00:01:10,534 --> 00:01:27,844
By the central limit theorem, we know that it would follow a normal distribution. Now, from the NumPy documentation on normal distributions, we see we can simulate draws from the normal using the hypothesized mean at 70 and the standard deviation of our sampling distribution in the following way.

8
00:01:27,844 --> 00:01:37,325
So here, the loc tells us that the mean of 70 will go in this value, and the scale is the standard deviation that we want to use.

9
00:01:37,325 --> 00:01:43,765
So that's the standard deviation of our sampling distribution. And we can simulate, say, 10,000 values from that.

10
00:01:43,765 --> 00:01:54,864
Each of the simulated draws here represents a possible mean from the null hypothesis. We can now ask the question of where the sample mean falls in this distribution.

11
00:01:54,864 --> 00:02:01,924
If we go back and look at what our sample mean was, we can see that it falls far below this distribution from the norm.

12
00:02:01,924 --> 00:02:12,405
If our sample mean were to fall closer to the center value of 70, it would be a value that we would expect from the null hypothesis and therefore, we think the null is more likely to be true.

13
00:02:12,405 --> 00:02:21,439
In this case, with our sample means so far out in the tail, it's far enough that we don't think it probably came from this null hypothesized value.


@@@
1
00:00:01,679 --> 00:00:20,674
In the previous video, you saw two methods for how we might choose between competing hypotheses. In the second method, we ask the question if the null hypothesis is true, what is the probability of obtaining the statistic we observed in our data or one more extreme in favor of the alternative hypothesis?

2
00:00:20,675 --> 00:00:31,779
This probability is what is called a P-value. Finding the P-value involves a mix of ideas that you've learned about, sampling distributions and conditional probability.

3
00:00:31,780 --> 00:00:42,259
Imagine we have a null that a population mean is equal to zero. Then we collect sample data and we find the sample mean to be five and the sample standard deviation to be two.

4
00:00:42,259 --> 00:00:54,525
Assuming the sampling distribution of your statistic follows the null hypothesis. What is the probability of observing the actual value of the statistic from your data in this distribution?

5
00:00:54,524 --> 00:01:00,995
If we want to know the probability that the population mean is actually greater than zero, you could update the hypothesis.

6
00:01:00,994 --> 00:01:12,260
Values like 6, 7, 10 all fall out here and are even more of an indication that the alternative is true, that our population mean is greater than zero.

7
00:01:12,260 --> 00:01:18,084
Additionally, what is the probability you observed something even more in favor of the alternative hypothesis?

8
00:01:18,084 --> 00:01:31,079
This shaded region here provides that probability, the P-value. Notice the P-value is dependent on your alternative hypothesis as it determines what is considered more extreme.

9
00:01:31,079 --> 00:01:38,359
If your alternative hypothesis is that your parameter is greater than zero, then we shade greater than the statistic as shown here.

10
00:01:38,359 --> 00:01:45,144
However, if we change our null and alternative to look like this, then we shade to the left of our statistic.

11
00:01:45,144 --> 00:01:54,754
Our shading for the P-value would now look like this. Notice that this would create a very large probability in this case as almost the entire distribution is shaded.

12
00:01:54,754 --> 00:02:03,194
And if you have a not equal sign in the alternative, then your shading is associated with extremes that are just far from the null in either direction.

13
00:02:03,194 --> 00:02:15,730
In these cases, we just care about statistics that are far from the null in either direction. So we end up marking on both sides and shading away from the null hypothesis to find our P-value.

14
00:02:15,729 --> 00:02:31,040
This often takes people a while to wrap their heads around. And there are two parts to understand. First, you have to fully conceptualize the definition of a P-value, which is the conditional probability of your data given that the null hypothesis is true.

15
00:02:31,039 --> 00:02:44,000
Then, you need to figure out what to compute, which is visually summarized by these three images. You will get some practice with both of these before moving on and applying the ideas of P-value to make decisions.


@@@
1
00:00:00,000 --> 00:00:13,485
In the last programming video, you saw how we could simulate draws from the null hypothesis, and that if our statistic was in the bulk of the distribution, this suggested that the statistic was likely from that null.

2
00:00:13,484 --> 00:00:22,765
However, if the statistic was farther out from the bulk of the distribution, this suggested that the null wasn't likely to have generated our statistic.

3
00:00:22,765 --> 00:00:36,490
Then you saw that we can calculate p-values based on the shaded region starting at the value of our observed statistic through the tail of the distribution, where the shaded region is dependent on our alternative.

4
00:00:36,490 --> 00:00:43,690
Based on the previous video, you had simulated values of the sampling distribution from the null, like this.

5
00:00:43,689 --> 00:00:55,375
Imagine that we have the alternative hypothesis that the population mean is greater than 70, then we could calculate the p-value as the proportion of the simulated draws that are larger than our sample mean.

6
00:00:55,375 --> 00:01:04,769
Here, you can see that that would give us a p-value of one. Remember, large p-value suggests that we shouldn't move away from the null hypothesis.

7
00:01:04,769 --> 00:01:15,079
In this case, that suggests that we should stay with the mean being less than 70. Here, we've calculated the null values that are greater than our sample mean.

8
00:01:15,079 --> 00:01:26,905
Since this is one, our p-value is large and therefore we wouldn't move away from the null hypothesis that suggests that our population mean is truly less than or equal to 70.

9
00:01:26,905 --> 00:01:33,995
If our new null and alternative hypotheses look like this instead, we would calculate our p-value a little differently.

10
00:01:33,995 --> 00:01:41,505
Here, because our alternative is less than 70, we would now look at the shaded region to the left of our statistic.

11
00:01:41,504 --> 00:01:55,400
This would change our p-value to the following. Now that our p-value is zero, this suggests that we reject the null hypothesis in favor of an alternative, suggesting that the population mean is less than 70.

12
00:01:55,400 --> 00:02:06,125
So if this was our null and alternative hypothesis, we would now want to look at the values that are more extreme than our sample mean in either direction away from the null hypothesis.

13
00:02:06,125 --> 00:02:12,625
That looks like this equation here. Let's take a look at where these values fall on the above histogram.

14
00:02:12,625 --> 00:02:21,344
You can see that if we were to shade more extreme than either of these regions, there are essentially no data points from our null hypothesis that fall outside of this region.

15
00:02:21,344 --> 00:02:28,030
Again, we would have evidence to suggest that the null hypothesized value did not generate our sample statistic.


@@@
1
00:00:02,149 --> 00:00:10,709
In the last video, we saw that a p-value is the probability of obtaining our data or more extreme values from the null hypothesis.

2
00:00:10,710 --> 00:00:22,474
So how does this connect to making decisions and the types of errors that we can make? If the p-value is really small, this suggests it's less likely to observe our statistic from the null.

3
00:00:22,475 --> 00:00:31,080
And it's more likely that it came from the alternative. But how small does the p-value need to be before we no longer stay with the null hypothesis?

4
00:00:31,079 --> 00:00:47,085
Well, I guess that depends on how willing you are to make certain types of errors. If you are willing to commit five percent of errors, where you choose the alternative incorrectly, then your p-value needs to be smaller than this threshold in order to choose the alternative.

5
00:00:47,085 --> 00:00:57,905
However, if your probability of getting the data from the null is, say, an eight percent chance, this is enough of a chance that you would stay with the null under a five percent type one error threshold.

6
00:00:57,905 --> 00:01:04,524
The fast rule is that if our p-value is less than a type one error rate, then professionals say we reject the null.

7
00:01:04,525 --> 00:01:12,459
That is, we choose the alternative hypothesis. If the p-value is greater than our type one error rate, then we fail to reject the null.

8
00:01:12,459 --> 00:01:22,039
That is, we stay with the null hypothesis as our decision. What's actually true? We can't really be sure in practice, but we now have a database way to make our decisions.


@@@
1
00:00:01,470 --> 00:00:28,750
When making a decision about whether you're choosing the null or alternative hypothesis, you might see certain professionals, specifically statisticians, cringe, with concluding remarks like, "So based on the data, we accepted the null hypothesis to be true," or, "Based on the data, we accepted the alternative." Remember, when setting up our null and alternative hypotheses, we automatically set the null to be true before any data were collected.

2
00:00:28,750 --> 00:00:40,875
Therefore, this statement is a default. It isn't like we're uncertain about what hypothesis we'll choose and then we choose one, rather, you are by default choosing the null.

3
00:00:40,875 --> 00:00:47,950
Going back to the judicial example, everyone is innocent until proven guilty. You don't choose someone as innocent.

4
00:00:47,950 --> 00:01:07,844
By default, everyone is innocent. Therefore, in hypothesis testing, we say, "Based on the data, we have evidence to reject the null hypothesis," or alternatively, if we don't have enough evidence to reject the null, we say, "Based on the data, we fail to reject the null hypothesis." Many people will only care about making the right decision.

5
00:01:07,844 --> 00:01:16,170
So this distinction might seem just a bit picky. But it stresses that the null is much more likely to be chosen as you start with the statement being true.


@@@
1
00:00:01,820 --> 00:00:13,404
When conducting a hypothesis test, there are really two components that you have control over. First, you should be asking yourself, is my sample representative of my population of interest?

2
00:00:13,404 --> 00:00:25,390
Are there ways to assure that everyone in my population is accurately represented in my sample? If your sample isn't representative, then your conclusions are likely to be incorrect.

3
00:00:25,390 --> 00:00:38,875
Second, you should know the impact of your sample size, and the role that it plays on your results. As your sample size increases, even the smallest differences between the two groups will appear as notable and statistically significant.

4
00:00:38,875 --> 00:00:50,469
With the really large sample sizes that are becoming more frequent, as a part of the data world, we are seeing a large change in techniques away from hypothesis tests for exactly this reason.

5
00:00:50,469 --> 00:00:58,935
Imagine we are interested in which of two coffee types will sell better on the shelf, and on average, maybe type one sells better than type two.

6
00:00:58,935 --> 00:01:06,204
However, there are hundreds, thousands, maybe even millions of individuals who still prefer a different type.

7
00:01:06,204 --> 00:01:15,159
A hypothesis test just tells us that, on average, type one will sell more than type two. But with large sample sizes, we should do better than this.

8
00:01:15,159 --> 00:01:24,759
Discussing averages leaves out an entire part of the population who preferred type two, or maybe did not care for either of these types, but like a different type.

9
00:01:24,760 --> 00:01:32,380
But with large sample sizes, we can do better than this. We can do better than hypothesis testing. And this is what more and more people are finding out.

10
00:01:32,379 --> 00:01:42,444
Using machine learning, we can individualize an approach. So maybe, we sell twenty types of coffee and we know what type every member in our population wants.

11
00:01:42,444 --> 00:01:52,825
So these large sample sizes prove to be pretty detrimental to a hypothesis test, but they are spectacular for individual-aimed approaches using machine learning techniques.

12
00:01:52,825 --> 00:01:59,000
We will get a glimpse of two of these machine learning techniques in the last lessons of this class.


@@@
1
00:00:02,270 --> 00:00:11,995
You've learned about the types of errors that are possible on hypothesis testing. And you've learned about how we can create a threshold for how often we allow these to happen.

2
00:00:11,994 --> 00:00:26,320
But consider if we run 20 of the exact same types of hypothesis tests. Even if the null is actually true, if we have a five percent type one error rate, we can expect one of these to have results where we choose the alternative.

3
00:00:26,320 --> 00:00:37,189
Often, researchers all over the world are performing very similar studies. So when one researcher comes up with significant results, how can we be sure that they are one of these type one errors?

4
00:00:37,189 --> 00:00:46,695
Well, the hard part is we can't really know in many cases. And that's a problem. But there are a few methods that statisticians have come up with to assist with this problem.

5
00:00:46,695 --> 00:01:01,894
One of the most conservative and common approaches is known as the Bonferroni Correction. This simple correction says that if you're performing m-tests, you should divide your type one error rate by m to assure that you actually maintain the error rate here.

6
00:01:01,895 --> 00:01:12,980
So if you really want to have a five percent type one error rate and you're performing 10 hypothesis test, your new threshold is actually 0.5 percent for choosing the alternative hypothesis.

7
00:01:12,980 --> 00:01:22,480
This is just one popular correction method. But other methods include the Tukey correction. And in the biomedical field, they commonly use a method called Q-Values.


@@@
1
00:00:01,899 --> 00:00:11,339
So you saw earlier that hypothesis tests and confidence intervals are pretty similar. But you might be surprised just how similar these two techniques are.

2
00:00:11,339 --> 00:00:26,655
It turns out that if you are to do a hypothesis test that contains a not equal in alternative hypothesis, your conclusions are identical to a confidence interval, as long as 100 minus your confidence interval level is the same as the type one error rate in your hypothesis test.

3
00:00:26,655 --> 00:00:42,909
That is, if you build a 95 percent confidence interval, this is the same as doing a two-sided hypothesis test with a five percent type one error rate, and building a 99 percent confidence interval will provide the same results as a hypothesis test with a one percent type one error rate.

4
00:00:42,909 --> 00:00:50,789
With this in mind, many academic journals are moving away from hypothesis tests as they find the results are often misinterpreted.

5
00:00:50,789 --> 00:00:58,135
Instead, more and more confidence intervals and other statistics like effect size or machine learning techniques are being used.

6
00:00:58,134 --> 00:01:06,000
There is more information on effect sizes as well as a link to a free course in the Udacity Nanodegree, link in instructor notes below.


@@@
1
00:00:02,060 --> 00:00:08,339
You now have learned how to set up a null and alternative hypothesis. You have determined type one and type two errors.

2
00:00:08,339 --> 00:00:23,009
And you can calculate which hypothesis you should choose based on error threshold. You also have learned about some of the dangers of the conclusions that you might make when having a really large sample size, or if you were to perform more than one hypothesis tests.

3
00:00:23,010 --> 00:00:29,164
Finally, you learned how confidence intervals and hypothesis tests are highly related to one another.

4
00:00:29,164 --> 00:00:36,195
The ideas associated with hypothesis tests are central to A/B testing and making database business decisions.


@@@
1
00:00:00,000 --> 00:00:10,904
Welcome to this case study on AB test. In this lesson, you all apply what you've learned in the previous lessons, to help a company decide whether to launch two new features on their web site.


@@@
1
00:00:01,840 --> 00:00:09,980
When companies want to test new features or versions of a web page, they often use a method called A/B testing.

2
00:00:09,980 --> 00:00:21,469
The way this works is that one set of users called the control group is shown the old version of a page while another set of users called the experiment group is shown the new version of a page.

3
00:00:21,469 --> 00:00:28,039
Based on how both groups respond, we can determine if the new version is better and should be launched.

4
00:00:28,039 --> 00:00:42,780
This is actually just an application of hypothesis testing where the null hypothesis would be that the new version is no better or even worse than the old version and the alternative, would be that the new version is better than the old version.

5
00:00:42,780 --> 00:00:52,484
A ton of companies use A/B testing to try out changes in features, layouts and even colors to increase a metric that measures interest from their users.

6
00:00:52,484 --> 00:01:01,760
For example, you could perform an A/B test to see if a new look for your site's landing page increases the likelihood of a visit or subscribing to your mailing list.

7
00:01:01,759 --> 00:01:09,634
Or you could perform an A/B test to see if offering personalized recommendations on a shopping site increases purchases.

8
00:01:09,635 --> 00:01:20,945
One thing to be aware of is that although A/B testing can be good to test if a new feature or version of a feature is better on a page, it's not useful for everything.

9
00:01:20,944 --> 00:01:33,045
For example, A/B testing can tell you the best way to rank a set of products on a site but it can't tell you that the site would really benefit from having two additional products added to the site.

10
00:01:33,045 --> 00:01:41,710
It's also not always great for testing whole new experiences for existing users due to factors like change aversion and novelty effect.

11
00:01:41,709 --> 00:01:51,340
Existing users may give an unfair advantage to the older version simply because they're unhappy with a change even if it's ultimately for the better.

12
00:01:51,340 --> 00:02:00,689
Alternatively, they may give an unfair advantage to the new version because they're excited or drawn to the change even if it isn't for the better.

13
00:02:00,689 --> 00:02:09,000
In addition to these, there are many other factors that can bias the results of an A/B test. You'll learn more about those in this lesson.


@@@
1
00:00:02,439 --> 00:00:13,804
In this case study, you'll be analyzing AB test results for an online education company called Audacity, that offers courses on finance.

2
00:00:13,804 --> 00:00:24,859
Let's first see what a typical user flow might look like on Audacity site. New users would land on the home page, and if they're interested, they would click to explore courses.

3
00:00:24,859 --> 00:00:34,814
As they browse the course list, a course may catch their eye, and they click on it to learn more. Once they're on the course overview page, they may decide to enroll.

4
00:00:34,814 --> 00:00:46,800
After enrolling, they'd hopefully complete the course. In this flow, commonly called the customer funnel, users lose interest, and leave at different stages of the funnel, and few make it to the end.

5
00:00:46,799 --> 00:00:59,299
Of course, this is a simplistic model, and isn't how all user experiences would play out. But it does capture pretty well the main steps from an initial exposure to course completion in order of decreasing probability.

6
00:00:59,299 --> 00:01:07,459
To increase student engagement, Audacity wants to perform AB tests to try out changes, at different points in this funnel.

7
00:01:07,459 --> 00:01:16,490
In this case study, we'll analyze test results for two particular changes they have in mind, and then make a recommendation on whether they should launch each change.


@@@
1
00:00:01,649 --> 00:00:12,684
The first change Audacity wants to try is on their home page. They hope that this new, more engaging design will increase the number of users that explore their courses.

2
00:00:12,685 --> 00:00:20,905
That is, move on to the second stage of the funnel. Now that we know the change we want to make, we need to choose a metric that measures that change.

3
00:00:20,905 --> 00:00:30,784
To measure how many people move on to the next stage, we can track how many people click on the Explore Courses button on the home page with the new design and the old design.

4
00:00:30,785 --> 00:00:39,509
However, using just the number of users doesn't make sense if more total users view the page in one version of the experiment.

5
00:00:39,509 --> 00:00:46,900
More total clicks could occur in one version even if there is a greater percentage of clicks in the other version.

6
00:00:46,899 --> 00:00:59,049
So instead, we could use the fraction of page visitors who clicked. That is the number of clicks on The View Courses button divided by the number of page views to the home page.

7
00:00:59,049 --> 00:01:14,519
This metric is commonly called click-through rate or CTR. You could take this metric one step further and make it the number of unique visitors who clicked at least once, divided by the number of unique visitors to view the page.

8
00:01:14,519 --> 00:01:22,750
Now that we have our metric, let's set up our known alternative hypothesis. Our alternative hypothesis is what we want to prove to be true.

9
00:01:22,750 --> 00:01:29,500
In this case, the new home page design has a higher click-through rate than the old home page design.

10
00:01:29,500 --> 00:01:43,010
And the new hypothesis is what we assumed to be true before analyzing any data, which is that the new home page design has a click-through rate that is less than or equal to that of our old home page design.


@@@
1
00:00:00,000 --> 00:00:11,905
As you saw in the last section, this dataset includes view and click actions on the home page of Audacity's site, from users that were shown the control and experimental versions of the A/B test.

2
00:00:11,904 --> 00:00:18,285
Our task is to analyze these actions to see if there was a significant difference in performance for the two versions.

3
00:00:18,285 --> 00:00:24,090
To do this, let's first compute the click-through rate for each group. Let's start with the control group.

4
00:00:24,089 --> 00:00:38,609
We can extract all the actions from the control group like this. Now, to compute the click-through rate, we'll divide the number of unique users who actually click the Explore courses button by the total number of unique users who viewed the page.

5
00:00:38,609 --> 00:00:44,935
This gives us a click-through rate of about 28 percent. Let's do the same thing for the experiment group.

6
00:00:44,935 --> 00:00:54,755
Again, we'll take all the click actions, get the unique number of users, and divide that number by the number of unique users who viewed the page.

7
00:00:54,755 --> 00:01:05,545
That gives us a click-through rate of about 31 percent. So in this sample, the experiment group's click-through rate was higher than the control group's click-through rate by about 3 percent.

8
00:01:05,545 --> 00:01:11,960
Now that we know the observed difference in this sample, we have to see if this difference is significant and not just due to chance.

9
00:01:11,959 --> 00:01:20,490
Let's bootstrap the sample to simulate the sampling distribution for the difference in proportions. Let's take a look at our sampling distribution.

10
00:01:20,489 --> 00:01:35,054
If you remember from the previous lesson, we can compute the p-value for our statistic which is the observed difference in proportions by simulating the distribution under the null hypothesis and then finding the probability that our statistic came from this distribution.

11
00:01:35,055 --> 00:01:44,530
To simulate from the null, we'll create a normal distribution centered at zero, with the same standard deviation as our sampling distribution we simulated here.

12
00:01:44,530 --> 00:02:00,015
We could see the null distribution here. And this is where our observed statistic falls. We can't find the p-value like this as these are all the null values that are more extreme than our statistic in favor of our alternative.

13
00:02:00,015 --> 00:02:08,754
With a p-value of approximately a half of percent, the difference in click-through rates for the control and experiment groups does appear to be significant.

14
00:02:08,754 --> 00:02:16,000
We can reject the null hypothesis, and based on these results, it looks like Audacity should launch the new version of the home page.


@@@
1
00:00:02,169 --> 00:00:18,765
The second change Audacity wants to try is on their course overview page. They created a new description for one of their courses to dedicate larger portions to connecting concepts in the course to career skills, and less on the details of each concept.

2
00:00:18,765 --> 00:00:29,029
They hope that this change may encourage more users to complete the course. For this experiment, instead of choosing just a single metric, we're going to analyze multiple.

3
00:00:29,030 --> 00:00:37,115
We'll track the enrollment rate, the average reading time on the course page, the average time spent in the classroom, and the course completion rate.


@@@
1
00:00:00,000 --> 00:00:06,790
In addition to computing the enrollment rate, we can also compute the average reading durations with this dataset.

2
00:00:06,790 --> 00:00:14,039
The two analyses so far were comparing proportions. With this metric, we'll be analyzing the difference in means.

3
00:00:14,039 --> 00:00:21,304
This analysis will be quite similar. Since we're comparing reading durations, we only care about view action.

4
00:00:21,304 --> 00:00:32,465
So let's filter by that first. And let's only count each unique user once by finding their average reading duration if they visited the site more than once.

5
00:00:32,465 --> 00:00:44,899
Well, also group by group, just so we keep track of that information. This isn't necessary, but resetting the index is nice just so we keep the ID and group as column names.

6
00:00:44,899 --> 00:00:56,750
And it also let's us continue working in a data frame instead of a multi index series. Now, we can find the average reading durations for each group like this.

7
00:00:56,750 --> 00:01:05,444
On average, it looks like users in the experiment group spent 15 more seconds on the course overview page than those in the control group.

8
00:01:05,444 --> 00:01:14,299
To see if this difference is significant, let's simulate the sampling distribution for the difference in mean reading durations with bootstrapping.

9
00:01:14,299 --> 00:01:26,619
Here's what the sampling distribution looks like. Now to find the P value, let's simulate the distribution under the null and find the probability that our observed statistic came from this distribution.

10
00:01:26,620 --> 00:01:33,634
We'll create the distribution centered at zero and they're having the same spread as our sampling distribution.

11
00:01:33,635 --> 00:01:43,985
Here's our null distribution. And here is where I observed statistic falls. Our statistic definitely doesn't look like it came from this null distribution.


@@@
1
00:00:02,299 --> 00:00:11,594
As you've seen in previous lessons, the more things that you test, the more likely you are to observe significant differences just by chance.

2
00:00:11,595 --> 00:00:21,594
This happens when we run evaluations from multiple metrics at the same time. The probability of any false positive increases as you increase the number of metrics.


@@@
1
00:00:00,000 --> 00:00:11,914
How do we make a recommendation when three out of our four metrics had a significant difference for testing each metric individually, but insignificant differences when we use the Bonferroni correction?

2
00:00:11,914 --> 00:00:23,464
The Bonferroni method is a conservative one, and since we expect the metrics to be correlated, this would be best handled by a more sophisticated method that ideally takes this correlation into account.

3
00:00:23,464 --> 00:00:35,810
In the instructor notes, there is a list of possible other methods that are less conservative. Should you choose one of these methods, you should ensure that the assumptions of the method are truly met in your scenario.

4
00:00:35,810 --> 00:00:48,000
Choosing a poorly suited test that simply provides the results that you want is not only bad practice but will likely lead to misguided decisions that harm your company's performance in the long run.


@@@
1
00:00:02,149 --> 00:00:21,420
Great job completing this case study. To summarize, you learned about the uses and values of AB testing, defining metrics that measure changes in your experiments, analyzing results with confidence intervals and hypothesis testing, handling multiple metrics, and common difficulties associated with AB testing.


@@@
1
00:00:00,000 --> 00:00:10,199
Hi I'm Louis. Welcome to the linear regression section of this Nanodegree. The two main families of algorithms and predictive machine learning are classification and regression.

2
00:00:10,199 --> 00:00:19,044
Classification answers questions of the form yes-no. For example, is this email spam or not, or is the patient sick or not.

3
00:00:19,045 --> 00:00:28,500
Regression answers questions of the form how much. For example, how much does this house cost? Or how many seconds do expect someone to watch this video?

4
00:00:28,500 --> 00:00:40,119
In this lesson we'll learn how to answer this last type of questions. First, we will learn linear regression, then some ways to improve it, and finally some ways to generalize it to non-linear cases.


@@@
1
00:00:00,000 --> 00:00:06,015
So let's say we're studying the housing market and our task is to predict the price of a house given its size.

2
00:00:06,015 --> 00:00:18,600
So we have a small house that costs $70,000 and a big house that costs $160,000. We'd like to estimate the price of these medium-sized house over here. So how do we do it?

3
00:00:18,600 --> 00:00:28,770
Well, first we put them in a grid where the x-axis represents the size of the house in square feet and the y-axis represents the price of the house.

4
00:00:28,770 --> 00:00:39,705
And to help us out, we have collected some previous data in the form of these blue dots. These are other houses that we've looked at and we've recorded their prices with respect to their size.

5
00:00:39,705 --> 00:00:49,680
And here we can see the small house is priced at $70,000 and the big one at $160,000. Now it's time for a small quiz.

6
00:00:49,680 --> 00:01:03,090
What do you think is the best estimate for the price of the medium house given this data? Would it be $80,000, $120,000 or $190,000?


@@@
1
00:00:00,000 --> 00:00:09,089
Well to help us out, we can see that these points can form a line. And we can draw the line that best fits this data.

2
00:00:09,089 --> 00:00:18,660
Now on this line, we can see that our best guess for the price of the house is this point here over the line which corresponds to $120000.

3
00:00:18,660 --> 00:00:31,914
So if you said $120000, that is correct. This method is known as linear regression. You can think of linear regression as a painter who would look at your data and draw the best fitting line through it.

4
00:00:31,914 --> 00:00:37,000
And you may ask, "How do we find this line?" Well, that's what the rest of the section will be about.


@@@
1
00:00:00,000 --> 00:00:08,505
So here's a trick that will help us fit a line through a set of points. Let's say that these are points and we start by drawing some random line.

2
00:00:08,505 --> 00:00:21,005
We're going to ask every point what it wants for the model to be better and then listen to them. So let's go one by one, let's take this point over here and ask it what would it want the line to do?

3
00:00:21,004 --> 00:00:32,890
Well, the point would want the line to move closer. So, we listen to the point and move closer. Now, if we ask all the points they will tell us the same thing, so we move closer to all of them.

4
00:00:32,890 --> 00:00:41,269
Then we'll listen to them again and we take another step that gets closer to all of them, now it's trickier because we have a point underneath that pulls it down but that's still okay.

5
00:00:41,270 --> 00:00:48,554
Basically, the idea is that we take a few steps that make us go closer to all the points and that's it, that's linear regression.


@@@
1
00:00:00,000 --> 00:00:18,674
So, let's have a little refresher on how we move lines by changing the parameters. So, if we have a line of equation y equals w_1x plus w_2, where w_1 and w_2 are constants, it looks like this, w_1 is the slope and w_2 is the y-intercept which is where the line intersects the y-axis.

2
00:00:18,675 --> 00:00:25,600
Now, what happens if we increase w_1? Well, we increase the slope so that means the line rotates like this.

3
00:00:25,600 --> 00:00:33,774
If we decrease w_1, then we're decreasing the slope so the line rotates like this. Now, what happens if we increase the y-intercept w_2?

4
00:00:33,774 --> 00:00:40,975
Then the line moves in a parallel way up like this. And if we decrease w_2, then the line moves like that to the bottom.


@@@
1
00:00:00,000 --> 00:00:04,734
So, here's our first trick that we will align closer to a point that we're going to use in a linear regression.

2
00:00:04,735 --> 00:00:14,919
It's called the absolute trick and it works like this, we start with a point, and a line, and the idea is that the point wants the line to come closer to it.

3
00:00:14,919 --> 00:00:23,179
So, let's put some numbers here, the point has coordinates p comma q, where p is a horizontal coordinate and q is the vertical coordinate.

4
00:00:23,179 --> 00:00:39,649
The line has equation y equals w_1x plus w_2. In here w_1 is the slope and w_2 is the y-intercept. So, an easy way to move the line closer to the point pq is to just add one to the y-intercept, and then there is, the line moves up.

5
00:00:39,649 --> 00:00:47,489
Now let's add something to the slope two to make the line rotating the direction of the point. This is going to look a little strange but it's going to make sense very soon.

6
00:00:47,490 --> 00:01:01,320
So, this distance over here is p because it's the horizontal distance from the y-axis to the point. So let's just add that, let's add p to the slope, and now our new slope is w_1 plus p, and that rotates the line in this direction.

7
00:01:01,320 --> 00:01:10,230
Now our new equation is y equals w_1 plus p times x plus w_2 plus one, and that's pretty much what the absolute trick is about.

8
00:01:10,230 --> 00:01:21,480
Notice some subtleties though, notice that we moved the line a little too much, and we actually went over the point and kept going, we don't want this, in general in machine learning we never want to take big steps like this.

9
00:01:21,480 --> 00:01:31,004
Instead what we want to do is take tiny steps. So, in order to take a tiny step we'll just do the exact same thing we did except we multiply everything by a small number.

10
00:01:31,004 --> 00:01:46,064
So let's take a small number called the learning rate, let's say alpha, and instead of adding one to the y-intercept and p to the slope, we will add alpha times one to the y-intercept, and alpha times p to the slope.

11
00:01:46,064 --> 00:01:51,674
Now the line moves up by a little bit and rotates a little bit, so we don't have that risk of going too far.

12
00:01:51,674 --> 00:02:03,625
Our new equation is going to be w_1 plus p times alpha times x plus w_2 plus alpha. So our new slope is w_1 plus p times alpha and our new y-intercept is w_2 plus alpha.

13
00:02:03,625 --> 00:02:09,560
So we're doing better but there's still a little subtlety. What happens if the point is not on top of the line but underneath the line?

14
00:02:09,560 --> 00:02:19,969
Well, same thing except now instead of adding we just subtract to get our new equation w_1 minus b times alpha times x plus w_2 minus alpha.

15
00:02:19,969 --> 00:02:28,919
The reason is if we subtract alpha to the y-intercept the line moves down instead of up and if we subtract p times alpha to the slope the line rotates in this direction instead.

16
00:02:28,919 --> 00:02:49,240
Something more interesting is this, and that will explain the reason p is there. If the point is not on the right of the y-axis but on the left of it, than we still add one to the y-intercept because we need the line to move up, but the fact that now we're adding p to the slope and p is now a negative number means that our line now rotates in this direction, so that's a reason for p to be there.

17
00:02:49,240 --> 00:02:57,504
Another reason for p to be there is this, check this out. If this distance is small then p is small, so we're adding a small number to the slope.

18
00:02:57,504 --> 00:03:07,490
Now, if the distance is large then p is large so we're adding a large number to the slope. It makes sense that if the point is really close to the y-axis we want to increase the slope by a little bit.

19
00:03:07,490 --> 00:03:23,875
Whereas if it's far want to move it by a lot more. So, let's do an example to make this more clear. Let's say we have the point 5 comma 15 and the line y equals 2_x plus three, so this means the distance from the point to the y-axis is five, now let's say our learning rate is 0.1.

20
00:03:23,875 --> 00:03:42,979
So, we're adding 0.1 times one to the y-intercept and that moves the line up by a little bit. Also we are taking five multiplying it by 0.1 and adding that to the slope which makes the line move in this direction, this means our new equation is y equals 2.5 x plus 3.1.

21
00:03:42,979 --> 00:03:57,000
Now, check out what happens if the point is in the left, we're still adding 0.1 to the y-intercept to move the line up, but now to the slope we're going to add the product of 0.1 and minus five, that's minus 0.5.

22
00:03:57,000 --> 00:04:08,159
This means our new equation is going to be 1.5 x plus 3.1. As you can see the slope moved in a different direction which made it go closer to the point.

23
00:04:08,159 --> 00:04:13,320
So that's it, that's the absolute trick, and we're going to use it extensively in linear regression.


@@@
1
00:00:00,000 --> 00:00:08,280
So here's another trick that will help us move a point closer to a line, and it's very similar to the absolute trick but it has a little bit of extra gravy.

2
00:00:08,279 --> 00:00:16,109
It's based on this premise. If we have a point that is close to a line, then this distance is small and we want to move the line very little.

3
00:00:16,109 --> 00:00:31,109
But if the point is far from the line, then want to move the line a lot more. The absolute trick that we learned previously does not have this property because if we remember the absolute trick adds alpha to the y-intercept w_2 and p times alpha to the slope w_1.

4
00:00:31,109 --> 00:00:37,969
This has nothing to do with how far the point is from the line, since P is just a horizontal distance.

5
00:00:37,969 --> 00:00:44,734
So let's just add the vertical distance into this formula. Let's look at this vertical distance between the point and the line.

6
00:00:44,734 --> 00:00:56,225
The point over the line has coordinates say,( p,q'). This distance is then q minus q' because q' is the value at the line and q is the value of the y coordinate at the point.

7
00:00:56,225 --> 00:01:04,570
So what we do here is very simple. We just take this q minus q' and multiply it into what we're adding to both the y-intercept and to the slope.

8
00:01:04,569 --> 00:01:18,044
This will again make the line go up by a bit and rotate in this direction except that now if the point is far from or close to the line, the amount the line moves will be affected and here's our new equation with a factor of q minus q'.

9
00:01:18,045 --> 00:01:33,849
Notice that here we get something for free. If the point is underneath the line instead of over the line, then q minus q' is actually a negative value and if this is the case then we're subtracting something from the slope thus rotating the line in this direction instead, still towards the point.

10
00:01:33,849 --> 00:01:40,189
So this trick also takes care of points that are under the line and we don't have to have two rules like we had on the absolute trick.

11
00:01:40,189 --> 00:01:57,259
We just have one same rule for both. Again, let's clarify this with an example. In this example over here, we have the point (5,15) and notice that this distance is going to be two because the line goes through the (5,13) as two times five plus three is 13.

12
00:01:57,260 --> 00:02:07,409
Now to change things a bit, we'll use zero point 0.01 as the learning rate. So the absolute trick would be adding 0.01 to the y-intercept and 0.05 to the slope.

13
00:02:07,409 --> 00:02:20,569
But now as our neutral access, we multiply the two numbers two adding to the slope and the y-intercept by two; which means we're adding 0.12 to the slope and 0.02 to the y-intercept.

14
00:02:20,569 --> 00:02:29,530
So the equation of our new line is y equals 2.1_x plus 3.02. Notice that that line is going to be closer to the point.


@@@
1
00:00:00,000 --> 00:00:08,504
so now that we've learned the absolute trick and the square trick, and how they're used in linear regression, we still want to have some intuition on how these things get figured out.

2
00:00:08,505 --> 00:00:14,595
These tricks still seem a little too magical, and we'd like to find their origin so let's do this in a much more formal way.

3
00:00:14,595 --> 00:00:21,757
Let's say we have our points on our plan is to develop an algorithm which will find the line that best fits this set of points.

4
00:00:21,757 --> 00:00:39,417
And the algorithm works like this, first it will draw a random line, and it'll calculate the error. The error is some measure of how far the points are from the line, in this drawing it looks like it's the sum of these distances but it could be any measure that tells us how far we are from the points.

5
00:00:39,417 --> 00:00:49,174
Now we're going to move the line around and see if we can decrease this error. We move in this direction and we see that the error kind of increases so that's not the way to go.

6
00:00:49,174 --> 00:00:56,359
So we move in the other direction and see that the error decreased, so we pick this one and stay there.

7
00:00:56,359 --> 00:01:03,680
Now we'll repeat the process many times over and over every time descending the error a bit until we get to the perfect line.

8
00:01:03,679 --> 00:01:11,840
So to minimize this error, we're going to use something called gradient descent. So let me talk a bit about gradient descent, the way it works is we're standing here on top of a mountain.

9
00:01:11,840 --> 00:01:22,234
This is called Mt rainierror as it measures how big our error is, and wanted descend from this mountain in order to descend from this mountain we need to minimize our height.

10
00:01:22,234 --> 00:01:30,844
And on the left, we have a problem of fitting the line to the data, which we can do by minimizing the error or the distance from the line to the points.

11
00:01:30,844 --> 00:01:43,228
So descending from the mountain is equivalent to getting the line closer to the points. Now, if we wanted to descend from the mountain we would look at the directions where we can walk down and find the one that makes us descend the most.

12
00:01:43,227 --> 00:01:51,149
And let's say this is the direction, so we descend a bit in this direction, this is equivalent to getting the line a little bit closer to the points.

13
00:01:51,150 --> 00:01:56,734
So now our height is smaller because we're closer to the points since our distance to them is smaller.

14
00:01:56,734 --> 00:02:02,112
And again and again we look at what makes us descend the most from the mountain and let's say we get here.

15
00:02:02,112 --> 00:02:08,585
Now we're at a point where we're descended from the mountain and on the right we found the line that is very close to our points.

16
00:02:08,585 --> 00:02:20,640
Thus, we've solved our problem and that is gradient descent. In a more mathematical way what happens is the following, we have a plot and here's a plot in two dimensions allowing a reality that plot will be in higher dimensions.

17
00:02:20,639 --> 00:02:34,435
We have our weights on the x-axis and our error on the y-axis. And we have an error function that looks like this, we're standing over here and the way to descend is to actually take the derivative or gradient of the error function with respect to the weights.

18
00:02:34,435 --> 00:02:44,140
This gradient is going to point to a direction where the function increases the most. Therefore, the negative of this gradient is going to point down in the direction where the function decreases the most.

19
00:02:44,139 --> 00:02:56,500
So what we do is we take a step in the direction of the negative of that gradient, this means we are taking our weights wi and changing them to wi minus the derivative of the error with respect to wi.

20
00:02:56,500 --> 00:03:01,855
In real life we'll be multiplying this derivative by the learning rate since we want to make small steps.

21
00:03:01,854 --> 00:03:11,514
This means the error function is decreasing and we're closer to the minimum. If we do this several times we get to either a minimum or a pretty good value where the error is small.


@@@
1
00:00:00,000 --> 00:00:05,990
In the last video, we'll learned how to decrease an error function by walking along the negative of its gradient.

2
00:00:05,990 --> 00:00:15,885
Now in this video, we're going to learn formulas for these error functions. The two most common error functions for linear regression are the mean absolute error, and the mean squared error.

3
00:00:15,884 --> 00:00:25,370
First, we'll learn the mean absolute error. So, here's a point and a line, the point has coordinates x, y, and the line is called Y-hat since it's the prediction.

4
00:00:25,370 --> 00:00:32,479
So, our prediction for this point is going to be the point in the line with the same x coordinate as our point, that is the point x, y hat.

5
00:00:32,479 --> 00:00:40,399
This means the vertical distance from the point to the line is y minus y hat, and that's what we'll be calling the error.

6
00:00:40,399 --> 00:00:50,770
Notice that this is not the actual distance to the line since this would be a perpendicular segment, but it's the vertical distance from the point to the line which is the distance between the point and its prediction.

7
00:00:50,770 --> 00:00:56,015
Now, our total error is going to be the sum of all these distances for all the points in our dataset.

8
00:00:56,015 --> 00:01:06,349
And sometimes we'll use this as our error, but in this case we'll use the average or the mean absolute error, which is the sum of all the errors divided by m, the number of points in our dataset.

9
00:01:06,349 --> 00:01:24,175
Using the sum or the average won't change our algorithms, since that would only scale our error by a constant, namely m. Notice something here which is that we have an absolute value around the y minus y hat, the reason is that if the point is on top of the line, the distance is y minus y hat, but if it's under the line then it's y hat minus y.

10
00:01:24,174 --> 00:01:29,899
We want the error to always be positive, otherwise negative errors will cancel with positive errors.

11
00:01:29,900 --> 00:01:47,449
Therefore, we take the absolute value of y minus y hat. So, our mean absolute error is the average of all absolute errors or in other words, the sum of all these absolute values divided by the number of points, which is m. We're going to plot these error over here in a graph.

12
00:01:47,450 --> 00:01:54,254
Obviously as we mentioned before, the graph has more dimensions but this is a two-dimensional simplification of that graph.

13
00:01:54,254 --> 00:02:03,120
As we descend in this graph using gradient descent, we get a better and better line until we find the best possible fit with the smallest possible mean absolute error.


@@@
1
00:00:00,000 --> 00:00:05,869
Now in this video we'll learn the Mean Squared Error. The Mean Squared Error is very similar to the Mean Absolute Error.

2
00:00:05,870 --> 00:00:13,099
Again, here we have our point and our prediction, but now instead of taking the distance we're actually going to draw a square with this segment as its side.

3
00:00:13,099 --> 00:00:20,789
So the areas precisely y minus y hat squared. Notice that this is always non-negative, so we don't need to worry about absolute values.

4
00:00:20,789 --> 00:00:28,269
And our mean squared error is going to be the average of all these series of squares, and we're going to have this extra factor of one half for convenience later.

5
00:00:28,269 --> 00:00:40,615
So in summary the area's one half times the average of the sum of all y minus y hat squared. Again, we can take the sum and call it the total square error, but we take the average and this won't make a difference in the algorithm.

6
00:00:40,615 --> 00:00:50,364
Notice, if the point is over the line or underneath the line the square is always going to be a non-negative number, because the square of a real number is always going to be non-negative.

7
00:00:50,365 --> 00:00:55,410
The one half is going to be there for convenience because later we'll be taking the derivative of this error.

8
00:00:55,409 --> 00:01:03,215
Again, we can multiply this error by any constant and the process of minimizing it will be the exact same thing, so this one half does not affect anything.

9
00:01:03,215 --> 00:01:12,245
So here's a pictorial representation of the error. Here we have our points, our line, and the error is the average of the areas of all these squares.

10
00:01:12,245 --> 00:01:22,770
Here's our graph of the error. As we descend from this mountain we get to the place where the error is the smallest possible, and that's the same as minimizing the average of the areas of the squares.


@@@
1
00:00:00,000 --> 00:00:14,609
So far, we've learned two algorithms that will fit a line through a set of points. One is using any of the tricks namely the absolute and the square trick, and the other one is minimizing any of the error functions namely the mean absolute error and the mean squared error.

2
00:00:14,609 --> 00:00:26,535
The interesting thing is that these two are actually the exact same thing. What I'm saying is that when we minimize the mean absolute error, we're using a gradient descent step and that gradient descent step is the exact same thing as the absolute trick.

3
00:00:26,535 --> 00:00:32,399
Likewise, when we minimize the squared error, the gradient descent step is the exact same thing as the square trick.

4
00:00:32,399 --> 00:00:43,034
So let's see why, let's start with the mean squared error. Here's a point with coordinates (x, y) and here's our line with equation y-hat equals w_1 times x plus w_2.

5
00:00:43,034 --> 00:00:52,155
So y-hat is our prediction and the line predicts the y coordinate of this point that prediction gives us a point on the line that matches the x coordinate of the point (x,y).

6
00:00:52,155 --> 00:01:04,500
So it's the point (x,y-hat). Now, the error for this point is 1.5 times y minus y-hat squared, and the mean squared error for this set of points is the average of all these errors.

7
00:01:04,500 --> 00:01:19,430
But since average is a linear function, then whatever we do here applies to the entire error. Now, we know that the gradient descent step uses these two derivatives, namely the derivative with respect to the slope w_1 and the derivative with respect to the y-intercept w_2.

8
00:01:19,430 --> 00:01:35,975
If we calculate the derivatives, and you can see the calculation in detail in the instructor notes, we get negative times y minus y-hat times x for the one respect to the slope and negative times y minus y-hat for the one with respect to the y-intercept w_2.

9
00:01:35,974 --> 00:01:44,924
And notice that the length of this red segment is precisely (y minus y-hat) and the length of this green segment is precisely x.

10
00:01:44,924 --> 00:01:59,339
And if you remember correctly, the square trick told us that we have to upgrade the slope by adding y minus y-hat times x times the learning rate alpha, and upgrade the y-intercept by adding y minus y-hat times the learning rate alpha.

11
00:01:59,340 --> 00:02:11,344
But that is precisely what this gradient descent step is doing. If you like, feel free to pause the video or actually write it down in a little piece of paper to verify that is the exact same calculation.

12
00:02:11,344 --> 00:02:18,199
So this is why the gradient descent step utilize when we minimize the mean squared error is the same as the square trick.

13
00:02:18,199 --> 00:02:25,680
We can do the same thing with the absolute trick. The procedure is very similar except we have to be careful about the sign.

14
00:02:25,680 --> 00:02:39,504
This is our error, the absolute value of y minus y-hat and the derivatives of the error with respect to w_1 and w_2 are plus or minus x and plus or minus one based on the point is on top or underneath the line.

15
00:02:39,504 --> 00:02:47,344
Since the distance is x, then you can also check that this is precisely what the gradient descent step does when we minimize the mean absolute error.

16
00:02:47,344 --> 00:02:54,570
So that's it, that's why minimizing these errors with gradient descent is the exact same thing that using the absolute and the square tricks.


@@@
1
00:00:00,000 --> 00:00:07,280
So here's a question, what is better the mean absolute error or the mean squared error? Well, there's no real answer to this.

2
00:00:07,280 --> 00:00:12,074
Both are used for a lot of different purposes, but here's one property that actually tells them apart.

3
00:00:12,074 --> 00:00:27,789
So here's a set of data and we're going to try to fit it by minimizing the mean absolute error. So let's say we have line A, line B, and line C. And the question is, which one of these lines gives us a smaller mean absolute error?


@@@
1
00:00:00,000 --> 00:00:08,714
Well that was a tricky quiz. If you said the same, then you were correct because as you can see, moving this line up and down actually keeps the mean absolute error the same.

2
00:00:08,714 --> 00:00:19,899
You can convince yourself by looking at the picture and checking that as you move a line up and down, you're adding a segment to two of the errors and removing a segment of equal length to the other two errors.

3
00:00:19,899 --> 00:00:28,490
Now, let's repeat the quiz but with the mean squared error. The question is, which one of these three lines A, B, and C would give us a smaller mean squared error?


@@@
1
00:00:00,000 --> 00:00:13,484
So this time there was a solution and the solution is B and here's the reason which is more subtle. Our mean squared error is actually a quadratic function and quadratic functions have a minimum at the point in the middle.

2
00:00:13,484 --> 00:00:22,829
Over here, we can see that the error for line A would be around here, the error for line C would be around here, and the one for line B would be somewhere around here.

3
00:00:22,829 --> 00:00:36,240
So we can see that the minimum mean square error is given by line B. Here's a diagram of the errors and you can check with your eyes or even drawing a small example and calculating the areas; that line B would be giving you the smallest mean square error.


@@@
1
00:00:00,000 --> 00:00:07,084
So in the previous example, we had a one column input and one column output. The input was the size of the house and the output was the price.

2
00:00:07,084 --> 00:00:16,179
So we had a two-dimensional problem. Our prediction for the price would be a line and the equation would just be a constant times size plus another constant.

3
00:00:16,179 --> 00:00:25,304
What if we had more columns in the input, for example size and school quality? Well, now we have a three dimensional graph because we have two dimensions for the input and one for the output.

4
00:00:25,304 --> 00:00:38,969
So now our points don't live in the plane, but they look like points flying in 3-dimensional space. What we do here is we'll feed a plane through them instead of fitting a line, and our equation won't be a constant times one variable plus another constant.

5
00:00:38,969 --> 00:00:46,140
It's going to be a constant times school quality plus another constant times size plus a third constant.

6
00:00:46,140 --> 00:00:56,524
That's what happens when we're in three dimensions. So what happens if we're in n dimensions? So in this case we have n minus one columns in the input and one in the output.

7
00:00:56,524 --> 00:01:05,304
So, for example the inputs are size, school quality, number of rooms, et cetera. Well, now we have the same thing except our data lives in n-dimensional space.

8
00:01:05,305 --> 00:01:15,909
So for our input, we have n minus one variables namely; x_1, x_2 up to x_n minus one and for the output of the prediction, we only have one variable y hat.

9
00:01:15,909 --> 00:01:38,269
Our prediction would be an n minus one dimensional hyperplane living in n dimensions. Since it's hard to picture n-dimensions just think of a linear equation in n variables, such as y hat equals w1x1 plus w2x2 plus all the way to w_n minus one x_n minus one plus w_n and that's how we do predictions for higher dimensions.

10
00:01:38,269 --> 00:01:44,725
In order to find the weights w_1 up to w_n the algorithm is exactly the same thing for two variables.

11
00:01:44,724 --> 00:01:53,890
We can either do the absolute or square root tricks, or we can calculate the mean absolute or square errors, and minimize using gradient descent.


@@@
1
00:00:00,000 --> 00:00:07,230
So here's an interesting observation; in order to minimize the mean squared error, we do not actually need to use gradient descent or the tricks.

2
00:00:07,230 --> 00:00:16,489
We can actually do this in a closed mathematical form. Let me show you. Here's our data x_1, y_1 all the way to x_m, y_m; and in this case, m is five.

3
00:00:16,489 --> 00:00:34,115
And the areas of the squares represent our squared error. So our input is x_1 up to x_m and our labels are y_1 up to y_m, and our predictions are of the form y_i hat equals w_1 x_i plus w_2, where w_1 is a slope of the line and w_2 is the y-intercept.

4
00:00:34,115 --> 00:00:48,130
And the mean squared error is given by this formula over here. Notice that I've written the error as a function of w_1 and w_2, since given any w_1 and w_2 we can calculate the predictions and the error based on these values of w_1 and w_2.

5
00:00:48,130 --> 00:00:57,689
Now, as we know from calculus, in order to minimize this error, we need to take the derivatives with respect to the two input variables w_1 and w_2 and set them both equal to zero.

6
00:00:57,689 --> 00:01:03,440
We calculate the derivatives and you can see the full calculation in the instructor notes and we get these two formulas.

7
00:01:03,439 --> 00:01:10,179
Now, we just need to solve for w_1 and w_2 for these two equations to be zero. So what do we have now?

8
00:01:10,180 --> 00:01:19,850
We have a system of two equations and two unknowns, we can easily solve this using linear algebra. So now the question is, why don't we do this all the time?

9
00:01:19,849 --> 00:01:25,680
Why do we have to go through many gradient descent steps instead of just solving a system of equations and unknowns?

10
00:01:25,680 --> 00:01:44,019
Well, think about this. If you didn't have only two dimensions in the input but you had n, then you would have n equations with n unknowns, and solving a system of n equations with n unknowns is very expensive because if n is big, then at some point of our solution, we have to invert an n by n matrix.

11
00:01:44,019 --> 00:01:50,574
Inverting a huge matrix is something that takes a lot of time and a lot of computing power. So this is simply not feasible.

12
00:01:50,575 --> 00:02:01,984
So instead this is why we use gradient descent. It will not give us the exact answer necessarily but it will get us pretty close to the best answer which will give us a solution that fits our data pretty well.

13
00:02:01,984 --> 00:02:08,090
But if we had infinite computing power, we would just solve this system and solve linear regression in one step.


@@@
1
00:00:00,000 --> 00:00:04,735
So what happens if we have data that looks like this where a line won't really do a good job fitting in?

2
00:00:04,735 --> 00:00:12,150
Maybe would like to have a curve or some polynomial. Maybe something along the lines of 2x cubed minus 8x squared, et cetera.

3
00:00:12,150 --> 00:00:20,769
This can be solved using a very similar algorithm than linear regression. All we have to do is instead of considering lines, we consider higher degree polynomials.

4
00:00:20,769 --> 00:00:29,105
This would give us more weights to solve our problem. For example, this problem here we'll make a solve for four weights; w_1, w_2, w_3, w_4.

5
00:00:29,105 --> 00:00:41,094
But the algorithm is the same thing. We just take the mean absolute or squared error and take the derivative with respect to the four variables and use gradient descent to modify these four weights in order to minimize the error.


@@@
1
00:00:00,000 --> 00:00:07,790
The following concept is one that works both for regression and classification. So in this video, we'll explain it using a classification problem.

2
00:00:07,790 --> 00:00:18,115
But as you will see, all the arguments here work with regression algorithms as well. The concept is called regularization, it's a very useful technique to improve our models and make sure they don't over fit.

3
00:00:18,114 --> 00:00:25,039
So let's look at some data, here's the data, and let's make two copies of it, and let's look at two models that classify this data.

4
00:00:25,039 --> 00:00:31,399
The first one is a line and the second one is a higher degree polynomial curve. So, the question is which one is better?

5
00:00:31,399 --> 00:00:41,335
Well, they both have their pros and cons, right? The one on the left makes a couple of mistakes. As you can see, there is a red point and a blue point in the wrong sides but it is much simpler.

6
00:00:41,335 --> 00:00:49,780
The one on the right makes zero mistakes but it's actually a bit more complicated. So let's say we want the one on the left because the one on the right over fits and it just doesn't generalize well.

7
00:00:49,780 --> 00:00:55,920
So the problem is that when we train the model, the one on the right will appear more likely and the reason is the following.

8
00:00:55,920 --> 00:01:06,295
When we're training the model, the model takes an error and minimizes it. So, the model on the left has a small error since it own misclassifies two points but it's an error nonetheless.

9
00:01:06,295 --> 00:01:16,670
The model in the right has a really small error since it does not misclassifies any of the points. So, if we train a model to minimize error, it will build a boundary like the one on the right, not the one on the left.

10
00:01:16,670 --> 00:01:27,965
So the question is, how do we pick the one in the left? Well, here's an idea. Let's look at the equation and let's say the equation of the line on the left is something like 3x_1 plus 4x_2 plus five equals zero.

11
00:01:27,965 --> 00:01:35,834
The equation of the polynomial is something more complex with high degree terms like x_1 squared x_1 x_2 x_2 cube, et cetera.

12
00:01:35,834 --> 00:01:46,490
If we look at the equation in the left, it's much simpler than the equation on the right. In particular, there are less coefficients, only three, four, five whereas the right one has many more.

13
00:01:46,489 --> 00:01:56,170
So, if we find a way to in commend the error by some function of these numbers, that would be very helpful because in some way the complexity of the model will be added into the error.

14
00:01:56,170 --> 00:02:08,689
So a complex model will have a larger error and then a simple model. So, let's do that and I'll show you the details later but the idea is that we take this three and four and notice that we're forgetting about the constant term, and there's a reason for that.

15
00:02:08,689 --> 00:02:18,629
But if we take this three and four and say add them to the error, we get a slightly bigger error. But what if we take all these coefficients and add them to the error here, now we get a huge error.

16
00:02:18,629 --> 00:02:32,724
Now, we can see that the modeling the left is better because it has a smaller combined error, so again, what we did is we took the complexity in the model into account, when we calculated the error and in that way, a simpler model has an edge over the complicated model.

17
00:02:32,724 --> 00:02:42,605
Simpler models have a tendency to generalize better so, that's what we want. So now, let me be more detailed on how to take the complexity of a model and turn into part of the error.

18
00:02:42,604 --> 00:02:52,875
In summary, will take this highlighted coefficients and somehow add them to the error, this method is called L1 regularization and it's very simple, here's how it works.

19
00:02:52,875 --> 00:02:58,930
What L1 regularization does, it takes the coefficient and just adds the absolute values of them to the error.

20
00:02:58,930 --> 00:03:07,704
So in this case, we're adding absolute value of two which is two plus absolute value of minus two, which is two again, et cetera and we see that they add to 21.

21
00:03:07,705 --> 00:03:18,920
In the linear case, we see that we're adding is the absolute value of three and four which is seven, so a seven is much less than 21, we can see that the complicated model gives us a much higher error.

22
00:03:18,919 --> 00:03:30,830
That's L1 regularization and the one is attached to the absolute value. L2 regularization is very similar and what we do here is instead of adding the absolute values, we add the squares of the coefficients.

23
00:03:30,830 --> 00:03:36,935
So for the complicated case, we get two squared plus minus two squared, et cetera which gives us 85.

24
00:03:36,935 --> 00:03:43,145
For the linear case, we get three squared plus four squared which is 25, which is much smaller than 85.

25
00:03:43,145 --> 00:03:53,019
So again, we see that the complex model gets punished a lot more than the simple model. But now the question is, what if we punish the complicated model too little or what if we punish it too much?

26
00:03:53,020 --> 00:04:11,000
Maybe some models, like a model to send a rocket to the moon or a medical model, have very little room for error and we're okay with some complexity, or maybe other models like a video recommendation model, or model recommending potential friends on a social network have more room for experimenting and need to be simpler and faster to run a big data.

27
00:04:11,000 --> 00:04:18,720
So we're okay with having some error. So it seems that for every case, we have to tune how much we want to punish complexity in each model.

28
00:04:18,720 --> 00:04:27,845
This can be fixed with a parameter and this parameter is called lambda. What we do with lambda, is we multiply the complexity part of the error as follows.

29
00:04:27,845 --> 00:04:38,795
Let's look at the two models again and let's remember that the yellow part of the error comes from the misclassified points and the green part comes from the complexity of the model, namely the coefficients in the polynomial.

30
00:04:38,795 --> 00:04:45,845
Let's say, we have a small lambda, so we take the green error and multiplied by small lambda which gives us something small.

31
00:04:45,845 --> 00:04:51,910
Therefore, the right model still wins because the complexity part of the error is small and it won't swing the balance.

32
00:04:51,910 --> 00:04:57,895
But if we have a large value for lambda, then we're multiplying the complexity part of the error by a lot.

33
00:04:57,894 --> 00:05:08,964
Which punishes the complex model more and then the simple model wins. So in summary, this is what happens, if we have a large lambda then we're punishing complexity by a large amount and we're picking a simpler model.

34
00:05:08,964 --> 00:05:15,689
Whereas if we have a small lambda, then we're punishing complexity by a small amount, so we're okay with having more complex models.

35
00:05:15,689 --> 00:05:23,259
Now the question is, which one to use L1 or L2 regularization? So here's a cheat sheet with some benefits for each one.

36
00:05:23,259 --> 00:05:31,660
L1 regularization is actually computationally inefficient even though it seems simpler because it has no squares, but actually those absolute values are hard to differentiate.

37
00:05:31,660 --> 00:05:37,834
Whereas, an L2 regularization squares have very nice derivatives. So, these are easy to deal with computation.

38
00:05:37,834 --> 00:05:55,290
The only times where L1 regularization is faster than L2 regularization is when the data is sparse. So let's say if you have a thousand columns of data but only 10 are relevant and the rest are mostly zeros, then L1 is faster, L2 is better for non-sparse outputs which is when the data is more equally distributed among the columns.

39
00:05:55,290 --> 00:06:04,139
L1 has one huge benefit which is that, it gives us feature selection. So let's say, we have again, data in a thousand columns but really only 10 of them matters and the rest are mostly noise.

40
00:06:04,139 --> 00:06:12,715
So, L1 will detect this and will make the relevant columns into zeroes. L2 on the other hand won't do this and it just take the columns and treat them similarly.


@@@
1
00:00:00,000 --> 00:00:05,235
Well that was it. In this lesson, you have learned linear regression and some of its generalizations.

2
00:00:05,235 --> 00:00:16,880
You have also gone hands-on and implemented the gradient descent algorithm for linear regression. You are now ready for the upcoming project in which you'll be implementing a regression neural network to analyze real data. Great job.


@@@
1
00:00:00,000 --> 00:00:09,096
Hello again and welcome to the Naive Bayes section. Naive Bayes is a more probabilistic algorithm which is based on playing with the concept of conditional probability.

2
00:00:09,096 --> 00:00:17,655
This algorithm has great benefits such as being easy to implement and very fast to train. We'll be studying one of very interesting applications, natural language processing.


@@@
1
00:00:00,000 --> 00:00:07,740
We'll start with an example. Let's say we're in an office and there are two people, Alex and Brenda, and they're both there the same amount of time.

2
00:00:07,740 --> 00:00:14,830
When they were in the office and we see someone passing by really fast, we can't tell who it is, but we'd like to take a guess.

3
00:00:14,830 --> 00:00:28,064
So far, with all we know, all we can infer is that since they're both in the office the same amount of time, the probability of the person being Alex is 50 percent and the probability of the person being Brenda is also 50 percent.

4
00:00:28,065 --> 00:00:37,859
But now, let's try to use more information so we can make a better guess of who the person is. When we saw the person running by, we notice that they were wearing a red sweater.

5
00:00:37,859 --> 00:00:49,579
So, we'll use that piece of information. We've known Alex and Brenda for a while, and actually we've noticed that Alex wears a red sweater two days a week, and Brenda wears a red sweater three days a week.

6
00:00:49,579 --> 00:00:57,554
We don't know which days, but we are sure of this fact. Also, when we say week we mean workweek, so five days, although at the end this won't matter much.

7
00:00:57,554 --> 00:01:12,060
So now what we'll do, is we'll use this piece of information to help us make a better guess. First off, since Alex wears a red sweater less than Brenda, It's easy to imagine that it's a bit less likely that the person we saw is Alex than that it is Brenda.

8
00:01:12,060 --> 00:01:22,939
But exactly how likely? Well, let's say that if we saw a person pass by five times, it would make sense to think that two of this times it was Alex, since he wears a red sweater twice a week.

9
00:01:22,939 --> 00:01:33,369
And the other three times it was Brenda, since she wears a red sweater three times a week. Therefore, from here we can infer that the probabilities are 40 and 60.

10
00:01:33,370 --> 00:01:40,439
We've used the formation about the color of the sweater to obtain better probabilities about who was the person who passed by.

11
00:01:40,439 --> 00:01:53,640
This is Bayes' theorem and we'll learn in more in detail in the next few videos. The initial guess we had, the 50/50 guess, is called the prior, since it's all we could infer prior to the new information about the red sweater.

12
00:01:53,640 --> 00:02:01,399
The final guess we have, the 60/40 guess is called the posterior, since we've inferred it after the new information has arrived.


@@@
1
00:00:00,000 --> 00:00:06,334
In the last video, we saw an example of Bayes theorem. But here's the main idea fit and it's a very powerful theorem.

2
00:00:06,334 --> 00:00:18,434
What it does is it switches from what we know to what we infer. What we know in this case is the probability that Alex wears red and the probability that Brenda wears red.

3
00:00:18,434 --> 00:00:25,724
And what we infer is the opposite, is the probability that someone wearing red is Alex or that someone wearing red is Brenda.

4
00:00:25,725 --> 00:00:35,265
In other words, initially we know the probability of an event A. And to give us more information, we introduce a new event R which is related to A.


@@@
1
00:00:00,000 --> 00:00:07,810
Bayes Theorem can get a little more complex. Let's take a look at a small example and what we'll do here is we'll mess a bit with the prior probability.

2
00:00:07,809 --> 00:00:14,849
So again, we have Alex and Brenda in the office, and we saw someone pass by quickly and we don't know who the person is.

3
00:00:14,849 --> 00:00:21,195
So let's say we look more carefully at their schedules and we realized that Alex actually works from the office most of the time.

4
00:00:21,195 --> 00:00:28,630
He comes by three days a week. And Brenda travels a lot for work, so, she actually comes to the office only one day a week.

5
00:00:28,629 --> 00:00:37,609
So initially, without knowing anything about the red sweater, all we know is that it's three times more likely to see Alex than to see Brenda.

6
00:00:37,609 --> 00:00:53,680
Therefore our prior probabilities are 0.75 for Alex and 0.25 for Brenda. And let's say that we have this happening throughout all the weeks, but now we use our extra knowledge which is that the person we saw had a red sweater.

7
00:00:53,679 --> 00:01:01,329
The rule is still as before, as Alex wears red twice a week and Brenda wears red three times a week.

8
00:01:01,329 --> 00:01:14,395
So, naively we would think that the real probabilities are not exactly 0.75 or 0.25 because Brenda wears a red sweater more than Alex, so they should be a little closer to each other.

9
00:01:14,394 --> 00:01:27,959
Let's calculate them. So, we'll do the following, let's think of the columns as weeks instead. So, now for each five-day work week, Alex wears red twice and Brenda three times.

10
00:01:27,959 --> 00:01:35,375
So, we colored the days they wore red. Now, since we know the person wore red, we forget about the times that they didn't.

11
00:01:35,375 --> 00:01:50,569
So we have nine times someone wore red. Six of them are Alex and three of them are Brenda. Therefore, among nine times we saw someone wearing red, two-thirds of the times it with Alex and one third of the time it was Brenda.

12
00:01:50,569 --> 00:02:03,194
Thus, our posterior probabilities are two-thirds or 0.67 for Alex and one third or 0.33 for Brenda. So it looks like we did a little bit of magic.

13
00:02:03,194 --> 00:02:19,185
Let's do this again in a more mathematical way. We saw a person and initially all we know is that it's Alex with a 75% probability and Brenda with a 25% probability since Alex comes to the office three times a week and Brenda once a week.

14
00:02:19,185 --> 00:02:29,229
But now new information comes to light which is that the person is wearing a red sweater and the data says that Alex wears red two times a week.

15
00:02:29,229 --> 00:02:40,974
So now we look at Alex. What is the probability that he's wearing red? Since a work week has five days and the probability of him wearing red is two-fifths or 0.4.

16
00:02:40,974 --> 00:02:58,215
And the probability of him not wearing red is the complement, so 0.6. Same thing with Brenda, since she wears red three a week, then the probability of her wearing red today is 0.6 and the probability of her not wearing red is 0.4.

17
00:02:58,215 --> 00:03:08,034
Now, by the formula of conditional probability, the probability that these two will happen is the product of the two probabilities P of Alex, times P of red given Alex.

18
00:03:08,034 --> 00:03:16,814
Therefore, the probability of the person we saw is Alex and that they're wearing red is precisely 0.75 times 0.4.

19
00:03:16,814 --> 00:03:28,219
We multiply them and put the result here. We calculate the other probabilities in the same way, that probability of the person we saw is Alex and that he's not wearing red is 0.75 times 0.6.

20
00:03:28,219 --> 00:03:35,799
The probability of the person we saw is Brenda and that she's wearing red, is again the product of these probabilities, which is 0.25 times 0.6.

21
00:03:35,800 --> 00:03:42,159
And finally, the probability of the person we saw is Brenda and she's not wearing red is 0.25 times 0.4.

22
00:03:42,159 --> 00:03:50,125
And now here's where the Bayesian magic happens, are you ready? We have four possible scenarios and you can check that these four probabilities add to one.

23
00:03:50,125 --> 00:04:02,364
But we know one thing, that the person we saw was wearing red. Therefore, out of these four scenarios, only two are plausible, the two when the person is wearing red.

24
00:04:02,365 --> 00:04:15,155
So, we forget about the other two. Now, since our new universe consists of only these two scenarios, then the probability should be higher, but their ratio should still be the same with respect to each other.

25
00:04:15,155 --> 00:04:22,615
This means, we need to normalize them or equivalently, divide them by something so that they now add to one.

26
00:04:22,615 --> 00:04:40,839
The thing we should divide them by, is the sum of the two. So, our new probability of the person being Alex is the top one, namely, 0.75 times 0.4 divided by the sum of the two, namely, 0.75 times four, plus 0.25 zero times 0.6.

27
00:04:40,839 --> 00:04:52,379
This is precisely two-thirds or 0.67, and now we can see that the complement is the probability that the person is Brenda, which is one third or 0.33.

28
00:04:52,379 --> 00:05:00,154
If we take Brenda's probability and divide it by the sum of both probabilities we can see that we get one third as desired.


@@@
1
00:00:00,000 --> 00:00:06,959
So, let's look at a formal version of Bayes Theorem. Initially, we start with an event, and this event could be A or B.

2
00:00:06,960 --> 00:00:26,804
The probabilities for each are here, P of A, and P of B. Now, we observe a third event, and that event can either happen or not happen both for A and for B. R is going to help us find more exact probabilities for A and B in the following way.

3
00:00:26,804 --> 00:00:37,479
Let's say we can calculate the probability of R given A, and also, of R complement which is node R given A.

4
00:00:37,479 --> 00:00:52,064
And similarly for R given B, and R complement given B. Now, our set of scenarios are these four, R n A, R complement n A, R n B, and R complement n B.

5
00:00:52,064 --> 00:01:05,250
But since we know R occurred, then we know that the second and the fourth events are not possible. So, our new universe consists of the two events, R n A and R n B.

6
00:01:05,250 --> 00:01:15,885
We calculate the probability for A n R or equivalently A intersection R, and by the Law of Conditional Probability, this is P of A times B of R given A.

7
00:01:15,885 --> 00:01:29,684
Similarly, for B intersection R. Now, since these probabilities do not add to one, we just divide them both by their sum so that the new normalized probabilities now do add to one.

8
00:01:29,685 --> 00:01:43,530
Thus, we get the following formulas for P of A given R, and P of B given R. These are our new and improved probabilities for A and B after we know that R occurred.

9
00:01:43,530 --> 00:01:50,970
Again, P of A and P of B are called the prior probabilities which is, what we knew before we knew that R occurred.

10
00:01:50,969 --> 00:01:59,775
P of A given R and P of B given R, are posterior probabilities which is, what we inferred after we knew that R occurred.


@@@
1
00:00:00,000 --> 00:00:11,669
Now, let's look at an interesting application of Bayes Theorem. Let's say we're not feeling very well and we go to the doctor, the doctor says there's a terrible disease going on, I'll administer a test for you.

2
00:00:11,669 --> 00:00:27,984
Moreover, she says that the test has 99 percent accuracy. More specifically, she says that for every 100 patients that are sick, the test correctly diagnosis 99 of them and for every 100 patients that are healthy, the test correctly diagnosis 99 of them.

3
00:00:27,984 --> 00:00:40,585
If we want to be tactical, these are actually called sensitivity and specificity. Then while we're waiting for our test, we researched the Internet and find that on average, one of every 10,000 people suffers from the disease.

4
00:00:40,585 --> 00:00:47,259
The next day the doctor calls with terrible news. She says that we have tested positive for the disease, so now we're panicking.

5
00:00:47,259 --> 00:00:54,439
But before panicking, let's turn to math and actually calculate what is the probability of being sick. So here's a quiz.

6
00:00:54,439 --> 00:01:06,344
Given the two pieces of information that the test has 99 percent accuracy, and that one out of every 10,000 people have the disease, what do you think the probability is that we're sick?


@@@
1
00:00:00,000 --> 00:00:12,290
Well, let's see. Let's use Bayes theorem to calculate it. We'll use the following notation, S will stand for sick, H will stand for healthy, and the plus sign will stand for testing positive.

2
00:00:12,289 --> 00:00:24,365
So since one out of every 10,000 people are sick, we get that P of S is 0.0001. Similarly, P of H is 0.9999.

3
00:00:24,364 --> 00:00:41,100
Since the test has 99 percent accuracy, both for sick and for healthy patients, we see that P of plus, given S is 0.99, the probability that the sick patient will get correctly diagnosed.

4
00:00:41,100 --> 00:00:50,710
And that P of plus given H is 0.01, the probability that the healthy patient will get incorrectly diagnosed as sick.

5
00:00:50,710 --> 00:01:02,125
So plugging that into the new formula, we get the probability of being diagnosed as positive when you're sick is exactly 0.0098, which is less than 1 percent.

6
00:01:02,125 --> 00:01:09,760
Really? Less than 1 percent? When the test has 99 percent accuracy? That's strange, but I guess that's the answer to the quiz.

7
00:01:09,760 --> 00:01:19,754
So, less than 1 percent falls in this category of 0 to 20 percent. I'm still puzzled though, why less than 1 percent If the test is correct 99 percent of the time.

8
00:01:19,754 --> 00:01:28,715
Well, let's explore. Let's go back to the tree of possibilities. Let's say we start with 1 million patients, and they have two options, healthy and sick.

9
00:01:28,715 --> 00:01:40,330
Now, since 1 out of every 10,000 patients is sick, then from this group of 1 million patients, 100 will be sick and the remaining 999,900 will be healthy.

10
00:01:40,329 --> 00:01:50,950
Now let's remember that for every 100 patients, 99 get correctly diagnosed and one gets incorrectly diagnosed, this happens both for sick and for healthy patients.

11
00:01:50,950 --> 00:02:05,034
So, let's see how many of these patients will get diagnosed positively or negatively. Out of the 100 sick ones, 99 will be correctly diagnosed as positive and one will be incorrectly diagnosed as negative.

12
00:02:05,034 --> 00:02:20,155
Now, out of the healthy ones, 1 percent or 9,999 will be incorrectly diagnosed as positive and the remaining 99 percent or 989,901 will be correctly diagnosed as negative.

13
00:02:20,155 --> 00:02:27,175
Now let's really examine these four groups. The first group is the sick people who we will send for more test or treatment.

14
00:02:27,175 --> 00:02:36,790
The second is the unlucky sick people that will be sent home with no treatment. The third is a slightly confused healthy people who will be sent for more tests.

15
00:02:36,789 --> 00:02:42,754
And the fourth group or the majority is the people who are healthy and were correctly diagnosed healthy and sent home.

16
00:02:42,754 --> 00:02:54,699
But now, here's the thing, we know we tested positively, so we must be among one of these two groups, the sick people who tested positively or the healthy people who tested positively.

17
00:02:54,699 --> 00:03:07,359
One group is much larger, it has 9,999 people, whereas the other one has only 99 people. The probability that we're in this group is much larger than that we're in this group.

18
00:03:07,360 --> 00:03:30,925
As a matter of fact, the probability that we are in the small group is 99 divided by the sum, 99 plus 9,999, which is, you guessed it, 0.0098, which is smaller than 1 percent, this is the probability of being sick if you are diagnosed as positive.

19
00:03:30,925 --> 00:03:37,745
But why is the group of healthy people who tested positively so much larger than the group of sick people who tested positively?

20
00:03:37,745 --> 00:03:48,825
The reason is because, even though the test only fails 1 percent of the time, that 1 percent is much, much larger than the one out of 10,000 rate of sickness among the population.

21
00:03:48,824 --> 00:03:57,219
In other words, in a group of 10,000 healthy people, 1 percent or a 100 of them will get misdiagnosed as sick.

22
00:03:57,219 --> 00:04:12,425
On the other hand, in a group of 10,000 people, around one will be sick, this is much less. So if you know you've tested positively, you are still more likely to be among the 100 errors than among the ones sick.

23
00:04:12,425 --> 00:04:20,730
And how much more likely? Around 100 times, and that's why our probability of being sick while being diagnosed positively is around 1 percent.

24
00:04:20,730 --> 00:04:26,930
This phenomenon is called the False Positive, and it has been a nightmare for the medical world, the legal world and many others.

25
00:04:26,930 --> 00:04:34,229
Search False Positives on Google, and you'll see many cases in which people have been misdiagnosed, misjudged etc.


@@@
1
00:00:00,000 --> 00:00:06,644
Now the question is, how do we use this wonderful Bayes theorem to do machine learning. And the answer is repeatedly.

2
00:00:06,644 --> 00:00:14,189
Let's look at this example, a spam email classifier. So let's say, we have some data in the form of a bunch of emails.

3
00:00:14,189 --> 00:00:33,090
Some of them are spam and some of them are not spam, which we call ham. Spam are, "Win money now!" "Make cash easy!" et cetera. And the ham are, "How are you?" "There you are!" et cetera. And now, what we'll do is, a new email comes in say, "easy money" and we want to check if it's spam or ham.

4
00:00:33,090 --> 00:00:41,085
So, we take it word by word. Of course, we can be more effective if we took into account the order of the words, but for this classifier, we won't.

5
00:00:41,085 --> 00:00:55,140
It's surprising how good it can be even if it doesn't take into account the order of the words. So let's study the first word say, "easy." We can see that the word "easy" appears once among the three spam emails and once among the five ham emails.

6
00:00:55,140 --> 00:01:05,280
And the word "money" appears twice among the three spam emails and once among the five ham emails. So, let's start with calculating some preliminary probabilities as an exercise.

7
00:01:05,280 --> 00:01:13,530
Given the data we have, what is the probability of an email containing the word "easy" given that it is spam? Here are some options.

8
00:01:13,530 --> 00:01:23,890
And let's also calculate it for the other word. Again given our data, what's the probability of an email being spam given that it contains the word "money"?


@@@
1
00:00:00,000 --> 00:00:10,245
So let's see. We have three spam emails and one of them contains the word 'easy,' which means the probability of an email containing the word 'easy' given that it's spam is one-third.

2
00:00:10,244 --> 00:00:18,675
Since two out of the three three spam emails containing the word 'money,' then the probability of an email containing the word money given that it's spam is two-thirds.

3
00:00:18,675 --> 00:00:29,070
And similarly, since there are five ham emails and one of them contains the word 'easy,' then the probability of an email containing the word 'easy' given that it is ham is one-fifth.

4
00:00:29,070 --> 00:00:52,590
And same thing for the word 'money.' And the main gist of Bayesian learning is the following, we go from what's known, which is P of 'easy' given spam and P of 'money' given spam, to what's inferred, which is P of spam given that it contains the word 'easy,' which is one-half, since there are two emails containing the word 'easy' and only one of them is spam.

5
00:00:52,590 --> 00:01:03,869
And P of spam given that it contains the word 'money,' which is two-thirds since there are three emails containing the word 'money' and two of them are spam.


@@@
1
00:00:00,000 --> 00:00:15,955
So, let's do this calculation a bit more in detail. Since we have eight emails in total and three of them are spam and five of them are non-spam or ham, then our prior probabilities are three over eight for spam and five over eight for ham.

2
00:00:15,955 --> 00:00:25,005
So, onto calculate the posteriors. Say we have a spam email, since there are three of them and one contains the word easy and two don't.

3
00:00:25,004 --> 00:00:32,129
Then, the probability for containing the word easy is one-third, and for not containing it is two-thirds, if you're spam.

4
00:00:32,130 --> 00:00:42,915
And as we had calculated before, the probability of containing the easy if your ham is one-fifth and of not containing it if your ham is four-fifths.

5
00:00:42,914 --> 00:00:52,620
Now, by the rule of conditional probability, probability of the email spam containing the word easy is the product of these two, three over eight times one-third, which one-eighth.

6
00:00:52,619 --> 00:00:58,204
In a similar way, we calculate the probability of being spam and not containing the word easy, which is one-fourth.

7
00:00:58,204 --> 00:01:04,424
And probabilities of being ham containing the word easy is one-eighth, and not containing it is one-half.

8
00:01:04,424 --> 00:01:14,295
Now, this is where we apply Bayes' rule. We know that the email contains the word easy, so our entire universe consists of only these two cases: when the is spam or ham.

9
00:01:14,295 --> 00:01:22,935
Those two have the same probability, one-eighth of happening. So, once we normalize the probabilities, they both turn into 50 percent.

10
00:01:22,935 --> 00:01:33,114
Thus, our two posterior probabilities are 50 percent. For ham emails, we can do the same procedure. Our prior are three over eight and five over eight as before.

11
00:01:33,114 --> 00:01:43,015
Our probabilities of containing the word money and not containing it are two-thirds and one-third for the spam emails, and one-fifth and four-fifths for the ham emails.

12
00:01:43,015 --> 00:01:54,424
Our products of probabilities are then one-quarter, one-quarter, one-eighth, and one-half. But since the email contains the word money, then we only care about these two.

13
00:01:54,424 --> 00:02:07,379
Since one-fourth is twice as much as one-eighth, when we normalize them we get two-thirds or 66.7 percent for spam, and one-third or 33.3 percent for ham.


@@@
1
00:00:00,000 --> 00:00:06,765
Now, here's where the word naive comes in Naive Bayes. We're going to make a pretty naive assumption here.

2
00:00:06,765 --> 00:00:15,090
Let's look at the probability of two events happening together, so P of A and B. We can also read this a P of A intersection B.

3
00:00:15,090 --> 00:00:22,770
And we're going to say that this is the product of P of A and P of B. Now, this only happens when the two events are independent.

4
00:00:22,770 --> 00:00:34,050
If they're not, then this is not true. For example, if A is the event of it being hot outside and B is the event of it being cold outside, then they both have a positive probability.

5
00:00:34,050 --> 00:00:42,570
But now, what's the probability of both events happening at the same time? This will be zero, since it can't be hot and cold at the same time.

6
00:00:42,570 --> 00:00:48,540
So, this formula doesn't follow because the events of being hot and being cold are dependent on each other.

7
00:00:48,540 --> 00:00:59,675
But in a Naive Bayes, we will assume that our probabilities are independent. This, as we said, is a false and naive assumption, but in practice, it works very well and it makes our algorithm very fast.

8
00:00:59,675 --> 00:01:06,024
Another formula I will use is a formula for conditional probability. These are two ways of writing P of A intersection B.

9
00:01:06,024 --> 00:01:14,935
And this is the basis for our base theorem. But the trick we'll use here is to forget about P of B. And now, we don't have these being equal.

10
00:01:14,935 --> 00:01:27,738
But we have P of A given B to be proportional to P of B given A times P of A. This will work very well because in the practice, P of B will cancel out, so the fact that these two are proportional is very useful.

11
00:01:27,738 --> 00:01:34,125
And now, here's what we want. We have an email that contains the words easy and money, and we want to know if it is spam.

12
00:01:34,125 --> 00:01:41,020
So we want this, the probability of the email being spam given that it contains the words easy and money.

13
00:01:41,020 --> 00:01:54,555
We'll start by using a conditional probability rule that we just reviewed to write it as a product of the probability that the email contains the words easy and money given that it is spam, times the probability of the email being spam.

14
00:01:54,555 --> 00:02:05,235
In this formula, A represents being spam and B represents containing the words easy and money. Now, we are ready to use our naive assumption.

15
00:02:05,235 --> 00:02:11,775
This first factor over here is a probability of the email containing the words easy and money given that it is spam.

16
00:02:11,775 --> 00:02:21,455
We can write it as a probability of the email containing the word easy given that it is spam, times the probability of the email containing the word money given that it is spam.

17
00:02:21,455 --> 00:02:31,075
Again, huge naive assumptions as these may be dependent. It could be that containing the word easy actually makes it more likely that the email contains the word money. But that's okay.

18
00:02:31,075 --> 00:02:36,550
In many cases, this assumption won't affect the results and it will make our calculations much easier.

19
00:02:36,550 --> 00:02:46,575
And this is the heart of the Naive Bayes algorithm. And now, we do the same thing for hand emails. We have both probabilities written as a product of factors.

20
00:02:46,575 --> 00:03:00,940
But what are these factors? Well, we've calculated them before based on our data. The first one P of containing the word easy given that it is spam, is one-third since there are three spam emails and one of them contains the word easy.

21
00:03:00,940 --> 00:03:07,995
For P of containing money given spam, that's two-thirds since there are three spam emails and two of them contain the word money.

22
00:03:07,995 --> 00:03:14,410
And P of spam is very simple. It's three over eight since there are eight emails and only three of them are spam.

23
00:03:14,410 --> 00:03:27,390
We do a similar calculation for the bottom one and get one-fifth, one-fifth, and five over eight. Now we multiply them and we get that the probability of spam given that it contains the word easy and money is proportional to one over 12.

24
00:03:27,390 --> 00:03:36,210
And for ham, it's proportional to one over 40. Now remember that these values are not the actual probabilities, they are proportional to the actual probabilities.

25
00:03:36,210 --> 00:03:45,545
So what do we do to get the actual probabilities? Here's the magic. We know that an email has to be either spam or ham, so these two should add to one.

26
00:03:45,545 --> 00:03:55,155
So we need to normalize them, namely, multiply them both by the same factor so that they are still proportional to one over 12 and one over 40, but they add to one.

27
00:03:55,155 --> 00:04:04,445
Let's try that in a quiz. Can you find two numbers that add to one and that they are in the same proportion to each other as one over 12 and one over 40?


@@@
1
00:00:00,000 --> 00:00:07,305
So the way to do this is to actually divide each one by the sum of both. This will make sure that they add to one.

2
00:00:07,305 --> 00:00:16,245
For the first one, we have one over 12 divided by one over 12 plus one over 40, which is 10 divided by 13.

3
00:00:16,245 --> 00:00:24,270
And for the second one, we have one over 40 divided by one over 12 plus one over 40, which is three over 13. So there we go.

4
00:00:24,270 --> 00:00:33,245
The answers are 10 over 13 for spam and three over 13 for ham. So, for this particular email, we conclude that it is very likely to be spam.

5
00:00:33,245 --> 00:00:39,310
Now, what happens in general? Well, let's say we have a bunch of words that we use as features to tell if the email is spam or not.

6
00:00:39,310 --> 00:00:55,000
Say, easy, money,cheap, et cetera. Our first step is to flip the event and the conditional to get this, then we make the naive assumption to split this into a product of simple factors that we can quickly calculate by looking at our data.

7
00:00:55,000 --> 00:01:05,515
We do this both for spam and ham, and we get some values that don't add to one. As a final step, we normalize to get our final probabilities of our email being spam or ham.


@@@
1
00:00:00,240 --> 00:00:13,210
So Katie, this is going to be a unit on unsupervised learning. &gt;&gt; Unsupervised learning is something that's very important, because most of the time, the data that you get in the real world doesn't have little flags attached that tell you the correct answer.

2
00:00:13,210 --> 00:00:20,620
So what are you to do as a machine learner in that case? You turn to unsupervised techniques to still figure something out about that data.

3
00:00:20,620 --> 00:00:27,930
&gt;&gt; Okay, let's talk about them. Given a dataset without labels over all the data points are of the same class.

4
00:00:27,930 --> 00:00:41,190
There are sometimes still things you can do to extract useful information. Like this dataset over here, where I would say this dataset is structured in a way that is useful to recognize for a machine learning algorithm.

5
00:00:41,190 --> 00:00:54,730
&gt;&gt; When we look at this by eye, it looks like there's clumps or clusters in the data. And if we could identify those clumps or clusters, we could maybe say something about a new, unknown data point and what its neighbors might be like.

6
00:00:54,730 --> 00:01:01,050
&gt;&gt; Or here's a second example of data. Maybe the data looks just like this. There's something we can say here as well.

7
00:01:01,050 --> 00:01:08,960
&gt;&gt; Right. So all the data in this example looks like it lives on some kind of line or some complicated shape that you seem to be drawing in there right now.

8
00:01:08,960 --> 00:01:16,200
&gt;&gt; Yeah. And it's, it's, it's used to be a two-dimensional space, with x and y over here. But some of it we can reduce it to a one-dimensional line.

9
00:01:16,200 --> 00:01:21,089
So that's called what? &gt;&gt; That's called dimensionality reduction, usually. &gt;&gt; Dimensionality reduction.

10
00:01:21,089 --> 00:01:25,830
So we learned about, a little bit about clustering. &gt;&gt; Clustering is what we'll learn in this lesson.

11
00:01:25,830 --> 00:01:31,890
&gt;&gt; And you can see here an example of something also called unsupervised learning of dimensionality reduction.

12
00:01:31,890 --> 00:01:39,938
&gt;&gt; Which we will get in a future lesson. &gt;&gt; So these kind of things where you find structure in the data without labels, they're called unsupervised learning.

13
00:01:39,938 --> 00:01:46,100
And we're now gong to dive into the wonderful, wonderful magical land of unsupervised learning. &gt;&gt; Sounds great, let's dive in.


@@@
1
00:00:00,370 --> 00:00:08,900
So here's an example that should make it intuitively clear the clustering sometimes make sense. So take Katie and me, we both have a movie collection at home.

2
00:00:08,900 --> 00:00:20,034
And just imagine that both of us look at each other's movies, and all movies, and Katie gets to rank them from really, really bad to great.

3
00:00:20,034 --> 00:00:28,600
And I to get to rank the same movies from bad to great. Now it so turns out that Katie and I have very different tastes.

4
00:00:28,600 --> 00:00:40,680
Maybe some movies that I love, like all my James Bond movies, but Katie doesn't like as much. And there's others or these chick flicks, that Katie loves, and I don't.

5
00:00:40,680 --> 00:00:49,270
So somewhat exaggerated. It mind end up, that our movies fall into different classes, depending on who likes which movies.

6
00:00:49,270 --> 00:00:57,010
So say, say you're Netflix, and and you look at both my queue and Katie's queue, and you graph it like that.

7
00:00:57,010 --> 00:01:07,270
Then you can conclude, wow, there's two different classes of movies. Without knowing anything else about movies, you would say, here's class A and class B.

8
00:01:07,270 --> 00:01:19,080
And they're very different in characteristics. And the reason why Netflix might want to know this is next time Katie comes in, you want to kind of propose a movie that fits into class B and on to class A.

9
00:01:19,080 --> 00:01:24,700
Otherwise, she's very unlikely to watch this movie. And conversely, for me, you want a reach into class A versus class B.

10
00:01:24,700 --> 00:01:30,440
In fact, if you were to look at those movies, you might find that old style westerns are right over here.

11
00:01:30,440 --> 00:01:37,360
And modern chick flicks might be sitting over here. Who knows? But that's an example of clustering. Because here, there's no target labels given.

12
00:01:37,360 --> 00:01:48,660
Then they move down for you if class A and B existed. But after looking at the data, you could, through clustering, deduce as two different classes, and you could even look at the movie titles to understand what these classes are all about.


@@@
1
00:00:00,230 --> 00:00:11,690
The perhaps the most basic algorithm for clustering, and by far the most used is called K-MEANS. And I'm going to work with you through the algorithm with many, many quizzes for you.

2
00:00:11,690 --> 00:00:21,430
Here is our data space. And suppose we are given this type of data. The first question is intuitively, how many clusters do you see?


@@@
1
00:00:00,270 --> 00:00:10,670
Okay, and I would argue it's 2. There's a cluster over here. And a cluster over here. And the cluster centers respectively lie right over here and somewhere over here.


@@@
1
00:00:00,320 --> 00:00:08,600
In k-means, you randomly draw cluster centers and say our first initial guess is, say, over here and over here.

2
00:00:08,600 --> 00:00:13,970
These are obviously not the correct cluster centers. You're not done yet. But k-means now operates in two steps.

3
00:00:13,970 --> 00:00:30,030
Step number is assign and step number two is optimize. So let's talk about the assignment. For classes in number one, I want you to click on exactly those of the red points that you believe are closer to center one than center two.


@@@
1
00:00:00,280 --> 00:00:22,800
And the answer is this guy is closer these guys over her are closer. And the way to see this is you can make a line between the cluster centers and then draw an equidistant and orthogonal line and that line separate the space into a half space that's closer to center number one, which is the one over here, and a half space that's closer to center number two, which is the space over here.


@@@
1
00:00:00,430 --> 00:00:08,630
So now we know that these four points correspond to the present class center one that was randomly chosen.

2
00:00:08,630 --> 00:00:15,200
And these three points over here correspond to class center in the middle. That's the assignment step.

3
00:00:15,200 --> 00:00:24,840
obviously that's not good enough. Now we have to optimize. And what we are optimizing is, you are minimizing the total quadratic distance.

4
00:00:24,840 --> 00:00:38,490
Of our cluster center to the points. We're now free to move our cluster center. I think of these little blue lines over here as rubber bands, and that we're trying to find the state of minimum energy for the rubber bands, where the total quadratic error is minimized.

5
00:00:38,490 --> 00:00:44,990
And for the top one, I'm going to give you three positions. They're all approximate, but pick the one that looks best in terms of minimization.

6
00:00:44,990 --> 00:00:52,800
And that's a not a trivial question at all. So one could be right over here, one could be at and one could be right over here.

7
00:00:52,800 --> 00:01:00,080
Which one do you think best minimize, or is minimize of the three positions, the total quadratic length of these rubber bands?


@@@
1
00:00:00,640 --> 00:00:07,550
So after the assign step, you can now see these little blue lines over here that marry data points to cluster centers.

2
00:00:07,550 --> 00:00:13,460
And now we're going to think of what this rubber bands. They're rubber bands that like to be as short as possible.

3
00:00:13,460 --> 00:00:22,320
In the optimize step, we're not allowed, now allowed to move the green cluster center to a point where the total rubber band is minimized.


@@@
1
00:00:00,560 --> 00:00:10,400
Same exercise for the optimization step for the center below. I give you a couple of hypotheses, four in total.


@@@
1
00:00:00,370 --> 00:00:09,880
And then argue this is the one that minimizes it. In fact, a center somewhere over here minimizes the total rubber bands in the bottom case.


@@@

@@@
1
00:00:00,079 --> 00:00:16,059
And this example is now easy. You can see that all the four over here fit with the green one. In fact, the separating line will be somewhere here and, in fact, after the next iteration of optimize, with the assignment of those point, four points of this cluster center and those three points of this cluster center.

2
00:00:16,059 --> 00:00:24,950
You can see that this cluster center will move straight into the center of those four points. And this cluster center will move to the center of those three points.

3
00:00:24,950 --> 00:00:41,890
Have we truly achieved our result? We now have assumed it's two clusters. But our algorithm of iteratively assigning and optimizing has moved the cluster center straight into what we would argue is actually the correct centroid for those two clusters over here.


@@@
1
00:00:00,230 --> 00:00:07,689
So you learn about k-means. Katie is going to give you one more example of how to apply k-means in practice.


@@@
1
00:00:00,280 --> 00:00:09,350
Now I want to show you a visualization tool that I found online that I think does a really great job of helping you see what k-means clustering does.

2
00:00:09,350 --> 00:00:19,890
And that should give you a good intuition for how it works. So I'd like to give a special shout out to Naftali Harris, who wrote this visualization and very kindly agreed to let us use it.

3
00:00:19,890 --> 00:00:24,640
I'll put a link to this website in the instructor notes that you can go and play around with it on your own.

4
00:00:24,640 --> 00:00:32,530
So it starts out by asking me how to pick the initial centroids of my clusters. I'll start out with Randomly right now.

5
00:00:32,530 --> 00:00:37,500
What kind of data would I like to use? There are a number of different things here, and I encourage you to play around with them.

6
00:00:37,500 --> 00:00:43,490
A Gaussian Mixture has been really similar to one of the simple examples we've done so far. So Gaussian mixture data looks like this.

7
00:00:43,490 --> 00:00:49,470
These are all the points that we have to classify. The first question for you is, how many centroids do you think is the correct number of centroids on this data?


@@@
1
00:00:00,490 --> 00:00:07,788
And I hope you said three, it's pretty obvious that there should be three centroids here. So let's add three, one, two, three.

2
00:00:07,788 --> 00:00:13,460
So they're all starting out right next to each other, but we'll see how as the algorithm progresses, they end up in the right place.


@@@
1
00:00:00,380 --> 00:00:10,790
One of the things that's immediately apparent once I start assigning my centroids, with these colored regions, is how all the points are going to be associated with one of the centroids, with one of the clusters.

2
00:00:10,790 --> 00:00:19,760
So you can see that the blue is probably already in reasonably good shape. I would say that we got a little bit lucky in where the, the initial centroid was placed.

3
00:00:19,760 --> 00:00:28,680
It looks like it's pretty close to the, the center of this blob of data. With the red and the green it looks like they're sitting kind of right on top of each other in the same cluster.

4
00:00:28,680 --> 00:00:34,690
So, let's watch as K-means starts to sort out this situation and get all the clusters properly allocated.

5
00:00:34,690 --> 00:00:41,720
So, I hit Go. The first thing that it does is it tells me explicitly which cluster each one of these points will fall into.

6
00:00:41,720 --> 00:00:47,150
So you see, we have a few blue that fall into the wrong cluster over here. And then, of course, the red and the green.

7
00:00:47,150 --> 00:00:55,480
So this is the association step is all the points are being associated with the nearest centroid. And then the next thing that I'll do is I'm going to update the centroid.

8
00:00:55,480 --> 00:01:08,850
So now, this is going to move the centroids to the, the mean of all of the associated points. So in particular, I, I expect this green point to be pulled over to the right by the fact that we have so many points over here.

9
00:01:08,850 --> 00:01:15,920
So let's update. Now this is starting to look much better. If we were to just leave everything as is, you can see how the clustering was before.

10
00:01:15,920 --> 00:01:22,090
So now all these points that use to be green are now about to become red. And likewise with a few blue points over here.

11
00:01:22,090 --> 00:01:29,460
You can see how even just in one step from this bad initial condition, we've already started to capture the structure in the data pretty well.

12
00:01:29,460 --> 00:01:34,590
So I'm going to reassign the points. Iterate through this again to reassign each point to the nearest centroid.

13
00:01:34,590 --> 00:01:45,840
And now things are starting to look very, very consistent. There's probably just one, one or two more iterations before we have the centroid's right at the middle of the clusters so I update and reassign points.

14
00:01:45,840 --> 00:01:51,070
No points have changed so this is the final clustering that would be assigned by k-means clustering.

15
00:01:51,070 --> 00:01:57,410
So in three or four steps, using this algorithm, I assigned every point to a cluster and it worked in a really beautiful way for this example.


@@@
1
00:00:00,100 --> 00:00:06,580
Now I'm going to show you another set of data that won't work out quite so perfectly, but you can see how k-means clustering is still.

2
00:00:06,580 --> 00:00:11,690
And the type of data that I'll use in this example is uniform points. This is what uniform points look like.

3
00:00:11,690 --> 00:00:24,700
It's just scattered everywhere. So I wouldn't look at this and say there's clear clusters in here that I want to pick out, but I might still want to be able to describe that, say, these points over here are all more similar to each other than these points over there.

4
00:00:24,700 --> 00:00:29,710
And k-means clustering could be one way of mathematically describing that, that fact about the data.

5
00:00:29,710 --> 00:00:37,200
So I don't a priori have a number of centroids that I know I want to use here, so I'll use two. Seems like a reasonable number.

6
00:00:37,200 --> 00:00:42,740
One, two. And then let's see what happens in this case. Few points are going to be reassigned. Move the centroids.

7
00:00:42,740 --> 00:00:52,560
If you can see that there's a few more little adjustments here. But in the end, it basically just ends up splitting the data along this axis.

8
00:00:52,560 --> 00:01:02,080
If I try this again, depending on the exact initial conditions that I have and the exact details of how these points are allocated, I can come up with something that looks a little bit different.

9
00:01:02,080 --> 00:01:13,360
So you can see here that I ended up splitting the data vertically rather than horizontally. And the way you should think about this is the initial placement of the centroids is usually pretty random and very important.

10
00:01:13,360 --> 00:01:20,770
And so depending on what exactly the initial conditions are, you can get clustering in the end that looks totally different.

11
00:01:20,770 --> 00:01:27,020
Now, this might seem like a big problem, but there is one pretty powerful way to solve it. So let's talk about that.


@@@
1
00:00:00,160 --> 00:00:08,780
Now that I've explained the theory of k-means clustering to you, I'm going to show you how to use the scikit-learn implementation to deploy it in your own studies.

2
00:00:08,780 --> 00:00:20,020
So I start over here at Google, and I find that there's a whole page on clustering in scikit-learn. The first thing that I notice when I get to this page is that there are many types of clustering, besides just k-means clustering.

3
00:00:20,020 --> 00:00:32,509
So all of these different columns right here are different types of clustering. We won't go into all of these, instead I want to use this page to navigate to the k-means documentation that you can get a little bit of a better idea of how this is handled in scikit-learn.

4
00:00:32,509 --> 00:00:41,350
So here's a list of all of the different clustering methods that I have. And here the first item on the list we see is k-means, and some summary information about the algorithm.

5
00:00:41,350 --> 00:00:55,870
And so one of the parameters that you have to define for k-means is the number of clusters. Remember, we had to say at the outset how many clusters we want to look for and this is one of the things that can be most challenging actually about using k-means is deciding how many clusters you want to try.

6
00:00:57,170 --> 00:01:04,928
Then he gives us some information about the scalability, which basically tells us how the algorithm performs as you start to have lots and lots of data, or lots of clusters.

7
00:01:04,928 --> 00:01:13,510
A use case, which gives us a little bit of information that this is good for general purpose when you have clusters that have even number of points in them and so on.

8
00:01:13,510 --> 00:01:21,720
And, last, that the way that k-means clustering works is based on the distances between the points. So, very consistent with what we've seen so far.

9
00:01:21,720 --> 00:01:29,470
Let's dig in a little bit deeper. Now we're at the k-means documentation page. And there are three parameters in particular that I want to call your attention do.

10
00:01:29,470 --> 00:01:42,790
First and most important one is n_clusters. The default value for n_clusters is eight. But of course we know that the number of clusters in the algorithm is something that you need to set on your own based on what you think makes sense.

11
00:01:42,790 --> 00:01:50,910
This might even be a parameter that you play around with. So you should always be thinking about whether you actually want to use this default value, or if you want to change it to something else.

12
00:01:50,910 --> 00:01:57,990
I can guarantee you that you're mostly going to want to change it to something else. The second parameter that I want to call your attention to is max_iter=300.

13
00:01:57,990 --> 00:02:10,500
Remember that when we're running k-means clustering we have an iteration that we go through as we're finding the clusters, where we assign each point to a centroid and then we move the centroid.

14
00:02:10,500 --> 00:02:20,078
Then we assign the points again. We move the centroids again. And each one of those assign and move, assign and move steps is an iteration of the algorithm.

15
00:02:20,078 --> 00:02:29,440
And so max_iter actually says how many iterations of the algorithm do you want it to go through. 300 will usually be a very reasonable value for you.

16
00:02:29,440 --> 00:02:35,700
In fact most of the time I would guess that it's going to terminate before it gets to this maximum number.

17
00:02:35,700 --> 00:02:42,520
But if you want to have a finer level of control over the algorithm and how many times it goes through that iteration process this is the parameter that you want.

18
00:02:42,520 --> 00:02:50,240
And then the last parameter that I'll mention, another one that's very important. Is the number of different initializations that you give it.

19
00:02:50,240 --> 00:02:58,220
Remember we said that k-means clustering has this challenge, that depending on exactly what the initial conditions are, you can sometimes end up with different clusterings.

20
00:02:58,220 --> 00:03:08,850
And so then you want to repeat the algorithm several times so that any one of those clusterings might be wrong, but in general, the ensemble of all the clusterings will give you something that makes sense.

21
00:03:08,850 --> 00:03:16,320
That's what this parameter controls. It's basically how many times does it initialize the algorithm, how many times does it come up with clusters.

22
00:03:16,320 --> 00:03:29,910
You can see that by default it goes through at ten times. If you think for some reason that your clustering might be particularly prone to bad initializations or challenging initializations, then this is the parameter that you want to change.

23
00:03:29,910 --> 00:03:39,580
Maybe bump the number of initializations up to a higher number. But again, just to reiterate, of all those parameters, number of clusters is definitely the one that's most important.


@@@
1
00:00:00,360 --> 00:00:09,290
Now this wraps up what we're going to talk about in terms of the k-means algorithm. What I'll have you do is practice much more in the coding aspects of this in the mini project.

2
00:00:09,290 --> 00:00:17,160
But before we do that, here are few thoughts on things that k-means is very valuable for and a few places where you need to be careful if you're going to try to use it.


@@@
1
00:00:00,250 --> 00:00:11,760
So now we look at the limits of what k-means can or cannot do, and you're going to try to break it. And specifically, talk about local minima and to do this, I want to ask you a question that you can think about and see if you get the answer right.

2
00:00:11,760 --> 00:00:22,680
Suppose you use a fixed number of cluster centers, two or three or four. Will the output for any fixed training set, always be the same?

3
00:00:22,680 --> 00:00:29,610
So given a fixed data set, given a fixed number of cluster centers, when you run k-means will you always arrive at the same result?


@@@
1
00:00:00,250 --> 00:00:09,400
And the answer is no, as I will illustrate to you. K-means is what's called a hill climbing algorithm, and as a result it's very dependent on where you put your initial cluster centers.


@@@
1
00:00:00,380 --> 00:00:09,770
So let's make another data set. In this case, you're going to pick three cluster centers and, then, conveniently, we'll draw three clusters onto my diagram.

2
00:00:09,770 --> 00:00:14,230
Obviously, for three cluster centers, you want a cluster to be here, right here, and right over here.

3
00:00:15,300 --> 00:00:26,370
So my question is, is it possible that all these data points over here are represented by one cluster, and these guys over here by two separate clusters.

4
00:00:26,370 --> 00:00:37,200
Given what you know about k-means, do you think it can happen that all these points here fall into one cluster and those two fall into two clusters as one what's called a local minimum for clustering.


@@@
1
00:00:00,240 --> 00:00:09,110
And the answer is positive, and I prove it to you. Suppose you put one cluster center right between those two points over here and the other two somewhere in here.

2
00:00:09,110 --> 00:00:20,330
It doesn't even have an error. In your assignment step, you will find that pretty much everything left of this line would be allocated to the left cluster center.

3
00:00:20,330 --> 00:00:27,410
And as a result, this is the point where the total rubber band distance is minimized. So this cluster is very stable.

4
00:00:27,410 --> 00:00:39,920
These two guys over here, however, separate between themselves the data on the right. And they will fight for the same data points and end up somewhere partitioning the cloud on the right side.

5
00:00:39,920 --> 00:00:49,380
And that is a stable solution because in the assignment step, nothing changes. This guy will still correspond to all the guys over here, and these guys will correspond to the guys over here.

6
00:00:49,380 --> 00:01:00,600
That's called a local minimum. And it really depends on the initialization of the cluster centers. If you had chosen these three cluster centers as your initial guesses, you would never move away from it.

7
00:01:00,600 --> 00:01:07,200
Thus, make sure it's really important in clustering to be aware of the fact it's a local hill climbing algorithm.

8
00:01:07,200 --> 00:01:11,520
And it can give you suboptimal solutions that, if you divide them again, it gives you a better solution.

9
00:01:11,520 --> 00:01:18,250
Obviously, in this case with three cluster centers, you want them over here, over here and just one on the right side over here.


@@@
1
00:00:00,370 --> 00:00:12,750
Let me give another example and ask you a quiz. Suppose we have data just like this over here. Do you think there could be a local minimum if you initialize this data set with two cluster centers?

2
00:00:13,840 --> 00:00:19,660
Is there a stable solution where which the two cluster would not end up one over here and one over here?


@@@
1
00:00:00,270 --> 00:00:10,180
And I would say the answer's yes. You could make it so that the cluster centers sit right on top of each other, and the separation line looks like this.

2
00:00:10,180 --> 00:00:17,370
And all the top points are associated to the top cluster center, and all the bottom points are associated to the bottom cluster center.

3
00:00:17,370 --> 00:00:26,600
Granted, it's unlikely, to have init-, ,initialization like this, but if it happens, then the algorithm would believe this is one cluster.

4
00:00:26,600 --> 00:00:33,120
And this is another cluster. If we re-run it and you initialize differently. Say one of the cluster centers sits over here.

5
00:00:34,180 --> 00:00:44,759
Then a separation line will fall like that. And the classes would automatically resolve themselves. It's unlikely, but there exists a bad local minimum, even in the example I showed you over here.

6
00:00:46,220 --> 00:00:56,170
Now as a rule of thumb, the more cluster centers you have, the more local minima you find. But they exist, as a result, you are forced to run the algorithm multiple times.


@@@
1
00:00:00,000 --> 00:00:08,015
Hello, and welcome to the Decision Trees section. Let me introduce the concept of decision trees by playing this fun game on the Internet. It's called the Akinator.

2
00:00:08,015 --> 00:00:14,879
And the way it works is the genie will ask you questions about some character, and based on these questions, it'll guess who it is.

3
00:00:14,880 --> 00:00:20,719
The questions, as you can see, will get more and more educated as the genie narrowed down who the person is.

4
00:00:20,719 --> 00:00:29,850
This is the exact same thing decisions trees do. They ask you questions and questions about the data until they narrow the information down well enough to make a prediction.

5
00:00:29,850 --> 00:00:35,234
So let's play it, and I'll choose one of my favorite character from history, the great mathematician Hypatia.

6
00:00:35,234 --> 00:00:43,080
And here we have her Wikipedia page to help us out, so let's start answering questions. First question, is your character an adult man?

7
00:00:43,079 --> 00:00:49,740
Nope. Is your character older than 18? Yes, way older than 18. Has your character ever been pregnant?

8
00:00:49,740 --> 00:01:00,225
Well, it says there were no kids known, so I'm going to go for no. Is your character a YouTuber? Well, she would have been a wonderful Youtuber, but she was born way before Youtube was a thing, so no.

9
00:01:00,225 --> 00:01:08,015
Has your character recorded any albums? Again, no. Does your character have a cell phone? Nope, way before cell phones.

10
00:01:08,015 --> 00:01:14,480
Has your character really existed? Of course. Is your character a citizen of the United States? Nope, she was Egyptian.

11
00:01:14,480 --> 00:01:20,254
Has your character ever been married? Well, Wikipedia is not sure, so I'm going to go for a 'don't know'.

12
00:01:20,254 --> 00:01:27,309
Has your character been dead for more than a hundred years? Yes, way more than a hundred years. Is your character in the Bible?

13
00:01:27,310 --> 00:01:36,025
No. Is your character an orphan? I don't think so. Her dad was known, so no. Is your character European?

14
00:01:36,025 --> 00:01:45,250
Well, she was in the Roman Empire but she was born in Egypt, so I'm going to go for no. Is your character from Eastern Europe? Again, no.

15
00:01:45,250 --> 00:01:53,500
Oh, we're getting there. Is your character Egyptian? Yes. Is your character obsessed with waffles? No. Is your character a woman?

16
00:01:53,500 --> 00:02:03,310
Yes. Did your character know Cleopatra? Well, they didn't live in the same period of time, so no. Was your character a Pharaoh? No, she wasn't.

17
00:02:03,310 --> 00:02:11,544
Does your character live in Utah? No. Is your character a queen? No, she was a great mathematician but not a queen.

18
00:02:11,544 --> 00:02:20,109
Was your character a murderer? Yes, she was brutally murdered. Is your character a princess? No. Can your character cast spells?

19
00:02:20,110 --> 00:02:28,949
No. Is your character bad? No, she was very good. Oh, and the genie did it. He found the Hypatia of Alexandria. Good job, genie.

20
00:02:28,949 --> 00:02:36,000
The way decision trees work is very similar to these examples. So, let's dive deeper into them and learn how they work and how they get built.


@@@
1
00:00:00,000 --> 00:00:07,570
So, let's start with an example. Let's say we're in charge of writing the recommendation engine for the App Store or for Google play.

2
00:00:07,570 --> 00:00:13,870
Our task is to recommend to people the app they're most likely to download, and we should do this based on previous data.

3
00:00:13,869 --> 00:00:24,070
Our previous data is this table with six people each in a row, and the columns are their gender, male or female, their occupation, work or study, and the app they downloaded.

4
00:00:24,070 --> 00:00:32,325
The options for the app are Pokemon Go, WhatsApp, and Snapchat. So, the model we'll create will take the first two columns and guess the third one.

5
00:00:32,325 --> 00:00:46,530
So let's start with some small quizzes to test our intuition with this data. The first quiz is the following: if we have a woman who works at an office, what app should we recommend to her, Pokemon Go, WhatsApp, or Snapchat?

6
00:00:46,530 --> 00:00:52,064
The second quiz is the following: if we have a man who works at a factory, what app should we recommend to him?

7
00:00:52,064 --> 00:00:57,270
And the third one says, if we have a girl who's in high school, what app do we recommend to her? Enter your answers below.


@@@
1
00:00:00,000 --> 00:00:07,050
Well, let's see. A woman who works at an office. In the table, there's two women who work and both downloaded WhatsApp.

2
00:00:07,049 --> 00:00:18,524
So, let's say, it's safe to assume that recommending WhatsApp to this new person is the best idea. Now, for the man who works at a factory, we'll see that there's another man in the table who works and he downloaded Snapchat.

3
00:00:18,524 --> 00:00:24,125
So, we'll proceed to recommend Snapchat to the new person. And finally, the girl that is in high school.

4
00:00:24,125 --> 00:00:33,120
Well, we see that in the table, there are three students and they all downloaded Pokemon Go. So, we'll go ahead and recommend Pokemon Go to this new person.

5
00:00:33,119 --> 00:00:48,734
But now, that's how a human thinks. For the computer, it's harder to repeat this procedure. However, the computer can ask a slightly more technical question like this one: between gender and occupation, which one seems more decisive for predicting what app will the users download?

6
00:00:48,734 --> 00:00:54,000
This question sounds slightly ambiguous, but if you think about it, the answer does make sense. Let's give it a try.


@@@
1
00:00:00,000 --> 00:00:10,455
Okay. Well, let's see what happens if we split them by gender. If we split them by gender, we see that the women downloaded WhatsApp and Pokemon Go, while the men downloaded Snapchat and Pokemon Go.

2
00:00:10,455 --> 00:00:20,864
This tells us a bit, but not much. On the other hand, if split by occupation, we can see that the students all downloaded Pokemon Go, whereas, the people who work downloaded other apps.

3
00:00:20,864 --> 00:00:27,105
This is a good piece of information since from now on, whenever a student comes in, we'll recommend them Pokemon Go.

4
00:00:27,105 --> 00:00:34,774
Thus, occupation is a better feature here for predicting what app will the users download. So, we can go ahead and make that decision.

5
00:00:34,774 --> 00:00:41,660
We'll do it by creating a node here that says, "To everybody that goes to school, we'll recommend Pokemon Go.

6
00:00:41,659 --> 00:00:48,020
And for the ones that work, let's see." If we forget about the students, now will look at the people who work.

7
00:00:48,020 --> 00:00:55,520
And now, it turns out that the gender split will help us. Because the women downloaded WhatsApp, and the men downloaded Snapchat.

8
00:00:55,520 --> 00:01:02,464
So, let's do that. Let's add another node here. The new node says "If you work, then I'll ask for your gender.

9
00:01:02,465 --> 00:01:08,590
And if the gender is female, we'll recommend WhatsApp. And if the gender is male, we'll recommend Snapchat." And we're done.

10
00:01:08,590 --> 00:01:15,224
So quick summary. If we got a student, well recommend them Pokemon Go. If we get someone who works, we'll ask for the gender.

11
00:01:15,224 --> 00:01:33,129
If it's a woman, we'll recommend her WhatsApp. And if it's a man, we'll recommend him Snapchat. Now, what we need to figure out is, how do we get the computer to measure the two features, and figure out that occupation is a better feature to split by? We will learn that later.


@@@
1
00:00:00,000 --> 00:00:09,000
Now, in the last example, we constructed a tree with categorical features, namely gender and occupation, but we can also create a tree with continuous features.

2
00:00:09,000 --> 00:00:24,030
Let's go to this example, which you may see in other parts of this class. The example is an Admissions Office which takes two pieces of data from the students, their score in a test and their grades, and the better they do on them, the more likely that they'll be accepted at a university.

3
00:00:24,030 --> 00:00:33,704
So in here, the blue points are accepted and the red points are rejected, and our model will have the task of determining a rule for future students to be accepted or rejected.

4
00:00:33,704 --> 00:00:40,350
So, let's ask the same question as before. Between grades and tests, which one determines student acceptance better?

5
00:00:40,350 --> 00:01:02,799
And in order to help you out, let me translate this sentence into graphical terms. Since test is the horizontal axis and grades is the vertical axis, then the question becomes, between a horizontal and a vertical line, which one would cut the data in a better way, namely separating the red and the blue points as much as possible? Give this one a try.


@@@
1
00:00:00,000 --> 00:00:11,579
Well, let's try both. The best horizontal line would be somewhere around here. It does an okay job, but it doesn't really separate the points that well, at least a lot of blue points in the red area and vice versa.

2
00:00:11,580 --> 00:00:22,304
So, what happens if we try a vertical line? Well, it seems that the best cut is here around five. That does a pretty good job and only leaves five red points on the blue side and five blue points on the red side.

3
00:00:22,304 --> 00:00:32,564
So, let's go for that one, and our answer is vertical line. This means that the best feature to separate this data is test, and the best threshold is five.

4
00:00:32,564 --> 00:00:43,880
Therefore, we can add our first node to the decision tree and this node asks, "Is your test greater than or equal to five or is it less than five?" And now, we can do even more.

5
00:00:43,880 --> 00:00:49,130
We can try dividing each of the two halves with a horizontal line, which is the equivalent of saying, "Okay.

6
00:00:49,130 --> 00:00:58,280
I've seen your tests. Now, let's see how you did in the grades." The left half can be cut with a vertical line over here at height seven.

7
00:00:58,280 --> 00:01:07,689
This means, if your test score is less than five, then you need seven or more in the grades to get accepted, otherwise, you get rejected.

8
00:01:07,689 --> 00:01:21,404
The right half can be separated with a vertical line at height two. This means, if your test score is greater than or equal to five, then you need to have two or more in your grades in order to get accepted, otherwise, you get rejected.

9
00:01:21,405 --> 00:01:31,329
So, we've built our Decision Tree in a similar way as before, except now at each node, we don't have a yes/no question, but we have a threshold which would cut the values in two.


@@@
1
00:00:00,000 --> 00:00:05,415
Now in order to go further with Decision Trees, we need to learn an important concept called entropy.

2
00:00:05,415 --> 00:00:16,769
Entropy comes from physics and to explain it, we'll use the example of the three states of water. These are solid, which is ice, liquid, and gas, which is water vapor.

3
00:00:16,769 --> 00:00:25,140
Let's think of the particles inside ice, water, and vapor. Ice is pretty rigid in that its particles don't have many places to go.

4
00:00:25,140 --> 00:00:31,214
They mostly stay where they are. Water is a little less rigid in which a particle has a few places to move around.

5
00:00:31,214 --> 00:00:37,664
And water vapor is in the other end of the spectrum. A particle has many possibilities of where to go and can move around a lot.

6
00:00:37,664 --> 00:00:51,195
So, entropy measures precisely this, how much freedom does a particle have to move around? Thus, the entropy of ice is low, the entropy of liquid water is medium, and the entropy of water vapor is high.

7
00:00:51,195 --> 00:00:57,465
The notion of entropy can also work in probability. Let's look at these three configurations of balls inside buckets.

8
00:00:57,465 --> 00:01:05,079
The first bucket has four red balls. The second one has three red and one blue, and the third one has two red and two blue.

9
00:01:05,079 --> 00:01:14,453
And let's say balls from each color are completely indistinguishable. So we could say that entropy is given by how much balls are allowed to move around if we put them in a line.

10
00:01:14,453 --> 00:01:22,384
We can see that the first bucket is very rigid. No matter how we organize the balls, we always get the same state, so it has low entropy.

11
00:01:22,385 --> 00:01:31,745
In the second one, we can reorganize the balls in four ways, so it has medium entropy. For the third one, we have six ways of reorganizing the balls, so it has high entropy.

12
00:01:31,745 --> 00:01:41,284
This is not the exact definition of entropy, but it gives us an idea, that the more rigid the set is or the more homogeneous, the less entropy you'll have, and vice versa.

13
00:01:41,284 --> 00:01:49,415
Another way to see entropy is in terms of knowledge. If we were to pick a random ball from each of the buckets, how much do we know about the color of this ball?

14
00:01:49,415 --> 00:01:58,519
In the first bucket, we know for sure that the ball is red, so we have high knowledge. In the second bucket, it's very likely to be red and not very likely to be blue.

15
00:01:58,519 --> 00:02:04,339
So if we bet that it's red, we'll be right most of the time. So we have medium knowledge of the color of the ball.

16
00:02:04,340 --> 00:02:10,834
In the third bucket, we know much less since it's equally likely to be blue or red. So here, we have low knowledge.

17
00:02:10,835 --> 00:02:17,330
And it turns out that knowledge and entropy are opposites. The more knowledge one has, the less entropy, and vice versa.

18
00:02:17,330 --> 00:02:24,859
Thus, we conclude that the first bucket has low entropy, the second one has medium entropy, and the third one has high entropy.

19
00:02:24,860 --> 00:02:32,044
Now when I say opposites, I don't mean additive inverse or multiplicative inverses. I only mean it in the colloquial sense of the word.

20
00:02:32,044 --> 00:02:44,000
When one of them is big, then the other one is small, and vice versa. Over the next few videos, we'll cook up a formula for entropy, namely, one that gives us low, medium, and high values for these buckets.


@@@
1
00:00:00,000 --> 00:00:12,150
In order to cook up a formula for entropy, we'll consider the following game. In this game, we'll start with a configuration of balls; say red, red, red, and blue, and we'll put them inside the bucket.

2
00:00:12,150 --> 00:00:22,440
Now, what we do is we pick four balls out of this bucket with repetition and we try to get the initial configuration; red, red, red, blue and if we do, we win.

3
00:00:22,440 --> 00:00:30,464
Let's say we pick our first ball and it's red. We record the color and put it back. Then, we pick a second ball and it's red.

4
00:00:30,464 --> 00:00:37,965
So, we again record the color and put it back. Then, we pick a third ball, say red and record the color and put it back.

5
00:00:37,965 --> 00:00:46,515
Notice that we pick the same ball again. This is completely okay. Finally, we pick a fourth ball. Now, we got blue, we record it, and put it back.

6
00:00:46,515 --> 00:00:56,789
We recorded red, red, red, blue. So, we win a lot of money. If say we got the colors red, blue, blue, red, then we will win no money at all.

7
00:00:56,789 --> 00:01:06,200
That's the game. Now, the quiz question is, which one of the three buckets is the best to play the game with and which one is the worst? Enter your answer below.


@@@
1
00:00:00,000 --> 00:00:07,065
Well, it seems that the first bucket is the best one, because no matter what we do, we'll always pick red, red, red, red so we'll win every time.

2
00:00:07,065 --> 00:00:17,595
We can see that although it's not very easy to win in any of the other two, it's easier to pick red, red, red, blue in the second one, and much harder to get red, red, blue, blue in the third one.

3
00:00:17,594 --> 00:00:24,750
Thus, the answers are: the best bucket is the first one, the next one is okay, and the third one is the worst.

4
00:00:24,750 --> 00:00:32,369
But by how much more specifically? Let's ask the following question. What is the probability of winning in each of these games?

5
00:00:32,369 --> 00:00:40,909
So, let's start by the easy one. How likely is it to winning this game? Well, to get the first ball to be red, the probability is actually one.

6
00:00:40,909 --> 00:00:51,649
Same thing for the second one, the third one, and the fourth one. Since we put the ball back after recording each color, then these events are completely independent.

7
00:00:51,649 --> 00:01:04,564
So, the probability that they all occur is the product of the four probabilities. This means the probability is one, which matches our intuition that no matter what we do, we'll always pick red, red, red, red.

8
00:01:04,564 --> 00:01:11,465
Now let's go to the red, red, red, blue case. What is the probability that the first ball we'd pick is red?

9
00:01:11,465 --> 00:01:21,980
Well, it's three over four or 0.75 since there are three red balls, and four in total. Same thing for the second, and the third balls.

10
00:01:21,980 --> 00:01:29,269
Now what's the probability of the fourth ball we pick his blue? Well now it's one over four since there's only one blue ball among four.

11
00:01:29,269 --> 00:01:43,250
Therefore again, since the events are independent the probability of the four of them happening is the product of the four probabilities, which is 0.75 times 0.75 times 0.75 times 0.25.

12
00:01:43,250 --> 00:01:53,805
This is 0.105 or around ten 10 percent probability of winning here. And for the last one, well the chances here of getting a red ball are 50 percent, since there are two red balls and two blue balls.

13
00:01:53,805 --> 00:02:07,859
And the chance of getting a blue ball are the same. Thus, the chance of these balls being red, red, blue, and blue is the product, which is 0.0625 or roughly six percent for winning in this game.

14
00:02:07,859 --> 00:02:18,539
We summarize these results in the table over here, where the first column has a probability of a ball being red, the second one off the ball being blue, and the last one we highlight the probability of winning.

15
00:02:18,539 --> 00:02:24,560
Now products are confusing mainly for two reasons. The first one is that if we have, say a a thousand balls.

16
00:02:24,560 --> 00:02:34,729
Now, we take the product of a thousand numbers all between zero and one. This could be very tiny. The other reason is that a small change in one of the factors could drastically alter their product.

17
00:02:34,729 --> 00:02:42,050
We want something more manageable. And what's better than products? Let's ask our friend here. Yes, he is right.

18
00:02:42,050 --> 00:02:49,579
Sums are better than products. And now we just need to turn the products into sums. Would any of the following functions be able to help us?


@@@
1
00:00:00,000 --> 00:00:08,385
Correct logarithm is the answer since it satisfies that beautiful identity that says, the logarithm of a product is the sum of the logarithms.

2
00:00:08,385 --> 00:00:17,100
Thus, our product numbers becomes a sum of the logarithms of the numbers. In this case, we get minus 3.245.

3
00:00:17,100 --> 00:00:22,500
Now in this class, we'll be using log as a logarithm base two, and the reason is information theory.

4
00:00:22,500 --> 00:00:34,315
So here's our summary. We have our three configuration of red and blue balls. Now we have the probability of the ball being red and blue, and the product of them according to the sequence, which is the probability of winning the game.

5
00:00:34,315 --> 00:00:40,055
In the next column, we'll take the logarithm base two. But since the numbers are less than one, the logarithm is negative.

6
00:00:40,055 --> 00:00:56,045
Thus, if we take the negative of the whole equation, we're now dealing with positive numbers. And in the last column, we'll just divide by four because what we'll use as definition of entropy is the average of the negatives of the logarithms of the probabilities of picking the balls in a way that we win the game.

7
00:00:56,045 --> 00:01:03,290
Thus, for the first bucket, we get 0 entropy, for the second one we get 0.81, and for the third one we get one.

8
00:01:03,290 --> 00:01:10,490
This is going to be our formula for entropy. In the slightly more general case of five red balls and three blue balls, we get the following.

9
00:01:10,490 --> 00:01:22,085
The negative of the sum of five times the logarithm of the probability of picking a red ball, which is five over eight, and three times the logarithm of the probability of picking a blue ball, which is three over eight.

10
00:01:22,085 --> 00:01:29,705
We can see that this is a large number since this set has a lot of entropy. In the more general case with m red balls and n blue balls, this is the formula.

11
00:01:29,705 --> 00:01:42,130
As the probability of picking a red ball is m divided by m plus n, and for a blue ball it's n divided by n plus n. And this is the general formula for entropy when the balls can be of two colors.


@@@
1
00:00:00,000 --> 00:00:09,929
Okay, so now, we'll use what we know about entropy and information gain to build Decision Trees. Let's say, we have our data in the form of these red and blue points and we want to split it into two.

2
00:00:09,929 --> 00:00:27,530
So, I'll show you three ways of splitting it, and here are the three ways. Now, let's have a small quiz: In which of these do you think we have gained more information about our data and in which one did we gain less information? Enter your answers below.


@@@
1
00:00:00,000 --> 00:00:08,085
Correct. The first way to split them does not help us at all. We ended up with very similar sets of red and blue points and we learn nothing about the data.

2
00:00:08,085 --> 00:00:14,669
The second way did a decent job. We managed to get most of the blue ones on one side and most of the red ones on the other side.

3
00:00:14,669 --> 00:00:23,039
So, now we know a bit more about it. And the third one did fantastic. It managed to send all the blue points on one side and all the red ones on the other side.

4
00:00:23,039 --> 00:00:34,299
Now we know a lot about our data. Actually, we'll learn how to calculate something called the information gain, and it will be zero for the first splitting, 0.28 for the second, and one for the third one.

5
00:00:34,299 --> 00:00:40,203
And the formula for information gain is very simple. It's just the change in entropy. Let me be more specific.

6
00:00:40,203 --> 00:00:49,630
For every node in the Decision Tree, we can calculate the entropy of the data in the parent node. And then, we calculate the entropies of the two children.

7
00:00:49,630 --> 00:00:56,890
The information gain is a difference between the entropy of the parent and the average entropy of the children.

8
00:00:56,890 --> 00:01:04,504
So, in our first example, you can calculate the entropy of the parent and its one. The entropy of each of the children is 0.72.

9
00:01:04,504 --> 00:01:14,060
So the average of the entropy of the children is actually 0.72. Thus, the change in entropy is one minus 0.72, which is 0.28.

10
00:01:14,060 --> 00:01:22,099
For this example, now, we can calculate the entropies of the children which are both one. Thus, the change in entropy is one minus one which is zero.

11
00:01:22,099 --> 00:01:33,664
This is a terrible splitting because they gave us zero information. And finally, in this example, the entropy of the children are both zero, since as we saw, a set where the points are the same color has zero entropy.

12
00:01:33,665 --> 00:01:47,054
Thus, the information gain is one minus zero which is one. This split gave us the largest information gain and as you can see, it is the best split because it managed to perfectly cut the data into the blue points and red points.

13
00:01:47,055 --> 00:01:58,200
So, here's our summary. The three cuts with the values of information gain. If the tree had to choose, it would go for the third cut, which is the one that gives it the largest amount of information gain.


@@@
1
00:00:00,000 --> 00:00:11,335
Okay, so now, let's go ahead and build a Decision Tree. Our algorithm will be very simple. Look at the possible splits that each column gives, calculate the information gain, and pick the largest one.

2
00:00:11,335 --> 00:00:17,574
So, let's calculate the entropy of the parent, which is this data. We'll calculate the entropy of the column of the labels.

3
00:00:17,574 --> 00:00:33,625
So there are three Pokemon Go's, two WhatsApp, and one Snapchat. The entropy is negative three over six, logarithm base two of three over six, minus two over six, logarithm base two of two over six, minus one over six, logarithm base two of one over six.

4
00:00:33,625 --> 00:00:46,174
This gives us 1.46. Now, if we split them by gender, we get two sets, one with one Pokemon Go and two WhatsApp, and one with one Snapchat and two Pokemon Go.

5
00:00:46,174 --> 00:01:01,845
The entropies for these sets are both 0.92. Thus, the average entropy of the children of this node is 0.92, and the information gain is 1.46 minus 0.92, which is zero point 54.

6
00:01:01,844 --> 00:01:09,269
Now, if we split by occupation, we get one set of three Pokemon Go's, and one of two WhatsApps, and one Snapchat.

7
00:01:09,269 --> 00:01:23,054
The first set has entropy zero, and the other has entropy 0.92. Therefore, the average of these is 0.46, and the information gain is 1.46 minus 0.46, which is one.

8
00:01:23,055 --> 00:01:31,664
So to summarize, splitting by the gender column gave us an information gain of 0.54, and splitting by the occupation column gave us an information gain of one.

9
00:01:31,665 --> 00:01:40,185
The algorithm says, pick the column with the highest information gain, which is occupation. So we split by occupation. We'll get two sets.

10
00:01:40,185 --> 00:01:46,304
One is very nice, since everybody downloaded Pokemon Go, and in the other one, we can still do better.

11
00:01:46,305 --> 00:01:55,754
We can split based on the gender column. Since now, we get two very nice sets, one where everybody downloaded WhatsApp, and the other one where everybody downloaded Snapchat.

12
00:01:55,754 --> 00:02:02,740
And we're done, here's our Decision Tree. If we want to do this for continuous features instead of discrete features, we can still do it.

13
00:02:02,739 --> 00:02:17,080
We'll let you think about the details. So basically, the idea is to think of all the possible vertical and horizontal cuts, and seeing which one maximizes the entropy, and then iterating over and over as we build a Decision Tree.

14
00:02:17,080 --> 00:02:25,134
Here, we can see that our first cut is vertical at the value five. Our next cut will be a horizontal cut at the height seven.

15
00:02:25,134 --> 00:02:34,270
And our final cut will be horizontal at height two. And finally, we have our Decision Tree that cuts our data in two.


@@@
1
00:00:00,000 --> 00:00:05,549
Now, here's a potential problem with Decision Trees. Let's say we have a humungous table with lots and lots of columns.

2
00:00:05,549 --> 00:00:11,339
So, we create our Decision Tree and let's say it looks like this. This is not a realistic tree, though, just an example.

3
00:00:11,339 --> 00:00:23,255
And we end up with answers like the following. If a client is male between 15 and 25 in the US, on Android, in school, likes tennis, pizza, but does not like long walks on the beach, then they're likely to download Pokemon Go.

4
00:00:23,254 --> 00:00:31,195
This is not good. This almost looks like the tree just memorized the data. It's overfitting. Decision Trees tend to overfit a lot.

5
00:00:31,195 --> 00:00:41,304
In the continuous case, this can also happen and it looks like this. The Decision Tree has many nodes which end up giving us a complicated boundary that pretty much borders every point with a small square.

6
00:00:41,304 --> 00:00:49,164
This is also overfitting as it doesn't generalize well to the data. So, how do we solve this? In the simplest possible way. Take a look at this.

7
00:00:49,164 --> 00:00:55,929
Let's take our data and say, pick some of the columns randomly. Build a Decision Tree in those columns.

8
00:00:55,929 --> 00:01:03,754
Now, pick some other columns randomly and build a Decision Tree in those, and do it again. And now, just let the trees vote.

9
00:01:03,755 --> 00:01:12,620
When we have a new data point, say this person over here, we just let all the trees make a prediction and pick the one that appears the most.

10
00:01:12,620 --> 00:01:22,129
For example, these trees decided that this person will download Snapchat, WhatsApp, and WhatsApp. So, the ensemble of trees will recommend WhatsApp.

11
00:01:22,129 --> 00:01:34,269
Since we used a bunch of trees on randomly picked columns, this is called a random forest. There are better ways to pick the columns than randomly and we'll see this in the ensemble methods section of this Nanodegree.


@@@
1
00:00:00,000 --> 00:00:08,005
In this next unit, I really want to take you to a world of amazing tools that we use to build self-driving cars.

2
00:00:08,005 --> 00:00:16,754
And these tools are called matrices, linear algebra or vectors. And they might look a bit scary at first, but they're very, very intuitive.

3
00:00:16,754 --> 00:00:24,699
In my experience, many students struggle with those tools. So my team has put together hopefully a very comprehensive introduction to those tools.

4
00:00:24,699 --> 00:00:34,734
If you're familiar, you're going to breeze through it. But hopefully you'll learn and demystify all these cryptic things that you find in Wikipedia when you google Kalman filters.


@@@
1
00:00:01,000 --> 00:00:08,000
Welcome to my second class on Kalman filters. I want to take you on a little tour to where it all began--Stanford University.

2
00:00:08,000 --> 00:00:17,000
Behind me is Vale, Stanford's Research Center. Let's go inside. This is Junior, Standord's most recent self-driving car.

3
00:00:17,000 --> 00:00:23,000
It's the child of Stanley, whom you can find in the National Museum of American History in Washington, D.C.

4
00:00:23,000 --> 00:00:34,000
Let me tell you something about the equipment that's on this car that makes it self-driving. This rotating thing over here is a laser-range finder that takes distance scans 10 times a second, about a million data points.

5
00:00:34,000 --> 00:00:42,000
It'll be really important for the Kalman filter class I'm teaching you today. It's major function is to spot other cars so you don't run into them.

6
00:00:42,000 --> 00:00:53,000
There is also a camera on top. There is a stereo camera system over here. In the rear there are antennas for a GPS--global positioning system-- that allows us to estimate where the car is in the world.

7
00:00:53,000 --> 00:01:00,000
This is a supplemental system to the localization class I just taught you. This is the data that comes from the laser.

8
00:01:00,000 --> 00:01:00,000
This is the car parked in the garage right now. We see the back wall. These are all range measurements that tell you how far things are away, and they are essential as the input to the Kalman filter that we're going to learn about today.


@@@
1
00:00:00,000 --> 00:00:10,495
So, I'd like to take my students onto a little journey to Stanford and show them our self-driving car that uses sensors to sense the environment.

2
00:00:10,494 --> 00:00:18,855
So let me dive into the class very much right now. Through our last class, we talked about localization.

3
00:00:18,855 --> 00:00:26,594
We had a robot that lived in an environment and that it could use its sensors to determine where in the environment it is.

4
00:00:26,594 --> 00:00:38,564
So, here you can see the Google self-driving car using a roadmap localizing itself. But in addition what's shown here in red are measurements of other vehicles.

5
00:00:38,564 --> 00:00:47,200
The car uses lasers and radars to track other vehicles. Today, we're going to talk about how to find other cars.

6
00:00:47,200 --> 00:01:07,615
The reason why we'd like to find other cars is because we wouldn't want to run into them. So, we have to understand how to interpret sensor data to make assessments not just where these other cars are as in the localization case, but also how fast they're moving, so that we can drive in a way that avoids collisions within the future.

7
00:01:07,614 --> 00:01:22,840
That's important not just for cars, it matters for pedestrians and bicyclists and understanding where the cars are and making prediction where they're going to move is absolutely essential for safe driving in the Google car potrait.

8
00:01:22,840 --> 00:01:29,000
So in this class, we'll talk about tracking. The technique I'd like to teach you is called the Kalman Filter.

9
00:01:29,000 --> 00:01:41,889
This is an insanely popular technique for estimating the state of a system. It's actually very similar to the probabilistic localization method we talked in the previous class, Monte Carlo localization.

10
00:01:41,890 --> 00:01:52,109
The primary differences are that Kalman Filters estimate a continuous state whereas in Monte Carlo localization, we are forced to chop the word in the discrete places.

11
00:01:52,109 --> 00:02:04,569
As a result, the Kalman Filter happens to give us a uni-modal distribution and I'll tell you in a second what that means, whereas Monte Carlo was fine with multi-modal distributions.

12
00:02:04,569 --> 00:02:11,610
Both of these techniques are applicable to robot localization and tracking other vehicles. Consider the car down here.

13
00:02:11,610 --> 00:02:25,644
Let's assume it seizes measurement, an object here, here, here, and here for the time is t equals zero, t equals one, two and three.


@@@
1
00:00:00,000 --> 00:00:09,195
You remember our Markov model, where the word was divided into discrete grids, and we assigned to each grid to the probability.

2
00:00:09,195 --> 00:00:31,140
Such a representation of probability of a space is called a histogram, and that it divides the continuous space into discrete, into finally many grid cells, and approximates the posterior distribution by a histogram over the original distribution, and the histogram is a mere approximation to this continuous distribution.

3
00:00:31,140 --> 00:00:44,250
In Kalman Filters, the distribution is given by what's called a Gaussian. Gaussian is a continuous function over the space of locations and the area underneath sums up to 1.

4
00:00:44,250 --> 00:01:02,969
So, use Gaussian again and if you call the space x, then the Gaussian is characterized by two parameters, the mean, often abbreviated with the Greek letter Mu, and the width of the Gaussian, often called the variance.

5
00:01:02,969 --> 00:01:19,780
For reasons I don't wanna go into is often written as a quadratic variable, Sigma square. So, any Gaussian in 1D, which means the parameter space over here is one-dimensional, is characterized by Mu and Sigma square.

6
00:01:19,780 --> 00:01:33,655
So, rather than estimating the entire distribution as a histogram, our task in common phases is to maintain a Mu and a Sigma square as our best estimate of the location of the object we are trying to find.

7
00:01:33,655 --> 00:01:44,230
The exact formula is an exponential of a quadratic function where we take the exponent of this complicated expression over here.

8
00:01:44,230 --> 00:01:55,034
The quadratic difference of our query point x, relative to the mean Mu, divided by Sigma square, multiply by minus a half.

9
00:01:55,034 --> 00:02:11,625
Now if x equals Mu, then the numerator becomes 0, and if x of 0, which is one. It turns out we have to normalize this by a constant, 1 over the square root of 2 Pi Sigma square.

10
00:02:11,625 --> 00:02:20,814
But for everything we talk about today, this constant won't matter, so we can ignore it. What matters is we have an exponential of a quadratic function over here.


@@@
1
00:00:00,000 --> 00:00:21,000
Starting with the following source code, I'm looking for a completion of this one line over here that returns the Gaussian function with arguments mu = 10, sigma2 = 4, and x = 8, and I want the output to be approximately 0.12.

2
00:00:21,000 --> 00:00:37,000
Here's my solution. This is the constant: 1/sprt(2pisigma2). Then I multiply with the exponential of (-.5(x-mu)*2/sigma2).

3
00:00:37,000 --> 00:00:37,000
Applying that to the following numbers over here gives me 0.12. Now, here's a question for you. How do I have to modify x the 8 to get the maximum return value for this function f?


@@@
1
00:00:01,000 --> 00:00:08,000
The answer is assess with the same value as mu, in which case this expression over here becomes zero, and we get the maximum.

2
00:00:08,000 --> 09:59:59,000
We get the peak of the Gaussian. We set x to the same value as mu, to 10, and the output is 0.2 approximately.


@@@
1
00:00:00,000 --> 00:00:09,480
In Kalman filters, we iterate measurement and motion is often called the measurement update and it's often called prediction.

2
00:00:09,480 --> 00:00:27,400
And the update will use Bayes rule, which is nothing else but a product or a multiplication. In this update, we use total probability which is a convolution or simply an addition.

3
00:00:27,399 --> 00:00:43,799
Let's talk first about the measurement cycle and then the prediction cycle using our great Gaussians for implementing those steps.

4
00:00:43,799 --> 00:00:54,990
Suppose you're localizing another vehicle and you have a prior distribution that looks as follows; it is a very wide Gaussian with the mean over here.

5
00:00:54,990 --> 00:01:10,704
And now say we get a measurement that tells us something about the localization vehicle and it comes in like this: it has a mean over here called mu and this example has a much smaller covariance for the measurement.

6
00:01:10,704 --> 00:01:20,109
This is an example in our prior we were fairly uncertain about location but the measurement told us quite a bit as to where the vehicle is.

7
00:01:20,109 --> 00:01:35,879
Here's a quiz for you with the new mean of the subsequent Gaussian B over here, over here, or over here.


@@@
1
00:00:00,000 --> 00:00:09,675
The answer is over here in the middle, it's between the two old means, the mean of the prior, and the mean of the measurement.

2
00:00:09,675 --> 00:00:18,780
It's slightly further on the measurement side because the measurement was more certain as to where the vehicle is than the prior.


@@@
1
00:00:00,500 --> 00:00:10,539
Now, here's a question that's really, really hard. When we graph the new Gaussian. Graph one is very wide.

2
00:00:10,730 --> 00:00:21,504
It's very peaky. So if I were to measure where the peak of the new Gaussian is, this will be a very narrow and skinny Gaussian.

3
00:00:21,504 --> 00:00:29,004
This would be one that was width is in between the two Gaussian, and this is one is even wider than the two original Gaussians.

4
00:00:29,004 --> 00:00:40,670
Which one do you believe is the correct posterior after multiplying these two Gaussians? This is an insanely hard question, I'd like you to take your chances here, and explain me the answer in just a second.


@@@
1
00:00:00,710 --> 00:00:15,434
And very surprisingly, the resulting Gaussian is more certain than the two-component Gaussians. That is the covariance is smaller than either of the two covariances in the installation.

2
00:00:15,435 --> 00:00:25,545
Intuitively speaking, this is the case because we actually gain information. The two Gaussians together with high information content in either Gaussian installation.

3
00:00:25,545 --> 00:00:36,799
So it look like something like this. And it is completely non-obvious. You might have to take this with faith, but I can actually prove it to you.


@@@
1
00:00:00,000 --> 00:00:17,290
Suppose we multiply two Gaussians, as in Bayes rule, a prior and a measurement probability. The prior has a mean of Mu and a variance of Sigma square, and the measurement has a mean of Nu, and a covariance of r-square.

2
00:00:17,290 --> 00:00:32,775
Then, the new mean, Mu prime, is the weighted sum of the old means. The Mu is weighted by r-square, Mu is weighted by Sigma square, normalized by the sum of the weighting factors.

3
00:00:32,775 --> 00:00:45,125
The new variance term, I want to write Sigma square prime here for the new one after the update, is given by this equation over here.

4
00:00:45,125 --> 00:01:01,550
So, let's put this into action. We have a weighted mean over here. Clearly, the prior Gaussian has a much higher uncertainty therefore Sigma square is larger and that means the nu is weighted much much larger than the Mu.

5
00:01:01,550 --> 00:01:13,875
So, the mean will be closer to the nu than the mu, which means it will be somewhere over here. Interestingly enough, the variance term is unaffected by the actual means, it just uses the previous variances.

6
00:01:13,875 --> 00:01:30,505
It comes up with a new one that's even peak here, so the result might look like this. So, this is the common situation for the measurement update step where this is the prior, this is the measurement probability and this is the posterior.

7
00:01:30,505 --> 00:01:42,350
So, let's practice these equations with a simple quiz. So, here are our equations again and suppose I use the following Gaussians.

8
00:01:42,350 --> 00:01:54,920
These are Gaussian with equal variance, but different means. They might look as follows. Compute for me the new mean after the update and the new sigma square.


@@@
1
00:00:00,000 --> 00:00:05,794
The answer for the new mean is just the one in the middle, and the reason is both weights over here are equivalent.

2
00:00:05,794 --> 00:00:20,755
So, we can take the mean between MU and NU, which is 11. Then the sigma square is two. If you take one over four plus one over four, then we get one over two.

3
00:00:20,754 --> 00:00:30,279
So, one over one over two equals two, which means the new variance term is half the size of the previous variance terms.


@@@
1
00:00:00,000 --> 00:00:10,000
Here's my answer. This is the expression for the mean. This is the one for the variance. I run it, and I get the exact same answer.

2
00:00:10,000 --> 00:00:22,000
I run it again for my other example of equal variances and 10 and 12 as means, and miraculously, the correct answer comes out-- 11 for the new mean and 2 for the new variance.

3
00:00:22,000 --> 00:00:30,000
If you programmed this correctly, then congratulations. You've just programmed an essential update step in the Kalman filter-- the measurement update step.

4
00:00:30,000 --> 09:59:59,000
That's really the difficult step in Kalman filtering. The other one--the prediction step or the motion step--is much, much easier to program.


@@@
1
00:00:00,000 --> 00:00:10,945
So, let's step a step back and look at what we've achieved. We knew there was a measurement update, an emotion update, which is also called prediction.

2
00:00:10,945 --> 00:00:16,344
We know that the measurement update is implemented by multiplication which is the same as Bayes rule.

3
00:00:16,344 --> 00:00:30,609
The motion update is done by total probability or an addition. So, we tackle the more complicated case, this is actually the hop from mathematically, and we solve this, we give an exact expression.

4
00:00:30,609 --> 00:00:37,494
We even derived mathematically, and you were able to write a computer program that implements this step of the Kalman Filter.

5
00:00:37,494 --> 00:00:43,809
I don't want to go into too much depth here. This is a really, really easy step. Let me write it down for you.

6
00:00:43,810 --> 00:00:50,995
Suppose you live in a world like this, this is your current best estimate of where you are and this is your uncertainty.

7
00:00:50,994 --> 00:00:58,520
Now, say you move to the right side, a certain distance, and that motion itself has its own set of uncertainty.

8
00:00:58,520 --> 00:01:08,100
Then you arrive at a prediction that adds the motion command to the mean, and it has an increased uncertainty over the initial uncertainty.

9
00:01:08,099 --> 00:01:21,554
Intuitively, this makes sense. If you move to the right by this distance in expectation you're exactly where you wish to be, but you've lost information because your motion tends to lose information as manifest by this uncertainty over here.

10
00:01:21,555 --> 00:01:30,265
Now, the math for this is really, really easy. A new mean, is your old mean plus the motion often called u.

11
00:01:30,265 --> 00:01:41,769
So, if you move over 10 meters, this will be 10 meters and you knew sigma square is your old sigma square plus variance of the motion Gaussian.

12
00:01:41,769 --> 00:01:48,605
This is all you need to know, it's just an addition, and I won't prove it to you because it's really trivial.

13
00:01:48,605 --> 00:02:00,439
But in summary, we have a Gaussian over here. We have a Gaussian for the motion with u as the mean and r square has its own motion uncertainty.

14
00:02:00,439 --> 00:02:12,169
The resulting Gaussian in the prediction step just adds these two things up, mu plus u and sigma square plus r square.

15
00:02:12,169 --> 00:02:24,155
Since it was so simple, let me quiz you, we have a Gaussian before the prediction step which mu equals 8 and sigma square equals 4.

16
00:02:24,155 --> 00:02:41,969
We then move to the right, a total of 10 with a motion uncertainty of 6. Now, describe to me the predicted Gaussian and give me the new mu and the new sigma square.


@@@
1
00:00:00,000 --> 00:00:06,000
[Thrun] Let's program this. I'm giving you a skeleton code. This is the same update function as before.

2
00:00:06,000 --> 00:00:19,000
Now I would like you to do the predict function, which takes our current estimate and its variance and the motion and its uncertainty and computes the new updated prediction: mean and variance.

3
00:00:19,000 --> 00:00:28,468
So for example, if our prior is 10 and 4, our motion is 12 and 4, I would like to get out to 22 and 8 according to the formulas I've just given you.


@@@
1
00:00:00,880 --> 00:00:13,121
And yes, it's as easy as this. We just add the two means and the two variances. It's amazing, this entire program over here implements a one-dimensional Kalman filter.


@@@
1
00:00:00,000 --> 00:00:12,000
So now let's put everything together. Let's write a main program that takes these 2 functions, update and predict, and feeds into a sequence of measurements and motions.

2
00:00:12,000 --> 00:00:21,000
In the example I've chosen here are the measurements of 5., 6., 7., 9., and 10. The motions are 1., 1., 2., 1., 1.

3
00:00:21,000 --> 00:00:31,000
This all would work out really well if the initial estimate was 5, but we're setting it to 0 with a very large uncertainty of 10,000.

4
00:00:31,000 --> 00:00:49,000
Let's assume the measurement uncertainty is constant 4, and the motion uncertainty is constant 2. When you run this, your first estimate for position should basically become 5-- 4.99, and the reason is your initial uncertainty is so large, the estimate is dominated by the first measurement.

5
00:00:49,000 --> 00:01:02,000
Your uncertainty shrinks to 3.99, which is slightly better than the measurement uncertainty. You then predict that you add 1, but the uncertainty increases to 5.99, which is the motion uncertainty of 2.

6
00:01:02,000 --> 00:01:15,000
You update again based on the measurement 6, you get your estimate of 5.99, which is almost 6. You move 1 again. You measure 7. You move 2. You measure 9. You move 1.

7
00:01:15,000 --> 00:01:30,000
You measure 10, and you move a final 1. And out comes as the final result, a prediction of 10.99 for the position, which is your 10 position moved by 1, and the uncertainty--residual uncertainty of 4.


@@@
1
00:00:00,000 --> 00:00:11,000
This piece of code implements the entire Kalman filter. It goes through all the measurement elements and quietly assumes there are as many measurements as motions indexed by n.

2
00:00:11,000 --> 00:00:22,000
It updates the mu and sigma using this recursive formula over here. If we plug in the nth measurement and the measurement uncertainty, it does the same with the motion, the prediction part over here.

3
00:00:22,000 --> 00:00:30,000
It updates the mu and sigma recursively using the nth motion and the motion uncertainty, and it prints all of those out.

4
00:00:30,000 --> 00:00:39,000
If I hit the Run button, I find that my first measurement update gets me effectively 5.0. It's 4.98.

5
00:00:39,000 --> 00:00:46,000
And that makes sense because we had a huge initial uncertainty, and [inaudible] of 5 with a relatively small measurement uncertainty.

6
00:00:46,000 --> 00:00:53,000
And in fact the resulting sigma square term is 3.98, which is better than 4 and 1,000, slightly better than 4.

7
00:00:53,000 --> 00:01:00,000
We're slightly more certain than the measurement itself. We now apply the motion of 1. We get to 5.9.

8
00:01:00,000 --> 00:01:13,000
Our uncertainty increases by exactly 2, from 3.9 to 5.98. And then the next update comes in at 6, and it gives us a measurement of 5.99 and now a reduced uncertainty of 2.39.

9
00:01:13,000 --> 00:01:21,000
And then we go to move to the right again by 1, which makes the prediction 6.99. Uncertainty goes up.

10
00:01:21,000 --> 00:01:31,000
We measure 7. We get to 6.99, almost 7. Uncertainty goes down. We move 2 to the right, measure 9, 1 to the right, measure 10, and move 1 again.

11
00:01:31,000 --> 00:01:40,000
The final thing is the motion. And if you look at the end result, our estimate is almost exactly 11, which is the result of 10 + 1.

12
00:01:40,000 --> 00:01:52,000
And the uncertainty is 4.0 after the motion and 2.0 after the measurement. This code that you just wrote implements a full Kalman filter for 1D.

13
00:01:52,000 --> 00:02:04,000
If you look at this, we have an update function that implements what actually is a relatively simple equation, and a prediction function which is an even simpler equation of just addition.

14
00:02:04,000 --> 00:02:14,000
And then you apply it to a measurement sequence and a motion sequence with certain uncertainties associated, and this little piece of code over here gives you a full Kalman filter in 1D.

15
00:02:14,000 --> 00:02:22,000
I find this really amazing. Let's plug in some other values. Suppose you're really certain about the initial position.

16
00:02:22,000 --> 00:02:31,000
It's wrong. It's 0. It should be 5, but it's 0. And now we assume a really small uncertainty. Guess what's going to happen to the final prediction?

17
00:02:31,000 --> 00:02:38,000
As I hit the Run button, we find this has an effect on the final estimate. It's not 11. It's only 10.5.

18
00:02:38,000 --> 00:02:46,000
And the way this takes place is initially, after our first measurement update, we believe in the position of 0.

19
00:02:46,000 --> 00:02:55,000
This is 1.24 to the - 10th, but a really small uncertainty, even smaller than this one over here. We apply our motion update. We add a 1.

20
00:02:55,000 --> 00:03:06,000
We have a higher uncertainty. And now when the next measurement comes in, 6, we are now more inclined to believe the measurement because uncertainty is now basically 2 as opposed to 0.001.

21
00:03:06,000 --> 00:03:15,000
We update our position to be 2.666, which is now a jump away from 1, and we reduce our uncertainty. Motion comes in, 3.66.

22
00:03:15,000 --> 00:03:26,000
Uncertainty goes up. We now are willing to update even more. As you see the 7, we're willing to go to 5.1, but not quite all the way because we feel fairly confident on our wrong prior estimate.

23
00:03:26,000 --> 00:03:36,000
And this confidence makes it all the way to the end when we predict 10.5 as opposed to 11 with an uncertainty of 3.98.

24
00:03:36,000 --> 09:59:59,000
We've corrected some of it. We were able to drag it into the right direction but not all the way because our false initial belief has such a strong weight in the overall equation.


@@@
1
00:00:00,000 --> 00:00:13,379
So, now we understand a lot about the 1D-Kalman Filter. You've programmed one, you understand how to incorporate measurements, you understand how to incorporate motion, and you really implement something that's actually really cool.

2
00:00:13,380 --> 00:00:22,330
Which is a full common filter for the 1D case. Now in reality, we often have many Ds, and then things become more involved.

3
00:00:22,329 --> 00:00:31,279
So, I'm going to just tell you how things work with an example, and why it's great to estimate in higher-dimensional state spaces.

4
00:00:31,280 --> 00:00:46,075
Suppose you have a two-dimensional state space of x and y, like a camera image, or in our case we might have a car that uses a radar to detect the location of a vehicle over time.

5
00:00:46,075 --> 00:01:00,100
Then what the 2D-Kalman filter affords to a something really amazing. And here's how it goes. Suppose at time t equals zero, you observe the object of interest to be at this coordinate.

6
00:01:00,100 --> 00:01:07,295
This might be another current traffic for the Google self-driving car. One time-step later you see it over here.

7
00:01:07,295 --> 00:01:16,885
Another time-step later you see it right over here. Where would you now expect at time t equals three the object to be?

8
00:01:16,885 --> 00:01:44,520
Let me give you three different places. And the answer is here. What the Kalman Filter does for you if you do estimation in higher dimensional spaces, is to not just go onto x and y spaces, but allows you to implicitly figure about the velocity of the object is, and then uses velocity estimate to make a really good prediction about the future.

9
00:01:44,519 --> 00:01:54,519
Now, notice the sensor itself only sees position. It never sees the actual velocity, the velocity is inferred from seeing multiple positions.

10
00:01:54,519 --> 00:02:11,969
So, one of the most amazing things about Kalman Filters in tracking applications is that it's able to figure out, even though it never directly measures it, the velocity of the object, and from there is able to make predictions about future locations that incorporate velocity.

11
00:02:11,969 --> 00:02:23,679
That is just really really really great. And it's one of the reasons why Kalman filters are such a popular algorithm in artificial intelligence, and in control theory at large.


@@@
1
00:00:00,000 --> 00:00:08,160
So let's start with two questions, what is deep learning, and what is it used for? The answer to the second question is pretty much everywhere.

2
00:00:08,160 --> 00:00:24,635
Recent applications include things such as beating humans in games such as Go, or even jeopardy, detecting spam in emails, forecasting stock prices, recognizing images in a picture, and even diagnosing illnesses sometimes with more precision than doctors.

3
00:00:24,635 --> 00:00:31,980
And of course, one of the most celebrated applications of deep learning is in self-driving cars. And what is at the heart of deep learning?

4
00:00:31,980 --> 00:00:41,299
This wonderful object called neural networks. Neural networks vaguely mimic the process of how the brain operates, with neurons that fire bits of information.

5
00:00:41,299 --> 00:00:50,719
It sounds pretty scary, right? As a matter of fact, the first time I heard of a neural network, this is the image that came into my head, some scary robot with artificial brain.

6
00:00:50,719 --> 00:00:56,179
But then, I got to learn a bit more about neural networks and I realized that there are actually a lot scarier than that.

7
00:00:56,179 --> 00:01:09,029
This is how a neural network looks. As a matter of fact, this one here is a deep neural network. Has lots of nodes, lots of edges, lots of layers, information coming through the nodes and leaving, it's quite complicated.

8
00:01:09,030 --> 00:01:14,540
But after looking at neural networks for a while, I realized that they're actually a lot simpler than that.

9
00:01:14,540 --> 00:01:23,900
When I think of a neural network, this is actually the image that comes to my mind. There is a child playing in the sand, with some red and blue shells and we are the child.

10
00:01:23,900 --> 00:01:32,299
Can you draw a line that separates the red and the blue shells? And the child draws this line. That's it. That's what a neural network does.

11
00:01:32,299 --> 00:01:38,800
Given some data in the form of blue or red points, the neural network will look for the best line that separates them.

12
00:01:38,799 --> 00:01:44,079
And if the data is a bit more complicated like this one over here, then we'll need a more complicated algorithm.

13
00:01:44,079 --> 00:01:49,799
Here, a deep neural network will do the job and find a more complex boundary that separates the points.


@@@
1
00:00:00,000 --> 00:00:07,339
So, let's start with one classification example. Let's say we are the admissions office at a university and our job is to accept or reject students.

2
00:00:07,339 --> 00:00:14,698
So, in order to evaluate students, we have two pieces of information, the results of a test and their grades in school.

3
00:00:14,698 --> 00:00:21,800
So, let's take a look at some sample students. We'll start with Student 1 who got 9 out of 10 in the test and 8 out of 10 in the grades.

4
00:00:21,800 --> 00:00:32,000
That student did quite well and got accepted. Then we have Student 2 who got 3 out of 10 in the test and 4 out of 10 in the grades, and that student got rejected.

5
00:00:32,000 --> 00:00:40,795
And now, we have a new Student 3 who got 7 out of 10 in the test and 6 out of 10 in the grades, and we're wondering if the student gets accepted or not.

6
00:00:40,795 --> 00:00:53,020
So, our first way to find this out is to plot students in a graph with the horizontal axis corresponding to the score on the test and the vertical axis corresponding to the grades, and the students would fit here.

7
00:00:53,020 --> 00:01:04,560
The students who got three and four gets located in the point with coordinates (3,4), and the student who got nine and eight gets located in the point with coordinates (9,8).

8
00:01:04,560 --> 00:01:12,075
And now we'll do what we do in most of our algorithms, which is to look at the previous data. This is how the previous data looks.

9
00:01:12,075 --> 00:01:20,859
These are all the previous students who got accepted or rejected. The blue points correspond to students that got accepted, and the red points to students that got rejected.

10
00:01:20,858 --> 00:01:29,805
So we can see in this diagram that the students would did well in the test and grades are more likely to get accepted, and the students who did poorly in both are more likely to get rejected.

11
00:01:29,805 --> 00:01:38,000
So let's start with a quiz. The quiz says, does the Student 3 get accepted or rejected? What do you think? Enter your answer below.


@@@
1
00:00:00,000 --> 00:00:12,210
Correct. Well, it seems that this data can be nicely separated by a line which is this line over here, and it seems that most students over the line get accepted and most students under the line get rejected.

2
00:00:12,210 --> 00:00:22,019
So this line is going to be our model. The model makes a couple of mistakes since there are a few blue points that are under the line and a few red points over the line.

3
00:00:22,019 --> 00:00:33,600
But we're not going to care about those. I will say that it's safe to predict that if a point is over the line the student gets accepted and if it's under the line then the student gets rejected.

4
00:00:33,600 --> 00:00:39,495
So based on this model we'll look at the new student that we see that they are over here at the point 7:6 which is above the line.

5
00:00:39,494 --> 00:00:47,070
So we can assume with some confidence that the student gets accepted. So if you answered yes, that's the correct answer.

6
00:00:47,070 --> 00:00:55,199
And now a question arises. The question is, how do we find this line? So we can kind of eyeball it. But the computer can't.

7
00:00:55,200 --> 00:01:04,000
We'll dedicate the rest of the session to show you algorithms that will find this line, not only for this example, but for much more general and complicated cases.


@@@
1
00:00:00,000 --> 00:00:11,329
So, first let's add some math. We're going to label the horizontal axis corresponding to the test by the variable x1, and the vertical axis corresponding to the grades by the variable x2.

2
00:00:11,330 --> 00:00:17,504
So this boundary line that separates the blue and the red points is going to have a linear equation.

3
00:00:17,504 --> 00:00:34,685
The one drawn has equation 2x1+x2-18=0. What does this mean? This means that our method for accepting or rejecting students simply says the following: take this equation as our score, the score is 2xtest+grades-18.

4
00:00:34,685 --> 00:00:42,710
Now when the student comes in, we check their score. If their score is a positive number, then we accept the student and if the score is a negative number then we reject the student.

5
00:00:42,710 --> 00:00:49,484
This is called a prediction. We can say by convention that if the score is 0, we'll accept a student although this won't matter much at the end.

6
00:00:49,484 --> 00:01:00,420
And that's it. That linear equation is our model. In the more general case, our boundary will be an equation of the following wx1+w2x2+b=0.

7
00:01:00,420 --> 00:01:09,835
We'll abbreviate this equation in vector notation as wx+b=0, where w is the vector w1w2 and x is the vector x1x2.

8
00:01:09,834 --> 00:01:19,000
And we simply take the product of the two vectors. We'll refer to x as the input, to w as the weights and b as the bias.

9
00:01:19,000 --> 00:01:26,189
Now, for a student coordinates x1x2, we'll denote a label as Y and the label is what we're trying to predict.

10
00:01:26,189 --> 00:01:36,809
So if the student gets accepted, namely the point is blue, then the label is Y+1. And if the student gets rejected, namely the point is red and then the label is Y=0.

11
00:01:36,810 --> 00:01:51,919
Thus, each point is in the form x1x2Y or Y is 1 for the blue points and 0 for the red points. And finally, our prediction is going to be called Y-hat and it will be what the algorithm predicts that the label will be.

12
00:01:51,920 --> 00:01:59,010
In this case, Y-hat is one of the algorithm predicts that the student gets accepted, which means the point lies over the line.

13
00:01:59,010 --> 00:02:05,864
And, Y-hat is 0 if the algorithm predicts that this didn't get rejected, which means the point is under the line.

14
00:02:05,864 --> 00:02:17,469
In math terms, this means that the prediction Y-hat is 1 if wx+b is greater than or equal to zero and 0 if wx+b is less than 0.

15
00:02:17,469 --> 00:02:24,810
So, to summarize, the points above the line have Y hat=1 and the points below the line have Y-hat=0.

16
00:02:24,810 --> 00:02:43,009
And, the blue points have Y=1 and the red points have Y=0. And, the goal of the algorithm is to have Y-hat resembling Y as closely as possible, which is exactly equivalent to finding the boundary line that keeps most of the blue points above it and most of the red points below it.


@@@
1
00:00:00,000 --> 00:00:08,070
Now, you may be wondering what happens if we have more data columns so not just testing grades, but maybe something else like the ranking of the student in the class.

2
00:00:08,070 --> 00:00:16,955
How do we fit three columns of data? Well the only difference is that now, we won't be working in two dimensions, we'll be working in three.

3
00:00:16,954 --> 00:00:26,899
So now, we have three axis: x_1 for the test, x_2 for the grades and x_3 for the class ranking. And our data will look like this, like a bunch of blue and red points flying around in 3D.

4
00:00:26,899 --> 00:00:34,009
On our equation won't be a line in two dimension, but a plane in three dimensions with a similar equation as before.

5
00:00:34,009 --> 00:00:43,950
Now, the equation would be w_1_x_1 plus w_2_x_2 plus w_3_x_3 plus b equals zero, which will separate this space into two regions.

6
00:00:43,950 --> 00:00:52,489
This equation can still be abbreviated by Wx plus b equals zero, except our vectors will now have three entries instead of two.

7
00:00:52,490 --> 00:01:00,825
And our prediction will still be y head equals one if Wx plus b is greater than or equal to zero, and zero if Wx plus b is less than zero.

8
00:01:00,825 --> 00:01:09,429
And what if we have many columns like say n of them? Well, it's the same thing. Now, our data just leaps in n-dimensional space.

9
00:01:09,430 --> 00:01:32,084
Now, I have trouble picturing things in more than three dimensions. But if we can imagine that the points are just things with n coordinates called x_1, x_2, x_3 all the way up to x_n with our labels being y, then our boundaries just an n minus one dimensional hyperplane, which is a high dimensional equivalent of a line in 2D or a plane in 3D.

10
00:01:32,084 --> 00:01:49,040
And the equation of this n minus one dimensional hyperplane is going to be w_1_x_1 plus w_2_x_2 plus all the way to w_n_x_n plus b equals zero, which we can still abbreviate to Wx plus b equals zero, where our vectors now have n entries.

11
00:01:49,040 --> 00:02:01,000
And our prediction is still the same as before. It is y head equals one if Wx plus b is greater than or equal to zero and y head equals zero if Wx plus b is less than zero.


@@@
1
00:00:00,000 --> 00:00:09,629
So let's recap. We have our data which is all these students. The blue ones have been accepted and the red ones have been rejected.

2
00:00:09,630 --> 00:00:22,140
And we have our model which consists of the equation two times test plus grades minus 18, which gives rise to this boundary which the point where the score is zero and a prediction.

3
00:00:22,140 --> 00:00:28,839
The prediction says that the student gets accepted of the score is positive or zero, and rejected if the score is negative.

4
00:00:28,839 --> 00:00:36,310
So now we'll introduce the notion of a preceptron, which is the building block of neural networks, and it's just an encoding of our equation into a small graph.

5
00:00:36,310 --> 00:00:42,475
The way we've build it is the following. Here we have our data and our boundary line and we fit it inside a node.

6
00:00:42,475 --> 00:00:52,399
And now we add small nodes for the inputs which, in this case, they are the test and the grades. Here we can see an example where test equals seven and grades equals six.

7
00:00:52,399 --> 00:00:59,259
And what the perceptron does is it blocks the points seven, six and checks if the point is in the positive or negative area.

8
00:00:59,259 --> 00:01:05,784
If the point is in the positive area, then it returns a yes. And if it is in the negative area, it returns and no.

9
00:01:05,784 --> 00:01:20,709
So let's recall that our equation is score equals two times test plus one times grade minus 18, and that our prediction consists of accepting the student if the score is positive or zero, and rejecting them if the score is negative.

10
00:01:20,709 --> 00:01:30,409
These weights two, one, and minus 18, are what define the linear equation, and so we'll use them as labels in the graph.

11
00:01:30,409 --> 00:01:38,655
The two and the one will label the edges coming from X1 and X 2 respectively, and the bias unit minus 18 will label the node.

12
00:01:38,655 --> 00:01:48,165
Thus, when we see a node with these labels, we can think of the linear equation they generate. Another way to grab this node is to consider the bias as part of the input.

13
00:01:48,165 --> 00:01:54,609
Now since W1 gets multiplied by X1 and W2 by X2, It's natural to think that B gets multiplied by a one.

14
00:01:54,609 --> 00:02:05,760
So we'll have the B labeling and and edge coming from a one. Then what the node does is it multiplies the values coming from the incoming nodes by the values and the corresponding edges.

15
00:02:05,760 --> 00:02:18,924
Then it adds them and finally, it checks if the result is greater that are equal to zero. If it is, then the node returns a yes or a value of one, and if it isn't then the node returns a no or a value of zero.

16
00:02:18,925 --> 00:02:23,474
We'll be using both notations throughout this class although the second one will be used more often.

17
00:02:23,474 --> 00:02:42,179
In the general case, this is how the nodes look. We will have our node over here then end inputs coming in with values X1 up to Xn and one, and edges with weights W1 up to Wn, and B corresponding to the bias unit.

18
00:02:42,180 --> 00:02:53,545
And then the node calculates the linear equation Wx plus B, which is a summation from I equals one to n, of WIXI plus B.

19
00:02:53,545 --> 00:03:05,844
This node then checks if the value is zero or bigger, and if it is, then the node returns a value of one for yes and if not, then it returns a value of zero for no.

20
00:03:05,844 --> 00:03:16,919
Note that we're using an implicit function, here, which is called a step function. What the step function does is it returns a one if the input is positive or zero, and a zero if the input is negative.

21
00:03:16,919 --> 00:03:28,782
So in reality, these perceptrons can be seen as a combination of nodes, where the first node calculates a linear equation and the inputs on the weights, and the second node applies the step function to the result.

22
00:03:28,782 --> 00:03:37,305
These can be graphed as follows: the summation sign represents a linear function in the first node, and the drawing represents a step function in the second node.

23
00:03:37,305 --> 00:03:43,385
In the future, we will use different step functions. So this is why it's useful to specify it in the node.

24
00:03:43,384 --> 00:03:54,370
So as we've seen there are two ways to represent perceptions. The one on the left has a bias unit coming from an input node with a value of one, and the one in the right has the bias inside the node.


@@@
1
00:00:00,000 --> 00:00:08,759
So you may be wondering why are these objects called neural networks. Well, the reason why they're called neural networks is because perceptions kind of look like neurons in the brain.

2
00:00:08,759 --> 00:00:20,504
In the left we have a perception with four inputs. The number is one, zero, four, and minus two. And what the perception does, it calculates some equations on the input and decides to return a one or a zero.

3
00:00:20,504 --> 00:00:27,058
In a similar way neurons in the brain take inputs coming from the dendrites. These inputs are nervous impulses.

4
00:00:27,059 --> 00:00:35,054
So what the neuron does is it does something with the nervous impulses and then it decides if it outputs a nervous impulse or not through the axon.

5
00:00:35,054 --> 00:00:46,130
The way we'll create neural networks later in this lesson is by concatenating these perceptions so we'll be mimicking the way the brain connects neurons by taking the output from one and turning it into the input for another one.


@@@
1
00:00:00,000 --> 00:00:08,980
So we had a question we're trying to answer and the question is, how do we find this line that separates the blue points from the red points in the best possible way?

2
00:00:08,980 --> 00:00:14,663
Let's answer this question by first looking at a small example with three blue points and three red points.

3
00:00:14,663 --> 00:00:21,980
And we're going to describe an algorithm that will find the line that splits these points properly. So the computer doesn't know where to start.

4
00:00:21,980 --> 00:00:31,609
It might as well start at a random place by picking a random linear equation. This equation will define a line and a positive and negative area given in blue and red respectively.

5
00:00:31,609 --> 00:00:39,399
What we're going to do is to look at how badly this line is doing and then move it around to try to get better and better.

6
00:00:39,399 --> 00:00:49,344
Now the question is, how do we find how badly this line is doing? So let's ask all the points. Here we have four points that are correctly classified.

7
00:00:49,344 --> 00:01:04,409
They are these two blue points in the blue area and these two red points in the red area. And these points are correctly classified, so they say, "I'm good." And then we have these two points that are incorrectly classified.

8
00:01:04,409 --> 00:01:15,754
That's this red point in the blue area and this blue point in the red area. We want to get as much information from them so we want them to tell us something so that we can improve this line.

9
00:01:15,754 --> 00:01:22,560
So what is it that they can tell us? So here we have a misclassified point, this red point in the blue area.

10
00:01:22,560 --> 00:01:31,084
Now think about this. If you were this point, what would you tell the line to do? Would you like it to come closer to you or farther from you?

11
00:01:31,084 --> 00:01:37,000
That's our quiz. Will the misclassified point want the line to come closer to it or farther from it?


@@@
1
00:00:00,000 --> 00:00:06,509
Well, consider this. If you're in the wrong area, you would like the line to go over you, in order to be in the right area.

2
00:00:06,509 --> 00:00:12,000
Thus, the points just come closer! So the line can move towards it and eventually classify it correctly.


@@@
1
00:00:00,000 --> 00:00:09,019
Now, let me show you a trick that will make a line go closer to a point. Let's say we have our linear equation for example, 3x1 + 4x2 -10.

2
00:00:09,019 --> 00:00:15,054
And that linear equation gives us a line which is the points where the equation is zero and two regions.

3
00:00:15,054 --> 00:00:26,750
The positive region drawn in blue where 3x1 + 4x2 - 10 is positive, and the negative region drawn in red with 3x1 + 4x2 - 10 is negative.

4
00:00:26,750 --> 00:00:35,039
So here we have our lonely misclassified point, the 0.4, 5 which is a red point in the blue area, and the point has to come closer.

5
00:00:35,039 --> 00:00:48,195
So how do we get that point to come closer to the line? Well, the idea is we're going to take the four and five and use them to modify the equation of the line in order to get the line to move closer to the point.

6
00:00:48,195 --> 00:00:57,640
So here are parameters of the line 3, 4 and -10 and the coordinates of the point are 4 and 5, and let's also add a one here for the bias unit.

7
00:00:57,640 --> 00:01:06,003
So what we'll do is subtract these numbers from the parameters of the line to get 3 - 4, 4 - 5, and -10 -1.

8
00:01:06,004 --> 00:01:19,009
The new line will have parameters -1, -1, -11. And this line will move drastically towards the point, possibly even going over it and placing it in the correct area.

9
00:01:19,010 --> 00:01:26,659
Now, since we have a lot of other points, we don't want to make any drastic moves since we may accidentally misclassify all our other points.

10
00:01:26,659 --> 00:01:33,099
We want the line to make a small move towards that point and for this, we need to take small steps towards the point.

11
00:01:33,099 --> 00:01:51,109
So here's where we introduce the learning rate, the learning rate is a small number for example, 0.1 and what we'll do is instead of subtracting four, five and one from the coordinates of the line, we'll multiply these numbers by 0.1 and then subtract them from the equation of the line.

12
00:01:51,109 --> 00:02:04,714
This means we'll be subtracting 0.4, 0.5, and 0.1 from the equation of the line. Obtaining a new equation of 2.6x1 + 3.5x 2 - 10.1 = 0.

13
00:02:04,715 --> 00:02:16,393
This new line will actually move closer to the point. In the same way, if we have a blue point in the red area, for example, the point 1,1 is a positively labeled point in the negative area.

14
00:02:16,393 --> 00:02:27,110
This point is also misclassified and it says, come closer. So what do we do here is the same thing, except now instead of subtracting the coordinates to the parameters of the line, we add them.

15
00:02:27,110 --> 00:02:40,500
Again, we multiply by the learning rate in order to make small steps. So here we take the coordinates of the point 1,1 and put an extra one for the constant term and now, we multiply them by the learning rates 0.1.

16
00:02:40,500 --> 00:02:50,640
Now, we add them to the parameters of the line and we get a new line with equation 3.1x1 + 4.1x2 - 9.9.

17
00:02:50,639 --> 00:02:59,000
And magic, this line is closer to the point. So that's the trick we're going to use repeatedly for the Perceptron Algorithm.


@@@
1
00:00:00,000 --> 00:00:11,494
Now, we finally have all the tools for describing the perceptron algorithm. We start with the random equation, which will determine some line, and two regions, the positive and the negative region.

2
00:00:11,494 --> 00:00:17,265
Now, we'll move this line around to get a better and better fit. So, we ask all the points how they're doing.

3
00:00:17,265 --> 00:00:31,484
The four correctly classified points say, "I'm good." And the two incorrectly classified points say, "Come closer." So, let's listen to the point in the right, and apply the trick to make the line closer to this point.

4
00:00:31,484 --> 00:00:45,094
So, here it is. Now, this point is good. Now, let's listen to the point in the left. The points says, "Come closer." We apply the trick, and now the line goes closer to it, and it actually goes over it classifying correctly.

5
00:00:45,094 --> 00:00:52,670
Now, every point is correctly classified and happy. So, let's actually write the pseudocode for this perceptron algorithm.

6
00:00:52,670 --> 00:01:02,004
We start with random weights, w1 up to wn and b. This gives us the question wx plus b, the line, and the positive and negative areas.

7
00:01:02,005 --> 00:01:23,664
Now, for every misclassified point with coordinates x1 up to xn, we do the following. If the prediction was zero, which means the point is a positive point in the negative area, then we'll update the weights as follows: for i equals 1 to n, we change wi, to wi plus alpha times xi, where alpha is the learning rate.

8
00:01:23,665 --> 00:01:33,840
In this case, we're using 0.1. Sometimes, we use 0.01 etc. It depends. Then we also change the bi as unit to b plus alpha.

9
00:01:33,840 --> 00:01:46,950
That moves the line closer to the misclassified point. Now, if the prediction was one, which means a point is a negative point in the positive area, then we'll update the weights in a similar way, except we subtract instead of adding.

10
00:01:46,950 --> 00:01:57,995
This means for i equals 1, change wi, to wi minus alpha xi, and change the bi as unit b to b minus alpha.

11
00:01:57,995 --> 00:02:07,425
And now, the line moves closer to our misclassified point. And now, we just repeat this step until we get no errors, or until we have a number of error that is small.

12
00:02:07,424 --> 00:02:14,000
Or simply we can just say, do the step a thousand times and stop. We'll see what are our options later in the class.


@@@
1
00:00:00,000 --> 00:00:08,905
Okay, so let's look more carefully at this model for accepting and rejecting students. Let's say we have this student four, who got nine in the test, but only one on the grades.

2
00:00:08,906 --> 00:00:15,525
According to our model this student gets accepted since it's placed over here in the positive region of this line.

3
00:00:15,525 --> 00:00:22,190
But let's say we don't want that since we'll say, "If your grades were terrible, no matter what you got on the test, you won't get accepted".

4
00:00:22,190 --> 00:00:32,329
So our data should look more like this instead. This model is much more realistic but now we have a problem which is the data can no longer be separated by just a line.

5
00:00:32,329 --> 00:00:40,574
So what is the next thing after a line? Maybe a circle. A circle would work. Maybe two lines. That could work, too.

6
00:00:40,575 --> 00:00:49,795
Or maybe a curve like this. That would also work. So let's go with that. Let's go with the curve. Now, unfortunately, the perceptron algorithm won't work for us this time.

7
00:00:49,795 --> 00:01:00,000
We'll have to come up with something more complex and actually the solution will be, we need to redefine our perceptron algorithm for a line in a way that it'll generalize to other types of curves.


@@@
1
00:00:00,000 --> 00:00:09,419
So the way we'll solve our problems from now on is with the help of an error function. An error function is simply something that tells us how far we are from the solution.

2
00:00:09,419 --> 00:00:18,114
For example, if I'm here and my goal is to get to this plant, an error function will just tell me the distance from the plant.

3
00:00:18,114 --> 00:00:28,875
My approach would then be to look around myself, check in which direction I can take a step to get closer to the plant, take that step and then repeat.


@@@
1
00:00:00,000 --> 00:00:12,425
Here is obvious realization of the error function. We're standing on top a mountain, Mount Errorest and I want to descend but it's not that easy because it's cloudy and the mountain is very big, so we can't really see the big picture.

2
00:00:12,425 --> 00:00:18,234
What we'll do to go down is we'll look around us and we consider all the possible directions in which we can walk.

3
00:00:18,234 --> 00:00:26,100
Then we pick a direction that makes us descend the most. Let's say it's this one over here. So we take a step in that direction.

4
00:00:26,100 --> 00:00:39,210
Thus, we've decreased the height. Once we take the step and we start the process again and again always decreasing the height until we go all the way down the mountain, minimizing the height.

5
00:00:39,210 --> 00:00:45,533
In this case the key metric that we use to solve the problem is the height. We'll call the height the error.

6
00:00:45,533 --> 00:00:51,234
The error is what's telling us how badly we're doing at the moment and how far we are from an ideal solution.

7
00:00:51,234 --> 00:00:57,715
And if we constantly take steps to decrease the error then we'll eventually solve our problem, descending from Mt.

8
00:00:57,715 --> 00:01:06,968
Errorest. Some of you may be thinking, wait, that doesn't necessarily solve the problem. What if I get stuck in a valley, a local minimum, but that's not the bottom of the mountain.

9
00:01:06,968 --> 00:01:11,870
This happens a lot in machine learning and we'll see different ways to solve it later in this Nanodegree.

10
00:01:11,870 --> 00:01:18,129
It's also worth noting that many times a local minimum will give us a pretty good solution to a problem.

11
00:01:18,129 --> 00:01:25,424
This method, which we'll study in more detail later, is called gradient descent. So let's try that approach to solve a problem.

12
00:01:25,424 --> 00:01:30,634
What would be a good error function here? What would be a good way to tell the computer how badly it's doing?

13
00:01:30,634 --> 00:01:38,385
Well, here's our line with our positive and negative area. And the question is how do we tell the computer how far it is from a perfect solution?

14
00:01:38,385 --> 00:01:44,810
Well, maybe we can count the number mistakes. There are two mistakes here. So that's our height. That's our error.

15
00:01:44,810 --> 00:01:52,665
So just as we did to descend from the mountain, we look around all the directions in which we can move the line in order to decrease our error.

16
00:01:52,665 --> 00:02:00,810
So let's say we move in this direction. We'll decrease the number of errors to one and then if we're moving in that direction, we'll decrease the number of errors to zero.

17
00:02:00,810 --> 00:02:14,990
And then we're done, right? Well, almost. There's a small problem with that approach. In our algorithms we'll be taking very small steps and the reason for that is calculus, because our tiny steps will be calculated by derivatives.

18
00:02:14,990 --> 00:02:23,631
So what happens if we take very small steps here? We start with two errors and then move a tiny amount and we're still at two errors.

19
00:02:23,631 --> 00:02:30,784
Then move a tiny amount again and we're still two errors. Another tiny amount and we're still at two and again and again.

20
00:02:30,783 --> 00:02:37,900
So not much we can do here. This is equivalent to using gradient descent to try to descend from an Aztec pyramid with flat steps.

21
00:02:37,900 --> 00:02:46,909
If we're standing here in the second floor, for the two errors and we look around ourselves, we'll always see two errors and we'll get confused and not know what to do.

22
00:02:46,908 --> 00:02:56,224
On the other hand in Mt. Errorest we can detect very small variations in height and we can figure out in what direction it can decrease the most.

23
00:02:56,223 --> 00:03:04,818
In math terms this means that in order for us to do gradient descent our error function can not be discrete, it should be continuous.

24
00:03:04,818 --> 00:03:15,699
Mt. Errorest is continuous since small variations in our position will translate to small variations in the height but the Aztec pyramid does not since the high jumps from two to one and then from one to zero.

25
00:03:15,699 --> 00:03:26,484
As a matter of fact, our error function needs to be differentiable, but we'll see that later. So, what we need to do here is to construct an error function that is continuous and we'll do this as follows.

26
00:03:26,485 --> 00:03:40,710
So here are six points with four of them correctly classified, that's two blue and two red, and two of them incorrectly classified, that is this red point at the very left and this blue point at the very right.

27
00:03:40,710 --> 00:03:50,150
The error function is going to assign a large penalty to the two incorrectly classified points and small penalties to the four correctly classified points.

28
00:03:50,150 --> 00:04:02,938
Here we are representing the size of the point as the penalty. The penalty is roughly the distance from the boundary when the point is misclassified and almost zero when the point is correctly classified.

29
00:04:02,938 --> 00:04:10,729
We'll learn the formula for the error later in the class. So, now we obtain the total error by adding all the errors from the corresponding points.

30
00:04:10,729 --> 00:04:19,509
Here we have a large number so it is two misclassified points add a large amount to the error. And the idea now is to move the line around in order to decrease these error.

31
00:04:19,509 --> 00:04:28,129
But now we can do it because we can make very tiny changes to the parameters of the line which will amount to very tiny changes in the error function.

32
00:04:28,129 --> 00:04:43,360
So, if you move the line, say, in this direction, we can see that some errors decrease, some slightly increase, but in general when we consider the sum, the sum gets smaller and we can see that because we've now correctly classified the two points that were misclassified before.

33
00:04:43,360 --> 00:04:51,240
So once we are able to build an error function with this property, we can now use gradient descent to solve our problem.

34
00:04:51,240 --> 00:04:57,810
So here's the full picture. Here we are at the summit of Mt. Errorest. We're quite high up because our error is large.

35
00:04:57,810 --> 00:05:14,629
As you can see the error is the height which is the sum of the blue and red areas. We explore around to see what direction brings us down the most, or equivalently, what direction can we move the line to reduce the error the most, and we take a step in that direction.

36
00:05:14,629 --> 00:05:22,684
So in the mountain we go down one step and in the graph we've reduced the error a bit by correctly classifying one of the points. And now we do it again.

37
00:05:22,684 --> 00:05:32,365
We calculate the error, we look around ourselves to see in what direction we descend the most, we take a step in that direction and that brings us down the mountain.

38
00:05:32,365 --> 00:05:45,430
So on the left we have reduced the height and successfully descended from the mountain and on the right we have reduced the error to its minimum possible value and successfully classified our points.


@@@
1
00:00:00,000 --> 00:00:10,814
In the last section we pointed out the difference between a discrete and a continuous error function and discovered that in order for us to use gradient descent we need a continuous error function.

2
00:00:10,814 --> 00:00:18,190
In order to do this we also need to move from discrete predictions to continuous predictions. Let me show you what I mean by that.


@@@
1
00:00:00,000 --> 00:00:06,220
The prediction is basically the answer we get from the algorithm. A discreet answer will be of the form yes, no.

2
00:00:06,219 --> 00:00:11,615
Whereas a continued answer will be a number, normally between zero and one which we'll consider a probability.

3
00:00:11,615 --> 00:00:25,679
In the running example, here we have our students where blue is accepted and red is rejected. And the discrete algorithm will tell us if a student is accepted or rejected by typing a zero for rejected students and a one for accepted students.

4
00:00:25,679 --> 00:00:32,505
On the other hand, the farther our point is from the black line, the more drastic these probabilities are.

5
00:00:32,505 --> 00:00:40,050
Points that are well into the blue area get very high probabilities, such as this point with an 85% probability of being blue.

6
00:00:40,049 --> 00:00:48,159
And points that are well into the red region are given very low probabilities, such as this point on the bottom that is given a 20% probability of being blue.

7
00:00:48,159 --> 00:00:57,335
The points over the line are all given a 50% probability of being blue. As you can see the probability is a function of the distance from the line.

8
00:00:57,335 --> 00:01:07,655
The way we move from discrete predictions to continuous, is to simply change your activation function from the step function in the left, to the sigmoid function on the right.

9
00:01:07,655 --> 00:01:14,629
The sigmoid function is simply a function which for large positive numbers will give us values very close to one.

10
00:01:14,629 --> 00:01:25,599
For large negative numbers will give us values very close to zero. And for numbers that are close to zero, it'll give you values that are close to point five.

11
00:01:25,599 --> 00:01:38,245
The formula is sigmoid effects equals σ(x) = 1/(1 + exp(-x)) So, before our model consisted of a line with a positive region and a negative region.

12
00:01:38,245 --> 00:01:47,969
Now it consists of an entire probability space or for each point in the plane we are given the probability that the label of the point is one for the blue points, and zero for the red points.

13
00:01:47,969 --> 00:02:02,975
For example, for this point the probability of being blue is 50% and of being red is 50%. For this point, the probabilities are 40% for being blue, and 60% for being red.

14
00:02:02,974 --> 00:02:14,655
For this one over here it's 30% for blue, and 70% for red. And for this point all over here is 80% for being blue and 25 percent for being red.

15
00:02:14,655 --> 00:02:22,800
The way we obtain this probability space is very simple. We just combine the linear function WX + b with the sigmoid function.

16
00:02:22,800 --> 00:02:32,630
So in the left we have the lines that represent the points for which WX + b is zero, one, two, minus one, minus two, etc.

17
00:02:32,629 --> 00:02:40,515
And once we apply the sigmoid function to each of these values in the plane, we then obtain numbers from zero to one for each point.

18
00:02:40,514 --> 00:02:50,454
These numbers are just the probabilities of the point being blue. The probability of the point being blue is a prediction of the model Y hat to sigmoid of W x plus b.

19
00:02:50,455 --> 00:02:58,419
Here we can see the lines for which the prediction is point five, point six, point seven, point four, point three, et cetera.

20
00:02:58,419 --> 00:03:11,115
As you can see, as we get more into the blue area, σ(Wx + b) gets closer and closer to one. And as we move into the red area, σ(Wx + b) gets closer and closer to zero.

21
00:03:11,115 --> 00:03:19,319
When we're over the main line, W x plus b is zero, which means sigmoid of W s plus b is exactly zero point five.

22
00:03:19,319 --> 00:03:29,944
So here on the left we have our old perceptron with the activation function as a step function. And on the right we have our new perceptron, where the activation function is the sigmoid function.

23
00:03:29,944 --> 00:03:40,935
What our new perceptron does, it takes the inputs, multiplies them by the weights in the edges and adds the results, then applies the sigmoid function.

24
00:03:40,935 --> 00:03:49,984
So instead of returning one and zero like before it returns values between zero and one such as 0.99 or 0.67 etc.

25
00:03:49,985 --> 00:03:58,000
Before it used to say the student got accepted or not, and now it says the probability of the student got accepted is this much.


@@@
1
00:00:00,000 --> 00:00:07,099
So far we have models that give us an answer of yes/no or the probability of a label being positive or negative.

2
00:00:07,099 --> 00:00:14,160
What if we have more classes? What if we want our model to tell us if something is red, blue, yellow or dog, cat, bird?


@@@
1
00:00:00,000 --> 00:00:06,365
Let's switch to a different example for a moment. Let's say we have a model that will predict if you receive a gift or not.

2
00:00:06,365 --> 00:00:18,759
So, the model use predictions in the following way. It says, the probability that you get a gift is 0.8, which automatically implies that the probability that you don't receive a gift is 0.2.

3
00:00:18,760 --> 00:00:26,064
And what does the model do? What the model does is take some inputs. For example, is it your birthday or have it been good all year?

4
00:00:26,065 --> 00:00:37,350
And based on those inputs, it calculates a linear model which would be the score. Then, the probability that you get the gift or not is simply the sigmoid function applied to that score.

5
00:00:37,350 --> 00:00:49,890
Now, what if you had more options than just getting a gift or not a gift? Let's say we have a model that just tell us what animal we just saw, and the options are a duck, a beaver and a walrus.

6
00:00:49,890 --> 00:01:02,689
We want a model that tells an answer along the lines of, the probability of a duck is 0.67, the probability of a beaver is 0.24, and the probability of a walrus is 0.09.

7
00:01:02,689 --> 00:01:09,377
Notice that the probabilities need to add to one. Let's say we have a linear model based on some inputs.

8
00:01:09,378 --> 00:01:17,599
The inputs could be, does it have a beak or not? Number of teeth. Number of feathers. Hair, no hair. Does it live in the water? Does it fly?

9
00:01:17,599 --> 00:01:30,975
Etc. We calculate linear function based on those inputs, and let's say we get some scores. So, the duck gets a score of two, and the beaver gets a score of one, and the walrus gets a score of zero.

10
00:01:30,974 --> 00:01:41,539
And now the question is, how do we turn these scores into probabilities? The first thing we need to satisfy with probabilities is as we said, they need to add to one.

11
00:01:41,540 --> 00:02:02,150
So the two, the one, and the zero do not add to one. The second thing we need to satisfy is, since the duck had a higher score than the beaver and the beaver had a higher score than the walrus, then we want the probability of the duck to be higher than the probability of the beaver, and the probability of the beaver to be higher than the probability of the walrus.

12
00:02:02,150 --> 00:02:20,644
Here's a simple way of doing it. Let's take each score and divide it by the sum of all the scores. The two becomes two divided by two plus one plus zero, the one becomes one divided by two plus one plus zero, and the zero becomes zero divided by two plus one plus zero.

13
00:02:20,645 --> 00:02:29,419
This kind of works because the probabilities we obtain are two thirds for the duck, one third for the beaver, and zero for the walrus.

14
00:02:29,419 --> 00:02:35,610
That works but there's a little problem. Let's think about it. What could this problem be? The problem is the following.

15
00:02:35,610 --> 00:02:43,250
What happens if our scores are negative? This is completely plausible since the scores are linear function which could give negative values.

16
00:02:43,250 --> 00:02:55,930
What if we had, say, scores of 1, 0 and (-1)? Then, one of the probabilities would turn into one divided by one plus zero plus minus one which is zero, and we know very well that we cannot divide by zero.

17
00:02:55,930 --> 00:03:06,694
This unfortunately won't work, but the idea is good. How can we turn this idea into one that works all the time even for negative numbers?

18
00:03:06,694 --> 00:03:14,299
Well, it's almost like we need to turn these scores into positive scores. How do we do this? Is there a function that can help us?

19
00:03:14,300 --> 00:03:26,219
This is the quiz. Let's look at some options. There's sine, cosine, logarithm, and exponential. Quiz. Which one of these functions will turn every number into a positive number?


@@@
1
00:00:00,000 --> 00:00:06,115
So, if you said exponential, you are correct. Because this is a function that returns a positive number for every input.

2
00:00:06,115 --> 00:00:15,089
E to the X is always a positive number. So, what we're going to do is exactly what we did before, except, applying it to the X to the scores.

3
00:00:15,089 --> 00:00:25,774
So, instead of 2,1, 0, we have E to the 2, E to the 1 and E to the 0. So, that 2 becomes E to the 2 divided by E to the two plus E to the 1 plus E to the 0.

4
00:00:25,774 --> 00:00:36,910
And, similarly for 1 and 0. So, the probabilities we obtain now are as 0.67, 0.24 and 0.09. This clearly add to 1.

5
00:00:36,909 --> 00:00:43,009
And, also notice that since the exponential function is increasing, then the duck has a higher probability than the beaver.

6
00:00:43,009 --> 00:00:51,289
And this one has a higher probability than the walrus. This function is called the Softmax function and it's defined formally like this.

7
00:00:51,289 --> 00:00:59,005
Let's say we have N classes and a linear model that gives us the following scores. Z1, Z2, up to ZN.

8
00:00:59,005 --> 00:01:15,210
Each score for each of the classes. What we do to turn them into probabilities is to say the probability that the object is in class I is going to be E to the power of the ZI divided by the sum of E to the power of Z1 plus all the way to E to the power ZN.

9
00:01:15,209 --> 00:01:24,234
That's how we turn scores into probabilities. So, here's a question for you. When we had two classes, we applied the sigmoid function to the scores.

10
00:01:24,234 --> 00:01:33,424
Now, that we have more classes we apply the softmax function to the scores. The question is, is the softmax function for N equals to the same as the sigmoid function?

11
00:01:33,424 --> 00:01:39,000
I'll let you think about it. The answer is actually, yes, but it's not super trivial why. And, it's a nice thing to remember.


@@@
1
00:00:00,000 --> 00:00:12,600
So, as we've seen so far, all our algorithms are numerical. This means we need to input numbers, such as a score in a test or the grades, but the input data will not always look like numbers.

2
00:00:12,599 --> 00:00:18,640
Sometimes it looks like this. Let's say the module receives as an input the fact that you got a gift or didn't get a gift.

3
00:00:18,640 --> 00:00:24,994
How do we turn that into numbers? Well, that's easy. If you've got a gift, we'll just say that the input variable is 1.

4
00:00:24,995 --> 00:00:34,515
And, if you didn't get a gift, we'll just say that the input variable is 0. But, what if we have more classes as before or, let's say, our classes are Duck, Beaver and Walrus?

5
00:00:34,515 --> 00:00:45,287
What variable do we input in the algorithm? Maybe, we can input a 0 or 1 and a 2, but that would not work because it would assume dependencies between the classes that we can't have. So, this is what we do.

6
00:00:45,287 --> 00:00:50,739
What we do is, we come up with one variable for each of the classes. So, our table becomes like this.

7
00:00:50,740 --> 00:00:58,109
That's one variable for Duck, one for Beaver and one for Walrus. And, each one has its corresponding column.

8
00:00:58,109 --> 00:01:06,780
Now, if the input is a duck then the variable for duck is 1 and the variables for beaver and walrus are 0.

9
00:01:06,780 --> 00:01:14,770
Similarly for the beaver and the walrus. We may have more columns of data but at least there are no unnecessary dependencies.


@@@
1
00:00:00,000 --> 00:00:05,525
So we're still in our quest for an algorithm that will help us pick the best model that separates our data.

2
00:00:05,525 --> 00:00:12,894
Well, since we're dealing with probabilities then let's use them in our favor. Let's say I'm a student and I have two models.

3
00:00:12,894 --> 00:00:20,859
One that tells me that my probability of getting accepted is 80% and one that tells me the probability is 55%.

4
00:00:20,859 --> 00:00:28,210
Which model looks more accurate? Well, if I got accepted then I'd say the better model is probably the one that says 80%.

5
00:00:28,210 --> 00:00:33,983
What if I didn't get accepted? Then the more accurate model is more likely the one that says 55 percent.

6
00:00:33,984 --> 00:00:47,085
But I'm just one person. What if it was me and a friend? Well, the best model would more likely be the one that gives the higher probabilities to the events that happened to us, whether it's acceptance or rejection.

7
00:00:47,085 --> 00:00:58,353
This sounds pretty intuitive. The method is called maximum likelihood. What we do is we pick the model that gives the existing labels the highest probability.


@@@
1
00:00:00,000 --> 00:00:10,740
So let me be more specific. Let's look at the following four points: two blue and two red and two models that classify them, the one on the left and the one on the right.

2
00:00:10,740 --> 00:00:26,625
Quick. Which model looks better? You are correct. The model on the right is much better since it classifies the four points correctly whereas the model in the left gets two points correctly and two points incorrectly.

3
00:00:26,625 --> 00:00:37,289
But let's see why the model in the right is better from the probability perspective. And by that, we'll show you that the arrangement in the right is much more likely to happen than the one in the left.

4
00:00:37,289 --> 00:00:48,240
So let's recall that our prediction is ŷ = σ(Wx+b) and that that is precisely the probability of a point being labeled positive which means blue.

5
00:00:48,240 --> 00:00:56,835
So for the points in the figure, let's say the model tells you that the probability of being blue are 0.9, 0.6, 0.3, and 0.2.

6
00:00:56,835 --> 00:01:03,649
Notice that the points in the blue region are much more likely to be blue and the points in the red region are much less likely to be blue.

7
00:01:03,649 --> 00:01:16,319
Now obviously, the probability of being red is one minus the probability of being blue. So in this case, the probability of some of the points being red are 0.1, 0.4, 0.7 and 0.8.

8
00:01:16,319 --> 00:01:23,844
Now what we want to do is we want to calculate the probability of the four points are of the colors that they actually are.

9
00:01:23,843 --> 00:01:39,290
This means the probability that the two red points are red and that the two blue points are blue. Now if we assume that the colors of the points are independent events then the probability for the whole arrangement is the product of the probabilities of the four points.

10
00:01:39,290 --> 00:02:00,388
This is equal to 0.1 × 0.6 × 0.7 × 0.2 = 0.0084. This is very small. It's less than 1%. What we mean by this is that if the model is given by these probability spaces, then the probability that the points are of these colors is 0.0084.

11
00:02:00,388 --> 00:02:10,485
Now let's do this for both models. As we saw the model on the left tells us that the probabilities of these points being of those colors is 0.0084.

12
00:02:10,485 --> 00:02:23,259
If we do the same thing for the model on the right. Let's say we get that the probabilities of the two points in the right being blue are 0.7 and 0.9 and of the two points in the left being red are 0.8 and 0.6.

13
00:02:23,258 --> 00:02:39,478
When we multiply these we get 0.3024 which is around 30%. This is much higher than 0.0084. Thus, we confirm that the model on the right is better because it makes the arrangement of the points much more likely to have those colors.

14
00:02:39,479 --> 00:02:50,780
So now, what we do is the following? We start from the bad modeling, calculate the probability that the points are those colors, multiply them and we obtain the total probability is 0.0084.

15
00:02:50,780 --> 00:03:03,104
Now if we just had a way to maximize this probability we can increase it all the way to 0.3024. Thus, our new goal becomes precisely that, to maximize this probability.


@@@
1
00:00:00,000 --> 00:00:08,969
Well we're getting somewhere now. We've concluded that the probability is important. And that the better model will give us a better probability.

2
00:00:08,970 --> 00:00:20,829
Now the question is, how we maximize the probability. Also, if remember correctly we're talking about an error function and how minimizing this error function will take us to the best possible solution.

3
00:00:20,829 --> 00:00:34,000
Could these two things be connected? Could we obtain an error function from the probability? Could it be that maximizing the probability is equivalent to minimizing the error function? Maybe.


@@@
1
00:00:00,000 --> 00:00:11,730
So a quick recap. We have two models, the bad one on the left and the good one on the right. And the way to tell they're bad or good is to calculate the probability of each point being the color it is according to the model.

2
00:00:11,730 --> 00:00:20,960
Multiply these probabilities in order to obtain the probability of the whole arrangement and then check that the model on the right gives us a much higher probability than the model on the left.

3
00:00:20,960 --> 00:00:29,690
Now all we need to do is to maximize this probability. But probability is a product of numbers and products are hard.

4
00:00:29,690 --> 00:00:35,642
Maybe this product of four numbers doesn't look so scary. But what if we have thousands of datapoints?

5
00:00:35,642 --> 00:00:50,545
That would correspond to a product of thousands of numbers, all of them between zero and one. This product would be very tiny, something like 0.0000 something and we definitely want to stay away from those numbers.

6
00:00:50,545 --> 00:00:57,630
Also, if I have a product of thousands of numbers and I change one of them, the product will change drastically.

7
00:00:57,630 --> 00:01:07,265
In summary, we really want to stay away from products. And what's better than products? Well, let's ask our friend here.

8
00:01:07,265 --> 00:01:19,939
Products are bad, but sums are good. Let's do sums. So let's try to turn these products into sums. We need to find a function that will help us turn products into sums.

9
00:01:19,938 --> 00:01:26,745
What would this function be? It sounds like it's time for a quiz. Quiz. Which function will help us out here?


@@@
1
00:00:00,000 --> 00:00:11,929
Correct. The answer is logarithm, because logarithm has this very nice identity that says that the logarithm of the product A times B is the sum of the logarithms of A and B.

2
00:00:11,929 --> 00:00:21,854
So this is what we do. We take our products and we take the logarithms, so now we get a sum of the logarithms of the factors.

3
00:00:21,855 --> 00:00:40,040
So the ln(0.6*0.2*0.1*0.7) is equal to ln(0.6) + ln(0.2) + ln(0.1) + ln(0.7) etc. Now from now until the end of class, we'll be taking the natural logarithm which is base e instead of 10.

4
00:00:40,039 --> 00:00:44,945
Nothing different happens with base 10. Everything works the same as everything gets scaled by the same factor.

5
00:00:44,945 --> 00:00:58,164
So it's just more for convention. We can calculate those values and get minus 0.51, minus 1.61, minus 0.23 etc. Notice that they are all negative numbers and that actually makes sense.

6
00:00:58,164 --> 00:01:05,594
This is because the logarithm of a number between 0 and 1 is always a negative number since the logarithm of one is zero.

7
00:01:05,594 --> 00:01:11,260
So it actually makes sense to think of the negative of the logarithm of the probabilities and we'll get positive numbers.

8
00:01:11,260 --> 00:01:23,180
So that's what we'll do. We'll take the negative of the logarithm of the probabilities. That sums up negatives of logarithms of the probabilities, we'll call the cross entropy which is a very important concept in the class.

9
00:01:23,180 --> 00:01:30,255
If we calculate the cross entropies, we see that the bad model on left has a cross entropy 4.8 which is high.

10
00:01:30,254 --> 00:01:37,454
Whereas the good model on the right has a cross entropy of 1.2 which is low. This actually happens all the time.

11
00:01:37,454 --> 00:01:52,599
A good model will give us a low cross entropy and a bad model will give us a high cross entropy. The reason for this is simply that a good model gives us a high probability and the negative of the logarithm of a large number is a small number and vice versa.

12
00:01:52,599 --> 00:02:01,470
This method is actually much more powerful than we think. If we calculate the probabilities and pair the points with the corresponding logarithms, we actually get an error for each point.

13
00:02:01,469 --> 00:02:17,859
So again, here we have probabilities for both models and the products of them. Now, we take the negative of the logarithms which gives us sum of logarithms and if we pair each logarithm with the point where it came from, we actually get a value for each point.

14
00:02:17,860 --> 00:02:36,544
And if we calculate the values, we get this. Check it out. If we look carefully at the values we can see that the points that are mis-classified has like values like 2.3 for this point or 1.6 one for this point, whereas the points that are correctly classified have small values.

15
00:02:36,544 --> 00:02:46,915
And the reason for this is again is that a correctly classified point will have a probability that as close to 1, which when we take the negative of the logarithm, we'll get a small value.

16
00:02:46,914 --> 00:02:57,594
Thus we can think of the negatives of these logarithms as errors at each point. Points that are correctly classified will have small errors and points that are mis-classified will have large errors.

17
00:02:57,594 --> 00:03:12,580
And now we've concluded that our cross entropy will tell us if a model is good or bad. So now our goal has changed from maximizing a probability to minimizing a cross entropy in order to get from the model in left to the model in the right.


@@@
1
00:00:00,000 --> 00:00:13,350
So this cross entropy, it looks like kind of a big deal. Cross entropy really says the following. If I have a bunch of events and a bunch of probabilities, how likely is it that those events happen based on the probabilities?

2
00:00:13,349 --> 00:00:21,299
If it's very likely, then we have a small cross entropy. If it's unlikely, then we have a large cross entropy. Let's elaborate.


@@@
1
00:00:02,440 --> 00:00:08,935
Let's look a bit closer into Cross-Entropy by switching to a different example. Let's say we have three doors.

2
00:00:08,935 --> 00:00:18,720
And no this is not the Monty Hall problem. We have the green door, the red door, and the blue door, and behind each door we could have a gift or not have a gift.

3
00:00:18,720 --> 00:00:26,900
And the probabilities of there being a gift behind each door is 0.8 for the first one, 0.7 for the second one, 0.1 for the third one.

4
00:00:26,900 --> 00:00:36,780
So for example behind the green door there is an 80 percent probability of there being a gift, and a 20 percent probability of there not being a gift.

5
00:00:36,780 --> 00:00:46,630
So we can put the information in this table where the probabilities of there being a gift are given in the top row, and the probabilities of there not being a gift are given in the bottom row.

6
00:00:46,630 --> 00:00:53,375
So let's say we want to make a bet on the outcomes. So we want to try to figure out what is the most likely scenario here.

7
00:00:53,375 --> 00:01:03,440
And for that we'll assume they're independent events. In this case, the most likely scenario is just obtained by picking the largest probability in each column.

8
00:01:03,440 --> 00:01:09,230
So for the first door is more likely to have a gift than not have a gift. So we'll say there's a gift behind the first door.

9
00:01:09,230 --> 00:01:14,995
For the second door, it's also more likely that there's a gift. So we'll say there's a gift behind the second door.

10
00:01:14,995 --> 00:01:21,015
And for the third door it's much more likely that there's no gift, so we'll say there's no gift behind the third door.

11
00:01:21,015 --> 00:01:36,665
And as the events are independent, the probability for this whole arrangement is the product of the three probabilities which is 0.8, times 0.7, times 0.9, which ends up being 0.504, which is roughly 50 percent.

12
00:01:36,665 --> 00:01:48,815
So let's look at all the possible scenarios in the table. Here's a table with all the possible scenarios for each door and there are eight scenarios since each door gives us two possibilities each, and there are three doors.

13
00:01:48,815 --> 00:01:57,245
So we do as before to obtain the probability of each arrangement by multiplying the three independent probabilities to get these numbers.

14
00:01:57,245 --> 00:02:05,955
You can check that these numbers add to one. And from last video we learned that the negative of the logarithm of the probabilities across entropy.

15
00:02:05,955 --> 00:02:16,345
So let's go ahead and calculate the cross-entropy. And notice that the events with high probability have low cross-entropy and the events with low probability have high cross-entropy.

16
00:02:16,345 --> 00:02:34,441
For example, the second row which has probability of 0.504 gives a small cross-entropy of 0.69, and the second to last row which is very very unlikely has a probability of 0.006 gives a cross entropy a 5.12.

17
00:02:34,441 --> 00:02:46,445
So let's actually calculate a formula for the cross-entropy. Here we have our three doors, and our sample scenario said that there is a gift behind the first and second doors, and no gift behind the third door.

18
00:02:46,445 --> 00:02:57,915
Recall that the probabilities of these events happening are 0.8 for a gift behind the first door, 0.7 for a gift behind the second door, and 0.9 for no gift behind the third door.

19
00:02:57,915 --> 00:03:14,070
So when we calculate the cross-entropy, we get the negative of the logarithm of the product, which is a sum of the negatives of the logarithms of the factors, which is negative logarithm of 0.8 minus logarithm of 0.7 minus logarithm 0.9.

20
00:03:14,070 --> 00:03:27,940
And in order to drive the formula we'll have some variables. So let's call P1 the probability that there's a gift behind the first door, P2 the probability there's a gift behind the second door, and P3 the probability there's a gift behind the third door.

21
00:03:27,940 --> 00:03:41,460
So this 0.8 here is P1, this 0.7 here is P2, and this 0.9 here is one minus P3. So it's a probability of there not being a gift is one minus the probability of there being a gift.

22
00:03:41,460 --> 00:03:49,750
Let's have another variable called Yi, which will be one of there's a present behind the ith door, and zero there's no present.

23
00:03:49,750 --> 00:04:00,210
So Yi is technically a number of presents behind the ith door. In this case Y1 equals one, Y2 equals one, and Y3 equals zero.

24
00:04:00,210 --> 00:04:08,305
So we can put all this together and derive a formula for the cross-entropy and it's this sum. Now let's look at the formula inside the summation.

25
00:04:08,305 --> 00:04:17,180
Noted that if there is a present behind the ith door, then Yi equals one. So the first term is logarithm of the Pi.

26
00:04:17,180 --> 00:04:28,355
And the second term is zero. Likewise, if there is no present behind the ith door, then Yi is zero. So this first term is zero.

27
00:04:28,355 --> 00:04:39,935
And this term is precisely logarithm of one minus Pi. Therefore, this formula really encompasses the sums of the negative of logarithms which is precisely the cross-entropy.

28
00:04:39,935 --> 00:04:53,485
So the cross-entropy really tells us when two vectors are similar or different. For example, if you calculate the cross entropy of the pair one one zero, and 0.8, 0.7, 0.1, we get 0.69.

29
00:04:53,485 --> 00:05:11,715
And that is low because one one zero is a similar vector to 0.8, 0.7, 0.1. Which means that the arrangement of gifts given by the first set of numbers is likely to happen based on the probabilities given by the second set of numbers.

30
00:05:11,715 --> 00:05:23,210
But on the other hand if we calculate the cross-entropy of the pairs zero zero one, and 0.8, 0.7, 0.1, that is 5.12 which is very high.

31
00:05:23,210 --> 00:05:32,030
This is because the arrangement of gifts being given by the first set of numbers is very unlikely to happen from the probabilities given by the second set of numbers.


@@@
1
00:00:00,000 --> 00:00:09,285
Now that was when we had two classes namely receiving a gift or not receiving a gift. What happens if we have more classes? Let's take a look.

2
00:00:09,285 --> 00:00:14,940
So we have a similar problem. We still have three doors. And this problem is still not the Monty Hall problem.

3
00:00:14,940 --> 00:00:23,575
Behind each door there can be an animal, and the animal can be of three types. It can be a duck, it can be a beaver, or it can be a walrus.

4
00:00:23,575 --> 00:00:39,080
So let's look at this table of probabilities. According to the first column on the table, behind the first door, the probability of finding a duck is 0.7, the probability of finding a beaver is 0.2, and the probability of finding a walrus is 0.1.

5
00:00:39,080 --> 00:00:45,745
Notice that the numbers in each column need to add to one because there is some animal behind door one.

6
00:00:45,745 --> 00:00:53,825
The numbers in the rows do not need to add to one as you can see. It could easly be that we have a duck behind every door and that's okay.

7
00:00:53,825 --> 00:01:04,805
So let's look at a sample scenario. Let's say we have our three doors, and behind the first door, there's a duck, behind the second door there's a walrus, and behind the third door there's also a walrus.

8
00:01:04,805 --> 00:01:18,925
Recall that the probabilities are again by the table. So a duck behind the first door is 0.7 likely, a walrus behind the second door is 0.3 likely, and a walrus behind the third door is 0.4 likely.

9
00:01:18,925 --> 00:01:27,900
So the probability of obtaining this three animals is the product of the probabilities of the three events since they are independent events, which in this case it's 0.084.

10
00:01:27,900 --> 00:01:37,065
And as we learn, that cross entropy here is given by the sums of the negatives of the logarithms of the probabilities.

11
00:01:37,065 --> 00:01:46,740
So the first one is negative logarithm of 0.7. The second one is negative logarithm of 0.3. And the third one is negative logarithm of 0.4.

12
00:01:46,740 --> 00:01:55,490
The Cross entropy's and the sum of these three which is actually 2.48. But we want a formula, so let's put some variables here.

13
00:01:55,490 --> 00:02:04,535
So P11 is the probability of finding a duck behind door one. P12 is the probability of finding a duck behind door two etc.

14
00:02:04,535 --> 00:02:19,285
And let's have the indicator variables Y1j D1 if there's a duck behind door J. Y2j B1 if there's a beaver behind door J, and Y3j B1 if there's a walrus behind door J.

15
00:02:19,285 --> 00:02:35,630
And these variables are zero otherwise. And so, the formula for the cross entropy is simply the negative of the summation from i_ equals_ one to n, up to summation from y_ equals_ j to m of Yij_ times_ the logarithm of Pij.

16
00:02:35,630 --> 00:02:48,555
In this case, m is a number of classes. This formula works because Yij being zero one, makes sure that we're only adding the logarithms of the probabilities of the events that actually have occurred.

17
00:02:48,555 --> 00:02:55,080
And voila, this is the formula for the cross entropy in more classes. Now I'm going to leave this equestion.

18
00:02:55,080 --> 00:03:04,240
Given that we have a formula for cross entropy for two classes and one for m classes. These formulas look different but are they the same for m_ equals_ two?

19
00:03:04,240 --> 00:03:11,000
Obviously the answer is yes, but it's a cool exercise to actually write them down and convince yourself that they are actually the same.


@@@
1
00:00:00,000 --> 00:00:08,934
So this is a good time for a quick recap of the last couple of lessons. Here we have two models. The bad model on the left and the good model on the right.

2
00:00:08,935 --> 00:00:19,259
And for each one of those we calculate the cross entropy which is the sum of the negatives of the logarithms off the probabilities of the points being their colors.

3
00:00:19,260 --> 00:00:29,269
And we conclude that the one on the right is better because a cross entropy is much smaller. So let's actually calculate the formula for the error function.

4
00:00:29,269 --> 00:00:42,480
Let's split into two cases. The first case being when y=1. So when the point is blue to begin with, the model tells us that the probability of being blue is the prediction y_hat.

5
00:00:42,479 --> 00:00:55,000
So for these two points the probabilities are 0.6 and 0.2. As we can see the point in the blue area has more probability of being blue than the point in the red area.

6
00:00:55,000 --> 00:01:04,010
And our error is simply the negative logarithm of this probability. So it's precisely minus logarithm of y_hat.

7
00:01:04,010 --> 00:01:17,585
In the figure it's minus logarithm of 0.6. and minus logarithm of 0.2. Now if y=0, so when the point is red, then we need to calculate the probability of the point being red.

8
00:01:17,584 --> 00:01:27,750
The probability of the point being red is one minus the probability of the point being blue which is precisely 1 minus the prediction y_hat.

9
00:01:27,750 --> 00:01:35,870
So the error is precisely the negative logarithm of this probability which is negative logarithm of 1 - y_hat.

10
00:01:35,870 --> 00:01:46,605
In this case we get negative logarithm 0.1 and negative logarithm 0.7. So we conclude that the error is a negative logarithm of y_hat if the point is blue.

11
00:01:46,605 --> 00:01:53,625
And negative logarithm of one - y_hat the point is red. We can summarize these two formulas into this one.

12
00:01:53,625 --> 00:02:16,495
Error = - (1-y)(ln( 1- y_hat)) - y ln(y_hat). Why does this formula work? Well because if the point is blue, then y=1 which means 1-y=0 which makes the first term 0 and the second term is simply logarithm of y_hat.

13
00:02:16,495 --> 00:02:27,680
Similarly, if the point is red then y=0. So the second term of the formula is 0 and the first one is logarithm of 1- y_hat.

14
00:02:27,680 --> 00:02:35,510
Now the formula for the error function is simply the sum over all the error functions of points which is precisely the summation here.

15
00:02:35,509 --> 00:02:45,330
That's going to be this 4.8 we have over here. Now by convention we'll actually consider the average, not the sum which is where we are dividing by n over here.

16
00:02:45,330 --> 00:03:05,094
This will turn the 4.8 into a 1.2. From now on we'll use this formula as our error function. And now since y_hat is given by the sigmoid of the linear function wx + b, then the total formula for the error is actually in terms of w and b which are the weights of the model.

17
00:03:05,094 --> 00:03:14,449
And it's simply the summation we see here. In this case y_i is just the label of the point x_superscript_i.

18
00:03:14,449 --> 00:03:23,210
So now that we've calculated it our goal is to minimize it. And that's what we'll do next. And just a small aside, what we did is for binary classification problems.

19
00:03:23,210 --> 00:03:28,490
If we have a multiclass classification problem then the error is now given by the multiclass entropy.

20
00:03:28,491 --> 00:03:39,139
This formula is given here where for every data point we take the product of the label times the logarithm of the prediction and then we average all these values.

21
00:03:39,139 --> 00:03:45,000
And again it's a nice exercise to convince yourself that the two are the same when there are just two classes.


@@@
1
00:00:00,000 --> 00:00:09,330
Okay. So now our goal is to minimize the error function and we'll do it as follows. We started some random weights, which will give us the predictions σ(Wx+b).

2
00:00:09,330 --> 00:00:17,324
As we saw, that also gives us a error function given by this formula. Remember that the summands are also error functions for each point.

3
00:00:17,324 --> 00:00:24,009
So each point will give us a larger function if it's mis-classified and a smaller one if it's correctly classified.

4
00:00:24,010 --> 00:00:36,439
And the way we're going to minimize this function, is to use gradient decent. So here's Mt. Errorest and this is us, and we're going to try to jiggle the line around to see how we can decrease the error function.

5
00:00:36,439 --> 00:00:56,000
Now, the error function is the height which is E(W,b), where W and b are the weights. Now what we'll do, is we'll use gradient decent in order to get to the bottom of the mountain at a much smaller height, which gives us a smaller error function E of W', b'.

6
00:00:56,000 --> 00:01:08,000
This will give rise to new weights, W' and b' which will give us a much better prediction. Namely,  σ(W'x+b').


@@@
1
00:00:00,000 --> 00:00:07,799
So let's study gradient descent in more mathematical detail. Our function is a function of the weights and it can be graph like this.

2
00:00:07,799 --> 00:00:13,639
It's got a mathematical structure so it's not Mt. Everest anymore, it's more of a mount Math-Er-Horn.

3
00:00:13,640 --> 00:00:35,924
So we're standing somewhere in Mount Math-Er-Horn and we need to go down. So now the inputs of the functions are W1 and W2 and the error function is given by E. Then the gradient of E is given by the vector sum of the partial derivatives of E with respect to W1 and W2.

4
00:00:35,924 --> 00:00:42,879
This gradient actually tells us the direction we want to move if we want to increase the error function the most.

5
00:00:42,880 --> 00:00:48,884
Thus, if we take the negative of the gradient, this will tell us how to decrease the error function the most.

6
00:00:48,884 --> 00:00:58,634
And this is precisely what we'll do. At the point we're standing, we'll take the negative of the gradient of the error function at that point.

7
00:00:58,634 --> 00:01:12,902
Then we take a step in that direction. Once we take a step, we'll be in a lower position. So we do it again, and again, and again, until we are able to get to the bottom of the mountain.

8
00:01:12,902 --> 00:01:19,724
So this is how we calculate the gradient. We start with our initial prediction Y had equals sigmoid of W Expo's B.

9
00:01:19,724 --> 00:01:25,250
And let's say this prediction is bad because the error is large since we're high up in the mountain.

10
00:01:25,250 --> 00:01:37,025
The prediction looks like this, Y had equal sigmoid of W 1 x 1 plus all the way to WnXn plus b. Now the error function is given by the formula we saw before.

11
00:01:37,025 --> 00:01:50,170
But what matters here is the gradient of the error function. The gradient of the error function is precisely the vector formed by the partial derivative of the error function with respect to the weights and the bias.

12
00:01:50,170 --> 00:02:00,230
Now, we take a step in the direction of the negative of the gradient. As before, we don't want to make any dramatic changes, so we'll introduce a smaller learning rate alpha.

13
00:02:00,230 --> 00:02:12,204
For example, 0.1. And we'll multiply the gradient by that number. Now taking the step is exactly the same thing as updating the weights and the bias as follows.

14
00:02:12,205 --> 00:02:23,880
The weight Wi will now become Wi prime. Given by Wi minus alpha times the partial derivative of the error, with respect to Wi.

15
00:02:23,879 --> 00:02:33,545
And the bias will now become b prime given by b minus alpha times partial derivative of the error with respect to b.

16
00:02:33,544 --> 00:02:48,055
Now this will take us to a prediction with a lower error function. So, we can conclude that the prediction we have now with weights W prime b prime, is better than the one we had before with weights W and b.


@@@
1
00:00:00,000 --> 00:00:06,830
And now we finally have the tools to write the pseudocode for the grading descent algorithm, and it goes like this.

2
00:00:06,830 --> 00:00:19,270
Step one, start with random weights w_one up to w_n and b which will give us a line, and not just a line, but the whole probability function given by sigmoid of w x plus b.

3
00:00:19,270 --> 00:00:29,230
Now for every point we'll calculate the error, and as we can see the error is high for misclassified points and small for correctly classified points.

4
00:00:29,230 --> 00:00:42,950
Now for every point with coordinates x_one up to x_n, we update w_i by adding the learning rate alpha times the partial derivative of the error function with respect to w_i.

5
00:00:42,950 --> 00:00:48,440
We also update b by adding alpha times the partial derivative of the error function with respect to be.

6
00:00:48,440 --> 00:01:05,215
This gives us new weights, w_i_prime and then new bias b_prime. Now we've already calculated these partial derivatives and we know that they are y_hat minus y times x_i for the derivative with respect to w_i and y_hat minus y for the derivative with respect to b.

7
00:01:05,215 --> 00:01:15,765
So that's how we'll update the weights. Now repeat this process until the error is small, or we can repeat it a fixed number of times.

8
00:01:15,765 --> 00:01:21,935
The number of times is called the epochs and we'll learn them later. Now this looks familiar, have we seen something like that before?

9
00:01:21,935 --> 00:01:31,640
Well, we look at the points and what each point is doing is it's adding a multiple of itself into the weights of the line in order to get the line to move closer towards it if it's misclassified.

10
00:01:31,640 --> 00:01:39,000
That's pretty much what the Perceptron algorithm is doing. So in the next video, we'll look at the similarities because it's a bit suspicious how similar they are.


@@@
1
00:00:00,000 --> 00:00:13,915
So let's compare the Perceptron algorithm and the Gradient Descent algorithm. In the Gradient Descent algorithm, we take the weights and change them from Wi to Wi_ plus_ alpha_ times_ Y hat_ minus_ Y_ times_ Xi.

2
00:00:13,915 --> 00:00:29,785
In the Perceptron algorithm, not every point changes weights, only the misclassified ones. Here, if X is misclassified, we'll change the weights by adding Xi to Wi if the point label is positive, and subtracting if negative.

3
00:00:29,785 --> 00:00:37,350
Now the question is, are these two things the same? Well, let's remember that in that Perceptron algorithm, the labels are one and zero.

4
00:00:37,350 --> 00:00:48,440
And the predictions Y-hat are also one and zero. So, if the point is correct, classified, then Y_ minus_ Y-hat is zero because Y is equal to Y-hat.

5
00:00:48,440 --> 00:00:55,950
Now, if the point is labeled blue, then Y_ equals_ one. And if it's misclassified, then the prediction must be Y-hat_ equals_ zero.

6
00:00:55,950 --> 00:01:04,105
So Y-hat_ minus_ Y is minus one. Similarly, with the points labeled red, then Y_ equals_ zero and Y-hat_ equals_ one.

7
00:01:04,105 --> 00:01:13,620
So, Y-hat_ minus_ Y_ equals_ one. This may not be super clear right away. But if you stare at the screen for long enough, you'll realize that the right and the left are exactly the same thing.

8
00:01:13,620 --> 00:01:23,305
The only difference is that in the left, Y-hat can take any number between zero and one, whereas in the right, Y-hat can take only the values zero or one.

9
00:01:23,305 --> 00:01:40,770
It's pretty fascinating, isn't it? But let's study Gradient Descent even more carefully. Both in the Perceptron algorithm and the Gradient Descent algorithm, a point that is misclassified tells a line to come closer because eventually, it wants the line to surpass it so it can be in the correct side.

10
00:01:40,770 --> 00:01:47,315
Now, what happens if the point is correctly classified? Well, the Perceptron algorithm says do absolutely nothing.

11
00:01:47,315 --> 00:01:58,875
In the Gradient Descent algorithm, you are changing the weights. But what is it doing? Well, if we look carefully, what the point is telling the line, is to go farther away.

12
00:01:58,875 --> 00:02:13,060
And this makes sense, right? Because if you're correctly classified, say, if you're a blue point in the blue region, you'd like to be even more into the blue region, so your prediction is even closer to one, and your error is even smaller.

13
00:02:13,060 --> 00:02:19,590
Similarly, for a red point in the red region. So it makes sense that the point tells the line to go farther away.

14
00:02:19,590 --> 00:02:30,315
And that's precisely what the Gradient Descent algorithm does. The misclassified points asks the line to come closer and the correctly classified points asks the line to go farther away.

15
00:02:30,315 --> 00:02:37,000
The line listens to all the points and takes steps in such a way that it eventually arrives to a pretty good solution.


@@@
1
00:00:00,000 --> 00:00:19,495
So, this is just a small recap video that will get us ready for what's coming. Recall that if we have our data in the form of these points over here and the linear model like this one, for example, with equation 2x1 + 7x2 - 4 = 0, this will give rise to a probability function that looks like this.

2
00:00:19,495 --> 00:00:29,570
Where the points on the blue or positive region have more chance of being blue and the points in the red or negative region have more chance of being red.

3
00:00:29,570 --> 00:00:36,314
And this will give rise to this perception where we label the edges by the weights and the node by the bias.

4
00:00:36,314 --> 00:00:44,689
So, what the perception does, it takes to point (x1, x2), plots it in the graph and then it returns a probability that the point is blue.

5
00:00:44,689 --> 00:00:54,000
In this case, it returns a 0.9 and this mimics the neurons in the brain because they receive nervous impulses, do something inside and return a nervous impulse.


@@@
1
00:00:00,000 --> 00:00:05,543
Now we've been dealing a lot with data sets that can be separated by a line, like this one over here.

2
00:00:05,543 --> 00:00:12,150
But as you can imagine the real world is much more complex than that. This is where neural networks can show their full potential.

3
00:00:12,150 --> 00:00:20,109
In the next few videos we'll see how to deal with more complicated data sets that require highly non-linear boundaries such as this one over here.


@@@
1
00:00:00,000 --> 00:00:14,185
So, let's go back to this example of where we saw some data that is not linearly separable. So a line can not divide these red and blue points and we looked at some solutions, and if you remember, the one we considered more seriously was this curve over here.

2
00:00:14,185 --> 00:00:20,519
So what I'll teach you now is to find this curve and it's very similar than before. We'll still use grading dissent.

3
00:00:20,518 --> 00:00:36,240
In a nutshell, what we're going to do is for these data which is not separable with a line, we're going to create a probability function where the points in the blue region are more likely to be blue and the points in the red region are more likely to be red.

4
00:00:36,240 --> 00:00:44,329
And this curve here that separates them is a set of points which are equally likely to be blue or red.

5
00:00:44,329 --> 00:00:52,000
Everything will be the same as before except this equation won't be linear and that's where neural networks come into play.


@@@
1
00:00:00,000 --> 00:00:06,058
Now I'm going to show you how to create these nonlinear models. What we're going to do is a very simple trick.

2
00:00:06,059 --> 00:00:13,769
We're going to combine two linear models into a nonlinear model as follows. Visually it looks like this.

3
00:00:13,769 --> 00:00:20,084
The two models over imposed creating the model on the right. It's almost like we're doing arithmetic on models.

4
00:00:20,085 --> 00:00:26,824
It's like saying "This line plus this line equals that curve." Let me show you how to do this mathematically.

5
00:00:26,824 --> 00:00:36,478
So a linear model as we know is a whole probability space. This means that for every point it gives us the probability of the point being blue.

6
00:00:36,478 --> 00:00:43,890
So, for example, this point over here is in the blue region so its probability of being blue is 0.7.

7
00:00:43,890 --> 00:00:52,170
The same point given by the second probability space is also in the blue region so it's probability of being blue is 0.8.

8
00:00:52,170 --> 00:01:00,225
Now the question is, how do we combine these two? Well, the simplest way to combine two numbers is to add them, right?

9
00:01:00,225 --> 00:01:09,890
So 0.8 plus 0.7 is 1.5. But now, this doesn't look like a probability anymore since it's bigger than one.

10
00:01:09,890 --> 00:01:20,980
And probabilities need to be between 0 and 1. So what can we do? How do we turn this number that is larger than 1 into something between 0 and 1?

11
00:01:20,980 --> 00:01:27,744
Well, we've been in this situation before and we have a pretty good tool that turns every number into something between 0 and 1.

12
00:01:27,745 --> 00:01:44,568
That's just a sigmoid function. So that's what we're going to do. We applied the sigmoid function to 1.5 to get the value 0.82 and that's the probability of this point being blue in the resulting probability space.

13
00:01:44,569 --> 00:01:51,243
So now we've managed to create a probability function for every single point in the plane and that's how we combined two models.

14
00:01:51,243 --> 00:01:59,334
We calculate the probability for one of them, the probability for the other, then add them and then we apply the sigmoid function.

15
00:01:59,334 --> 00:02:07,849
Now, what if we wanted to weight this sum? What, if say, we wanted the model in the top to have more of a saying the resulting probability than the second?

16
00:02:07,849 --> 00:02:15,698
So something like this where the resulting model looks a lot more like the one in the top then like the one in the bottom. Well, we can add weights.

17
00:02:15,699 --> 00:02:24,240
For example, we can say "I want seven times the first model plus the second one." Actually, I can add the weights since I want.

18
00:02:24,241 --> 00:02:43,293
For example, I can say "Seven times the first one plus five times the second one." And when I do get the combine the model is I take the first probability, multiply it by seven, then take the second one and multiply it by five and I can even add a bias if I want.

19
00:02:43,294 --> 00:02:54,914
Say, the bias is minus 6, then we add it to the whole equation. So we'll have seven times this plus five times this minus six, which gives us 2.9.

20
00:02:54,913 --> 00:03:02,680
We then apply the sigmoid function and that gives us 0.95. So it's almost like we had before, isn't it?

21
00:03:02,680 --> 00:03:17,650
Before we had a line that is a linear combination of the input values times the weight plus a bias. Now we have that this model is a linear combination of the two previous model times the weights plus some bias.

22
00:03:17,650 --> 00:03:30,573
So it's almost the same thing. It's almost like this curved model in the right. It's a linear combination of the two linear models before or we can even think of it as the line between the two models.

23
00:03:30,574 --> 00:03:43,228
This is no coincidence. This is at the heart of how neural networks get built. Of course, we can imagine that we can keep doing this always obtaining more new complex models out of linear combinations of the existing ones.


@@@
1
00:00:00,000 --> 00:00:07,734
So in the previous session we learn that we can add to linear models to obtain a third model. As a matter of fact, we did even more.

2
00:00:07,735 --> 00:00:17,785
We can take a linear combination of two models. So, the first model times a constant plus the second model times a constant plus a bias and that gives us a non-linear model.

3
00:00:17,785 --> 00:00:26,784
That looks a lot like perceptrons where we can take a value times a constant plus another value times a constant plus a bias and get a new value.

4
00:00:26,785 --> 00:00:33,210
And that's no coincidence. That's actually the building block of Neural Networks. So, let's look at an example.

5
00:00:33,210 --> 00:00:42,689
Let's say, we have this linear model where the linear equation is 5x1 minus 2x2 plus 8. That's represented by this perceptron.

6
00:00:42,689 --> 00:00:52,045
And we have another linear model with equations 7x1 minus 3x2 minus 1 which is represented by this perceptron over here.

7
00:00:52,045 --> 00:01:06,420
Let's draw them nicely in here and let's use another perceptron to combine these two models using the Linear Equation, seven times the first model plus five times the second model minus six.

8
00:01:06,420 --> 00:01:16,480
And now the magic happens when we join these together and we get a Neural Network. We clean it up a bit and we obtain this. All the weights are there.

9
00:01:16,480 --> 00:01:31,629
The weights on the left, tell us what equations the linear models have. And the weights on the right, tell us what the linear combination is of the two models to obtain the curve non-linear model in the right.

10
00:01:31,629 --> 00:01:40,204
So, whenever you see a Neural Network like the one on the left, think of what could be the nonlinear boundary defined by the Neural Network.

11
00:01:40,204 --> 00:01:50,394
Now, note that this was drawn using the notation that puts a bias inside the node. This can also be drawn using the notation that keeps the bias as a separate node.

12
00:01:50,394 --> 00:02:04,160
Here, what we do is, in every layer we have a bias unit coming from a node with a one on it. So for example, the minus eight on the top node becomes an edge labelled minus eight coming from the bias node.


@@@
1
00:00:00,000 --> 00:00:11,931
Neural networks have a certain special architecture with layers. The first layer is called the input layer, which contains the inputs, in this case, x1 and x2.

2
00:00:11,932 --> 00:00:18,855
The next layer is called the hidden layer, which is a set of linear models created with this first input layer.

3
00:00:18,855 --> 00:00:26,614
And then the final layer is called the output layer, where the linear models get combined to obtain a nonlinear model.

4
00:00:26,614 --> 00:00:36,600
You can have different architectures. For example, here's one with a larger hidden layer. Now we're combining three linear models to obtain the triangular boundary in the output layer.

5
00:00:36,600 --> 00:00:43,460
Now what happens if the input layer has more nodes? For example, this neural network has three nodes in its input layer.

6
00:00:43,460 --> 00:00:59,820
Well, that just means we're not living in two-dimensional space anymore. We're living in three-dimensional space, and now our hidden layer, the one with the linear models, just gives us a bunch of planes in three space, and the output layer bounds a nonlinear region in three space.

7
00:00:59,820 --> 00:01:06,780
In general, if we have n nodes in our input layer, then we're thinking of data living in n-dimensional space.

8
00:01:06,780 --> 00:01:14,209
Now what if our output layer has more nodes? Then we just have more outputs. In that case, we just have a multiclass classification model.

9
00:01:14,209 --> 00:01:27,930
So if our model is telling us if an image is a cat or dog or a bird, then we simply have each node in the output layer output a score for each one of the classes: one for the cat, one for the dog, and one for the bird.

10
00:01:27,930 --> 00:01:36,090
And finally, and here's where things get pretty cool, what if we have more layers? Then we have what's called a deep neural network.

11
00:01:36,090 --> 00:01:45,364
Now what happens here is our linear models combine to create nonlinear models and then these combine to create even more nonlinear models.

12
00:01:45,364 --> 00:01:54,434
In general, we can do this many times and obtain highly complex models with lots of hidden layers. This is where the magic of neural networks happens.

13
00:01:54,435 --> 00:02:01,049
Many of the models in real life, for self-driving cars or for game-playing agents, have many, many hidden layers.

14
00:02:01,049 --> 00:02:08,370
That neural network will just split the n-dimensional space with a highly nonlinear boundary, such as maybe the one on the right.


@@@
1
00:00:00,000 --> 00:00:10,278
We briefly mentioned multi-class classification in the last video but let me be more specific. It seems that neural networks work really well when the problem consist on classifying two classes.

2
00:00:10,278 --> 00:00:18,625
For example, if the model predicts a probability of receiving a gift or not then the answer just comes as the output of the neural network.

3
00:00:18,625 --> 00:00:26,849
But what happens if we have more classes? Say, we want the model to tell us if an image is a duck, a beaver, or a walrus.

4
00:00:26,850 --> 00:00:37,408
Well, one thing we can do is create a neural network to predict if the image is a duck, then another neural network to predict if the image is a beaver, and a third neural network to predict if the image is a walrus.

5
00:00:37,408 --> 00:00:45,073
Then we can just use SoftMax or pick the answer that gives us the highest probability. But this seems like overkill, right?

6
00:00:45,073 --> 00:00:52,545
The first layers of the neural network should be enough to tell us things about the image and maybe just the last layer should tell us which animal it is.

7
00:00:52,545 --> 00:01:07,730
As a matter of fact, as you'll see in the CNN section, this is exactly the case. So what we need here is to add more nodes in the output layer and each one of the nodes will give us the probability that the image is each of the animals.

8
00:01:07,730 --> 00:01:15,989
Now, we take the scores and apply the SoftMax function that was previously defined to obtain well-defined probabilities.


@@@
1
00:00:00,000 --> 00:00:10,495
So now that we have defined what neural networks are, we need to learn how to train them. Training them really means what parameters should they have on the edges in order to model our data well.

2
00:00:10,495 --> 00:00:16,800
So in order to learn how to train them, we need to look carefully at how they process the input to obtain an output.

3
00:00:16,800 --> 00:00:27,195
So let's look at our simplest neural network, a perceptron. This perceptron receives a data point of the form x1, x2 where the label is Y=1.

4
00:00:27,195 --> 00:00:41,595
This means that the point is blue. Now the perceptron is defined by a linear equation say w1, x1 plus w2, x2 plus B, where w1 and w2 are the weights in the edges and B is the bias in the note.

5
00:00:41,595 --> 00:00:49,820
Here, w1 is bigger than w2, so we'll denote that by drawing the edge labelled w1 much thicker than the edge labelled w2.

6
00:00:49,820 --> 00:00:57,240
Now, what the perceptron does is it plots the point x1, x2 and it outputs the probability that the point is blue.

7
00:00:57,240 --> 00:01:03,795
Here is the point is in the red area and then the output is a small number, since the point is not very likely to be blue.

8
00:01:03,795 --> 00:01:11,070
This process is known as feedforward. We can see that this is a bad model because the point is actually blue.

9
00:01:11,070 --> 00:01:18,570
Given that the third coordinate, the Y is one. Now if we have a more complicated neural network, then the process is the same.

10
00:01:18,570 --> 00:01:35,025
Here, we have thick edges corresponding to large weights and thin edges corresponding to small weights and the neural network plots the point in the top graph and also in the bottom graph and the outputs coming out will be a small number from the top model.

11
00:01:35,025 --> 00:01:47,280
The point lies in the red area which means it has a small probability of being blue and a large number from the second model, since the point lies in the blue area which means it has a large probability of being blue.

12
00:01:47,280 --> 00:01:57,485
Now, as the two models get combined into this nonlinear model and the output layer just plots the point and it tells the probability that the point is blue.

13
00:01:57,485 --> 00:02:03,750
As you can see, this is a bad model because it puts the point in the red area and the point is blue.

14
00:02:03,750 --> 00:02:13,070
Again, this process called feedforward and we'll look at it more carefully. Here, we have our neural network and the other notations so the bias is in the outside.

15
00:02:13,070 --> 00:02:23,310
Now we have a matrix of weights. The matrix w superscript one denoting the first layer and the entries are the weights w1, 1 up to w3, 2.

16
00:02:23,310 --> 00:02:36,115
Notice that the biases have now been written as w3, 1 and w3, 2 this is just for convenience. Now in the next layer, we also have a matrix this one is w superscript two for the second layer.

17
00:02:36,115 --> 00:02:43,700
This layer contains the weights that tell us how to combine the linear models in the first layer to obtain the nonlinear model in the second layer.

18
00:02:43,700 --> 00:02:51,000
Now what happens is some math. We have the input in the form x1, x2, 1 where the one comes from the bias unit.

19
00:02:51,000 --> 00:03:01,250
Now we multiply it by the matrix w1 to get these outputs. Then, we apply the sigmoid function to turn the outputs into values between zero and one.

20
00:03:01,250 --> 00:03:08,280
Then the vector format these values gets a one attatched for the bias unit and multiplied by the second matrix.

21
00:03:08,280 --> 00:03:16,290
This returns an output that now gets thrown into a sigmoid function to obtain the final output which is y-hat.

22
00:03:16,290 --> 00:03:23,275
Y-hat is the prediction or the probability that the point is labeled blue. So this is what neural networks do.

23
00:03:23,275 --> 00:03:32,825
They take the input vector and then apply a sequence of linear models and sigmoid functions. These maps when combined become a highly non-linear map.

24
00:03:32,825 --> 00:03:42,995
And the final formula is simply y-hat equals sigmoid of w2 combined with sigmoid of w1 applied to x.

25
00:03:42,995 --> 00:04:00,405
Just for redundance, we do this again on a multi-layer perceptron or neural network. To calculate our prediction y-hat, we start with the unit vector x, then we apply the first matrix and a sigmoid function to get the values in the second layer.

26
00:04:00,405 --> 00:04:13,315
Then, we apply the second matrix and another sigmoid function to get the values on the third layer and so on and so forth until we get our final prediction, y-hat.

27
00:04:13,315 --> 00:04:20,000
And this is the feedforward process that the neural networks use to obtain the prediction from the input vector.


@@@
1
00:00:00,000 --> 00:00:05,950
So, our goal is to train our neural network. In order to do this, we have to define the error function.

2
00:00:05,950 --> 00:00:18,900
So, let's look again at what the error function was for perceptrons. So, here's our perceptron. In the left, we have our input vector with entries x_1 up to x_n, and one for the bias unit.

3
00:00:18,900 --> 00:00:30,275
And the edges with weights W_1 up to W_n, and b for the bias unit. Finally, we can see that this perceptor uses a sigmoid function.

4
00:00:30,275 --> 00:00:44,175
And the prediction is defined as y-hat equals sigmoid of Wx plus b. And as we saw, this function gives us a measure of the error of how badly each point is being classified.

5
00:00:44,175 --> 00:00:53,415
Roughly, this is a very small number if the point is correctly classified, and a measure of how far the point is from the line and the point is incorrectly classified.

6
00:00:53,415 --> 00:01:03,740
So, what are we going to do to define the error function in a multilayer perceptron? Well, as we saw, our prediction is simply a combination of matrix multiplications and sigmoid functions.

7
00:01:03,740 --> 00:01:12,000
But the error function can be the exact same thing, right? It can be the exact same formula, except now, y-hat is just a bit more complicated.

8
00:01:12,000 --> 00:01:20,000
And still, this function will tell us how badly a point gets misclassified. Except now, it's looking at a more complicated boundary.


@@@
1
00:00:00,000 --> 00:00:06,449
So now we're finally ready to get our hands into training a neural network. So let's quickly recall feedforward.

2
00:00:06,450 --> 00:00:19,804
We have our perceptron with a point coming in labeled positive. And our equation w1x1 + w2x2 + b, where w1 and w2 are the weights and b is the bias.

3
00:00:19,804 --> 00:00:25,405
Now, what the perceptron does is, it plots a point and returns a probability that the point is blue.

4
00:00:25,405 --> 00:00:36,164
Which in this case is small since the point is in the red area. Thus, this is a bad perceptron since it predicts that the point is red when the point is really blue.

5
00:00:36,164 --> 00:00:42,284
And now let's recall what we did in the gradient descent algorithm. We did this thing called Backpropagation.

6
00:00:42,284 --> 00:00:59,894
We went in the opposite direction. We asked the point, "What do you want the model to do for you?" And the point says, "Well, I am misclassified so I want this boundary to come closer to me." And we saw that the line got closer to it by updating the weights.

7
00:00:59,895 --> 00:01:07,239
Namely, in this case, let's say that it tells the weight w1 to go lower and the weight w2 to go higher.

8
00:01:07,239 --> 00:01:19,490
And this is just an illustration, it's not meant to be exact. So we obtain new weights, w1' and w2' which define a new line which is now closer to the point.

9
00:01:19,489 --> 00:01:35,857
So what we're doing is like descending from Mt. Errorest, right? The height is going to be the error function E(W) and we calculate the gradient of the error function which is exactly like asking the point what does is it want the model to do.

10
00:01:35,856 --> 00:01:43,969
And as we take the step down the direction of the negative of the gradient, we decrease the error to come down the mountain.

11
00:01:43,969 --> 00:01:53,480
This gives us a new error, E(W') and a new model W' with a smaller error, which means we get a new line closer to the point.

12
00:01:53,480 --> 00:02:02,760
We continue doing this process in order to minimize the error. So that was for a single perceptron. Now, what do we do for multi-layer perceptrons?

13
00:02:02,760 --> 00:02:19,554
Well, we still do the same process of reducing the error by descending from the mountain, except now, since the error function is more complicated then it's not Mt. Errorest, now it's Mt. Kilimanjerror. But same thing, we calculate the error function and its gradient.

14
00:02:19,555 --> 00:02:32,719
We then walk in the direction of the negative of the gradient in order to find a new model W' with a smaller error E(W') which will give us a better prediction.

15
00:02:32,719 --> 00:02:40,149
And we continue doing this process in order to minimize the error. So let's look again at what feedforward does in a multi-layer perceptron.

16
00:02:40,149 --> 00:02:50,570
The point comes in with coordinates (x1, x2) and label y = 1. It gets plotted in the linear models corresponding to the hidden layer.

17
00:02:50,569 --> 00:02:58,280
And then, as this layer gets combined the point gets plotted in the resulting non-linear model in the output layer.

18
00:02:58,280 --> 00:03:05,060
And the probability that the point is blue is obtained by the position of this point in the final model.

19
00:03:05,060 --> 00:03:11,094
Now, pay close attention because this is the key for training neural networks, it's Backpropagation.

20
00:03:11,094 --> 00:03:19,365
We'll do as before, we'll check the error. So this model is not good because it predicts that the point will be red when in reality the point is blue.

21
00:03:19,365 --> 00:03:35,195
So we'll ask the point, "What do you want this model to do in order for you to be better classified?" And the point says, "I kind of want this blue region to come closer to me." Now, what does it mean for the region to come closer to it?

22
00:03:35,194 --> 00:03:42,735
Well, let's look at the two linear models in the hidden layer. Which one of these two models is doing better?

23
00:03:42,735 --> 00:03:50,230
Well, it seems like the top one is badly misclassifying the point whereas the bottom one is classifying it correctly.

24
00:03:50,229 --> 00:04:02,519
So we kind of want to listen to the bottom one more and to the top one less. So what we want to do is to reduce the weight coming from the top model and increase the weight coming from the bottom model.

25
00:04:02,520 --> 00:04:12,014
So now our final model will look a lot more like the bottom model than like the top model. But we can do even more.

26
00:04:12,014 --> 00:04:28,635
We can actually go to the linear models and ask the point, "What can these models do to classify you better?" And the point will say, "Well, the top model is misclassifying me, so I kind of want this line to move closer to me.

27
00:04:28,634 --> 00:04:41,670
And the second model is correctly classifying me, so I want this line to move farther away from me." And so this change in the model will actually update the weights.

28
00:04:41,670 --> 00:04:57,589
Let's say, it'll increase these two and decrease these two. So now after we update all the weights we have better predictions at all the models in the hidden layer and also a better prediction at the model in the output layer.

29
00:04:57,589 --> 00:05:06,649
Notice that in this video we intentionally left the bias unit away for clarity. In reality, when you update the weights we're also updating the bias unit.

30
00:05:06,649 --> 00:05:12,070
If you're the kind of person who likes formality, don't worry, we'll calculate these gradients in detail soon.


@@@
1
00:00:00,000 --> 00:00:06,279
Okay. So, now we'll do the same thing as we did before, painting our weights in the neural network to better classify our points.

2
00:00:06,280 --> 00:00:18,879
But we're going to do it formally, so fasten your seat belts because math is coming. On your left, you have a single perceptron with the input vector, the weights and the bias and the sigmoid function inside the node.

3
00:00:18,879 --> 00:00:27,050
And on the right, we have a formula for the prediction, which is the sigmoid function of the linear function of the input.

4
00:00:27,050 --> 00:00:38,799
And below, we have a formula for the error, which is the average of all points of the blue term for the blue points and the red term for the red points.

5
00:00:38,798 --> 00:00:55,630
And in order to descend from Mount Errorest, we calculate the gradient. And the gradient is simply the vector formed by all the partial derivatives of the error function with respect to the weights w1 up to wn and and the bias b.

6
00:00:55,630 --> 00:01:05,405
They correspond to these edges over here, and what do we do in a multilayer perceptron? Well, this time it's a little more complicated but it's pretty much the same thing.

7
00:01:05,405 --> 00:01:13,503
We have our prediction, which is simply a composition of functions namely matrix multiplications and sigmoids.

8
00:01:13,504 --> 00:01:23,799
And the error function is pretty much the same, except the ŷ is a bit more complicated. And the gradient is pretty much the same thing, it's just much, much longer.

9
00:01:23,799 --> 00:01:29,984
It's a huge vector where each entry is a partial derivative of the error with respect to each of the weights.

10
00:01:29,983 --> 00:01:45,765
And these just correspond to all the edges. If we want to write this more formally, we recall that the prediction is a composition of sigmoids and matrix multiplications, where these are the matrices and the gradient is just going to be formed by all these partial derivatives.

11
00:01:45,765 --> 00:02:06,125
Here, it looks like a matrix but in reality, it's just a long vector. And the gradient descent is going to do the following; we take each weight, w_i_j super k and we update it by adding a small number, the learning rate times the partial derivative of E with respect to that same weight.

12
00:02:06,125 --> 00:02:20,270
This is the gradient descent step, so it will give us new updated weight w_i_j super k prime. That step is going to give us a whole new model with new weights that will classify the point much better.


@@@
1
00:00:00,000 --> 00:00:08,080
So before we start calculating derivatives, let's do a refresher on the chain rule which is the main technique we'll use to calculate them.

2
00:00:08,080 --> 00:00:41,704
The chain rule says, if you have a variable x on a function f that you apply to x to get f of x, which we're gonna call A, and then another function g, which you apply to f of x to get g of f of x, which we're gonna call B, the chain rule says, if you want to find the partial derivative of B with respect to x, that's just a partial derivative of B with respect to A times the partial derivative of A with respect to x.

3
00:00:41,704 --> 00:01:14,130
So it literally says, when composing functions, that derivatives just multiply, and that's gonna be super useful for us because feed forwarding is literally composing a bunch of functions, and back propagation is literally taking the derivative at each piece, and since taking the derivative of a composition is the same as multiplying the partial derivatives, then all we're gonna do is multiply a bunch of partial derivatives to get what we want. Pretty simple, right?


@@@
1
00:00:00,000 --> 00:00:12,775
So, let us go back to our neural network with our weights and our input. And recall that the weights with superscript 1 belong to the first layer, and the weights with superscript 2 belong to the second layer.

2
00:00:12,775 --> 00:00:22,830
Also, recall that the bias is not called b anymore. Now, it is called W31, W32 etc. for convenience, so that we can have everything in matrix notation.

3
00:00:22,830 --> 00:00:37,860
And now what happens with the input? So, let us do the feedforward process. In the first layer, we take the input and multiply it by the weights and that gives us h1, which is a linear function of the input and the weights.

4
00:00:37,860 --> 00:01:07,540
Same thing with h2, given by this formula over here. Now, in the second layer, we would take this h1 and h2 and the new bias, apply the sigmoid function, and then apply a linear function to them by multiplying them by the weights and adding them to get a value of h. And finally, in the third layer, we just take a sigmoid function of h to get our prediction or probability between 0 and 1, which is ŷ.

5
00:01:07,540 --> 00:01:33,540
And we can read this in more condensed notation by saying that the matrix corresponding to the first layer is W superscript 1, the matrix corresponding to the second layer is W superscript 2, and then the prediction we had is just going to be the sigmoid of W superscript 2 combined with the sigmoid of W superscript 1 applied to the input x and that is feedforward.

6
00:01:33,540 --> 00:01:52,611
Now, we are going to develop backpropagation, which is precisely the reverse of feedforward. So, we are going to calculate the derivative of this error function with respect to each of the weights in the labels by using the chain rule.

7
00:01:52,611 --> 00:02:02,760
So, let us recall that our error function is this formula over here, which is a function of the prediction ŷ.

8
00:02:02,760 --> 00:02:13,540
But, since the prediction is a function of all the weights wij, then the error function can be seen as the function on all the wij.

9
00:02:13,540 --> 00:02:23,500
Therefore, the gradient is simply the vector formed by all the partial derivatives of the error function E with respect to each of the weights.

10
00:02:23,500 --> 00:02:31,210
So, let us calculate one of these derivatives. Let us calculate derivative of E with respect to W11 superscript 1.

11
00:02:31,210 --> 00:02:41,650
So, since the prediction is simply a composition of functions and by the chain rule, we know that the derivative with respect to this is the product of all the partial derivatives.

12
00:02:41,650 --> 00:02:57,650
In this case, the derivative E with respect to W11 is the derivative of either respect to ŷ times the derivative ŷ with respect to h times the derivative h with respect to h1 times the derivative h1 with respect to W11.

13
00:02:57,650 --> 00:03:10,235
This may seem complicated, but the fact that we can calculate a derivative of such a complicated composition function by just multiplying 4 partial derivatives is remarkable.

14
00:03:10,235 --> 00:03:16,430
Now, we have already calculated the first one, the derivative of E with respect to ŷ. And if you remember, we got ŷ minus y.

15
00:03:16,430 --> 00:03:25,193
So, let us calculate the other ones. Let us zoom in a bit and look at just one piece of our multi-layer perceptron.

16
00:03:25,193 --> 00:03:44,670
The inputs are some values h1 and h2, which are values coming in from before. And once we apply the sigmoid and a linear function on h1 and h2 and 1 corresponding to the biased unit, we get a result h. So, now what is the derivative of h with respect to h1?

17
00:03:44,670 --> 00:03:55,940
Well, h is a sum of three things and only one of them contains h1. So, the second and the third summon just give us a derivative of 0.

18
00:03:55,940 --> 00:04:08,715
The first summon gives us W11 superscript 2 because that is a constant, and that times the derivative of the sigmoid function with respect to h1.

19
00:04:08,715 --> 00:04:27,600
This is something that we calculated below in the instructor comments, which is that the sigmoid function has a beautiful derivative, namely the derivative of sigmoid of h is precisely sigmoid of h times 1 minus sigmoid of h. Again, you can see this development underneath in the instructor comments.

20
00:04:27,600 --> 00:04:35,635
You also have the chance to code this in the quiz because at the end of the day, we just code these formulas and then use them forever, and that is it.


@@@
1
00:00:00,920 --> 00:00:07,642
One of the most fundamental ways we learn about our world and environment is through our sense of sight.

2
00:00:07,642 --> 00:00:17,039
Our ability to see permeates our very being. It has come to define, not only how we see the physical world, but also how we see the abstract.

3
00:00:17,039 --> 00:00:27,274
We speak of a company or an entrepreneur having a vision; we may say, "I see what you mean", to a colleague or send on the,"I see what you did there, Jeff".

4
00:00:27,274 --> 00:00:45,493
Our ability to see, our vision, in a very real sense determines how we interact with the world. Computer Vision strives to give a similar ability to a machine and if you want to build an agent that uses vision to extract, analyze and understand useful information then you're in the right place.

5
00:00:45,493 --> 00:00:53,524
We have Cezanne Camacho, a resident expert in the subject to help you learn about Computer Vision and its fascinating applications.

6
00:00:53,524 --> 00:00:58,929
Hey everyone. I'm Cezanne and I'm excited to guide you through some important Computer Vision topics.

7
00:00:58,929 --> 00:01:14,000
Computer Vision is a fascinating set of techniques, inspired by mathematics, statistics and probability theory and in this lesson we'll be learning about the basics of Computer Vision and about the role it plays in artificial intelligence systems. Let's get started.


@@@
1
00:00:00,349 --> 00:00:10,814
So, what is vision? At its most basic, vision or visual perception is the act of observing patterns and objects through sight or visual input.

2
00:00:10,814 --> 00:00:22,515
But vision is so much more than that. It allows us to build a model of the physical world. For example, take a look around you and think about what you see, maybe you see a computer screen or a mobile phone.

3
00:00:22,515 --> 00:00:30,649
You may also see a desk or different items in a room. When I look around the studio I see a camera in front of me, and tables and chairs besides me.

4
00:00:30,649 --> 00:00:39,130
We use site to learn about our physical surroundings and where they are in relation to us. And with this information, we build a physical model of our world.

5
00:00:39,130 --> 00:00:46,379
This is how typical human vision works, but different creatures have evolved vision systems that are tailored to their specific needs.

6
00:00:46,380 --> 00:01:00,560
Bees for instance have compound eyes which are many tiny eyes bundled together. Their eyes have really low resolution so they're not so great at recognizing things from a distance, but they're very sensitive to motion which is essential while flying fast.


@@@
1
00:00:00,390 --> 00:00:08,710
Some of you are probably wondering how vision fits into artificial intelligence. Why do AI systems need to see?

2
00:00:08,710 --> 00:00:18,054
Why do they need vision? Well, the basis of any AI system is that it can: one, perceive its environment and two, take actions based on those perceptions.

3
00:00:18,053 --> 00:00:27,804
And as we noted previously vision is a central part of how we perceive the world. One example of an AI system we're working on Udacity is a self-driving car.

4
00:00:27,803 --> 00:00:34,640
The car sees the world through cameras and sensors and then uses that input to safely navigate roads by itself.

5
00:00:34,640 --> 00:00:42,085
Computer vision is used to analyze camera images and intelligently identify objects like other cars or pedestrians.

6
00:00:42,085 --> 00:00:48,274
It's also used to recognize where the car is on the road and in the world based on the car's surroundings.

7
00:00:48,274 --> 00:00:59,000
So, self-driving cars and many other AI systems need vision to build a physical model of the world just like humans do to identify physical surroundings and respond to them.


@@@
1
00:00:00,699 --> 00:00:08,184
When we think of computer vision's role in AI, we often focus on smart object and behavior recognition.

2
00:00:08,185 --> 00:00:20,600
We've already talked about the self-driving car example, in which computer vision is used to recognize cars and pedestrians and also recognize their behavior, like whether they're moving fast or slow or even moving erratically.

3
00:00:20,600 --> 00:00:27,710
But there's a plethora of other far-reaching uses that one wouldn't necessarily think of when talking about computer vision.

4
00:00:27,710 --> 00:00:33,719
It's used in state-of-the-art medical technology to analyze medical images and identify points of interest.

5
00:00:33,719 --> 00:00:43,820
An AI system can even learn to recognize cancerous tissue and help with early detection and diagnostics by analyzing data about cancer and images of cancer cells.

6
00:00:43,820 --> 00:00:54,560
On a more mundane level, many of us have phones that use computer vision on a daily basis. Computer vision is used to identify and categorize personal images and video.

7
00:00:54,560 --> 00:01:05,114
My phone recognizes me when I take a selfie and even knows that some of my friends faces look like. Some systems are smart enough to recognize behavior and images and even caption it correctly.

8
00:01:05,114 --> 00:01:14,628
So, AI systems can learn to recognize a multitude of objects and behaviors using computer vision and by analyzing masses of images or video data.

9
00:01:14,629 --> 00:01:26,385
We've already spoken of AI systems using computer vision in a similar manner to the human visual system to recognize visual surroundings, but there's another essential use that humans have for visual data.

10
00:01:26,385 --> 00:01:42,430
We use it to gauge the emotional state of our fellow humans. That is, vision not only let's us recognize objects, but when we're communicating with another person, we use vision to look at a person's face and body language to help us identify their mental and emotional states.


@@@
1
00:00:00,630 --> 00:00:11,195
Today I'm with Dr. Rana el Kaliouby, Co-founder and CEO at Affectiva, a company that uses computer vision to build systems that are emotionally intelligent.

2
00:00:11,195 --> 00:00:24,309
Rana, I've been talking to our students about the role vision plays at a basic level in AI systems by helping to recognize objects and behavior but your work is in recognizing and responding to human emotion.

3
00:00:24,309 --> 00:00:40,060
What do you think makes this such important work? When we think of human intelligence, we tend to think of cognitive intelligence or IQ, but having emotional intelligence, which is the ability to empathize and read emotions, is central to how we all interact with the world.

4
00:00:40,060 --> 00:00:49,570
So the question becomes, how can we build systems that are artificially emotionally intelligent? Right. And this is the challenge we work on at Affectiva.

5
00:00:49,570 --> 00:00:58,149
We use computer vision and machine learning to recognize emotions as they manifest on your face. Can you tell what I'm feeling right now?

6
00:00:58,149 --> 00:01:08,739
Maybe a combination of surprise, a little bit of horror? Yeah, definitely happy surprise. So Affectiva software can recognize emotion just like you can?

7
00:01:08,739 --> 00:01:21,864
Yes, by observing your facial features, so how your eyebrows and your mouth and your cheeks move. And once a computer app, you know, or other application can recognize emotions, it can respond to them in real time.

8
00:01:21,864 --> 00:01:28,429
And this creates a lot of opportunity for improving and personalizing how we and machines interact with each other.


@@@
1
00:00:00,460 --> 00:00:10,929
So Rana, at Affectiva you build software that reads emotion from facial expressions. I was wondering if you could tell us a bit more about your background and how you got involved in this work.

2
00:00:10,929 --> 00:00:22,859
Sure. So I studied computer science as an undergraduate at the American University in Cairo and I was really interested in how computers change how we connect and communicate with one another.

3
00:00:22,859 --> 00:00:36,024
And then I got an opportunity to study at Cambridge University for my doctorate degree and that was really my first living abroad experience, and I realized I was spending more hours with my laptop than I did with any other human.

4
00:00:36,024 --> 00:00:47,740
Yet it was completely oblivious to my mental and emotional state. So that got me thinking. What if that computer could read and respond to my emotions just the way an awesome friend would?

5
00:00:47,740 --> 00:00:53,329
I think when I think of emotional intelligence, I don't automatically link it with artificial intelligence.

6
00:00:53,329 --> 00:01:06,030
So I was also wondering if you could talk about some current applications of this technology. Yeah. Artificial emotional intelligence, or what we're calling emotion AI, is the core AI and it cuts across many many industries.

7
00:01:06,030 --> 00:01:17,549
I'll give you two examples. One is social robots. Social robots are expected to build a rapport with its users and understand and respond to social and emotional skills.

8
00:01:17,549 --> 00:01:30,269
Autonomous vehicles is another area where emotion AI plays a really critical role. Understanding the sentiment inside the car and also deciding when to relinquish control back to a driver.

9
00:01:30,269 --> 00:01:39,375
And is that driver paying attention? Is that driver even awake? That's exciting to hear. These are problems we're working on teaching at Udacity too.

10
00:01:39,375 --> 00:01:46,650
I have one last question for you. What kind of advice would you give to a student who might be wanting to pursue a career in AI?


@@@
1
00:00:00,540 --> 00:00:07,169
Let me walk you through a sequence of steps that you need to analyze facial expressions and emotions.

2
00:00:07,168 --> 00:00:14,894
Other computer vision tasks have different desired outputs and corresponding algorithms, but they use a similar overall pipeline.

3
00:00:14,894 --> 00:00:23,254
First off, a computer receives visual input from an imaging device like a camera. This is typically captured as a sequence of images or frames.

4
00:00:23,254 --> 00:00:29,050
Each frame is then sent through some preprocessing steps that enhance the quality and detail of the image.

5
00:00:29,050 --> 00:00:54,234
You may also perform other transformations here such as changing from color to grayscale. Next, to these images are analyzed and our software recognizes certain facial features of interest like the curve of the mouth and shape of the eyes, and then data about these features is fed into a so-called trained model that from previously known data can recognize patterns in these facial expressions and finally identify a certain emotion.

6
00:00:54,234 --> 00:01:06,010
It does so with a certain probability that's reported back. Finally, having recognized an emotion, an application can then act on this and interact with a human in a way that takes their emotional state into account.


@@@
1
00:00:00,320 --> 00:00:10,455
So, I've just described a computer vision pipeline that takes in a sequence of images and through a series of steps can recognize different facial expressions and emotions.

2
00:00:10,455 --> 00:00:18,535
But it still seems kind of mysterious. Can you talk a bit about how exactly a model like this can be trained to recognize different facial expressions?

3
00:00:18,535 --> 00:00:28,710
Sure. The process is similar to the pipeline you just described. We have 45 facial muscles that drive thousands of different expressions on our face.

4
00:00:28,710 --> 00:00:37,795
But let's take a specific example. Let's say we are training an algorithm to discriminate between a smile and a smirk.

5
00:00:37,795 --> 00:00:46,600
We collect tens of thousands of examples of people smiling – the more diverse, the better – and then tens of thousands of examples of people smirking.

6
00:00:46,600 --> 00:00:55,780
We feed those prerecorded images along with their labels to the system. The algorithm then looks for visual differences between the two expressions.

7
00:00:55,780 --> 00:01:08,125
For instance, when you smile, your teeth might show, but that's not the case with a smirk. So, you give the model lots of examples of smiles and smirks and other facial expressions until it learns to recognize them.

8
00:01:08,125 --> 00:01:26,353
It sounds like how a baby learns, by lots of examples. Exactly. And similar to how humans learn, at the beginning of the training phase, the model typically performs very badly, but then it monitors the errors it makes and uses those to improve the performance each time it sees more images.

9
00:01:26,353 --> 00:01:37,545
After many iterations, the model converges on the right set of parameters once the error rate becomes acceptable, and that's when we consider the model to be fully trained.

10
00:01:37,545 --> 00:01:46,949
Now, this is a very high-level view of how to train any machine learning model, and the details will vary based on the type of model you use and the training algorithm you choose.

11
00:01:46,950 --> 00:01:57,480
For instance, you can use a convolutional neural network trained using gradient descent. Next, let's see how this computer vision pipeline works in a real-time application.


@@@
1
00:00:00,409 --> 00:00:08,365
The best way to understand how emotion AI works is by example. Would you like to see a live demo? Sure. Why not?

2
00:00:08,365 --> 00:00:25,989
Everyone wants their computers to understand them better. All right, so here's the demo. Basically, what's happening here is that the algorithm is looking for faces and it's detecting your face by, you know, drawing this bounding box around your different facial features.

3
00:00:25,989 --> 00:00:31,079
And then it's tracking the movement of your facial features, like your eyebrows, your mouth, your nose over time.

4
00:00:31,079 --> 00:00:37,199
That's what these dots are for? Exactly, and it's mapping it into a probability score for each emotion.

5
00:00:37,200 --> 00:00:51,030
So let's give this a try. For instance, yeah, let's try smiling. Smile. We can see that the probability score of the joy classifier goes up.

6
00:00:51,030 --> 00:01:03,219
Let's try sadness. It's also mapping your most dominant emotion into an emoji, so you can experiment with, you know, eliciting a bunch of different emojis.

7
00:01:03,219 --> 00:01:09,618
Yeah, that's interesting. It also detects the presence of glasses and gender. So it's identified us both as female.


@@@
1
00:00:00,609 --> 00:00:07,844
Thank you for sharing this with us. It sounds like you and your team have put in an incredible amount of effort to build this service.

2
00:00:07,844 --> 00:00:21,327
I'm wondering if we can use this to build apps of our own. Absolutely. At Affectiva we've made it really easy for you to incorporate emotion AI into your own applications and projects through a variety of STKs and APIs.

3
00:00:21,327 --> 00:00:32,649
Great. This way we can include emotion recognition software in a variety of applications. Next, it will be up to our students to get some practice with computer vision by using the software in a project of their own.


@@@
1
00:00:00,000 --> 00:00:08,460
Our first section will be taught by Arpan Chakraborty. Arpan has a Ph.D. in Computer Science and for several years, has taught at Udacity and at Georgia Tech. Hi Arpan.

2
00:00:08,460 --> 00:00:14,519
Hi Luis. Hello everyone. I'm glad to get you started on your journey through Natural Language Processing.

3
00:00:14,519 --> 00:00:21,070
Everything in NLP starts with raw text typically, produced by humans like you and me, and Luis here.

4
00:00:21,070 --> 00:00:29,955
This text is first processed using some simple transformations such as, splitting it into individual words, reducing verbs to their root form, et cetera.

5
00:00:29,954 --> 00:00:40,230
You need to do this before performing any other analysis or training complex models. This stage may sound simple but you have to be careful about how you process your raw text.


@@@
1
00:00:00,000 --> 00:00:13,525
Welcome to Natural Language Processing. Language is an important medium for human communication. It allows us to convey information, express our ideas, and give instructions to others.

2
00:00:13,525 --> 00:00:24,184
Some philosophers argue that it enables us to form complex thoughts and reason about them. It may turn out to be a critical component of human intelligence.

3
00:00:24,184 --> 00:00:32,420
Now consider the various artificial systems we interact with every day, phones, cars, websites, coffee machines.

4
00:00:32,420 --> 00:00:41,810
It's natural to expect them to be able to process and understand human language, right? Yet, computers are still lagging behind.

5
00:00:41,810 --> 00:00:50,240
No doubt, we have made some incredible progress in the field of natural language processing, but there is still a long way to go.

6
00:00:50,240 --> 00:01:09,609
And that's what makes this an exciting and dynamic area of study. In this lesson you will not only get to know more about the applications and challenges in NLP, you will learn how to design an intelligent application that uses NLP techniques and deploy it on a scalable platform.


@@@
1
00:00:00,000 --> 00:00:12,750
What makes it so hard for computers to understand us? One drawback of human languages, or feature depending on how you look at it, is the lack of a precisely defined structure.

2
00:00:12,750 --> 00:00:19,879
To understand how that makes things difficult let's first take a look at some languages that are more structured.

3
00:00:19,879 --> 00:00:30,809
Mathematics, for instance, uses a structured language. When I write y equals 2x plus 5 there is no ambiguity in what I want to convey.

4
00:00:30,809 --> 00:00:40,679
I'm saying that the variable y is related to the variable x as two times x plus five. Formal logic also uses a structure language.

5
00:00:40,679 --> 00:00:58,535
For example, consider the expression parent(x,y) and parent(x,z) implies sibling(y, z). This statement is asserting that if x is a parent of y and x is a parent of z, then y and z are siblings.

6
00:00:58,534 --> 00:01:06,359
A set of structure languages that may be more familiar to you are scripting and programming languages.

7
00:01:06,358 --> 00:01:22,805
Consider this SQL statement. SELECT name, email FROM users WHERE name LIKE A%. We are asking the database to return the names and e-mail addresses of all users whose names begin with an A.

8
00:01:22,805 --> 00:01:30,000
These languages are designed to be as unambiguous as possible and are suitable for computers to process.


@@@
1
00:00:00,000 --> 00:00:09,134
Structured languages are easy to parse and understand for computers because they are defined by a strict set of rules or grammar.

2
00:00:09,134 --> 00:00:18,510
There are standard forms of expressing such grammars and algorithms, that can parse properly formed statements to understand exactly what is meant.

3
00:00:18,510 --> 00:00:26,683
When a statement doesn't match the prescribed grammar, a typical computer doesn't try to guess the meaning, it simply gives up.


@@@
1
00:00:00,000 --> 00:00:15,734
The languages we use to communicate with each other also have defined grammatical rules. And indeed, in some situations we use simple structured sentences but for the most part human discourse is complex and unstructured.

2
00:00:15,734 --> 00:00:24,210
Despite that, we seem to be really good at understanding each other and even ambiguities are welcome to a certain extent.

3
00:00:24,210 --> 00:00:41,780
So, what can computers do to make sense of unstructured text? Here are some preliminary ideas. Computers can do some level of processing with words and phrases, trying to identify key words, parts of speech, named entities, dates, quantities, etc.

4
00:00:41,780 --> 00:00:49,630
Using this information they can also try to parse sentences, at least ones that are relatively more structured.

5
00:00:49,630 --> 00:01:07,394
This can help extract the relevant parts of statements, questions, or instructions. At a higher level computers can analyze documents to find frequent and rare words, assess the overall tone or sentiment being expressed, and even cluster or group similar documents together.

6
00:01:07,394 --> 00:01:17,000
You can imagine that building on top of these ideas, computers can do a whole lot with unstructured text even if they cannot understand it like us.


@@@
1
00:00:00,000 --> 00:00:12,300
So what is stopping computers from becoming as capable as humans in understanding natural language? Part of the problem lies in the variability and complexity of our sentences.

2
00:00:12,300 --> 00:00:30,059
Consider this excerpt from a movie review. "I was lured to see this on the promise of a smart witty slice of old fashioned fun and intrigue. I was conned. " Although it starts with some potentially positive words it turns out to be a strongly negative review.

3
00:00:30,059 --> 00:00:37,839
Sentences like this might be somewhat entertaining for us but computers tend to make mistakes when trying to analyze them.

4
00:00:37,840 --> 00:00:52,164
But there is a bigger challenge that makes NLP harder than you think. Take a look at this sentence. "The sofa didn't fit through the door because it was too narrow." What does "it" refer to?

5
00:00:52,164 --> 00:01:05,819
Clearly "it" refers to the door. Now consider a slight variation of this sentence. "The sofa didn't fit through the door because it was too wide." What does "it" refer to in this case?

6
00:01:05,819 --> 00:01:20,230
Here it's the sofa. Think about it. To understand the proper meaning or semantics of the sentence you implicitly applied your knowledge about the physical world, that wide things don't fit through narrow things.

7
00:01:20,230 --> 00:01:34,000
You may have experienced a similar situation before. You can imagine that there are countless other scenarios in which some knowledge or context is indispensable for correctly understanding what is being said.


@@@
1
00:00:00,000 --> 00:00:09,845
Natural language processing is one of the fastest growing fields in the world. NLP Is making its way into a number of products and services that we use every day.

2
00:00:09,845 --> 00:00:32,265
Let's begin with an overview of how to design an end-to-end NLP pipeline. Not that kind of pipeline; a natural language processing pipeline, where you start with raw text, in whatever form it is available, process it, extract relevant features, and build models to accomplish various NLP tasks.

3
00:00:32,265 --> 00:00:41,094
Now that I think about it, that is kind of like refining crude oil. Anyways, you'll learn how these different stages in the pipeline depend on each other.

4
00:00:41,094 --> 00:00:47,489
You'll also learn how to make design decisions, how to choose existing libraries, and tools to perform each step.


@@@
1
00:00:00,000 --> 00:00:09,975
Let's look at a common NLP pipeline. It consists of three stages, text processing, feature extraction and modeling.

2
00:00:09,974 --> 00:00:26,714
Each stage transforms text in some way and produces a result that the next stage needs. For example, the goal of text processing is to take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.

3
00:00:26,714 --> 00:00:38,535
Similarly, the next stage needs to extract and produce feature representations that are appropriate for that type of model you're planning to use and the NLP task you're trying to accomplish.

4
00:00:38,534 --> 00:00:53,094
When you're building such a pipeline, your workflow may not be perfectly linear. Let's say, you spend some time implementing text processing functions, then make some simple feature extractors, and then design a baseline statistical model.

5
00:00:53,094 --> 00:01:02,909
But then, maybe you are not happy with the results. So you go back and rethink what features you need, and that in turn, can make you change your processing routines.

6
00:01:02,909 --> 00:01:11,000
Keep in mind that this is a very simplified view of natural language processing. Your application may require additional steps.


@@@
1
00:00:00,000 --> 00:00:08,550
Let's take a closer look at text processing. The first question that comes to mind is, why do we need to process text?

2
00:00:08,550 --> 00:00:16,125
Why can we not feed it in directly? To understand that, think about where we get this text to begin with.

3
00:00:16,125 --> 00:00:28,489
Websites are a common source of textual information. Here's a portion of a sample web page from Wikipedia and the corresponding HTML markup, which serves as our raw input.

4
00:00:28,489 --> 00:00:38,984
For the purpose of natural language processing, you would typically want to get rid of all or most of the HTML tags, and retain only plain text.

5
00:00:38,984 --> 00:00:50,440
You can also remove or set aside any URLs or other items not relevant to your task. The Web is probably the most common and fastest growing source of textual content.

6
00:00:50,439 --> 00:01:02,679
But you may also need to consume PDFs, Word documents or other file formats. Or your raw input may even come from a speech recognition system or from a book scan using OCR.

7
00:01:02,679 --> 00:01:17,185
Some knowledge of the source medium can help you properly handle the input. In the end, your goal is to extract plain text that is free of any source specific markers or constructs that are not relevant to your task.

8
00:01:17,185 --> 00:01:26,840
Once you have obtained plain text, some further processing may be necessary. For instance, capitalization doesn't usually change the meaning of a word.

9
00:01:26,840 --> 00:01:35,799
We can convert all the words to the same case so that they're not treated differently. Punctuation marks that we use to indicate pauses, etc.

10
00:01:35,799 --> 00:01:43,350
can also be removed. Some common words in a language often help provide structure, but don't add much meaning.

11
00:01:43,349 --> 00:01:56,000
For example, a, and, the, of, are, and so on. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.


@@@
1
00:00:00,000 --> 00:00:07,700
Okay. We now have clean normalized text. Can we feed this into a statistical or a machine learning model?

2
00:00:07,700 --> 00:00:19,115
Not quite. Let's see why. Text data is represented on modern computers using an encoding such as ASCII or Unicode that maps every character to a number.

3
00:00:19,114 --> 00:00:27,445
Computer store and transmit these values as binary, zeros and ones. These numbers also have an implicit ordering.

4
00:00:27,445 --> 00:00:43,814
65 is less than 66 which is less than 67. But does that mean A is less than B, and B is less and C? No. In fact, that would be an incorrect assumption to make and might mislead our natural language processing algorithms.

5
00:00:43,814 --> 00:00:54,689
Moreover, individual characters don't carry much meaning at all. It is words that we should be concerned with, but computers don't have a standard representation for words.

6
00:00:54,689 --> 00:01:04,409
Yes, internally they are just sequences of ASCII or Unicode values but they don't quite capture the meanings or relationships between words.

7
00:01:04,409 --> 00:01:14,580
Compare this with how an image is represented in computer memory. Each pixel value contains the relative intensity of light at that spot in the image.

8
00:01:14,579 --> 00:01:23,420
For a color image, we keep one value per primary color; red, green, and blue. These values carry relevant information.

9
00:01:23,420 --> 00:01:32,099
Two pixels with similar values are perceptually similar. Therefore, it makes sense to directly use pixel values in a numerical model.

10
00:01:32,099 --> 00:01:39,974
Yes, some feature engineering may be necessary such as edge detection or filtering, but pixels are a good starting point.

11
00:01:39,974 --> 00:01:49,019
So the question is, how do we come up with a similar representation for text data that we can use as features for modeling?

12
00:01:49,019 --> 00:01:56,099
The answer again depends on what kind of model you're using and what task you're trying to accomplish.

13
00:01:56,099 --> 00:02:06,660
If you want to use a graph based model to extract insights, you may want to represent your words as symbolic nodes with relationships between them like WordNet.

14
00:02:06,659 --> 00:02:16,349
For statistical models however, you need some sort of numerical representation. Even then, you have to think about the end goal.

15
00:02:16,349 --> 00:02:28,229
If you're trying to perform a document level task, such as spam detection or sentiment analysis, you may want to use a per document representations such as bag-of-words or doc2vec.

16
00:02:28,229 --> 00:02:39,979
If you want to work with individual words and phrases such as for text generation or machine translation, you'll need a word level representation such as word2vec or glove.

17
00:02:39,979 --> 00:02:48,000
There are many ways of representing textual information, and it is only through practice that you can learn what you need for each problem.


@@@
1
00:00:00,000 --> 00:00:20,009
The final stage in this process is what I like to call modeling. This includes designing a model, usually a statistical or a machine learning model, fitting its parameters to training data using an optimization procedure, and then using it to make predictions about unseen data.

2
00:00:20,010 --> 00:00:26,984
The nice thing about working with numerical features is that it allows you to utilize pretty much any machine learning model.

3
00:00:26,984 --> 00:00:34,229
This includes support vector machines, decision trees, neural networks, or any custom model of your choice.

4
00:00:34,229 --> 00:00:41,234
You could even combine multiple models to get better performance. How you utilize the model is up to you.

5
00:00:41,234 --> 00:00:51,274
You can deploy it as a web-based application, package it up into a handy mobile app, integrate it with other products, services, and so on.


